\documentclass[]{article}

%******************************************************************************
% Prelude
%******************************************************************************
\newif\iffinal
\newif\ifverbose
\finalfalse\verbosetrue % see also other newif in Macros.tex

%------------------------------------------------------------------------------
% Packages
%------------------------------------------------------------------------------
\usepackage{docs/latex/noweb}
 \noweboptions{footnotesizecode,nomargintag}
 %note: allow chunk on different pages, less white space at bottom of pages
 \def\nwendcode{\endtrivlist \endgroup}
 \let\nwdocspar=\par
\usepackage{xspace}
\usepackage{verbatim}
%note: required by noweblatexpad for the \t \l \n in this file
\usepackage{fancyvrb}
\usepackage{url}
\usepackage{hyperref}
 \hypersetup{colorlinks=true}
\usepackage{cite}
\usepackage[pageref]{backref}
 \def\backref{{\footnotesize cited page(s)}~}
%\usepackage{cleveref} %\cref
%\usepackage{multirow}
\usepackage{booktabs} 
 \newcommand{\otoprule}{\midrule[\heavyrulewidth]}
\usepackage{graphicx}

%\usepackage[margin=1in]{geometry}

%------------------------------------------------------------------------------
% Macros
%------------------------------------------------------------------------------
\input{docs/latex/Macros}

%------------------------------------------------------------------------------
% Config
%------------------------------------------------------------------------------
%\setcounter{tocdepth}{1}

\begin{document}
%******************************************************************************
% Title
%******************************************************************************
\title{
{\Huge 
Principia Softwarica
}\\
Literate System Programs for the New Millenium\\
{version 0.1}
}

\author{
Yoann Padioleau\\
\texttt{yoann.padioleau@gmail.com}\\
\\
with code from\\
Ken Thompson, 
Rob Pike, 
Dave Presotto, 
Phil Winterbottom,\\
Tom Duff,
Andrew Hume,
Russ Cox,\\
Xavier Leroy,
Fabrice Le Fessant,
Francois Rouaix,
Yoann Padioleau
}

\maketitle 
\l the elements of ...?
\l source code of an entire "modern" operating system, Plan 9 graphics & network
\l foundational literate programs

\hrule
\input{docs/latex/Copyright}
\hrule

\begingroup
\hypersetup{linkcolor=blue}
\tableofcontents
\endgroup

%******************************************************************************
% Body
%******************************************************************************

\section{Introduction}
\n What

\principia is a serie of books explaining how things work in a
{computer} by describing with full details all the {source code} of all
the {essential software} used by a {programmer}.
\n machine, computer, systems, software, programs, source code, code, processes

Among those essential software are 
the {kernel}, 
the {shell}, 
the {windowing system}, 
the {compiler}, 
the {linker}, 
the {debugger}, etc.
Each software will be covered by a separate book. 
\l https://en.wikipedia.org/wiki/Computer#Software nice categories

Each software in \principia is a {literate program}, that is a document
containing both source code and documentation where the code
is organized and presented in a way that is more adapted
for human comprehension than compiler constraints.
The actual code and the book are derived both from this literate
program, automatically. The code and its documentation are strongly connected.
\l see section~\ref{sec:lp} for more information about literate programming
\l should I talk about plan9 already here?

The sections below will give a quick overview of the \principia software.
They form together the foundation on top of which all applications can
be built.
Similar to {\em Principia Mathematica}~\cite{principia}, 
which is a serie of books
covering the foundations of mathematics, the goal of \principia
is to cover the foundational software.
\l principia informatica would be the books that cover foundation of cs
\l principia algorithmica would be taocp
\n goal: http://www.nytimes.com/library/books/042999best-nonfiction-list.html
Those software are mostly all {meta} programs, that is
programs in which the input and/or output are other programs.
For instance the kernel is a program that {\em manages} other programs,
the compiler is a program that {\em generates} other programs.
%
Those programs are also sometimes referred as {system software}
\l https://en.wikipedia.org/wiki/System_software (short)
\l https://en.wikipedia.org/wiki/Computing#System_software
in opposition to {application software} (e.g. spreadsheets,
word processors, email clients) which we will not cover in \principia.


\subsection{Motivations}
\n Why

Why writing those books?
%
The main reason is that I, Yoann, have always been curious and always
wanted to understand how things work under the hood, fully, to the 
smallest detail.
Software is now running the world; it is thus important to
understand those software, to be computer literate,
and source code is what defines those software.

There are already lots of books explaining how computers work, explaining
the {concepts}, {theories}, and {algorithms} behind software such as
kernels, compilers, a few also about debuggers, but
all those books rarely explain everything with full details, which
is what source code is all about. 
%
There are a few books which
include the whole source code of the software
described, e.g. the books about Minix~\cite{minix}, XINU~\cite{xinu}, 
lcc~\cite{lcc}, 
but those books cover only
a few essential software, mostly always either
the kernel or the compiler, and they do not form a coherent set.

Enter \principia, a set of books covering all essential
software, in a coherent way. In addition to the kernel and compiler,
\principia covers also
the graphics stack, 
the networking stack, 
the windowing system, 
the assembler, 
the linker, 
and many other foundational software
that have never been fully explained before to the best of our knowledge.
\l with full details, as deep as possible, with all the source code.

We want to demistify those software by showing their code
and by showing that they are actually not that complicated.
As a side effect it will also maybe help people imagine better systems.
Indeed it can be very intimidating to invent something completely new
if you have no clue that it is actually not that hard to build from
scratch a complete operating system.

\l CAR Hoare wanted that (see Oberon book intro), show real code, good to learn
\l from the master

Another motivation for those books, in addition to satisfy our curiosity,
as well as your curiosity,
is that we think you are a better programmer if you understand
how things work under the hood. 
%
In our opinion, you become a better C programmer
when you understand roughly what code generates the C compiler.
A good way to write more efficient code, or to avoid
writing really slow code, is to have some ideas of the
assembly code that will be generated for your code by
the compiler. 
%
In the same way, you can better use resources, e.g. memory, if you
have some ideas about how the kernel manages for you
these resources. You can better fix latency issues if
you understand how the networking stack works.
The \principia books can complete the excellent book
{\em Computer systems: a programmer's pespective}~\cite{cs-bryant}
by illustrating the many concepts this book introduces with concrete code.

We also think it is easier to debug
programs if you better understand the environment in which
those programs evolve, if you understand the whole
software stack, and how things interact with each other.
Indeed, to fully understand certain error messages, e.g. from the kernel,
from the networking stack, or from the linker, it is very useful
to have some ideas about what those software do.
Moreover, even if in almost all situations the bug or a performance
issue is in your program,
it could also be sometimes in a core library, in the kernel,
in the compiler, or in the linker. 
It is very rare, but when
those situations happen, if you have no clue about
the environment in which you program runs, you will
never been able to fix your problem.
\l it happens at facebook, and those people like maurer, paul saab are valued



Here are a few questions we hope the \principia books will answer:
\begin{itemize}
\item What exactly happens when the user type [[ls]] in a terminal?
What is the set of software involved in such a command? What
is the trace of such a command through the different layers
of the software stack, from the keyboard interrupt to
the display of text glyphs on the screen?

\item How a debugger works? Does it rely on special
services from the processor or from the kernel to implement
breakpoints?

\item How source code gets compiled, assembled, linked,
and finally loaded in memory? What is the memory image
of a program? How it relates to the original source code?
How a debugger can display debugging information on a binary?

\item Which software does memory allocation? The kernel?
The C library? How [[malloc()]] is implemented?
How it relates to the [[sbrk()]] system call?

\item How certain graphical elements, rectangles, ellipses, fonts, are
rendered on the screen? How the graphics card helps? How
the kernel helps? How things are intercepted
by the windowing system to make sure applications can not
draw in other windows?

\item What happens when you open a connection to another machine?
When you type a URL in your web browser? How one can
achieve a reliable communication on an unreliable physical network?

\item What does it take to port an entire operating system
on a new architecture, e.g. on the Raspberry Pi?

\end{itemize}





\subsection{The ideal teaching operating system}
\n How

Very few programmers actually fully understand how everything work, 
how things interact with each other. It is easy to find
programming languages specialists, or kernel specialists, but
it is very rare to find people who can write compilers and
at the same time have a very good knowledge of the kernel
or networking stack and can optimize things also at this level.
%
Such programmers though are tremendously useful in companies
because they can think at multiple levels and optimize things globally.
\l saab, maurer at facebook

It is even more difficult those days 
to find those knowledgable programmers as
the code of the major operating systems, Linux, Windows, and MacOS
is just gigantic, with hundreds of millions lines of code (LOC).
It is impossible to understand such codebase.
\l too bad because under all those layers are beautiful code, beautiful algos

We think there is hope though, that it is possible to understand fairly
well everything, that it is possible to answer all of the questions
mentioned in the previous section,
by focusing on the {essence} of those major operating systems.
Here by operating system we mean not only the kernel but also
the software that are useful to operate the machine (the compiler,
the windowing system, etc).
\l provide a platform for running application software
We think it is possible to design an operating system
with capabilities similar to the mainstream operating systems,
but with a fraction of their code size.

Here are the requirements for the ideal teaching
operating system which we want to use as the basis for our
\principia books:

\begin{itemize}
\item {open source}: we want to show (lots of) code, 
and we want people to be able
to play with such code, to modify it, so our ideal operating
system must be open source

\item {small}: so the software can be described in
books of reasonable size. The software needs to be a bit minimalistic.

\item {as simple as possible, but not simpler}: we want
small code, but we don't want to show toy code.
%
For instance ARM is probably a simpler architecture than x86
so it makes sense to present the code of an ARM assembler
rather than an x86 assembler, but we don't want to show
a toy assembler for a toy architecture.
%
In the same way we can present the code of a C compiler
without certain advanced state of the art
optimisations, to simplify the presentation,
but we don't want to show a toy compiler for a toy language.

\item {real}: it has to run on real machines, e.g. an x86 desktop,
a Mac laptop, a Raspberry Pi.

\item {complete}: it needs to not only have a kernel and a compiler
but also a graphics stack, a networking stack, a windowing system, etc.

\item {coherent}: the whole set of software must form a coherent
set, so that the interactions between those software can also be
described, succintely.

\item {self hosting}: we want to be able
to recompile and run everything under the system itself.

\end{itemize}

\l jean louis gasse nostalgia about the time solo programmers could do huge
%  things, when things were still manageable (os, compiler, etc)
%http://www.mondaynote.com/2015/08/24/a-salute-to-solo-programmers/

We think this ideal teaching operating system actually already exists, 
we think it is \plan!

\subsection{\plan}
\n Where

\plan
\furl{plan9.bell-labs.com/plan9/}
is the successor of \unix. It was designed, from scratch,
by a small team of great programmers (Rob Pike, Dave Presotto, 
Phil Winterbottom) including the original creator of \unix (Ken Thompson).
Their goal was to redesign \unix to better integrate graphics
and networking. In some sense \plan is a kind of \unix 2.0.

The choice of \plan as the basis for our \principia books
may not be obvious, but it is in our opinion
the simplest and at the same time fairly complete operating system.
%
If you look at the screenshot of \plan in action in Figure~\ref{fig:plan9},
you'll see many features:
\begin{itemize}
\item multiple shells running independent commands at the 
same time

\item a screen with basic graphics and multiple
windows

\item a simple clock graphical application

\item a simple program communicating through 
the network

\end{itemize}


\begin{figure*}
\includegraphics[angle=90, height=18cm]{plan9}
%height=7cm
\caption{\plan in action}
\label{fig:plan9}
\end{figure*}
\t one showing efuns! with nice colors! one showing mmm would be nice too :)

By comparison, if you look at 
\l screenshots of 
other systems,
you'll see that \plan in essence provides the same core services
than MacOS, Linux or Windows,
without certain bells and whistles,
\l multi tasking, graphics, windows, IO, etc
but with a codebase that is significantly smaller.
Indeed, our fork of \plan\footnote{\urlprincipia}, which includes all the
essential software described in all our \principia books, and even a 
few more programs, 
is less than 400 000 LOC.
\n via make loc from the root directory

The \plan software are minimalistic but powerful, simple and small.
The creators of \plan were not afraid to rethink everything,
not just the kernel, even if it meant not being backward compatible 
with \unix.
This led to software with more elegant designs, to the removal
of many ugly corners in \unix, and ultimately to less source code
while still providing more services.

For instance, thanks to a few novel ideas in the kernel, namely
{per-process namespace}, 
{file servers} and
{union mount},
every application who wish to interact with the {console}
can just use the uniform [[/dev/cons]] device file, whether
this console is attached directly to the terminal, 
to a remote machine, or wether it is one of the terminal window
of the graphical user interface.
Thanks to this design, the \plan windowing system, [[rio]], 
could be implemented with only 10 500 LOC, including the code for
terminal windows.

By comparison, Linux and X Window introduced the separate concepts 
of [[tty]] devices and pseudo [[pty]] terminals, 
and the code of [[xterm]], which is just a terminal 
for X Window, is made of 88 000 LOC.
\l Why is X so big - Ken thompson / http://c2.com/cgi/wiki?PlanNineFromBellLabs
This is partly because [[xterm]] carries the historical baggage of
standards invented in the 70's, e.g. special control sequences
for VT100 terminals. 

With the exception of the web browser, \plan contains
all the essential software used by a programmer.
They form a coherent set because they were all designed from scratch
by a small team of programmers.

\l russ cox: if switch from windows -> linux, see inconsistencies and
\l inelegance in windows, same when do linux -> plan9

\subsection{The C language and ARM architecture}
\l Our basis
\l ocaml for a few books
\n hopefully later a bit Rust, safer than C, 

The goal of books such as {\em Principia Mathematica}~\cite{principia} is
to describe the foundations of a field starting from
a very small basis, e.g. a small set of axioms
and inference rules, from which all the rest can be derived
or built.
%
The great book {\em The Elements of Computing Systems}~\cite{tecs}
attempts a similar thing for computers,
starting from the [[nand]] logic gate and building gradually
an additioner, an arithmetic unit, a flip-flop, memory, and finally a 
simple processor.
\n nand2tetris had bootstrapping issue too. my hack assembler was in ocaml :)

In \principia we are interested in software though, not hardware.
Because most of the software described in the \principia books
are written in C, our basis in some sense
is the C programming language~\cite{k-r}.
C is a fairly large language, and so our basis is unfortunately 
fairly large too.
\l hard also for software because they all depend on each other strongly

Note that one of the \principia book describes the
C compiler which targets the ARM architecture. So, in principle
our basis could be reduced to the ARM machine language, which
is fairly simple. Another \principia book describes actually
an ARM emulator. But both the C compiler and the ARM emulator
are written in C itself, which brings us back to C as our basis.
\l conceptually could run compiler in your head manually 
\l and hand translate all the code, which reduced to ARM assembly.
\l We will focus on ARM architecture, so assume ARM machine.
\l also ocaml is written in ocaml ... so big basis there too

A more elegant alternative, avoiding self-reference,
would be to build a tower of increasingly powerful languages. Starting
from raw machine code, e.g. for the ARM architecture, which is
a pretty small basis, one could
gradually build more sophisticated languages
through a series of {bootstrapping} steps.
In fact such a project 
exists\furl{http://homepage.ntlworld.com/edmund.grimley-evans/bcompiler.html},
but with already six steps, its author could not reach a language
as expressive as C.

Another alternative, chosen by Donald Knuth
for his encyclopedic books {\em The Art of Computer Programming}~\cite{taocp}
was to choose as a basis a very simple computer he invented
called MIX, and a very simple assembly language, MIXAL.
\l didn't start from turing machine though
\l could start from turing machine?? tower on top of that? enter initial
\l prog via punch card? via manual bit switch?
Using assembly is maybe ok for describing algorithms,
but we think it would not be productive for writing entire
programs, and this would lead to very long \principia books.

We think the ARM machine, a real but fairly simple machine, and the C language,
a higher level language than assembly, are better basis 
for \principia.
\l more practical basis
\l Raspberry Pi target machine? Qemu?


\subsection{Literate programming}

We want to show source code because code is the ultimate
explanation for what a software does.
%
We think though 
that showing pages and pages of listings in an appendix, 
as done for instance in the Minix book~\cite{minix},
\l also for oberon, xv6, unix v6
even when this appendix is preceded by documentation chapters, is not
the best way to explain code. We think the code and its documentation
should be mixed together, as done for instance in the Xinu book~\cite{xinu},
so one does not have to switch back and forth between the code
in an appendix and the documentation chapters.

Literate programming~\cite{lp-book} is a technique invented by
Donald Knuth to make it easy to mix code and documentation
in a document in order to better develop and better explain programs. 
Such documents are called {literate programs}. All \principia
software are literate programs.

Note that literate programming is different from using 
{API documentation generators} such as 
javadoc\furl{http://www.oracle.com/technetwork/articles/java/index-jsp-135444.html}, or 
doxygen\furl{http://www.stack.nl/~dimitri/doxygen/}. 
Noweb\furl{http://www.cs.tufts.edu/~nr/noweb/}, the literate
programming tool we will be using, does not provide the same kind
of services. 
\l cf sexp_int.mli, too many functions, given equal importance. 
\l could reorder, but then that's the point, use LP. 
\l Moreover 2 views will always be better than just one view.
%
Indeed, literate programming allows programmers to explain their
code in the order they think the flow of their thoughts and their code would
be best understood, rather than the order imposed by the compiler.
It allows among other things to explain code piece by piece, with the
possibility to present a high-level view first of the code,
to switch between top-down and bottom-up explanations, and to
separate concerns.
\l AOP a bit

For instance, the [[Proc]] data structure which we will
see in the \book{Kernel}, 
represents some information about a process. It is a huge structure
with more than 90 fields. Many of those fields are used only
for advanced features of the kernel. The C compiler imposes
to define this structure in one place. Noweb, which
can be seen essentially as a macro-processing language, allows
to present this structure piece by piece, gradually, in different
chapters. One can show first the code of the structure with 
the most important fields,
and delay the exposition of other fields to advanced topics chapters.
This greatly facilitates the understanding of the code, by not
submerging the reader with too much details first.

In the same way, the [[main()]] function in most
software is rather large and mixes together many concerns:
command line processing, error managment, debugging options,
optimisations, and usually a call to the main algorithm.
Showing in one listing the whole function would hide behind
noise this call to the main algorithm. The main flow
of the program tough is arguably the most important thing to understand
first. Using literate programming
one can show code where the most important parts are kept, and where
other concerns are hidden and presented later. 

In fact, lots of the effort in writing the \principia books
has been in transforming the \plan software in literate
programs and in reorganizing again and again the \plan code to find
the best way, the best order, the best separation of concerns
in which we think the reader will more easily understand the code.


\subsection{Getting started}

To play with the different software described in the \principia books,
we recommend to install our fork of \plan. See \urlprincipia.
\l what is special about this fork?
Section~\ref{sec:code-orga} explains the directory structure of our
\plan repository.

The installation consists first in cloning our fork of \plan
and cloning our fork of Ken Thompson's C (cross) compilers, [[kencc]].
You can then compile [[kencc]] using a regular C compiler,
e.g. [[gcc]], from your host operating system, e.g. from MacOS or Linux.
Then you can {cross compile} the \plan kernel,
the C standard library, and then the whole \plan operating
system, with all its libraries and software,
using the \plan C cross compilers ([[kencc]]) installed in the previous step,
and so build from scratch your own \plan{} {distribution}.
\l also need plan9port

Finally, you can run this \plan distribution either under 
Qemu\furl{http://www.qemu.org}, which makes it
easy to experiment, or you can also install it on a
real machine such as a Raspberry Pi.


\subsection{Requirements}

The \principia books are not introductions to programming,
computer science, or to any of its subfields.
%
For instance our book about the kernel is not an introduction
to operating systems. Indeed, we will assume the reader 
has already a vague idea of how an operating system
works and so is already familiar 
with concepts such as 
virtual memory, 
critical regions, 
interrupts, 
system calls, etc.

We will present with full details the source
code of different software, but we assume you already know
most of the {concepts}, {theories}, and {algorithms} behind those software.
The \principia books are there to cover the {practice}.
\n Software Practice and Experience style!
We assume our readers are mainly students in computer science with
a bachelor or PhD, who desire to consolidate their knowledge
by reading the ultimate computer science explanations: source code.

As said earlier, because most of the books are made of C source code, 
you'll need to have a good knowledge of 
the C programming language~\cite{k-r} to understand them.
%
Because all the software we describe are \plan software, 
and because \plan has a lot in common with its ancestor \unix,
you'll need to be familiar with those systems.
We recommend to read~\cite{unix-pike} for \unix
and to read the two \plan tutorials [[docs/articles/9.ps]] and
[[docs/articles/names.ps]] available in our \plan repository.

\subsection{Copyright}

Most of the software in \principia are \plan software with copyrights
from Lucent Technologies Inc. They are open source though; permission
is granted to copy, distribute and/or modify the source code.

\subsection{Acknowledgments}

We would like to acknowledge of course \plan's authors who wrote
in some sense most of the content of the \principia books: 
Ken Thompson, Rob Pike, Dave Presotto, Phil
Winterbottom, Russ Cox, and many other people from Bell Labs.








\section{Overview}

\n https://en.wikipedia.org/wiki/Computer#Software nice categories
\n https://en.wikipedia.org/wiki/Computer_software

\subsection{Code organisation}
\label{sec:code-orga}

Table~\ref{tab:code-orga} presents a short description
of the main directories in our \plan fork
as well as the corresponding sections in this document
in which the software associated with the directory is discussed.
%
Table~\ref{tab:other-dirs} presents less important directories.

A few \plan software have some architecture specific parts,
with support for x86 and ARM in our \plan fork.
The LOC in Table~\ref{tab:code-orga} accounts only for the code to support
one of the architecture: the ARM.

\begin{table*}[tbh!]
\begin{tabular}{lllr}
\toprule
{\bf Directory} & {\bf Description} & {\bf Section} & {\bf LOC} \\
\otoprule
[[kernel/]] & The \plan kernel (x86, no graphics, no network) & \ref{sec:kernel} & 60 000 \\
            & graphics stack, kernel code & \ref{sec:graphics}  & 10 000 \\
            & network stack, kernel code & \ref{sec:network} & 23 000 \\
[[include/]] & The header files (e.g. [[core/libc.h]]) & 
   \ref{sec:libc} & 5 500 \\
% no x86, just ARM
[[lib_core/]] & The core C library (with ARM and x86 assembly) & \ref{sec:libc} & 21 500 \\
% no x86, just ARM
[[shells/]] & The shell & \ref{sec:shell} & 7 500 \\

\midrule
[[machine/]] & The ARM emulator & \ref{sec:emulator} & 4 400\\
[[assemblers/]] & The ARM and x86 assemblers & \ref{sec:assembler} & 4 100 \\
% includes also include/arm/5.out.h, no x86 just ARM
[[linkers/]] & The ARM and x86 linkers & \ref{sec:linker} & 9 600 \\
% includes also include/debug/a.out.h and a few other header files, no x86
[[compilers/]] & The C compiler with ARM and x86 backends & \ref{sec:compiler} &25 000 \\
[[generators/]] & The code generators [[lex]] and [[yacc]] & \ref{sec:generators} & 3 500\\
%ocaml

\midrule
[[editors/]] & The editor & \ref{sec:editor} & 10 000\\
%ocaml, but not the pfff major modes
[[builders/]] & The [[mk]] tool & \ref{sec:builder} & 4 800 \\
[[debuggers/]] & The debuggers [[db]] and [[acid]] & \ref{sec:debugger} & 17 000\\
% no x86 just ARM
[[profilers/]] & The profilers & \ref{sec:profiler} & 4 900 \\

\midrule
[[lib_graphics/]] & A large part of the graphics stack & \ref{sec:graphics} & 16 000 \\
% does not include kernel/, but include include/graphics/
[[windows/]] & The windowing system & \ref{sec:windowing} & 10 500\\

\midrule
[[lib_networking/]] & A small part of the network stack & \ref{sec:network} & 1 000\\
[[networking/]] & Networking applications, clients and servers & \ref{sec:network} & 75 000\\
%ocaml
[[browsers/]] & The web browser & \ref{sec:browser} & 24 000\\

\midrule
[[interpreters/]] & The Scheme interpreter & \ref{sec:scheme} & 4 600\\
[[languages/]] & The OCaml compiler & \ref{sec:ocaml} & 30 000\\
%ocaml, but not the native code compiler, nor external/libs nor debugger


\midrule
[[utilities/]] & Utilities such as [[ls]], [[cp]], [[mv]] & \ref{sec:utilities} & 16 000\\
               & [[grep]], [[gzip]], [[tar]], [[sed]], [[diff]], & \\
               & [[xargs]], [[ps]],  etc & \\
\otoprule
Total & & & 363 900 \\
\bottomrule
\end{tabular}
\caption{Main directories of the \plan source code repository}
\label{tab:code-orga}
\end{table*}
\n for the LOC it's a mix of cm -test_loc CROSS xxx/, or make loc in the dir


\begin{table*}[tbh!]
\begin{tabular}{llll}
\toprule
{\bf Directory} & {\bf Description} & \\
\otoprule
[[docs/]] & Articles and man pages  \\
[[ROOT/]] & The target directory where compiled binaries and libraries will \\
          & be installed which will form the \plan{} {distribution} \\
[[sys/]] & Backward compatible directory containing \\
         & mostly symbolic links and build scripts  \\
[[ape/]] & ANSI and POSIX environment, to ease \\
         & the compilation of legacy \unix applications & \\
[[CROSS/]] & Scripts to support cross compilation  \\

\midrule
[[security/]] & Security, authentification, cryptography \\
[[lib_math/]] & Arbitrary precision mathematics library \\
[[lib_misc/]] & String, regexps, and compression libraries  \\
[[typesetting/]] & Tools to generate documents [[troff]], [[tbl]], [[pic]], [[grap]], [[eqn]] and [[man]] \\
[[database/]] & Simple key-value database, mostly used for storing network information \\
[[games/]] & Video games \\
[[applications/]] & Small applications \\
[[BIG/]] & software of interest not yet included \\
[[version_control/]] &  \\
[[lib_audio/]] & \\
\bottomrule
\end{tabular}
\caption{Other directories}
\label{tab:other-dirs}
\end{table*}


\subsection{Software architecture}
%old: was Architecture overview but oxymoron, archi is already an overview

Many of the \principia programs are mutually dependent
on each other. Indeed, to {run} a compiler or an editor you need
a kernel (and a shell), but to {create} this kernel in the first
place you need a compiler and an editor.
In the same way, the C compiler uses code from the 
core C library, but to create this library you need a C compiler.
In fact the C compiler is written in C itself, so there are even
self dependencies.
\t Figure with all deps, split binary/source so partially solve mutual deps

It is possible though to layer things when one looks more
at how things are organized in memory.
\l rather than what depends on what codewise or runtimewise
\t Big figure with everything
A first separation to be made is 
code running in {kernel space} versus 
code running in {user space}.
%
Most of the code running
in kernel space is in [[kernel/]], as well as some code
in [[lib_graphics/]] and [[lib_networking/]]. Some of the
code in the C library, the memory pool library
\footnote{[[lib_core/libc/port/pool.c]]}
and a few utility functions and globals are used both in the kernel and in user
programs but they are the exceptions.
The rest of the codebase runs in user space.

The boundary between user programs and the kernel is provided
by the system calls API. They are declared in [[include/core/libc.h]]
and implemented in [[lib_core/libc/9syscall/]]. This last directory
contains one assembly file per system call and each of those files contain
mostly the software interrupt instruction with a special code to
call the appropriate code in the kernel.
The dispatching kernel code is in [[kernel/syscalls/]].
\l figure with content of one such file for ARM?
Some of those system calls are process related,
e.g. [[rfork()]], [[exec()]], [[exit()]]
\footnote{with implementations in [[kernel/processes/]]},
some are memory related, e.g. [[brk()]]
\footnote{with implementation in [[kernel/memory]]},
and other are used for file input and output,
e.g. [[open()]], [[close()]], [[read()]], [[write()]]
\footnote{with implementation in [[kernel/files/]]}.
\l table with all syscalls and a description?

In fact those IO system calls provide an extended way
for programs to request services from the kernel
via the filesystem hierarchy. Indeed, under \unix everything
is a file, including devices. 
A process using [[/dev/cons]] will trigger code in the kernel
in [[kernel/console/devcons.c]] which itself will trigger
code that handles the keyboard device in [[kernel/devices/keyboard/]]
and the screen device in [[kernel/devices/screen/]].

The concept was pushed even further under \plan where everything is 
a file server.
\l hard to understand now
The graphics API is accessible though files under [[/dev/draw/]]
and triggers code in [[kernel/devices/screen/devdraw.c]] and
[[lib_graphics/]].
In a similar way the networking API is accessible through files
under [[/net/]] and triggers code in [[kernel/network/]].

A second separation to be made is
library code versus
application code.
All the user programs in \plan rely first on the core C library
in [[lib_core/libc/]] and exposed via the [[include/core/libc.h]] header
file. All programs are linked with [[libc.a]]. 
The [[lib_xxx/]] directories contain other general purpose functions
and data structures which are used by different programs. By
using a library one can more easily reuse source code.

One of the first user program using the C library is the shell.
The assembler, linker, and compiler also rely on the C library
as well as other header files, e.g. [[include/arch/arm/5.out.h]]
which declares the set of ARM opcodes.

All the graphical applications, e.g. the clock but also the windowing
system rely also on the [[lib_graphics/lib_draw/]] library, which
is exposed in the [[include/graphics/draw.h]] header file.
The graphics library is essentially a thin wrapper over the protocol
used by the [[/dev/draw/]] device files.


\l like in cs illuminated, show the different layers. information/.../...

\subsection{Bootstrapping}

\begin{verbatim}
chicken and egg.
40 years of history :) 
toy kernel, written punch cards (no editor, no assembler)
keyboard/screen/disk
better kernel
toy assembler (punch cards)
toy editor written in assembly
better assembler
better editor
toy compiler
better compiler
better editor
better kernel
nowaday.
\end{verbatim}

To bootstrap \plan you'll need only a machine with
a C compiler and an operating system that can write
on a VFAT filesystem. That's it. So, Linux, MacOS, and Windows
are possible host operating systems you can use to
compile from scratch and then run \plan. \plan itself
is also a valid host as one can compile \plan under \plan.

\begin{verbatim}

kencc is rougly the one included, but ported to many
different OS (macos, linux) so more noisy code that
we got rid in principia.

put mkfile that builds plan9? 
and then run under real raspberry machine or qemu!
talk about kencc! but then show how can compile under itself!
(even kernel? yes if can make a floppy under qemu or rpi ...)

ROOT/ a distribution from scratch!

explain gradual things, kernel with sh on a floppy,
kernel with hdd vfat, kernel with rc, kernel with graphics,
kernel with rio, etc.

not complete in some sense, rely on 5i for the bottom
which is an incomplete simulator, and even loop because
rely on C. 
Nand2tetris is more complete in this sense. Its basis
is just the nand gate! everything is built on top of that after!
A bit like in math where can just rely on gentzen sequent calculus
and build everything on top of it (principia mathematic does that?)
We could even been build physically Hack (hmm but how enter
program in its memory? still an issue)

can then compile under plan9 itself?

qemu vs raspberry?
\end{verbatim}

% \subsection{Books structures? Serie structure?}


\section{The Core System}

What we call the {core system} is the minimal set of software that 
are needed to reach the point where the programmer can 
interactively launch other programs.
In the case of \plan this minimal set is made of 
the kernel, 
the shell, and the
standard C library.

The C library is used by the shell as the library provides
the necessary {bridge} to call the kernel from user programs.
The library also provides memory allocation routines and a few
general utility functions. In fact, part of the C library
is also used internally by the kernel.

\l figure? showing relation between kernel/shell/libc?

\subsection{The kernel, [[9]]}
\label{sec:kernel}

The kernel is the program that {manages} other programs.
%
It is arguably the most important program; without the kernel
\l with the compiler
no other program can run. 
%
The kernel provides
the main abstractions of the computer and
{demultiplex} its resources (the processor,
the memory, the input and output devices)
to multiple programs at the same time.

The kernel is the biggest program and so the biggest book in the serie.
The \plan kernel is called simply [[9]].

\subsection{The core library, [[libc]]}
\label{sec:libc}

The C library 
\l is the program 
contains code that is {used} by all the other 
programs (including the kernel).
%
In the case of \plan this library contains
the memory allocation routines ([[malloc()]], [[free()]]),
the bridge to call the kernel (the system calls),
the unicode functions, and many other
utility functions.
\l also describe thread library

\subsection{The shell, [[rc]]}
\label{sec:shell}

The shell is the program that allows users to {launch} other programs.
It is the primary user interface to the services offered
by the kernel, the so called {command line interface} (we will
see later a graphical user interface).

In \plan, and also in its ancestor \unix, the shell is a regular
user program; it does not have to be included in the kernel.
The user can actually change shell without changing the kernel.

The shell will be the first user program we will describe in the
\principia serie. It illustrates very well the many features
provided by the kernel because it is using internally many
system calls ([[fork()]], [[exec()]], [[chdir()]], [[pipe()]], etc).

The \plan shell is called [[rc]] (for [[r]]un [[c]]ommand).

\l requiring compiler/assembler/linker (and actually relying also on lex/yacc)
\l mv later though?


\section{The Development Toolchain}
\l bootstrapping issue

Once you have a terminal where you can launch programs,
you'll need tools to produce
those {binary} programs
from {source} code. 
%
The {development toolchain}
is a set of tools working together to be able to produce
{executables}.
This toolchain is made essentially of
a C compiler,
an assembler,
and a linker.

\n note that actually could be part of core system because need compiler
\n to compile the kernel ...

\l figure? showing the compilation pipeline?

\subsection{The C compiler, [[5c]]}
\label{sec:compiler}

\begin{verbatim}
looping issue again here, bootstrapping.

More high level than assembler language ...
\end{verbatim}

\subsection{The assembler, [[5a]]}
\label{sec:assembler}

\begin{verbatim}
produce object files
\end{verbatim}

\subsection{The linker, [[5l]]}
\label{sec:linker}

\begin{verbatim}
link them! produce executable
\end{verbatim}

%\subsection{The macro processor, [[cpp]]}

\subsection{The processor emulator, [[5i]]}
\label{sec:emulator}

An emulator (or simulator) is a program that 
emulates a computer and can {run} another (binary) program.

We will describe [[5i]], an ARM processor emulator.
The [[i]] probably means interpreter and the
[[5]] comes from the \plan convention to name architecture
with a number or single letter (0 is MIPS, 5 is ARM, 8 is x86, etc).
This is why the ARM assembler, linker, and compiler we will
describe later are called respectively [[5a]], [[5l]], and [[5c]].

An emulator is not really a part of the development toolchain but
because the assembler, linker, and compiler
target an architecture, it is useful to describe
first the {instruction set} (ISA) of an architecture via
its emulator.

\l also contain a debugger and profiler

\subsection{The code generators, [[lex]] and [[yacc]]}
\label{sec:generators}

Code generators are programs that {generate} other programs.
%
We will describe two such generators, [[lex]] and [[yacc]].
They provide both a domain specific language (DSL) to
help respectively scan and parse languages.
They are used by a few other programs in \principia:
the assembler, the C compiler, the shell, the builder,
which is why we consider [[lex]] and [[yacc]] essential software.
\l actually lex is not used that much
The full understanding of the C compiler for instance would
be incomplete if one could not understand the code generated
from the C grammar specification by [[yacc]].
\t in ocaml! not plan9 (actually unix)
\l also self reference there too, lex and yacc use lex and yacc :)

\section{The Developer Tools}

The {developer tools} are a set of tools which are not
strictly necessary to produce programs, like the software
development toolchain we've seen in the previous section, but which 
are really useful in the software development process.
%
Those tools are
the text editor,
the builder,
the debugger,
and the profiler.

\l figure? showing the source code at the center and the tools around?

\subsection{The text editor, [[efuns]]}
\label{sec:editor}

The text editor is a program to help {write} programs.
\l actually also to help read
%
It is perhaps the most important tool for a programmer,
and certainly the program to which programmers are the
most emotionally attached to.
\l editor wars
Without an editor you would have to read or modify source code with
tools like [[cat]], [[ed]], etc.
\l or even worse use punch cards.
\t efuns, not plan9 software. Also not C, but ocaml! 
\t will also use a bit the graphical stack! so first graphic app!

\t in fact attachment which is why choose present emacs clone
\t instead of sam or acme \plan text editor :)

\subsection{The builder, [[mk]]}
\label{sec:builder}

The builder is a program to help {build} programs
by calling appropriately tools from the development toolchain.
\l such as the compiler, the assembler, the linker, etc.
%
The whole \plan operating system can be built from scratch
with one simple command, [[mk all]], thanks to the builder program
called [[mk]] and a few configuration files (the [[mkfile]]s).

\subsection{The debugger, [[db]]}
\label{sec:debugger}

The debugger is a program that {commands} and {inspect} another program.
%
Programming is so difficult that invariably we make mistakes
when writing code. Having tools to help find those bugs is essential
and the debugger is the most important of those tools.
Even if many programmers, including great programmers,
\l cite coders at work?
use just [[printf()]] tracing commands to debug their programs, 
we think a debugger can greatly accelerate the time it takes to find bugs. 
%
Debuggers are also great tools to help understand programs, especially programs
written by other people.

The \plan debugger is called simply [[db]].
\t also acid, and strace.

\subsection{The profiler, [[prof]]}
\label{sec:profiler}

The profiler is a program that {generates statistics} about another program.
%
It is mainly used to optimize code by finding where the program
spends most of its time. It can be useful also as a tool to find bugs
because unexpected statistics can sometimes be good hints to
fix code where the programmer was expecting different results.
\l cf jon bentley

\t will see prof? tprof? 

\t Version control system

\section{Graphics}

Up until now we have mostly seen {command line} tools
and interactions through a simple text-based terminal.
One of the most important invention in computer science though is the 
{graphical user interface} (GUI). Even regular
command line tools benefit from a graphical user interface
as one can run multiple tools in different {windows}
at the same time.

\l figure? showing screenshot and what the graphics stack does and what rio do?

\subsection{The graphics stack, [[/dev/draw]]}
\label{sec:graphics}

The graphics stack provides the basis on top of which
graphical applications can be built, including the graphical
user interface.
GUI elements such as menus,
windows, lines, or texts are ultimately rendered
on the screen using simple graphic operations provided
by the graphics stack.

Under \plan the graphics services are accessible through
the [[/dev/draw/]] device directory which is connected
to the screen device driver in the kernel. 
\l a bit ugly design, lots of graphic code in the kernel

\subsection{The windowing system, [[rio]]}
\label{sec:windowing}

One of the most important graphical application is
the windowing system. In some sense it
is an extension of the kernel and the shell; it is a
program that also {manages and launches} other programs,
represented visually by separate {windows}.

The \plan windowing system is called [[rio]]. It is essentially
a demultiplexer of the [[/dev/cons]], [[/dev/mouse]], 
[[/dev/keyboard]] and [[/dev/draw]] device files.
Indeed [[rio]] is a {file server} that internally uses 
the mouse, keyboard, and screen {physical} devices,
and provides a {virtual} mouse, virtual keyboard,
and virtual screen to its windows using
{views} of those same device files
(thanks to the per-process amespace feature in the kernel)
In fact, an unusual feature of [[rio]] is that it can be run inside itself.
\l small xterm, beautiful, also as powerful as X11

\section{Networking}

Up until now we have described software running on 
a single machine. We'll now switch to programs
that can communicate with each other on different machines.
\l also core invention of CS, leading ultimately to the web! wikipedia!
\l people working together

\l figure? ???

\subsection{The network stack, [[/net]]}
\label{sec:network}

The network stack is a part of the kernel, fairly
large, which provides the necessary abstractions
for programs to communicate with each other on
different machines, using different protocols.

Under \plan the network services are accessible through
the [[/net/]] device directory.
\l small NFS, beautiful

\subsection{The web browser, [[mmm]]}
\label{sec:browser}

\l in ocaml!
\l actually main issue of plan9 was lack of web browser. A few tentatives
\l but never as complete as firefox, and was difficult to port firefox
\l to plan9

\section{Advanced Languages}

C is a great programming language. 
\l huge improvment over assembly
It is arguably
the best language to implement system programs
such as kernels, virtual machines, just-in-time compilers, etc.
\l maybe Rust is better now, or modula-3, but not C plus plus for sure
It can be seen as a portable assembler, which makes it
also a great language to implement efficiently core libraries,
which can even be used by other programming languages.

For many applications though, especially applications
without strong constraints on memory or speed, it can
be far more productive for the programmer to use higher-level
languages. Programming in C is indeed very error-prone, with
recurring bugs such as buffer overflows, memory corruptions,
or security holes.
%
We will present below two languages more expressive
and less error-prone than C. It is almost impossible to
have the bugs mentioned above while programming
in those higher-level languages.

%%%\subsection{The toy frontend compiler, [[tigerc]]}
%%%\subsection{The backend compiler, [[c--]]}

\subsection{The Scheme interpreter, [[s9]]}
\label{sec:scheme}

\begin{verbatim}
dynamic, very powerful
\end{verbatim}

\subsection{The OCaml (light) compiler, [[ocamlc]]}
\label{sec:ocaml}

\l needed because we expose ocaml code before, so for completness
\l need to describe ocaml compiler :)

\begin{verbatim}
static like C, but more expressive.
ultimate lang IMHO, even in 2015!
\end{verbatim}






\section{Utilities}
\label{sec:utilities}

In addition to all the software mentioned above,
a programmer very often use small utilities to perform or automate
certain tasks. 
The code of those utilities is usually pretty small as they
have often a single and simple function.
One of the \principia books will
be dedicated to describe the code of a few of those utilities,
the most important one for the programmer.

\subsection{File and directory utilities, [[ls]], [[cp]], etc}

The shell has very few builtins. To create, delete, or modify
files or directories a programmer needs to use special
programs. Those programs are essentially small wrappers
over the file and directory system calls provided by the kernel.

Here are the file and directory utilities we will describe:
\begin{itemize}
\item [[touch]] and [[mkdir]]
\item [[cat]] and [[ls]]
\item [[rm]], [[cp]], [[mv]]
\item [[chmod]], [[chgrp]], [[mtime]]
\end{itemize}

\l find?

\subsection{String processing utilities, [[grep]], [[sed]], etc}

Source code is text, which is a set of lines, which
are a set of strings.
It is thus normal that string processing utilities
are very often used by programmers to either search, modify,
or compare text.

Here are the string processing utilities we will describe:
\begin{itemize}
\item [[grep]], one of the most versatile and useful tool for a programmer
\item [[sed]] and [[tr]]
\item [[diff]]
\l very useful programmer tool
\end{itemize}

\subsection{Process utilities, [[ps]], [[kill]], etc}

Source code ultimately is transformed in binary programs which ultimately
become processes when run. A programmer needs a set of utilities
to manipulate those processes. Under \plan the [[/proc]]
directory can be used to manipulate those processes.
An alternative is to use command line programs which
are often small wrappers over [[/proc]].

Here are a few process utilities we will describe:
\begin{itemize}
\item [[ps]] and [[pstree]]
\item [[kill]]
\l actually it's a script
\item [[sleep]]
\end{itemize}

\subsection{Archive utilities, [[tar]], [[gzip]]}

Once a program has been written, a programmer often wants
to share it with other people.
In this case, he can count on a few utilities to package
and compress his code.

Here are a few utilities used to help disseminate software:
\begin{itemize}
\item [[tar]]
\item [[gzip]]
\end{itemize}


%\subsection{Misc utilities}

%\subsection{Date utilities}

%\begin{itemize}
%\item [[date]]
%\item [[cal]]
%\end{itemize}

%\subsection{Byte utilities}

%\begin{itemize}
%\item [[dd]]
%\item [[split]]
%\item [[xd]]
%\end{itemize}

\section{Applications}

We decided to limit ourselves in \principia to system software
that are the most relevant to the programmer.
We do not cover application software. We 
do not describe the code of applications such
as a spreadsheet, a word processor, a calendar, an email client, 
a video game, etc.

We encourage though other people to find such
applications, find the one with a small codebase, the one using 
a minimalistic approach, and to publish their literate programs.


%%%\subsection{The videogame, [[soldat]]}
%%%\subsection{The videogame, [[quake]]}

\section{Related works}

Here are other operating systems that were considered for \principia
but which were ultimately discarded:
\n (helps to appreciate even more how good our plan9 choice is):
\n dup: readme.txt

\begin{itemize}

\item \unix V6 (Ken Thompson et al.)
\furl{http://minnie.tuhs.org/cgi-bin/utree.pl},
fully commented in the classic book by John Lions~\cite{lions},
\l tuhs = the unix heritage society
or its modern incarnation xv6
\furl{http://pdos.csail.mit.edu/6.828/2014/xv6.html},
are great resources to fully understand a \unix kernel.
But the kernel is too simple: there is no support for graphics
or networking.

\item XINU (Douglas Comer)
\furl{http://minnie.tuhs.org/cgi-bin/utree.pl?file=Xinu7},
fully documented in two books~\cite{xinu,xinu2}
has a network stack but the operating system itself
is too simple with no virtual memory for instance.
\t and has multi processor support?

\item Minix (Andrew Tannenbaum et al.)
\furl{http://minnie.tuhs.org/cgi-bin/utree.pl?file=Minix1.1},
also fully documented~\cite{minix},
is fairly small but it is just a kernel. Minix does not provide
for instance its own windowing system; it relies on X Window instead
which is far more complicated than \plan windowing system.


\item GNU/Linux/X11 (Richard Stallman et al.)
\furl{http://www.gnu.org/}
\furl{http://www.kernel.org/}
\furl{http://www.freedesktop.org}
is far bigger than \plan.
If you take the source code of 
the Linux kernel, 
the glibc,
the bash shell,
the GNU assembler (gas) and linker (ld) part of the binutils package,
gcc,
the Emacs editor, 
make,
the debugger gdb, 
the profiler gprof,
the lex and yacc clones flex and bison,
and X11, you'll get
orders of magnitude more source code than \plan, even though
\plan provides in essence the same core services. 
Of course the Linux kernel contains thousands of specific device drivers,
glibc and gcc handle a multitude of different architectures, and X11
can support lots
of graphic cards; all of those things could be discarded when
presenting the core of those software. But their core is still far
bigger than the equivalent core in \plan.
\n Linux 1991, Plan9 1992 (but open source only in 2000)



\item Hack (Noam Nisan and Shimon Shocken)
\furl{http://www.nand2tetris.org/},
is a toy computer introduced in the excellent
book {\em The Elements of Computing Systems}~\cite{tecs}.
This book is great for understanding processors, assemblers,
and even compilers, but the operating system part is really too simple.


\item MMIX (Donald Knuth)
\furl{http://www-cs-faculty.stanford.edu/~uno/mmix-news.html}
and its ancestor MIX
are computers designed by Donald Knuth
and used in his classic book serie 
{\em The Art of Computer Programming}~\cite{taocp}.
A book, {\em MMIXware}~\cite{mmixware} using literate programming 
has even been written
to explain the full code of the MMIX simulator and assembler.
But, similar to Hack, very few software has been written for
this machine. For instance the book assumes the presence of an operating
system called NNIX, but nobody has ever written it.


\item STEPS (Alan Kay et al.)
\furl{http://vpri.org/html/writings.php}
is a project to reinvent from scratch programming.
It has a far more ambitious goal than \principia:
have a full system in 20 000 LOC. It is 
unfortunately not finished yet.
\l the kernel book is 30 000 Lines, and it's just the kernel (and actually not all of it), so hmmm

\item Oberon (Niklaus Wirth)
\furl{http://www.projectoberon.com/} 
is an operating system,
compiler, 
and windowing system 
designed from scratch. It is
a great system, very compact, and fully documented in a 
book~\cite{project-oberon}.
It imposes though strong restrictions on the programmer:
only applications written in Oberon can be run. This is
very different from \unix (and \plan) which is more like a universal operating
system which can run anything, as long as your program can
be compiled into a binary.
\l   javascript emulator: http://schierlm.github.io/OberonEmulator/

\item TempleOS (Terry A. Davis)
\furl{http://www.templeos.org/} is an operating system
single handedly created over a decade. It contains a kernel,
a windowing system, a compiler for a dialect of C, and
even some games.
It has graphics capabilities but there is no network support.

\end{itemize}

\l too many other to list here, see the comment in the .tex
%\item Squeak (Alan Kay et al.),
%\furl{http://squeak.org/}
% - nuttx, ils ont meme une graphic stack, widget, window manager
% - http://www.homebrewcpu.com/, full stack too, LCC, minix 2, tcp/ip, etc
% - RECC collection (Robert Elder)
%   http://recc.robertelder.org/ 
% - short canonical programs:
%   http://aosabook.org/blog/2014/01/call-for-technical-reviewers-for-500-lines-or-less/
%   https://github.com/aosabook/500lines
% - A tiny hand crafted CPU emulator, C compiler, and Operating System
%   https://github.com/rswier/swieros
% - http://menuetos.net/, written in assembly
% - boyer moore special machine, assembler, etc

\section{Conclusion}

We hope the \principia books will greatly consolidate
your computer science knowledge and give you a better and
more complete picture of what is going on in your computer.

We think the \principia software form together the minimal foundation
on top of which all applications can be built. Even though there are
19 books in the serie, we still think it is the
minimal foundation because it is hard to remove any of those software.
Indeed they all depend on each other.
%
Of course, you need to rely first on a kernel (hence the name), but
the shell and the C library are also essential. 
% => 3
But those software
are coded in C and assembly so you need a C compiler and
an assembler, and because source code is usually split in many
files you also need a linker.
\l But to run those tools you need a kernel :) interdependent
The C compiler itself usually uses DSLs like lex and yacc.
(which are themselves written in C and actually use also lex and yacc
themselves).
% => 7
To write all this code in the first place you need an editor.
With so many source files you need
a builder to automate and optimize the compilation process.
Because the software we just mentioned inevitably have bugs
or non optimial parts, you will need a debugger and a profiler.
% => 11
Finally, nowadays
it is inconceivable to not use a graphical user interface
and to not work with multiple windows opened at the same time.
In the same way it is also not conceivable to work
in isolation; programmers collaborate with each other
especially via the web.
This means you need a graphical and networking stack as well
as a windowing system and a web browser.
% => 15
% (miss 2 languages, 1 utility, 1 emulator => 19)
As we said earlier, it is hard to remove any of the software
from the serie.


We hope those books will answer many of your questions, 
even those that seems very simple at first,
such as what happens
when the user type [[ls]] in a terminal window, but which
involves as you will see many software layers (the shell,
the C library, the kernel, the graphics stack, the windowing system)
and quite a lot of code.

The books in the serie can be read in any order. You don't have
to read them all. We recommend to pick the software you are 
the most interested in, for instance
the one you are more curious about because you have 
only a fuzzy idea of how they are implemented, and 
read the corresponding book in the serie.
\l In our case, the graphics stack, windowing systems 


\addcontentsline{toc}{section}{References}

\bibliography{docs/latex/Principia}
\bibliographystyle{plain}

%******************************************************************************
% Postlude
%******************************************************************************

\end{document}
