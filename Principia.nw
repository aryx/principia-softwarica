\documentclass[]{report}

%******************************************************************************
% Prelude
%******************************************************************************
\newif\iffinal
\newif\ifverbose
\finaltrue\verbosefalse % see also other newif in Macros.tex

%------------------------------------------------------------------------------
% Guidelines: 
%------------------------------------------------------------------------------

% When writing, don't do everything at once, keep things for later:
% - concept identification: just use {} around words you think are concepts.
% - command vs file vs code: just use [[]] first (what does texinfo?).
% - whether it's self sufficient: assumes a lot first, it's ok. This is
%   why you don't have to em concepts as you can assume people know them.
% - whether things are at the right place: at least say it once somewhere.
% - whether it forms a coherent and fluid document: make a first draft first! 
%   use narrow/widen to focus on the different parts independently.
%   add the %trans: %toc: later.
% - perfect english: write down the rule you are not sure of and at night
%    review relevant sections in Bugs in Writing and fix all occurences.
% - perfect style: same; write down the rule and review Bugs in writing.
% => better done than perfect!

% Also a few specific guidelines for principia:
% - take the point of view of the toolmaker, so for instance when explaining
%   assembly, describe more the features that have implementation implications
% - concrete examples! %example:
% - real world sections? where try to say how done in GNU/Linux, mac, Windows

%------------------------------------------------------------------------------
% Packages
%------------------------------------------------------------------------------
\usepackage{xspace}

\usepackage{verbatim}
%note: required by noweblatexpad for the \t \l \n in this file
\usepackage{fancyvrb}

\usepackage{url}
\usepackage{hyperref}
 \hypersetup{colorlinks=true}
\usepackage{cite}
\usepackage[pageref]{backref}
 \def\backref{{\footnotesize cited page(s)}~}
% not in basicTex
%\usepackage{cleveref} %\cref

% not in basicTex
%\usepackage{multirow}
\usepackage{booktabs} 
 \newcommand{\otoprule}{\midrule[\heavyrulewidth]}

\usepackage{graphicx}

%\usepackage{minitoc}

%\usepackage{docs/latex/noweb}
% \noweboptions{footnotesizecode,nomargintag}
% %note: allow chunk on different pages, less white space at bottom of pages
% \def\nwendcode{\endtrivlist \endgroup}
% \let\nwdocspar=\par

%------------------------------------------------------------------------------
% Macros
%------------------------------------------------------------------------------
\input{docs/latex/Macros}

%------------------------------------------------------------------------------
% Config
%------------------------------------------------------------------------------
\setcounter{tocdepth}{2}

%\usepackage[margin=1in]{geometry}
%\usepackage[margin=0.5in]{geometry}
%  but eat the bottom when very low
%\usepackage{fullpage} is deprecated 
% => do the more manual below:
%\addtolength{\oddsidemargin}{-.850in}
%\addtolength{\evensidemargin}{-.850in}
%\addtolength{\textwidth}{1.70in}
%\addtolength{\topmargin}{-.850in}
%\addtolength{\textheight}{1.70in}
% too big constraints when in portrait mode, cause some WEIRD errors
% in landscape it's ok though

\begin{document}
%******************************************************************************
% Title
%******************************************************************************
\title{
{\Huge 
Principia Softwarica
}\\
Fundamental Literate System Programs\\
version 0.4
}
\author{
Yoann Padioleau\\
\texttt{yoann.padioleau@gmail.com}\\
\\
with code from\\
Ken Thompson, 
Rob Pike, 
Dave Presotto, 
Phil Winterbottom,\\
Tom Duff,
Andrew Hume,
Russ Cox,\\
Xavier Leroy,
Fabrice Le Fessant,
and Francois Rouaix.
}
% add myself for ocamlyacc2? Doligez? other?
% (put on separate line otherwise too wide and authors not centered correctly)

% Other great programmers:
%# academics
% - Donald Knuth (tex/metafont, mmix, web/cweb)
% - Niklaus Wirth (pascal, p-code compiler, modula-2, oberon)
% - Andrew Tanenbaum (minix, ack)
% - McCarty (Lisp, CTSS idea)
% - Guy Steele (scheme rabbit, Java)
% - Gosling (Andrew window system, NEWS windowing system, emacs for unix, Java)
% - Dennis Ritchie (C, Unix)
% - Dijkstra, Hoare, Liskov, Parnas (software engineering)
% - Alan Kay, Butler Lampson, Goldberg, Thacker (Xerox Alto)
% - Per Brinch Hansen? 
%# open source movement
% - Richard Stallman (emacs, gcc, gdb, ...)
% - Linus Torvalds (linux, git)
% - Jim Gettys, Bob Scheiffer, Keith packard (X11, cairo)
% - Bruce Evans (dev86)
% - Larry Wall (perl, patch)
%# industry
% - Jeff Dean (GFS, map-reduce, tensor-flow, ...)
% - John Carmack (doom, quake, ...)
% - Ole Agesen (vmware)
%# exotic
% - Fabrice Bellard (qemu, fbcc, ffmpeg, qemacs)


\maketitle 
\l The Elements of ...? ... for the New Millennium?
\l Literate Programs of an Entire Operating System (Plan 9 graphics & network)

\hrule
\input{docs/latex/Copyright}
\hrule

\begingroup
\hypersetup{linkcolor=blue}
\tableofcontents
\endgroup

%******************************************************************************
% Body
%******************************************************************************

\chapter{Introduction}
\n What. (Better to start with the golden Why? My 'what' has some implicit Why)

{\em \principia} is a series of books explaining how things work in a
{computer} by describing with full details all the {source code} of all
the essential {programs} used by a {programmer}.
\n machine/computer, system/software/program/sourcecode/code, process
\l fundamental software (alok)
%
Among those essential programs there are
the {kernel},
the {shell},
the {windowing system},
%
the {compiler},
the {linker},
%
the {editor}, or
the {debugger}.
%
Each program will be covered by a separate book. 

The books not only describe the implementations of essential programs,
{\em they are} the implementations of those programs. Indeed,
each program in \principia comes from a 
{\em literate program}~\cite{lp-book}, 
which is a document containing both source code and documentation
and where the code is organized and presented in a way 
to facilitate its comprehension.
The actual code and the book are derived both automatically from this literate
program.
\l juju: fait rever plus, cf gmail email, look also lcc book intro
\l  once have read a book, you'll have seen all the code! understand fully!
\l which is more adapted for human comprehension than compiler constraints.
\l The code and its documentation are strongly connected.
\l \footnote{Section~\ref{sec:lp} more information about literate programming}

\l should I talk about plan9 already here?

The goal of the report you are reading now is to introduce the series and to 
\l Chapter~\ref{chap:lp} will
give a quick overview of the \principia programs.
They form together the foundation on top of which all applications can
be built.
Similar to {\em Principia Mathematica}~\cite{principia}, 
which is a series of books
covering the foundations of mathematics, the goal of \principia
is to cover the fundamental programs.
\n principia informatica  = foundation of cs
\n principia algorithmica = taocp
\n goal: http://www.nytimes.com/library/books/042999best-nonfiction-list.html :)
Those programs are mostly all {\em meta programs}, which are
programs in which the input and/or output are other programs.
For instance, the kernel is a program that {manages} other programs;
the compiler is a program that {generates} other programs.
%
Those programs are also sometimes referred as {\em system software},
in opposition to {\em application software} (e.g., spreadsheets,
word processors, email clients), which I will not cover in \principia.


\section{Motivations}
\n Why

%Writing is nature's way of showing us how sloppy our thinking is.
% Leslie Lamport
\l add quotes in each section?

Why did I write those books?
%
The main reason is that I have always been curious and always
wanted to understand how things work under the hood, fully, to the 
smallest detail.
Programs are now running the world;
\l cite? the quote is actually software is eating the world
it is thus important to understand those programs, to be computer literate,
and source code is what defines those programs.

There are already lots of books explaining how computers work, explaining
the {concepts}, {theories}, and {algorithms} behind programs such as
kernels or compilers. There are also a few books about debuggers. But,
all those books rarely explain everything with full details, which
is what source code is all about. 
%
There are a few books that
include the whole source code of the program
described, for instance, the books about 
Minix~\cite{minix}, XINU~\cite{xinu}, or LCC~\cite{lcc}.
\n Oberon, Software Tools, with LP: Tex the program, mmixware, 
But, those books cover only
a few essential programs, and mostly always either
the kernel or the compiler. Moreover, they do not form a coherent set.

Enter \principia, a set of books covering all essential
programs, in a coherent way. 
In addition to the kernel and compiler, \principia covers also
the graphics stack, 
the networking stack, 
the windowing system, 
the assembler, 
the linker, 
and many other fundamental programs
that have never been fully explained before to the best of my knowledge.
\l with full details, as deep as possible, with all the source code.

\l there are also too many papers, too much knowledge, too many tools,
\l we need to stop and organize this knowledge!

I want to demystify those programs by showing their code,
and by showing that they are actually not that complicated.
%
I hope to remove some of the mental barriers people have that
prevent them from extending the tools you use every day: text editors,
compilers, even kernels.
%
As a side effect it will also maybe help people imagine better systems.
Indeed it can be very intimidating to invent something completely new
if you have no clue that it is actually not that difficult to build from
scratch a complete operating system.

Another motivation for those books, in addition to satisfy my curiosity,
as well as your curiosity,
is that I think you are a better programmer if you understand
how things work under the hood. 
%
In my opinion, you become a better C programmer
when you understand roughly what code generates the C compiler.
A good way to write more efficient code, or to avoid
writing really slow code, is to have some ideas of the
assembly code generated for your code by the compiler. 
%
In the same way, you can better use resources, for instance, memory, if you
have some ideas about how the kernel manages for you
these resources; you can better fix latency issues if
you understand how the networking stack works.
%
The \principia books can complement the excellent book
{\em Computer systems: a programmer's perspective}~\cite{cs-bryant}
by illustrating the many concepts this book introduces with concrete code.
\n but principia is really 'Computer systems: a builder's perspective'!

I also think it is easier to debug
programs if you better understand the environment in which
those programs evolve, if you understand the whole
software stack, and how things interact with each other.
%
Indeed, to fully understand certain error messages, e.g., from the kernel,
from the networking stack, or from the linker, it is very useful
to have some ideas about what those programs do.
Moreover, even if in almost all situations the bug or the performance
issue is in your program,
it could also be sometimes in a core library, in the kernel,
in the compiler, or in the linker. 
It is very rare, but when
those situations happen, if you have no clue about
the environment in which your program runs, you will
never be able to fix your problem.
\l it happens at facebook, and those people like maurer, paul saab are valued

Finally, by showing code written by great programmers, I hope
you will also learn how to write better programs. In other engineering
fields it is quite common for the students to learn from the
work of the masters of their fields.
\l Hoare wanted that! (see Oberon book intro)


Here are a few questions I hope the \principia books will answer:
\begin{itemize}

\item What happens when the user turns on the computer? Which program
gets executed first? What does this program?

\item What exactly happens when the user types [[ls]] in a terminal?
What is the set of programs involved in such a command? What
is the trace of this command through the different layers
of the software stack, from the keyboard interrupt to
the display of text glyphs on the screen?

\item How work [[C-c]] and [[C-z]], which respectively interrupts 
and suspends processes?
\t mv in kernel book? or shell book?

\item How source code gets compiled, assembled, linked,
and finally loaded in memory? What is the memory image
of a program? How it relates to the original source code?
How a debugger can display debugging information from a binary?

\item How a debugger works? Does it rely on special
services from the processor or from the kernel to implement
breakpoints?

\item Which program contains the memory allocator? The kernel or
The C library? How [[malloc()]] is implemented?
\n How it relates to the [[brk()]] {system call}? (too much)
How garbage collection works?

\item How certain graphical elements (e.g., rectangles, ellipses, characters)
are rendered on the screen? How the graphics card helps? How
the kernel helps? How things are intercepted
by the windowing system to make sure applications can not
draw in other windows?

\item What happens when you open a connection to another machine?
When you type a URL in your web browser? How can you
achieve a reliable communication on an unreliable physical network?

\item What does it take to port an entire operating system
to a new machine, for instance, on 
the Raspberry Pi\furl{https://www.raspberrypi.org/}?

\end{itemize}

\n list of courses that are never taught but should be! (via HN)
%https://news.ycombinator.com/item?id=10201611

\section{The ideal teaching operating system}
\n How

%%Very few programmers actually fully understand how everything work, 
%%how things interact with each other. It is easy to find
%%programming languages specialists, or kernel specialists, but
%%it is very rare to find people who can write compilers and
%%at the same time have a very good knowledge of the kernel
%%or networking stack and can optimize things also at this level.
%%%
%%Such programmers though are tremendously useful in companies
%%because they can think at multiple levels and optimize things globally.
%%\l saab, maurer at facebook
%%It is even more difficult those days 
%%to find those knowledgable programmers as

The question now is which actual source code to present?
The code of the major operating systems (e.g., GNU/Linux, MacOS, Windows)
is gigantic with hundreds of millions lines of code (LOC).
\n http://www.wired.com/2015/09/google-2-billion-lines-codeand-one-place/
% http://www.informationisbeautiful.net/visualizations/million-lines-of-code/
Here by operating system I mean not only the kernel but also
the programs that provide the platform for running application software
(the windowing system, the compiler, etc).
It is impossible to understand such large codebases.
\l even less to make books out of it
\l too bad because under all those layers are beautiful code, beautiful algos

I think there is hope though, that it is possible to understand fairly
well everything, that it is possible to answer all of the questions
mentioned in the previous section
by focusing on the {\em essence} of those major operating systems.
I think it is possible to design a teaching operating system
with capabilities similar to the mainstream operating systems,
but with a fraction of their code size.
%
Here are the requirements for the ideal teaching
operating system I want to use as the basis for the
\principia books:

\begin{itemize}
\item {\em Open source}: I want to show code, lots of code,
and I want you to be able
to play with such code, to modify it, so the ideal operating
system must be open source.

\item {\em Small}: I want programs that can be described in
books of reasonable size. The program needs to be a bit minimalist.

\item {\em As simple as possible, but not simpler}: I want
small code, but I do not want to show toy code.
%
For instance,
ARM~\cite{arm-refman} 
is a simpler architecture than 
x86~\cite{x86-refman},
so it makes sense to present the code of an ARM assembler
rather than an x86 assembler,
\l still show same essence
but I do not want to show a toy assembler for a toy architecture.
%
In the same way I can present the code of a C compiler
without certain advanced state-of-the-art
optimizations, to simplify the presentation,
but I do not want to show a toy compiler for a toy language.

\item {\em Real}: It has to run on real machines (e.g., an x86 desktop,
a Mac laptop, a Raspberry Pi), so you can play with it.

\item {\em Complete}: It needs to not only have a kernel and a compiler
but also a graphics stack, a networking stack, a windowing system, etc.

\item {\em Coherent}: The whole set of programs must form a coherent
set, so that the interactions between those programs can also be
described, and described succinctly.

\item {\em Self hosting}: I want to be able to improve,
to recompile, and to run everything under the system itself.
\l you improve yourself from yourself!
\n bootstrapable? hmm it's different than self hosting

\end{itemize}

\n jean louis gasse nostalgia about the time solo programmers 
%  could do huge things, when things were still manageable (os, compiler, etc)
%http://www.mondaynote.com/2015/08/24/a-salute-to-solo-programmers/

%trans:
Fortunately, this ideal teaching operating system already exists;
it is \plan.
\t I discuss a few other candidates in Section \ref{related-work}

\section{The \plan operating system}
\n How/Where

\plan
\furl{plan9.bell-labs.com/plan9/}
is the successor of \unix. It was designed from scratch
by a small team of great programmers (Rob Pike, Dave Presotto, 
Phil Winterbottom), including the original creator of \unix (Ken Thompson).
Their goal was to redesign \unix to better integrate 
graphics and networking,
which both became popular after \unix was originally invented.
\l "Why is X so big" Ken thompson (http://c2.com/cgi/wiki?PlanNineFromBellLabs)
In some sense, \plan is a kind of \unix 2.0.

The choice of \plan as the basis for the \principia books
may not be obvious, but it is in my opinion
the simplest and at the same time fairly complete operating system.
%
If you look at the screenshot of \plan in action in Figure~\ref{fig:plan9},
you will see many features:
\begin{itemize}

\item A screen with basic graphics and multiple windows

\item Multiple shells running independent commands at the same time

\item A simple clock graphical application

\item A simple program communicating through the network

\end{itemize}


\begin{figure*}
\includegraphics[angle=90, height=18cm]{plan9}
\caption{\plan in action.}
\label{fig:plan9}
\end{figure*}
\t one showing efuns! with nice colors! one showing mmm would be nice too :)
\t remove the .Trashes, and issue with uppercase dirs cos of VFAT retard
\l (even though efuns and mmm are not really plan9 applications)
\l WMP (not WIMP)

By comparison, if you look at 
\t screenshots of Mac, Windows, Linux, matching closely the one of plan9!
other systems
you will see that \plan {\em in essence} provides the same core services
than GNU/Linux/Xorg, Mac OS, or Microsoft Windows,
without certain bells and whistles,
\l multi tasking, graphics, windows, IO, etc
and with a significantly smaller codebase.
%
Indeed, my fork of \plan\footnote{\urlprincipia}, which includes all the
essential programs described in all the \principia books, and even a 
few more programs, 
is less than 350 000 LOC.
\n via make loc from the root directory (but use skip list ... so cheat)
% http://www.informationisbeautiful.net/visualizations/million-lines-of-code/

The \plan programs are minimalist but powerful, and their source
code is simple and small.
The creators of \plan were not afraid to rethink everything,
not just the kernel, even if it meant not being backward compatible 
with \unix.
This led to programs with more elegant designs, to the removal
of many ugly corners in \unix, 
\l footnote? russ cox: if switch from windows -> linux, see inconsistencies and
\l inelegance in windows, same when do linux -> plan9
and ultimately to less source code while still providing more services.
%
For instance, thanks to a few novel ideas in the kernel, namely
{per-process namespace}, 
{user-space filesystems}, and
{union mount},
every application who wish to interact with the {console}
can just use the uniform [[/dev/cons]] device file, whether
this console is attached directly to the physical terminal, 
to a remote machine, or whether it is one of the terminal window
of the graphical user interface.
%
Thanks to this design, the \plan windowing system [[rio]]
could be implemented with only 10 000 LOC, including the code
to emulate terminals in windows.

By comparison, Linux and X Window introduced the separate concepts 
of teletype [[tty]] devices and pseudo [[pty]] terminals.
The code of [[xterm]], which is just a terminal 
for X Window, not X Window itself, has already 88 000 LOC.
This is partly because [[xterm]] carries the historical baggage of
standards invented in the 70's (e.g., special control sequences
for VT100 terminals). 

\plan contains all the essential programs used by a programmer.
They form a coherent set because they were all designed from scratch
by a small team of programmers.
\n With the exception of the web browser? hmm they have one, just not complete
\t Chapter X talks about other operating systems, but they suck compare

\section{The C language and ARM architecture}
\l Our basis, Our starting point, Our axioms? Our language?
\l ocaml for a few books so put in title of section too? 
\n hopefully later a bit Rust, safer than C, 

The goal of books such as 
{\em The Elements}~\cite{elements} or
{\em Principia Mathematica}~\cite{principia} 
is to describe the foundation of a field starting from
a very small basis, 
for instance, a logic {language} with a small set of {axioms} and inference {rules}, 
\n e.g., predicate logic language (not that small), gentzen calculus, many defs
on top of which all the rest can be derived or built.
\n Hobbes (Law), Spinoza (Ethics?), Newton (Physics) did their own Elements
%
The book {\em The Elements of Computing Systems}~\cite{tecs}
does a similar thing for computers.
Starting only from the [[nand]] logic gate, the book builds gradually
the [[and]], [[or]], and [[not]] logic gates, a multiplexer,
a flip-flop, memory banks, an additioner, an arithmetic 
and logic unit, and finally a simple processor.
\n nand2tetris has bootstrapping issues too. my hack assembler was in ocaml :)

However, in \principia we are interested in software, not hardware.
Because most of the programs described in the \principia books
are written in C, our basis in some sense
is the C programming language~\cite{k-r}.
C is a fairly large language, with non trivial semantics, so 
our basis is unfortunately fairly large too.
%
Note that one of the \principia book describes a
C compiler that targets the ARM~\cite{arm-refman} architecture,
so in principle
our basis could be reduced to the ARM machine language, which
is fairly simple. 
\l conceptually you could run the compiler in your head, a known boot technique
\l because you are a human computer and C interpreter!
\l and so manually translate all the code, which reduced to ARM assembly
Another \principia book describes
an ARM emulator. But, both the C compiler and the ARM emulator
are written in C itself, which brings us back to C as our basis.
\l also ocaml is written in ocaml lex/yacc too ... so big basis
\l put here why chose ARM archi over x86?

A more elegant alternative, avoiding self-reference,
would be to start from a simple machine and
build a tower of increasingly powerful languages. Starting
from raw binary machine code, we could gradually build more 
sophisticated languages through a series of {\em bootstrapping} steps.
%
In fact, such a project partially
exists\furl{http://homepage.ntlworld.com/edmund.grimley-evans/bcompiler.html},
but with already six bootstrapping steps its author was still far
away from a language like C.
\n I was using "as expressive as C", but it is dangerous, Turing tarpit
Appendix~\ref{sec:bootstrapping} outlines a similar (long) process
to bootstrap \plan from scratch.
\l we dont because describing all the intermediates programs is long

Another alternative, chosen by Donald Knuth
for his encyclopedic books {\em The Art of Computer Programming}~\cite{taocp},
was to pick as a basis a very simple computer he invented
called MIX, and a very simple assembly language called MIXAL.
Using assembly is maybe OK for describing algorithms,
but I think it would not be productive for writing entire
programs; this would lead to very long \principia books.

I think that starting directly from the ARM machine, 
a real but fairly simple machine,
and the C language, a higher level language than assembly, 
is maybe less elegant but more practical for \principia.
\l more practical basis. because less intermediate books, see appendix list :)
\l Raspberry Pi target machine? Qemu?

\t C is great, and still great for many of the programs I present such as
\t  kernel. A bit old though and lots of advances in PL. For some programs
\t  dealing with languages, a bit old. So I will also use
\t  for a few books OCaml! with ocaml interpreter written in C so complete.

%% was in Machine.nw
%%We chose ARM over x86 because even if x86 is the processor in most
%%desktop machines today, ARM has a far simpler architecture.
%%\n actually there is no 8i :) It's not that bad according to 
%%\n torvalds: http://yarchive.net/comp/linux/x86.html but still.
%%Indeed, \co{RISC} (Reduced Instruction Set Computer) machines such as an ARM, as
%%their name suggest, have a smaller set of instructions
%%than \co{CISC} (Complex Instruction Set Computer) machines such as an x86,
%%and have also simpler instructions. The code
%%of an ARM emulator is thus smaller and simpler to describe, while
%%still conveying in our opinion the essence of all processors.
%%
%%We chose ARM over MIPS, because even if the MIPS is a RISC machine
%%probably simpler than the ARM, 
%%\l LOC vi=?
%%there is not much remaining MIPS
%%machines around. The ARM on the other hand is very much alive;
%%it is the most popular processor in phones today. It is also the
%%processor of the very cheap Raspberry Pi\furl{https://www.raspberrypi.org/} 
%%machine, a machine
%%used by many electronic hobbyist, which
%%makes it also a great candidate for our teaching purpose.

\section{Literate programming}
\n How

I want to show source code because it is the ultimate
explanation for what a program does.
%
However, I think
that showing pages and pages of listings in an appendix, 
as done for instance in the Minix book~\cite{minix},
\n also the case for oberon, xv6, unix v6
even when this appendix is preceded by documentation chapters, is not
the best way to explain code. I think the code and its documentation
should be mixed together, as done for instance in the Xinu book~\cite{xinu},
\n or even better lcc
so you do not have to switch back and forth between
an appendix and multiple chapters.

{\em Literate programming}~\cite{lp-book} is a technique invented by
Donald Knuth to make it easy to mix code and documentation
in a document in order to better develop and better explain programs. 
Such documents are called {\em literate programs}. All \principia
programs are literate programs.

Note that literate programming is different from using 
{API documentation generators} such as 
javadoc\furl{http://www.oracle.com/technetwork/articles/java/index-jsp-135444.html} or 
doxygen\furl{http://www.stack.nl/~dimitri/doxygen/}.
%
Noweb\furl{http://www.cs.tufts.edu/~nr/noweb/}, the literate
\l link to appendix on noweb tutorial? hello world of literate programming(alok)
programming tool I used, does not provide the same kind
of services. 
\l cf sexp_int.mli, too many functions, given equal importance. 
\l could reorder, but then that's the point, use LP. 
\l Moreover 2 views will always be better than just one view.
%
Indeed, literate programming allows programmers to explain their
code in the order they think the flow of their thoughts and their code would
be best understood, rather than the order imposed by the compiler.

Literate programming
allows, among other things, to explain the code piece by piece, with the
possibility to present a high-level view first of the code,
to switch between {top-down} and {bottom-up} explanations, and to
separate {concerns}.
\l AOP a bit
%
For instance, the [[Proc]] data structure --- which we will
see in the \book{Kernel} is a data structure that
represents some information about a process --- is a huge structure
with more than 90 fields. Many of those fields are used only
for advanced features of the kernel. The C compiler imposes
to define this structure in one place.
\l but with cpp, which is also a macro-processing language, I can cheat
Noweb
% which can be seen as a macro-processing language, 
allows to present this structure piece by piece, gradually, in different
chapters. I can show first the code of the structure with 
the most important fields,
and delay the exposition of other fields to advanced topics chapters.
This greatly facilitates the understanding of the code, by not
submerging you with too much details first.
\t see appendix example

In the same way, the [[main()]] function in most
programs is rather large and mixes together many concerns:
command line processing, error management, debugging output,
optimizations, and usually a call to the main algorithm.
Showing in one listing the whole function would hide behind
noise this call to the main algorithm. The main flow
of the program though is arguably the most important thing to understand
first. Using literate programming,
I can show code where the most important parts are highlighted, and where
other concerns are hidden and presented later. 

In fact, I spent lots of time during the writing the \principia books
in transforming the \plan programs in literate
programs, and in reorganizing again and again the \plan code to find
the best way, the best order, the best separation of concerns
in which I think you would more easily understand the code.


\section{Getting started}
\label{sec:getting-started}

To play with the different programs described in the \principia books,
I recommend to use my fork of \plan. See \urlinstall.
\l what is special about this fork? less arch (but still 2), better orga, etc
\l did modifs, renamed, mv around, add types (bool, kern_addr, error, etc)
\l mostly to ease the presentation (and have ocaml spirit!)
\l all the // in the code are mine.
\l it's kinda of the opposite of 9atom! support less features :)
Section~\ref{sec:code-orga} will explain later the directory structure
of this \plan repository.


To compile and install my fork of \plan, you will need a machine with
a C compiler, 
a \unix-like operating system, and 
a kernel that can write on a VFAT filesystem. 
So, GNU/Linux and MacOS are possible {host} operating systems.
\plan is also a valid host as you can build \plan under \plan.
\l also could provide a virtual box (alok)
%For the curious reader, Appendix~\ref{sec:bootstrapping} describes
%how to bootstrap and install \plan from scratch.
%not really in the end

The installation consists first in downloading my fork of \plan.
Because the source code of \plan uses a special dialect of C
and some non standard assembly, you can not use directly popular
C compilers and assemblers such as [[gcc]] and [[gas]].
To compile \plan, you will also need to download
my fork of Ken Thompson's C (cross) compilers called [[kencc]].
\l =~ one included, but support unix/macos/plan9, but noise for principia
You can compile [[kencc]] using a regular C compiler
(e.g., [[gcc]]) from your host operating system (e.g., from MacOS or Linux).
Then you can compile 
\l does compiling for same arch but other OS considered cross compiling?
the \plan kernel,
the \plan C standard library, and then the whole \plan operating
system with all its libraries and programs,
by using the [[kencc]] compiler installed in the previous step.
This will build from scratch a \plan{} {distribution}.
\l install bin and lib and few other files on disk or virtual disk
\l also need plan9port
\n nice to see that it's actually not that complicated to build distrib!

Finally, you can run this \plan distribution either under 
Qemu\furl{http://www.qemu.org}, which makes it
easy to experiment, or by installing the distribution on a
physical machine such as a Raspberry Pi.


\section{Requirements}

The \principia books are not introductions to 
programming,
\n read htdp/2e for that
computer science, 
\n read brookshear
or to any of its subfields.
%
For instance, the book on the kernel is not an introduction
to operating systems. Indeed, I will assume you
have already a rough idea of how a kernel
works and so that you are already familiar 
with concepts such as 
virtual memory, 
critical regions, 
interrupts, 
or system calls.
%
I will present with full details the source
code of different programs, but I assume you already know
most of the {concepts}, {theories}, and {algorithms} behind those programs.
The \principia books are there to cover the {practice}.
\l actually except for kernel and compiler, I may cover concepts too
\n Software Practice and Experience style!
%
I assume the main readers of \principia are students in computer science, with
a bachelor or PhD, who desire to consolidate their knowledge
by reading the ultimate computer science explanations: source code.

As said earlier, because most of the books are made of C source code,
you will need to have a good knowledge of 
the C programming language~\cite{k-r}.
\l also ocaml for a few books, and lex and yacc
%
Because most of the programs I describe are \plan programs, 
and because \plan has a lot in common with its ancestor \unix,
you will also need to be familiar with those systems.
I recommend to read~\cite{unix-pike} for \unix,
and to read the \plan tutorials [[docs/articles/9.ps]] and
[[docs/articles/names.ps]] available in my \plan repository.

\section{Copyright}

Most of the programs in \principia are from \plan with copyrights
from Lucent Technologies Inc. 
However, they are open source; permission
is granted to copy, distribute and/or modify the source code.
%
The remaining programs have copyrights from INRIA. They are also open source.
\l mention ocaml?
\l Q license though ... still actually?

\section{Acknowledgments}

I would like to acknowledge of course \plan's authors who wrote
in some sense most of the content of the \principia books: 
Ken Thompson, Rob Pike, Dave Presotto, Phil Winterbottom, Russ Cox, 
and many other people from Bell Labs.
%
I would like also to thank Xavier Leroy, Fabrice Le Fessant, 
Francoix Rouaix, and many other people from 
INRIA Rocquencourt who wrote the remaining programs
I describe in \principia.
\l mention ocaml? but then need to speak about ocaml before

%Thanks to Knuth for TeX, LP, and for encouragments in this project.

%reviewers?:
% - Pascal Garcia
% - Julien Verlaguet
% - Alok Menghrani
% - Tao Stein?
% - Ding Yuan?
% - ??? Pichardie? Besson? Ridoux?
% - ??? Julia?
% - Keith Adams? Jason Evans?




\chapter{Overview}

%trans:
Before delving in the description of the different \principia
programs in the next chapter,
%toc:
I first give here an overview of how the code is organized,
and how the different programs depend on each other.

\section{Code organization}
\label{sec:code-orga}

Table~\ref{tab:code-orga} presents short descriptions
of the main directories in my fork of \plan,
as well as the corresponding sections in this document
in which the program associated with the directory is discussed.
%
%%Table~\ref{tab:other-dirs} presents the remaining (less important)
%%directories.

A few \plan programs have some architecture specific parts,
with support for x86 and ARM in my fork of \plan.
The LOC column in Table~\ref{tab:code-orga} accounts only for 
the code to support one of the architecture: the ARM.
\t use instead a star next to some LOC and remove the ``ARM and x86'' in cells
\t  below
This is the only architecture-specific code I will show in 
the \principia books.
\t except for the kernel for now
In the same way, even if some directories have a plural form, e.g., [[editors/]],
the LOC column accounts only for one variant of
this program category, e.g., one of the editor, the one I chose
to present in \principia.

\begin{table*}[tbh!]
\begin{tabular}{lllr}
\toprule
{\bf Directory} & {\bf Description} & {\bf Section} & {\bf LOC} \\
\otoprule
[[kernel/]]         & The \plan basic kernel (for ARM and x86)        & \ref{sec:kernel}     & 60 000 \\
% no ARM :( just x86 for now

                    & $+$ graphics stack (kernel code)                & \ref{sec:graphics}   & 10 000 \\
                    & $+$ network stack (kernel code)                 & \ref{sec:network}    & 23 000 \\
[[include/]]        & The header files (e.g., [[libc.h]])             & \ref{sec:libc}       & 5 500 \\
% no x86, just ARM
[[lib_core/]]       & The core C library (for ARM and x86)            & \ref{sec:libc}       & 21 500\\
% no x86, just ARM
[[shells/]]         & The shell                                       & \ref{sec:shell}      & 7 500 \\

\midrule
[[compilers/]]      & The C compiler (for ARM and x86)                & \ref{sec:compiler}   &25 000\\
% no x86, just ARM
[[assemblers/]]     & The ARM and x86 assemblers                      & \ref{sec:assembler}  & 4 100 \\
% includes also include/obj/5.out.h, no x86 just ARM
[[linkers/]]        & The ARM and x86 linkers                         & \ref{sec:linker}     & 7 900 \\
% includes also include/debug/a.out.h and a few other header files, no x86
[[machine/]]        & The ARM emulator                                & \ref{sec:emulator}   & 4 400\\
[[languages/]]      & The OCaml bytecode compiler/interpreter         & \ref{sec:ocaml}      & 30 000\\
% in ocaml, but not the native code compiler, nor external/libs nor debugger
% but right now OCaml.nw is actually 15000 LOC so why 30 000?????

[[generators/]]     & The code generators Lex and Yacc                & \ref{sec:generators} & 3 500\\
% in ocaml

\midrule
[[editors/]]        & The editor                                      & \ref{sec:editor}     & 10 000\\
% in ocaml, but not the pfff major modes
[[builders/]]       & The build system                                & \ref{sec:builder}    & 4 800 \\
[[debuggers/]]      & The debuggers and tracers                       & \ref{sec:debugger}   & 17 000\\
% no x86 just ARM
[[profilers/]]      & The profilers                                   & \ref{sec:profiler}   & 4 900 \\

\midrule
[[lib_graphics/]]   & A large part of the graphics stack              & \ref{sec:graphics}   & 16 000 \\
% does not include kernel/, but includes include/graphics/
[[windows/]]        & The windowing system                            & \ref{sec:windowing}  & 10 500\\

\midrule
[[lib_networking/]] & A small part of the network stack               & \ref{sec:network}    & 1 000\\
[[networking/]]     & Networking applications (clients and servers)   & \ref{sec:network}    & 51 000\\
[[browsers/]]       & The web browser                                 & \ref{sec:browser}    & 24 000\\
%ocaml

%\midrule
%[[interpreters/]]  & The Scheme interpreter                          & \ref{sec:scheme}     & 4 600\\

\midrule
[[utilities/]]      & Utilities such as [[ls]], [[cp]], [[mv]]        & \ref{sec:utilities}  & 16 000\\
                    & [[grep]], [[gzip]], [[tar]], [[sed]], [[diff]], & \\
                    & [[xargs]], [[ps]],  etc                         & \\
\otoprule
Total               &                                                 &                      & 333 600 \\
\bottomrule
\end{tabular}
\caption{Main source code directories of my \plan repository.}
\label{tab:code-orga}
\end{table*}
\n for the LOC it's a mix of cm -test_loc CROSS xxx/, or make loc in the dir
\l could have a column with language? C/ASM, C, OCaml/C




%%\begin{table*}[tbh!]
%%\begin{tabular}{llll}
%%\toprule
%%{\bf Directory} & {\bf Description} & \\
%%\otoprule
%%[[docs/]] & Articles and man pages  \\
%%[[typesetting/]] & Tools to generate documents [[troff]], [[tbl]], [[pic]], [[grap]], [[eqn]] and [[man]] \\
%%
%%[[ROOT/]] & The target directory where compiled binaries and libraries will \\
%%          & be installed which will form the \plan{} {distribution} \\
%%[[sys/]] & Backward compatible directory containing \\
%%         & mostly symbolic links and build scripts  \\
%%
%%[[ape/]] & ANSI and POSIX environment, to ease \\
%%         & the compilation of legacy \unix applications & \\
%%[[CROSS/]] & Scripts to support cross compilation  \\
%%
%%%\midrule
%%[[security/]] & Security, authentification, cryptography \\
%%[[lib_math/]] & Arbitrary precision mathematics library \\
%%[[lib_misc/]] & String, regexps, and compression libraries  \\
%%[[database/]] & Simple key-value database, mostly used for storing network information \\
%%
%%[[games/]] & Video games \\
%%[[applications/]] & Small applications \\
%%[[BIG/]] & software of interest not yet included \\
%%[[version_control/]] &  \\
%%[[lib_audio/]] & \\
%%\bottomrule
%%\end{tabular}
%%\caption{Other directories of our \plan repository.}
%%\label{tab:other-dirs}
%%\end{table*}


\section{Software architecture}
%old: was Architecture overview but oxymoron, archi is already an overview

Many of the \principia programs are mutually dependent
on each other. Indeed, to {run} a compiler or an editor you need
a kernel (and a shell), but to {create} this kernel in the first
place you need an editor and a compiler.
In a similar way the C compiler uses code from the 
core C library, but to create this library you need a C compiler.
In fact the C compiler is written in C itself, so there are even
self-dependencies\footnote{
Appendix~\ref{sec:bootstrapping} describes
how to solve those mutual and self-dependencies issues.
}
\l Figure with all deps, split binary/source so partially solve mutual deps

It is possible though to {layer} things by looking
at how things are organized in memory.
\l rather than what depends on what codewise or runtimewise
A first separation to make is between
code running in {\em kernel space} and 
code running in {\em user space}, as shown in the left of
Figure~\ref{fig:soft-archi}.
%
Most of the code running
in kernel space is in [[kernel/]], as well as some code
in [[lib_graphics/]] and [[lib_networking/]]. Some of the
code in the C library (the memory pool library
\footnote{[[lib_core/libc/port/pool.c]]}
and a few utility functions and globals) are used both in the kernel 
and in user programs, but they are the exceptions.
The rest of the codebase runs in user space.

\begin{figure}[!]\centering
\begin{verbatim}
          +---+    +---+    +---+
          | a |    | l |    | c |                 applications/-+
          | s |    | i |    | o |                 | +---------+ |
          | s |    | n |    | m |                 | | clock.c | |
          | e |    | k |    | p |                 | +---------+ |
          | m |    | e |    | i |                 | +---------+ |
          | b |    | r |    | l |                 | |colors.c | | windows/rio+
          | l |    | s |    | e |                 | +---------+ | |          |
          | e |    | / |    | r |                 |             | |          |
 U        | r |    |   |    | s |                 |             | |          |
 S        | s |    |   |    | / |                 |             | |          |
 E        | / |    |   |    |   |    shells/-+    +-------------+ +----------+
 R        +---+    +---+    +---+    |       |   lib_graphics/libdraw/--------+
        +-------------------------+  |       |   |  include/graphics/draw.h   |
 S      |include/obj/5.out.h      |  |       |   |                            |
 P      +-------------------------+  +-------+   +----------------------------+
 A    lib_core/libc/-----------------------------------------------------------+
 C    |                        include/core/libc.h                             |
 E    |  +-------+ +-------+ +-----+  +-------+ +-------+ +-----+ +-------+    |
      |  | Math  | |Unicode| | Fmt |  |Memory | |Strings| | IO  | |System |    |
      |  +-------+ +-------+ +-----+  +-------+ +-------+ +-----+ +-------+    |
      |9syscall/--------------------------------------------------------------+|
      ||          rfork() | brk() | open() | read() | write() | ...           ||
      ++----------------------------------------------------------------------++
 -----------------------------------------------------------------------------------
      kernel/--------------------------------------------------+  +-----+  +-----+
      |  +---+    processes/---------+   devices/-----------+  |  |  l  |  |  l  |
 K    |  |   |    |   sysrfork()     |   |  screen/------+  |  |  |  i  |  |  i  |
 E    |  |   |    |   sysexec()      |   |  |            |  |  |  |  b  |  |  b  |
 R    |  | s |    +------------------+   |  +------------+  |  |  |  _  |  |  _  |
 N    |  | y |    memory/------------+   |  keyboard/----+  |  |  |  g  |  |  n  |
 E    |  | s |    |  sysbrk()        |   |  |            |  |  |  |  r  |  |  e  |
 L    |  | c |    +------------------+   |  +------------+  |  |  |  a  |  |  t  |
      |  | a |    files/-------------+   +------------------+  |  |  p  |  |  w  |
 S    |  | l |    |  sysopen()       |   console/-----------+  |  |  h  |  |  o  |
 P    |  | l |    |  sysread()       |   |                  |  |  |  i  |  |  r  |
 A    |  | s |    |  syswrite()      |   +------------------+  |  |  c  |  |  k  |
 C    |  | / |    +------------------+                         |  |  s  |  |  i  |
 E    |  |   |    network/-----------+                         |  |  /  |  |  n  |
      |  |   |    |                  |                         |  |     |  |  g  |
      |  |   |    +------------------+                         |  |     |  |  /  |
      |  +---+                                                 |  |     |  |     |
      +--------------------------------------------------------+  +-----+  +-----+

\end{verbatim}
\caption{Software architecture of \plan{}.}\label{fig:soft-archi}
\end{figure}


The boundary between user programs and the kernel is provided
by the {\em system calls} application programming interface (API).
The functions of this API are declared 
in [[include/core/libc.h]] (as well as many utility functions).
The user-space part of the system calls are implemented in 
[[lib_core/libc/9syscall/]]. This directory
contains one assembly file per system call and each of those files contains
mostly the software interrupt instruction with a special value
in one of the argument in order to
be dispatched to the appropriate code in the kernel.
The dispatcher kernel code is in [[kernel/syscalls/]].
\l figure with content of one such file for ARM? 
\l figure where see dispatching, many arrows in to central dispatcher, and out
Some of those system calls are process related
(e.g., [[rfork()]], [[exec()]], [[exit()]]
\footnote{with implementation in [[kernel/processes/]]}),
some are memory related
(e.g., [[brk()]]
\footnote{with implementation in [[kernel/memory]]}),
and other are used for file input and output (IO)
(e.g., [[open()]], [[close()]], [[read()]], [[write()]]
\footnote{with implementation in [[kernel/files/]]}).
\l table with all syscalls and a description? 

In fact, those IO system calls provide an extended way
for programs to request services from the kernel
by using the filesystem hierarchy. Indeed, one of the
philosophy of \unix{} is that {\em everything is a file}, including 
the devices. 
A process using the [[/dev/cons]] {\em device file} will trigger code 
in the kernel
in [[kernel/console/devcons.c]]; this code will then trigger
code that handles the keyboard device in [[kernel/devices/keyboard/]]
(when reading from [[/dev/cons]]),
or the screen device in [[kernel/devices/screen/]] 
(when writing to [[/dev/cons]]).

This philosophy was pushed even further under \plan where 
{\em everything is a {filesystem}}.
\n Pike says file server, but hard to understand I think
The graphics API for instance is accessible though many 
files under [[/dev/draw/]],
and triggers code in [[kernel/devices/screen/devdraw.c]] and
[[lib_graphics/]].
In a similar way, the networking API is accessible through files
under [[/net/]] and triggers code in [[kernel/network/]].

A second separation to be made is between
{\em library code} and
{\em application code}.
All the user programs in \plan rely first on the core C library
in [[lib_core/libc/]], exposed via the [[include/core/libc.h]] header
file; all programs are linked with [[libc.a]]. 
The other [[lib_xxx/]] directories contain other general-purpose functions
and data structures, which are used by different programs. By
using libraries, source code can be reused more easily.

The shell is a regular program using also the C library.
The assembler, linker, and compiler rely on the C library
as well as other header files, for instance, [[include/obj/5.out.h]],
which declares the set of ARM opcodes.

All the graphical applications, for instance, the clock, but also the windowing
system, rely also on the [[lib_graphics/lib_draw/]] library, which
is exposed in the [[include/graphics/draw.h]] header file.
The graphics library is essentially a thin wrapper over the protocol
used by the [[/dev/draw/]] device files.
%
In the same way, the [[lib_networking/libip/]] library,
exposed in the [[include/net/ip.h]] header file, is a thin
wrapper over the protocol used by the [[/net/]] device files.
Note that the core C library contains also network related
functions such as [[dial()]].

\l like in cs illuminated, show the different layers. information/.../...

% \section{Books structures? Serie structure?}


\chapter{The Literate Programs}
\label{chap:lp}

%toc:
I now switch to quick descriptions of the \nbbooks programs, and
so \nbbooks books, composing the \principia series.
I organized those programs in six different groups.

\section{The Core system}

The first group, which I call the {\em core system}, is made of the minimal
set of programs that are needed to reach the point where the programmer can
interactively launch other programs.
%toc:
In the case of \plan, this minimal set is made of 
the kernel, 
the shell, and the
standard C library.

The C library is necessary because it is used by the shell.
Indeed, the C library provides the necessary {bridge} to call the kernel 
from user-space programs.
The library provides also memory allocation routines and a few
general utility functions. 
In fact, a small part of the C library is also used internally by the kernel.

\l figure? showing relation between kernel/shell/libc?

\subsection{The kernel, [[9]]}
\label{sec:kernel}

A {\em kernel} is a program that {manages} all the other programs.
%
It is arguably the most important program; 
\l with the compiler
without the kernel no other program can run. 
%
The kernel provides
the main abstractions of the computer and
{multiplex} its resources 
(e.g., the processor, the memory, the input and output devices)
to multiple programs at the same time.

The kernel is the biggest program and so the biggest book in the series.
The \plan kernel is called simply [[9]].

\l real world: far less devices in 9, less opti (e.g., no RCU), few arch though

% virtual cpu, memory, file

\subsection{The core library, [[libc]]}
\label{sec:libc}

The C library defines data structures and functions
that are {used} by all the other programs (including the kernel).
%
In the case of \plan, this library contains
the memory allocation routines ([[malloc()]], [[free()]]),
the bridge to call the kernel (the system calls),
the Unicode functions, and many other
utility functions.
\l also describe thread library
Note that some of the code in the C library is written in assembly.
\l to optimize, but also because required because kernel interaction

\subsection{The shell, [[rc]]}
\label{sec:shell}

A {\em shell} is a program that allows a user to {launch} other programs.
It is the primary user interface of \plan,
the so called {\em command-line interface} 
(we will see later another interface).
\n a graphical user interface

In \plan, and also in its ancestor \unix, the shell is a regular
user-space program; it is not part of the kernel.
The user can actually change shell without changing the kernel.
\n shell is kinda bootstrapped, from the first user process and from init

The shell will be the first user program I will describe in the
\principia series. It illustrates many features
provided by the kernel, because the shell is using internally many
system calls (e.g., [[rfork()]], [[exec()]], [[chdir()]], [[pipe()]]).

The \plan shell is called [[rc]] (for [[r]]un [[c]]ommand).

\l requiring compiler/assembler/linker, so transition?
\l actually relying also on lex/yacc

\section{The development toolchain}

%trans:
Once you have a terminal where you can launch programs,
you will need tools to produce those
{binary} programs from 
{source} code. 
%
The {\em development toolchain}
\l software development toolchain?
is a set of tools working together to produce
{executables}.
%toc:
This toolchain is made essentially of
a compiler,
an assembler,
and a linker.

\n could be part of core system because need compiler to compile the kernel

\l figure? showing the compilation pipeline? c -> asm -> obj -> binary, and link

\subsection{The C compiler, [[5c]]}
\label{sec:compiler}

A {\em compiler} is a program that transforms source code
written in one high-level language (e.g., C), into another 
lower-level language (e.g., assembly).
%
The C compiler is probably the most important program 
in \plan after the kernel. Indeed, the \plan kernel itself is written
in C and so needs a C compiler to become executable by a machine.
In fact, almost all programs in \principia are written in C, including
the compiler itself\footnote{
Appendix~\ref{sec:bootstrapping} describes
how to solve this self-reference issue.
},
so they all need the C compiler to become executables.

I will describe [[5c]], a C compiler targeting
ARM assembly. The [[5]] in [[5c]]
comes from the \plan convention to name architectures
with a number or single letter (0 is MIPS, 5 is ARM, 8 is x86, etc).
This is why the ARM assembler and linker I will
describe later are called respectively [[5a]] and [[5l]].
%
I chose the ARM architecture because it is one of the
simplest architecture while being also one of the most
used architecture in the world. Indeed, almost every phone
contains an ARM processor.
%
It is also the processor of the extremely cheap Raspberry 
Pi\furl{https://www.raspberrypi.org/}, a machine
used by many electronic hobbyists. Thus,
ARM is a great candidate for my teaching purpose.

\l real world: far less opti?

%\section{The macro processor, [[cpp]]}

\subsection{DONE The assembler, [[5a]]}
\label{sec:assembler}

An {\em assembler} is a kind of compiler;
it translates source code written in an {assembly} programming language
into {machine code}, or into an {object code} close to machine code.
%
Some of the kernel code, as well as code in the C library, are written
in assembly.

To be consistent with the compiler, I will 
describe [[5a]], the \plan ARM assembler.
\l 5a is actually written in C :) and also use yacc. more high level :)

\subsection{DONE The linker, [[5l]]}
\label{sec:linker}

A {\em linker} is a program taking as input multiple files
called the {\em object files}, which contain object code
generated either by the assembler or compiler, and creates
the final executable.
%
In theory, compiling, assembling, and linking could be done by
a single program;
\l in fact could remove linking, just use cat
\l also at the beginning you had full program in one file
the C compiler could take as input multiple C source
files and produce directly an executable. 
\n ocamlc kinda does that. in fact gcc does that too (but shell out gas/ld)
But, from a software engineering point of view,
\l and also for separate compilation? well could be smart and see cache.
it is better to separate concerns and have three separate
tools for those three separate tasks.
\n and indeed ocamlc has separate code for the compiler, interpreter, linker

I will describe [[5l]], the \plan ARM linker.
\n also bootstrapping issues if 5l coded using separate files, but easy to fix

\subsection{The processor emulator, [[5i]]}
\label{sec:emulator}

An {\em emulator} is a program that 
acts like a computer and can {interpret} another (binary) program.
%
An emulator is not really a part of the development toolchain. But,
because the assembler, linker, and compiler
target an architecture, it is useful to describe
the {instruction set} of an architecture (ISA) through
its emulator.
%
Moreover, an emulator can be extremely useful when transforming
a compiler in a {cross compiler}, to test quickly the generated
machine code.

I will describe [[5i]], an ARM emulator, which we can
view as a kind of interpreter for a low-level language, hence the [[i]].
\l I'm not even sure actually
In fact, [[5i]] can also act as a (slow) debugger and profiler.


\subsection{The OCaml compiler, [[ocamlc]] and [[ocamlrun]]}
\label{sec:ocaml}
\l bytecode compiler/interpreter
\n why ocaml? could use sam/mothra/lex/yacc. but it's 2015! cannot just use C.

C is a great programming language, and a real improvement over assembly.
It is arguably the best language to implement system programs
such as kernels, virtual machines, just-in-time compilers, etc.
\n maybe Rust is better now, or modula-3, but not C plus plus for sure
C can also be used as a portable assembler; this makes C
a great language to implement efficiently core libraries,
which can even be used from programs written in different programming languages.
%
For many applications though, especially applications
without strong constraints on memory, speed, or latency, it can
be far more productive for the programmer to use higher-level
languages. Programming in C is indeed error-prone, with
recurring bugs such as buffer overflows, segmentation faults,
or security holes.

This is why a few programs in the \principia series 
\n lex/yacc,  efuns, mmm (and ocamlc itself)
are written in the OCaml programming language
\furl{http://ocaml.org/}.
\l cite? a good book on ocaml? the real world one?
OCaml is a statically-typed functional language, more expressive
and less error-prone than C. 
\l ultimate lang IMHO, even in 2015! actually early ML were in Bell Labs too
It is almost impossible to
have many of the bugs mentioned above while programming
in OCaml.
%
Just like I prefer, when possible, to describe programs written in C
rather than assembly, because the resulting code is smaller
and easier to understand, I also prefer, when possible, to describe programs
written in OCaml rather than C, because the resulting code is also smaller
and easier to understand.
\l some stuff has to be in assembly, some has to be in C, for the rest: ocaml!
\l also turns out were lucky and I liked efuns and mmm (too bad Camp not in ML)
\l great for compiler stuff actually :) in fact lex and yacc easier in ocaml

For completeness, I need then to present also the code of an OCaml compiler
or interpreter. Interestingly, OCaml uses a mixed approach. Indeed,
the OCaml system
includes first a compiler, [[ocamlc]],
written in OCaml itself,
\n hmm bootstrapping again, bigger basis now
generating {\em bytecode} for a {\em virtual machine},
instead of object code for a real machine. Then, a bytecode
interpreter, [[ocamlrun]], written in C, implements this
virtual machine and so can interpret the bytecode generated by the compiler.
Any machine with a C compiler can then by transitivity execute
OCaml (byte)code.
\l making OCaml as portable as C.

\subsection{The code generators, [[ocamllex]] and [[ocamlyacc]]}
\label{sec:generators}

{\em Code generators} are programs that {generate} automatically
source code.
Compilers are a kind of code generators as they generate assembly code.
%
I will describe two more generators in \principia: Lex and Yacc.
They provide both a {\em domain specific language} (DSL) to
help respectively scan and parse languages.
%
They are used by a few other programs in \principia:
the assembler, the C compiler, the shell, and the build system.
This is why I consider Lex and Yacc essential programs.
\n actually lex is not used that much
The full understanding of the C compiler, for instance, would
be incomplete if you could not understand the code generated
from the C grammar specification by Yacc.

There are many clones of Lex and Yacc written in many
different languages. The [[lex]] and [[yacc]] programs
included in \plan are actually not really \plan programs;
they are the original [[lex]] and [[yacc]], written in C,
copied without much modification directly from \unix.
Because they were written a long time ago, their C source code
is not as clear and modern as the code of the other \plan programs.
%
This is why I decided to present instead [[ocamllex]]
and [[ocamlyacc]], some Lex and Yacc clones written in OCaml.
\l and part of ocaml distrib for lex, and written by me for yacc
\n also self-reference, bootstrapping too, lex and yacc use lex and yacc :)


\section{The developer tools}

%trans:
The {\em developer tools} are a set of tools that are 
not strictly necessary to produce programs, like the software
development toolchain I have described in the previous section, but 
which are really useful in the software development process.
%toc:
Those tools are
the text editor,
the build system,
the debugger,
and the profiler.

\l figure? showing the source code at the center and the tools around?

\subsection{The text editor, [[efuns]]}
\label{sec:editor}
\n should be called screen text editor, because ed is also a text editor but ..

A {\em text editor} is a program to help {read} and {write} programs.
\l modify, ... = edit
%
It is perhaps the most important tool for a programmer.
Indeed, even if the kernel and compiler are essentials
to run and produce software, their source code was first
entered in a computer by a programmer using a text editor. In fact, 
text editors are used to enter the code of the text editors themselves, 
leading to self-refence issues just like for a compiler\footnote{
See Appendix~\ref{sec:bootstrapping} for 
discussions on how to bootstrap an editor.
}.

The text editor is certainly also the program to which programmers are the
most emotionally attached to\furl{https://en.wikipedia.org/wiki/Editor_war}.
In fact, this is the main reason I chose
%%%, for the first time in the \principia series,
to not present a \plan program,
even though \plan has a few text editors ([[ed]], [[sam]], and [[acme]]).
Instead I will present an Emacs clone.
%
Another deviation is that this program is not written in C but in OCaml.
This clone is called [[efuns]]\furl{https://github.com/aryx/fork-efuns}.
%%%a language arguably more expressive and convenient than C to code
%%%such programs. 
%%%Note that one of the previous \principia book describes an OCaml compiler. 

The text editor will be the first graphical program in the \principia series.
Indeed, until now all the programs I described were command-line programs.
The graphics stack used by [[efuns]] will be described later.


\subsection{The build system, [[mk]]}
\label{sec:builder}

A {\em build system} is a program to help automate the {compilation} 
of programs
by calling appropriately the tools from the development toolchain.
%
The whole \plan operating system can be built from scratch
with one simple command, [[mk all]], thanks to the build system
called [[mk]] and a few configuration files (the [[mkfile]]s).
\n also bootsrapping issue, but easy, a simple inefficient compile-all.sh

\subsection{The debugger, [[db]]}
\label{sec:debugger}

A {\em debugger} is a program that {commands} and {inspects} another program.
%
Programming is so difficult that invariably we make mistakes
when writing code. Having tools to help find those mistakes is essential,
and the debugger is the most important of those tools.
%
Even if many programmers, including great programmers\cite{coders-at-work},
use just [[printf()]] tracing commands to debug their programs,
I think a debugger can greatly accelerate the time it takes to find bugs. 
%
Debuggers are also great tools to help understand programs, especially programs
written by other people.
\l also required to get stack trace when program dies

The \plan debugger is called simply [[db]].
\l also acid, and strace.

\subsection{The profiler, [[prof]]}
\label{sec:profiler}

A {\em profiler} is a program that {generates statistics} about another
program.
%
It is mainly used to optimize code by finding where the program
spends most of its time. It can be useful also to find bugs
because unexpected statistics can sometimes be good hints to
fix code where the programmer was expecting different results.
\l cite jon bentley

\t will see prof? tprof?  actually a few profiling programs, different
\t statistics

%section{The version control system, [[gitocaml]]}
%pijul?

%section{The documentation system, [[TeX]]}
% or troff? TeX is actually a literate program documented in 'Tex the Program'
% or my own?

\section{Graphics}

%trans:
Up until now, we have mostly seen {command-line} tools
interacting with the programmer through simple text-based terminals.
%
However, one of the most important invention in computer science is the 
{\em graphical user interface} (GUI),
introduced in the 1970's with the Xerox Alto~\cite{alto}.
%
Even regular command-line programs benefit from a graphical user interface
as you can run multiple programs in different {windows} at the same time.
\l so multiple activities
\n alternative is sh-job-control/screen/tmux/virtual-terminals-under-linux/...

\l figure? showing screenshot and what the graphics stack does and what rio do?

\subsection{SEMI The graphics stack, [[draw]]}
\label{sec:graphics}

A {\em graphics stack} provides the basis on top of which
graphical applications can be built.
{GUI elements} such as 
menus,
windows, 
cursors, or 
texts are ultimately rendered
on the screen using simple graphic operations provided by the graphics stack:
lines,
circles, 
rectangles,
arcs, 
etc.
\l also event loop style, very different from batch programs

Under \plan, the graphics services are accessible through
the [[/dev/draw/]] device directory, which is connected
to the screen device driver in the kernel. 
\l a bit ugly design to have lots of graphics related code in the kernel 
\l (actually not that much LOC)
\t called [[draw]]


\subsection{SEMI The windowing system, [[rio]]}
\label{sec:windowing}

One of the most important graphical application is
the {\em windowing system}. In some sense, a windowing system
is an extension of the kernel and the shell; it is a
program that also {manages and launches} other programs,
which are represented visually by separate {windows}.

The \plan windowing system is called [[rio]]. It is essentially
a multiplexer of the [[/dev/cons]], [[/dev/mouse]], 
and [[/dev/draw]] device files.
\n actually rio does not demultiplex /dev/draw, instead shared /dev/draw
[[rio]] is a {file server} that internally uses 
the keyboard, mouse, and screen {physical} devices,
and provides a {virtual} keyboard, virtual mouse,
and virtual screen to its windows using
{views} of those same device files
(thanks to the per-process namespace feature in the kernel).
In fact, an unusual feature of [[rio]] is that it can be run inside itself.
\l as powerful as X11, works with networking too (see below)

% virtual screen

\section{Networking}

%trans:
Up until now, I have described programs that can run on 
isolated machines. I will now switch to programs
that can communicate with each other on different machines.
%
Just like the graphical user interface, {\em networking}
\n The Alto introduced both! and also the mouse! (well NLS before too) and OO
(and internetworking, also known as the Internet)
has been one of the most important invention in computer science.
It has led ultimately to the creation of the {Web} where networking
programs (e.g., web browsers and servers) made possible great things,
for instance, the worldwide collaborative project Wikipedia.

\l figure? ???

\subsection{The network stack, [[net]]}
\label{sec:network}

A {\em network stack} is a part of the kernel, usually fairly
large, that provides the necessary abstractions
for programs to communicate with each other on
different machines, by using different {protocols}.

Under \plan, the network services are accessible through
the [[/net/]] device directory.

\t in fact RPC, 9P, so rio is a graphic program and networking program!
\t small NFS, beautiful
\t can do X! when rio put on top of NFS!

\l real world: similar to the stack in Linux? (alok)

% virtual directory

\subsection{The web browser, [[mmm]]}
\label{sec:browser}

There are too many networking protocols 
and too many networking programs (clients and servers)
to present.
\n indeed 75 000 LOC in tab:code-orga
UUCP, Email, News, FTP, Telnet, IRC, or Gopher 
are all useful networking programs. However, I decided to focus
on what I think is the most important one: 
the {\em Web browser}. 
You can actually use Email, News, FTP, and
many other things by using just a Web browser. It has become
so versatile that it can be considered almost an operating
system on its own.
\l in fact OS papers about web browsers now, and can run linux in it

One of the reason \plan did not get a wider adoption was that
it did not include a complete Web browser. A few prototypes
were written, but they were limited. Moreover, because \plan
is not fully compatible with \unix, it was difficult to port
Web browsers such as Firefox to \plan.

In \principia, I will describe [[mmm]]
\furl{http://pauillac.inria.fr/mmm/}, 
a Web browser written in OCaml.
It is also more limited than popular Web browsers such
as Firefox or Chrome, but it can handle popular websites such as Wikipedia.
\l facebook, not so sure :)
Moreover, its codebase is far easier to understand and present
than the codebases of popular browsers.


\l what about sound engine? book about MP3 mentioned by Knuth?
\l  explain sampling, frequency, etc?

%%%\chapter{Advanced Languages}
%%%
%%%We will present below two languages more expressive
%%%and less error-prone than C. It is almost impossible to
%%%have the bugs mentioned above while programming
%%%in those higher-level languages.
%%%
%%%\section{The toy frontend compiler, [[tigerc]]}
%%%\section{The backend compiler, [[c--]]}
%%%\section{The Scheme interpreter, [[s9]]}
%%%\label{sec:scheme}
%%%
%%%Scheme, a dialect of Lisp,
%%%is a minimalist but very powerful dynamic functional language. It has
%%%a very small set of orthogonal features which can be combined in many
%%%different  ways. In fact, its 
%%%full specificiation~\cite{r4rs} has 55 pages.
%%%This fits well the minimalistic approach of \principia.
%%%
%%%We will describe [[s9]]
%%%\furl{http://www.t3x.org/s9book/}
%%%a scheme interpreter written in C which was written for education purpose.
%%%In fact, a literate programming book~\cite{s9book} has
%%%already been written about it.

%%%\section{The OCaml compiler}

%%%\chapter{Video games}

%%%\section{[[soldat]]}
%%%\section{[[quake]]}



\section{Utilities}
\label{sec:utilities}

%trans:
In addition to all the programs mentioned above,
%
a programmer very often uses small utilities to perform 
or automate certain tasks. 
The code of those utilities is usually pretty small because those utilities
have often a single and simple function.
One of the \principia books will
be dedicated to describe the code of a few of those utilities,
the most important ones for the programmer.

\subsection*{File and directory utilities}
\l if not in TOC, then no need in title , [[ls]], [[cp]], etc

%trans:
The shell has very few {builtins};
%
to create, delete, or modify
files or directories, a programmer needs to use special
programs. Those programs are essentially small wrappers
around the file and directory system calls provided by the kernel.

Here are the file and directory utilities I will describe:
\begin{itemize}
\item [[touch]] and [[mkdir]]
\item [[cat]] and [[ls]]
\l ls!! ls trace though multiplate layers :)
\item [[rm]], [[cp]], and [[mv]]
\item [[chmod]] and [[chgrp]]
\l [[mtime]] ??
\t find? http://doc.cat-v.org/unix/find-history, not designed by unix people!
\t  which is why it is actually ugly with weird interface like -o
\l can use du -a apparently
\t put a Tag? Creation: Listing: Deletion: Security:
\end{itemize}

\subsection*{String processing utilities}
\l , [[grep]], [[sed]], etc

%trans:
Source code is stored in text files, which are made of a set of lines, which
are a set of strings (words).
%
It is thus normal that {string-processing} utilities
are very often used by programmers to search, modify,
or compare source code.
\l Software tools book is the LP book about string/text processing
%
Here are the string-processing utilities I will describe:
\begin{itemize}

\item [[grep]]:
\l g re p,  global regexp p?
It is one of the most versatile and useful tool for a programmer.
It can be used to find code using {regular expressions}.
\l with pipes can do find | grep !! power of unix

\item [[sed]]: It can be used to help refactor code.
\l stream editor
\l and [[tr]]

\item [[diff]]: It can be used to compare different versions
of the same codebase.
\l very useful programmer tool, also dual of patch
\end{itemize}
\l awk?

\subsection*{Process utilities}
\l , [[ps]], [[kill]], etc

%trans:
Source code ultimately is transformed into 
binary programs, which ultimately become 
{processes} when run. 
%
A programmer needs a set of utilities
to manipulate those processes. Under \plan, the [[/proc]]
directory can be used to inspect and manipulate those processes.
An alternative is to use command-line programs that
are often small wrappers over [[/proc]].

Here are a few process utilities I will describe:
\begin{itemize}

\item [[ps]] and [[pstree]]

\item [[kill]]
\l actually it's a script
\l \item [[sleep]]

\end{itemize}

\subsection*{Archive utilities}
\l , [[tar]], [[gzip]]

%trans:
Once a program has been written, a programmer often wants
to share it with the world.
%
In this case, he can count on a few utilities to {package}
and compress code.

Here are a few utilities used to help disseminate programs:
\begin{itemize}

\item [[tar]]
\l kind of a filesystem format, good for bootstrapping too, portable fs!
\l [[t]]ape [[ar]]chive :)

\item [[gzip]]

\end{itemize}

%\section{Date utilities}

%\begin{itemize}
%\item [[date]]
%\item [[cal]]
%\end{itemize}

%\section{Byte utilities}
% maybe useful in bootstrapping context where need to do low-level
% things such as patching the first sector of a tape.

%\begin{itemize}
%\item [[dd]]
%\item [[split]]
%\item [[xd]]
%\end{itemize}

\section{Applications}

I decided to limit \principia to system programs,
and to the system programs that are the most relevant to the programmer.
I do not cover applications such as 
spreadsheets, word processors, calendars, email clients, 
or video games.
%
However, I encourage other people to find 
applications with small codebases (using minimalist approaches), 
and to write and publish their literate programs.
\n Frank from STEPS is less 1800 LOC in KScript/KWorld so maybe good inspiration.


\chapter{Conclusion}

I hope the \principia books will greatly consolidate
your computer science knowledge, and give you a better and
more complete picture of what is going on in your computer.

I think the \principia programs form together the minimal foundation
on top of which all applications can be built. Even though there are
\nbbooks books in the series, I still think it is the
minimal foundation. Indeed, it is hard to remove any of those programs
because they depend on each other.
%
First, you need to rely on a kernel (hence the name), but
the shell and the C library are also essential. 
% => 3
But, because those programs
are coded in C and assembly, you also need a C compiler and
an assembler, and because source code is usually split in many
files you also need a linker.
The C compiler itself usually uses DSLs like Lex and Yacc.
\l which are themselves written in C and actually use also lex and yacc themselves
% => 7
To write all this code in the first place you need an editor.
\l But to run those tools you need a kernel :) interdependent
\l but btw editor run in an OS and coded in a (another) language
Then, with so many source files you need
a build system to automate and optimize the compilation process.
Because the programs I just mentioned inevitably have bugs
or non optimal parts, you will need a debugger and a profiler.
% => 11
Finally, nowadays
it is inconceivable to not use a graphical user interface
and to not work with multiple windows opened at the same time.
In the same way it is also not conceivable to work
in isolation; programmers collaborate with each other,
especially via the Web.
This means you need a graphical and networking stack as well
as a windowing system and a Web browser.
% => 15
% (miss 1 language, 1 utility, 1 emulator => 18)
As I said earlier, it is hard to remove any of the programs
from the series.


I hope those books will answer many of your questions, 
even those that seem very simple at first
such as ``What happens when the user type [[ls]] in a terminal window?''.
The answer to this question
involves many software layers (the shell,
the C library, the kernel, the graphics stack, and the windowing system)
and lots of code.

The books in the series can be read mostly in any order.
\t do graph of order and independent order! and make figure with graphviz!
\l  maybe can have two kinds of arrows, hard-deps and our-opinion-order-deps
\l  with some arrows and some dashed arrows
\l hmm maybe for compiler/assembler/linker/emulator there are some orders
You do not have to read them all.
%
I recommend to pick the program you are 
the most interested in, for instance,
the one you are the most curious about because you have 
only a vague idea of how they are implemented, and 
read the corresponding book in the series.
\l In our case, the graphics stack, windowing systems 

Enjoy!


\appendix

\chapter{Literate Program Example TODO}
\t goal show advantage of using literate programming

\chapter{Other Teaching Operating Systems}

Here are a few teaching operating systems that I considered for \principia,
but which I ultimately discarded:
\n operating system in the general sense
\n helps to appreciate even more how good our plan9 choice is
%dup: readme.txt

\begin{itemize}

\item \unix V6 (Ken Thompson et al.)
\furl{http://minnie.tuhs.org/cgi-bin/utree.pl},
\n tuhs = the unix heritage society
fully commented in the classic book by John Lions~\cite{lions},
or its modern incarnation xv6
\furl{http://pdos.csail.mit.edu/6.828/2014/xv6.html},
are great resources to fully understand a \unix kernel.
But, this kernel is too simple; there is no support for graphics
or networking for instance.

\item XINU (Douglas Comer)
\furl{http://minnie.tuhs.org/cgi-bin/utree.pl?file=Xinu7},
fully documented in two books~\cite{xinu,xinu2},
has a network stack, but the kernel
is still too simple with no virtual memory for instance.
\l and has multi processor support?
\t new edition in 2015! seems to have virtual memory now

\item Minix (Andrew Tannenbaum et al.)
\furl{http://minnie.tuhs.org/cgi-bin/utree.pl?file=Minix1.1},
also fully documented~\cite{minix},
is fairly small, but it is just a kernel. Minix does not provide
for instance its own windowing system; it relies instead on X Window,
which is far more complicated than the \plan windowing system.
\l it has a compiler though, ack, but was open sourced only in 2003

\item Hack (Noam Nisan and Shimon Shocken)
\furl{http://www.nand2tetris.org/}
is a toy computer introduced in the excellent
book {\em The Elements of Computing Systems}~\cite{tecs}.
This book is great for understanding processors, assemblers,
and even compilers, but the kernel part is really too simple.
\n and actually no code, only code is the different emulators and debuggers

\item MMIX (Donald Knuth)
\furl{http://www-cs-faculty.stanford.edu/~uno/mmix-news.html}
and its ancestor MIX
are computers designed by Donald Knuth
and used in his classic book series 
{\em The Art of Computer Programming}~\cite{taocp}.
Donald Knuth also wrote a book using literate programming, 
{\em MMIXware}~\cite{mmixware},
to explain the full code of the MMIX simulator and assembler.
%
However, similar to Hack, very few programs have been written for
this machine. For instance, the book assumes the presence of a kernel
called NNIX, but nobody has ever written it.
\n but as kunth told me, gcc has been ported to mmix as well as Linux

\item STEPS (Alan Kay et al.)
\furl{http://vpri.org/html/writings.php}
is a project to reinvent from scratch programming.
It has a far more ambitious goal than \principia:
write a full operating system in 20 000 LOC. It is 
unfortunately not finished yet.
\l our kernel book is 30 000 LOC, and it's just the kernel (and actually not all of it), so hmmm
\l seems more oriented to apps though, minimal OS, and single lang I think
%\item Squeak (Alan Kay et al.),%\furl{http://squeak.org/}

\item Oberon (Niklaus Wirth et al.)
\furl{http://www.projectoberon.com/} 
is a kernel,
compiler, 
and windowing system 
designed from scratch. It is
a great operating system, very compact, and fully documented in a 
book~\cite{project-oberon}.
%
However, it imposes strong restrictions on the programmer:
only applications written in the Oberon programming language
can be run. 
This simplifies many things, 
\n singularity was a bit like that too
but operating systems like
\unix (and \plan) are more universal; they
can run any program in any language, as long as the program 
can be interpreted or compiled into a binary.
\l  javascript emulator: http://schierlm.github.io/OberonEmulator/
\n new edition in 2014!

\item TempleOS (Terry A. Davis)
\furl{http://www.templeos.org/} is an operating system
single handedly created over a decade. It contains a kernel,
a windowing system, a compiler for a dialect of C, and
even some games.
It has graphics capabilities but there is no network support.

\end{itemize}

Note that GNU\furl{http://www.gnu.org/}/Linux\furl{http://www.kernel.org/}/Xorg\furl{http://www.freedesktop.org}
(Richard Stallman et al.)
together are far bigger than \plan.
If you take the source code of 
the Linux kernel, 
the GNU C library ([[glibc]]),
the [[bash]] shell,
the GNU C compiler ([[gcc]]),
the GNU assembler ([[gas]]) and linker ([[ld]]) part of the [[binutils]] package,
the GNU Lex and Yacc clones ([[flex]] and [[bison]]),
the Emacs editor, 
GNU [[make]],
the GNU debugger ([[gdb]]), 
the GNU profiler ([[gprof]]),
and the X Window system ([[Xorg]]), you will get
orders of magnitude more source code than \plan, even though
\plan provides in essence the same core services. 
%
In fact, almost all of the programs above individually use more source code
than the whole \plan system.
Of course,
the Linux kernel contains thousands of specific device drivers,
[[gcc]] handles a multitude of different architectures, and 
[[Xorg]] supports lots of graphic cards.
All of those things could be discarded when
presenting the core of those programs. But their core is still far
bigger than the equivalent core in \plan programs.
\n Linux 1991, Plan9 1992 (but open source only in 2000), paper in 1990


\l too many other to list here, see the comment in the .tex
%#industry/mainstream
% - MacOSX, nice kernel, but lots of private code (e.g., quartz graphics stack)
% - Windows, closed source
% - BSD? which one? FreeBSD? OpenBSD? NetBSD?
% - QNX, see 1.44MB challenge, OS, GUI, web browser, ... in 1.44MB, wow
%   http://marc.info/?l=freebsd-chat&m=103030933111004
%#complete
% - nuttx(NX), ils ont meme une graphic stack, widget, window manager
% - http://www.homebrewcpu.com/, full stack too, LCC, minix 2, uIP tcp/ip,
%   with homemade cpu! homemade assembler and linker. But more hardware
%   project than software (even though he said he spent more time on the soft)
% - AROS (=~ amiga OS)
% - HelenOS
% - redox, rust-based OS, seems to have windowing system, network, editor, etc
% - menuetos/kolibriOS, has also GUI, network, but written all in assembly!?
%   http://menuetos.net/ http://kolibrios.org/en/index
%#history
% - unix v1 on pdp11 simulator in assembler 
%   https://github.com/c3x04/Unix-1st-Edition-jun72
%   (github export of https://code.google.com/p/unix-jun72/)
% - Xerox Alto, had so many things, network, graphics
% - Smalltalk 80, 3 books, also a bit encyclopedic
%    * blue book = design and implementation,
%    * red book  = IDE
%    * purple book = language
%#toys
% - RECC collection (Robert Elder)
%   http://recc.robertelder.org/ 
% - short canonical programs:
%   http://aosabook.org/blog/2014/01/call-for-technical-reviewers-for-500-lines-or-less/
%   https://github.com/aosabook/500lines
% - A tiny hand crafted CPU emulator, C compiler, and Operating System (xv6)
%   https://github.com/rswier/swieros
% - http://menuetos.net/, written in assembly
% - https://moonforth.github.io/
% - urbit http://urbit.org/preview/~2015.9.25/materials/whitepaper seems
%   very cryptic, but the abstract is nice, 25K LOC for lots of stuff
%#proved:
% - boyer moore special machine, assembler, etc



\chapter{Bootstrapping from Scratch}
\label{sec:bootstrapping}
\n bootstrapping always from scratch? not necesseraly.
\l or 60 Years of Software History in 10min :)

%alternative would be to use the Cuneiform tablets system of Alan Kay
% and preserve what we have now running! write an emulator for
% ARM basically and save all the ARM targets (rio.5, cc.5, etc).

%found after I made this chapter, but meh:
%https://www.reddit.com/r/programming/comments/9x15g/programming_thought_experiment_stuck_in_a_room/c0ewj2c
%https://www.quora.com/If-an-operating-system-like-windows-7-is-to-be-written-by-a-single-good-programmer-how-long-will-it-take

%trans:
It is currently fairly easy to install \plan on a new machine,
as explained in Section~\ref{sec:getting-started}.
%
This is because there are already in the world
computers running operating systems that include
executable C compilers and text editors, 
with standard formats for data as well as compatible storage devices.
This is also because the source code of \plan
can easily be downloaded on the storage device of those
computers through the network.
%
All of this makes it easy to compile (or {cross compile})
\plan on one machine and 
install it on the disk of a new machine that can be {booted} on. 
In fact, you can even use an emulator like Qemu and run \plan from
the same machine.
\l could show T diagram of how currently done: with gcc, kencc, qemu
\n does compiling for same arch but other OS considered cross compiling?
%

But, what if we had to start from {scratch}?
\begin{itemize}

\item What if there was no executable C compiler?
As mentioned in Section~\ref{sec:compiler}, the C compiler 
I describe in \principia
is itself written in C. This self-reference leads to a 
{\em chicken and egg} problem: How was compiled the code 
of the first compiler?
%
This is also true for OCaml, Lex, and Yacc, which are also written 
in themselves.
\n solution: T diagram. Lower-level -> higher-level -> higher-level until self
\l how to create (binary) programs from scratch? human assembler :)

\item What if there was no digital text of \plan and no text editor? If the
only representation of \plan was the printed \principia books, 
\n see actually history about restoration of unix, maybe cite?
the source code of \plan would have to be entered first in a computer.
But without a text editor, how to enter and save text in a computer?
How was entered the source code of the first text editor?
\n solution: physical editing outside computer, e.g., punched cards, or kbd
\l how to enter (text) programs from scratch? human punch :)

\item What if there was no kernel? 
How to interact then with a new machine?
%More generally, what if there was no existing software at all?
If most programs are loaded into memory by the kernel,
which can be seen as an interactive program loader, 
how was loaded in memory the first loader?
\n solution: booting, simple program hardwired, firmware, basic loader
\l how to load programs from scratch? human switches :)

\end{itemize}

In fact, the answer to all those questions is a technique called
{\em bootstrapping}. According to Wikipedia, ``{bootstrapping} in general
refers to the starting of a self-sustaining {process} that is supposed
to proceed without external input''. In our case, bootstrapping is a
process where you start from something simple and gradually build
something self-sustaining and more complex.

In the book 
{\em The Knowledge: How to Rebuild our World from Scratch}~\cite{knowledge},
its author Lewis Dartnell imagines our world after an apocalypse and describes
the key knowledge one needs to start rebuilding civilization from
scratch. He does not reach though the computer and software age.
\l information age?
%
What if after this apocalypse there was no more software?
What if the only programs we had was the printed \principia books
retrieved from a time capsule?
Would that be enough to {bootstrap} \plan?
%
The kernel, compiler, and editor being mutually dependent on each other,
\l as well as self-dependent, 
it is difficult to imagine where to start.
Which program to write first? For which machine? In which language? 
And how to enter this program in the machine?

%toc:
In the following sections I outline a bootstrapping process
for \plan where I start from scratch. 
%
I think it is an interesting intellectual exercise.
\l makes you realize certain things, also make principia more complete
\l learn history, see more beauty and needs of some features we take 4 granted
\n 40 years of history :) but we gonna do leap frog, e.g., directly keyboard
%
By scratch I mean with no existing software, but
because in \principia I am interested in software and not hardware, 
I assume the existence of a basic computer though.

\section{A basic computer}
\label{sec:basic-computer}

Software and hardware need each other in order to be useful,
but hardware is the concrete starting point. 
Before executing programs, we first need a physical machine,
a computer.
\l link to turing machines appendix? Here it's Von Newman architecture!

The main components of a basic computer are as follows:
\begin{itemize}
\item A processor: This is an interpreter for a simple low-level
language where instructions are encoded in a binary format.
\l list of instructions? ARITH, MOV, JMP, (SWI), IN/OUT?
\l also need hard interrupt mechanism (soft later), and boot procedure

\item Some memory: This
\l physical memory, will see virtual memory concept later
contains data but also the code of programs. The
{\em stored-program} concept is one of the major invention of computer science.
\l related is notion of registers, just faster dedicated memory

\item Input and output devices: This is used for {interactivity} with
the user.
\l need mechanism in processor, either memory-mapped or special instructions

\end{itemize}

To learn how to build a simple processor and memory
I recommend to read~\cite{tecs} in which
starting only from the [[nand]] logic gate, you learn how
to build gradually the 
[[and]], [[or]], and [[not]] logic gates, 
a multiplexer, 
flip-flops, 
memory banks, 
an additioner, 
an arithmetic 
and logic unit (ALU), 
and finally a simple processor (CPU).
\l for a very simple machine language.
\l also http://www.homebrewcpu.com/? good technical documentation?

Regarding the external devices, a basic computer typically has three:
\begin{itemize}

\item An input device, e.g., a keyboard, for reading data. 
The press of a key can trigger
an {interrupt} in the processor that will trigger the execution
of a special program. The value of the key could be read at
a certain memory location (memory-mapped IO) or via a special
instruction.
%
Before keyboards, switches on panels or punched cards
with card readers connected to the computer were the main ways
to enter data in a computer.

\item An output device, e.g., a screen, for displaying data. 
Again the device could be
memory-mapped, in which case the writing of data at certain memory
locations would trigger the display of special characters on the screen.
\l framebuffer
%
Before screens, diods on panels or printers connected to the computer
were the main ways to display data from the computer.
\l after bitmap came Mouse for input (and feedback with cursor on screen)

\item A storage device, e.g., a magnetic tape with its tape drive,
\l it's both input and output
to permanently store data, 
for instance programs in executable forms and source forms. 
Again, access to this device could be provided by special instructions
or by writing and reading certain codes at certain memory locations.

\end{itemize}

All those things can be built from scratch. They are not trivial to build,
but they do not require any software; they are physical artifacts.
\l mechanics, optics, eclectric, electronic, quantic

\n will see later necessary extensions: timer int, virtual mem, rings, ???

\section{A booting procedure}

%trans:
Once you have a basic computer, an important question
is what happens when a human presses the On/Off button of the computer?
%
The initialization of a computer system is called {\em booting},
which is the shortened form of the word bootstrapping.
The machine needs to initialize itself and execute the first program
from memory.
If most programs are loaded into memory by other
programs already running on the computer, what is the mechanism
to {load} the very first program?

There are multiple ways to load an initial program in memory.
%
The very first computers used switches on panels
where the initial configuration of memory could be manually
toggled, or at least part of the memory could be toggled.
Then, the computer by construction when turned on would start to execute
the code located at a specific memory address, e.g., [[0x10]].
%
This initial program, usually small, is called the {\em boot loader}. Its
job is just to load a larger program stored on the main storage device:
the kernel (which itself will load other programs).

Later, computers used punched cards and a card reader
connected to the computer to load automatically their first program in memory.
\l but so button of computer is connected to a program that
\l tells to do this card reader loading, or maybe it is a simple
\l instruction of the computer, so the button really is connected to
\l one instruction activation
Modern computers use special read-only memory (ROM) chips to store
the initial program. Such programs are also called {\em firmware},
because they are in the middle between the hard and soft.

In any case, all those techniques are similar: a program is
hardwired at a persistent location (panel, punched card, tape, ROM)
and loaded in memory at a specific address by the computer
when the human presses a special button. The computer then
jumps to this memory address and starts executing the program.
\l it's mechanical again, human button

\section{Physical programs}

%trans:
We have just seen different ways to enter programs in a computer
without a text editor;
%
switches and punched cards
are ways to write data, and so also programs, in the physical world.
Thanks to panels and card readers connected to the computer
those {physical programs} are made {machine readable}.
\l could sculpt directly machine tape? in the end everything is physical ...

Different kinds of punched cards and card readers can be used
to write different kinds of information:
\begin{itemize}
\item Different holes for different bits can be used to represent
binary data such as machine instructions.

\item Different holes for different letters or numbers can be used
for textual data used by programs.

\item Different holes for different instructions of specific languages
can be used for the source code of languages such as assembly.
\l JCL card?

\end{itemize}

Note that a card reader (or a keypunch machine)
could be built in such a way to behave like a primitive
{mechanical assembler}. The holes in the punched card could correspond
to different assembly mnemonic instructions and be
transmitted to the computer as the assembled binary machine instructions.
\l can reuse typing machine to generate physically those cards.

Once those physical programs are loaded into memory, they 
can be stored back on magnetic tapes thanks to special 
computer instructions and the tape drive.
\l could do that directly? sculpt magnetic holes? like for songs?

\bigskip
%trans:
We now have everything we need to start producing software:
\begin{itemize}
\item A basic computer, 
which can be seen as a machine-language {\em physical interpreter}.
This will allow us to bootstrap the {compiler}.

\item A way to enter programs in the computer, with punched cards,
a keypunch machine, and a card reader,
which can be seen together as a {\em physical text (and binary) editor}.
This will allow us to bootstrap the text editor.

\item A simple booting procedure
which will load the boot loader from a fixed {\em physical location}.
This will allow us to bootstrap the kernel.

\end{itemize}

%toc:
The following sections describe the process to bootstrap
\plan from scratch by writing a series of programs.
The process is decomposed in different
{phases} depending on the programming language used.

\section{Phase 1: Machine code}


\subsection*{[[BOOT-M-CARD]]}

The very first program to write is a boot loader,
\n of course :) it's the very first program loaded :)
which I call [[BOOT-M-CARD]]. 
[[M]] stands for machine code and [[CARD]] for punched card.
%
The first program has to be written directly in machine code
and entered in the computer via a punched card 
as there is no assembler and no text editor yet.
%
The bits of the instructions of this program can be encoded 
as holes in the punched card.
%
This card can then be inserted in the first position
of the card reader and loaded at boot time by the computer
according to its booting procedure.

One card should be enough to encode a very simple boot loader.
With just a few instructions this program could load in memory
the rest of the punched cards from the card deck (or data
from the tape), and so {{bootstrap}} an hypothetical kernel.
\l start of self sustain because can load other stuff without button/card 
It must load this code after
is own code though, to not overwrite itself,
e.g., at address [[0x100]], and then jump to this address.
\l can switch by being interactive and ask via keyboard.
\n which is why lots of boot loader first move themselves elsewhere :)
\n 0x100 - 0x10 =~ 0x100 = 256 which should be enough. But 1 card might not be

To create [[BOOT-M-CARD]] we could have created first
[[BOOT-ASM-PAPER]], the boot loader program written in
assembly on paper. 
Then, thanks to a very powerful (but error-prone) computer and assembler, 
the {human}\footnote{
Not just its brain, but also its powerful IO devices: the hands that
can type on a keypunch machine.
}, we could have translated this program in binary and then generate
[[BOOT-M-CARD]].
\n diagram with human?
\l brain is useful for bootstrap :) in fact Wadler says computer before
\l mean a person following a set of instructions :)

\l example of output on screen :)

\subsection*{[[KERNEL0-M-CARD]]}

The next program to write is a simple kernel, which
\l use operating system instead ?
I call [[KERNEL0-M-CARD]]. It is essentially an interactive program loader. 
With this loader you could type on the keyboard after a {prompt}
the location of a program in order to get this program
loaded in memory and jumped to.
\l after all main role of OS is generalized loader (better than boot loader)

The organization of different programs on the tape or in the
card reader can be trivial:
the different programs could be stored one after the other.
The location of a program could then be specified simply by 
a letter and two numbers: the letter to select
the card reader or tape drive, the two numbers for
the start and end locations of the program on the tape
or in the card deck.
Moreover, by convention programs could be loaded at the memory
address [[0x1000]] and executed from there.
The last instruction of a program could be to jump to
the address [[0x100]] to give back control to the kernel.
\l could put kernel in high memory :) reserved space.
The following diagram describes the memory layout
after a program was loaded by the kernel:

\begin{verbatim}
       ||                  ||
       || Loaded program   ||
       ||                  ||
0x1000 +--------------------+
       |                    |
       |                    |
       +--------------------+
       ||                  ||
       ||                  ||
       ||     Kernel       ||
       ||                  ||
       ||                  ||
 0x100 +--------------------+
       |                    |
       +--------------------+
       ||   Boot Loader    ||
       ||                  ||
  0x10 +--------------------+
       |                    |
   0x0 +--------------------+
\end{verbatim}
\n kernel could overwrite boot loader to free space, but detail

There is no need yet for a real filesystem or an executable format.
The code of this kernel can be very simple; it should
require only a few punched cards. 
%
Those cards can then be placed just after the punched card of the boot
loader in the cards deck.

\l example of session :) M 1 23, and also output of screen of booting :)


\l cp command, so can save on tape!

\subsection*{[[ASSEMBLER0-M-CARD]]}

The final program to write in machine code is a rudimentary assembler,
which I call [[ASSEMBLER0-M-CARD]].
Again, we could write first [[ASSEMBLER0-ASM-PAPER]] and then
translate this program in machine code by using a human assembler.
\l T-diagram

Because we do not have yet a text editor, 
this assembler when loaded could ask first a series of
questions to the programmer,
and then ask interactively to the programmer
to enter via the keyboard the full content of the assembly
program (without mistakes).
\l or basic line editing, backspace, leap frog keyboard here
The assembler could also take as input the content of an assembly program 
from cards or the tape, depending on the answer to the initial questions.
Another question could be used to specify where to save
the generated (assembled) machine-code program.
\n no need command line arguments yet. just use scanf.

The punched cards of this assembler program can be put just
after the cards of the kernel in the card deck.
You can then count the number of cards in the deck to know the location
numbers to enter in the prompt of the kernel to load this assembler program
in memory.

\l example of session where type stuff

\section{Phase 2: Assembly}

%trans:
Thanks to [[BOOT-M-CARD]], [[KERNEL0-M-CARD]], and [[ASSEMBLER0-M-CARD]],
we can now enter via the keyboard
programs in assembly, a major productivity improvement 
over punched cards and binary machine-code.

\subsection*{[[EDITOR0-ASM0-KBD]]}

For improving even more productivity, the first assembly 
program to write is an editor, which I call [[EDITOR0-ASM0-KBD]].
It is written in the rudimentary assembly [[ASM0]] supported
by the rudimentary assembler [[ASSEMBLER0-M-CARD]],
and {{bootstrapped}} via the keyboard input
of the assembler.
\l self-sustain, can edit yourself!
The assembler can then generate [[EDITOR0-M-TAPE]], an executable
editor, which can be loaded from tape.
\t FIGURE? complex see all the actors involved, CARD, different programs,
\t  and now KBD, which generate EDITOR0-M-TAPE.

Thanks to this editor further programs can be entered via
the keyboard, edited, and saved
on tape. We can make mistake and fix the mistake easily.
\l need FS? where to store on tape? need to manage yourself :) editor = hexdump
%
From now on I assume every programs will be entered
via the text editor and saved on tape, so there is no
need anymore for the [[CARD]], [[TAPE]], or [[KBD]] suffixes
in the program names.

\l not trivial to use screen, need curses :) ed was simpler.
\l talk about ed? imaginary cursor? fact that there was no screen so we leaped

\subsection*{[[ASSEMBLER1-ASM0]], [[ASSEMBLER2-ASM1]], etc}

%trans:
Thanks to [[ASSEMBLER0-M]] (previously called [[ASSEMBLER0-M-CARD]])
we can now {{bootstrap}}
an assembler written in itself: [[ASSEMBLER1-ASM0]].
\l bootstrap because start self-sustain here, in itself!
\t FIGURE, too complex.
%
In fact, we can write a series of increasingly powerful assemblers.
Each assembler can add features, be assembled by
the previous generation assembler, which will generate
a new binary assembler, e.g., [[ASSEMBLER1-M]],
enabling now programmers to write assembly programs
using those new features, including the assembler program itself.
\l T diagram, can rewrite assembler in assembly!
\l cite multi-steps bcompiler?
Hopefully at some point there will be no need for more features
or optimizations and we will reach a {fixpoint} with an assembly language
I call [[ASM]] from now on.

\subsection*{[[EDITOR1-ASM]]}

%trans:
Given this feature-rich assembly language [[ASM]] and assembler,
we could rewrite the editor that was originally using the 
rudimentary assembly [[ASM0]]. 
%
I call [[EDITOR1-ASM]] this
new editor. Again, we could write a series of increasingly
powerful editors where each new feature added could be used
to edit more quickly the code of the next version of the editor.
\l copy, paste, move editing window to different sector, pre-allocate space

\subsection*{[[KERNEL1-ASM]]}
\l KERNELFS1-ASM?

%trans:
We can now use a nice editor and program in a powerful assembly
language.
%
But, until this point we are still using the basic program
loader [[KERNEL0-M]]. With this kernel, you need
to enter the start and end locations on tape of a program
to get it executed, which is inconvenient.
\l the editor and assembler also work in terms of tape location
%
Indeed, with the multitude of programs
we previously created, you need to maintain a physical
map of the tape to remember where the programs (in source and
binary forms) are located.
%
Moreover, the programs have to be stored contiguously on the tape.
%
Finally, you must take care when using the editor to not create
data that would overwrite another program.
\l start/end could be first question of text editor too, to pre-allocate space
\l the editor is really more like hexdump, hmmm

All of this management of data, currently manual and error-prone,
can be done instead by the computer. I call [[KERNEL1-ASM]]
a new kernel written in assembly with one important new
feature: a {\em filesystem}.
\l DOS is Disk Operating system ...
%
Transitioning to this new kernel can be facilitated by having
two tape drives connected to the computer and two
programs [[FSINIT-ASM]] and [[CP-ASM]], which can
respectively initialize a new filesystem on a tape, and
transfer data from one ``raw'' tape to another tape
managed by a filesystem, and to give a {filename} to this data.
\t FIGURE

The start of a tape managed by a filesystem can still
be reserved to contain the boot loader and the kernel.
%
By preparing carefully the second tape with the boot loader, the new kernel,
the filesystem with the previous programs (assembler, editor, kernel)
in source and executable forms appropriately named, you
could then switch the first tape with the second and boot the new kernel.

The interactive program loader in [[KERNEL1-ASM]] 
would now take after its prompt {symbolic filenames} instead of tape
locations to execute commands.
\l no need args still I think, just ask questions if needed
Note though that the copied text editor and assembler programs
([[EDITOR1-M]] and [[ASSEMBLER2-M]])
would still use tape locations to operate so you must take care when executing
those commands to not corrupt the filesystem. 
%
One solution is to now use the second tape as a {scratch} and use
[[CP-ASM]] to {export} and {import} data from this tape;
the old editor and assembler could read and write data on this second
tape without risking to corrupt the filesystem of the first tape.

Because the code required to manage a filesystem can be large,
the kernel might not fit anymore in the original [[0x100-0x1000]] 
memory interval. After being loaded by the boot loader at [[0x100]],
the kernel could copy itself in higher memory and just put at [[0x100]]
a few instructions to jump back to higher memory. That way programs
could still be loaded at [[0x1000]] without overwriting code
from the kernel, as in the following diagram:


\begin{verbatim}
       ||                  ||
       ||                  ||
       ||     Kernel       ||
       ||                  ||
0x10000+--------------------+
       |                    |
       |                    |
       |                    |
       +--------------------+
       ||                  ||
       ||                  ||
       ||                  ||
       || Loaded program   ||
       ||                  ||
0x1000 +--------------------+
       |                    |
       |                    |
       |                    |
       +--------------------+
       || jump 0x10000     ||
 0x100 +--------------------+
       |                    |
       +--------------------+
       ||   Boot Loader    ||
  0x10 +--------------------+
       |                    |
   0x0 +--------------------+
\end{verbatim}

Note though that very large programs could
still overwrite the kernel if their code size would reach 
the higher memory area where the kernel resides in. We will
see later mechanisms to protect the kernel from such programs.
\l at the same time when load the program the kernel could detect this case
\l but can still not prevent small program to overwrite high-memory area

\l need basic shell, command line arguments to assembler? not yet, kbd fine.
\l executable format? => modif to assembler to generate this new format. meh


\subsection*{[[LINKER0-ASM]], [[ASSEMBLER3-ASM-OBJ]]}

%trans:
Now that we have a filesystem, we can easily create many
files. We can now split big programs in multiple files 
to separate concerns and also to factorize code among those 
programs in library files.
\l but how split? [[SPLIT-ASM]]? how create pre-allocate editor and CP-ASM?

To create an executable from multiple files we could
create first a program [[CAT-ASM]] that would concatenate
multiple files into one. We could then use
the old program [[ASSEMBLER2-M]] on this single file
(after it has been exported by [[CP-ASM]] on the scratch tape).
%
An alternative is to create intermediate machine-code files,
{\em object files}, and to create a {\em linker} I call [[LINKER0-ASM]]
to produce the final executable from those object files.
This program is more complicated than [[CAT-ASM]] but it
enables {\em separate compilation}, which in the long term
will save compilation time.
\l other advantages?

To create those new object files we need to modify the assembler
though and create [[ASSEMBLER3-ASM-OBJ]]. Moreover, we can
use the opportunity to update also the interface of the assembler
to now take filenames as arguments for input and output
instead of tape locations.
\l but still via questions, still no need argv

\subsection*{[[LIB0-ASMs]]}
\l LIBFS-ASMs?

For [[ASSEMBLER3-ASM-OBJ]] to work
with filenames, code in the kernel related to the filesystem
had to be copied and integrated in the assembler.
\l or need mechanism to jump to kernel code ... but then need link
Moreover, lots of code dealing with tapes, punched cards, 
and other devices had to be duplicated in different programs.
This is unfortunate.
%
Thanks to the linker we can now factorize code
in one library source file and get the 
resulting object file linked in different programs.

I call [[LIB0-ASMs]] a core library written in assembly
using multiple files (hence the [[s]] in [[ASMs]]).
This library contains reusable code among all the previous programs.
This library can provide functions to interact
with devices, the filesystem, files, memory, etc.
%
In some sense this library can also play the role of
a kernel as it can abstract hardware
and provide low-level functions to other programs.
A {system call} is then simply a library call.
\l in fact exo kernel are like that. mirage too.

\n need archive format? no, could take .o for library code too

\subsection*{[[LINKER1-ASMs]], [[ASSEMBLER4-ASMs]], [[EDITOR2-ASMs]]}

%trans:
We can now reorganize the code of the previous
assembly programs by splitting their code in multiple assembly files,
%
to better separate concerns, and by removing the code
that was factorized in the [[LIB0-ASMs]] library.
\l how split? SPLIT-ASM?
I call those new programs [[LINKER1-ASMs]], [[ASSEMBLER4-ASMs]],
and [[EDITOR2-ASMs]].
The interface of all those programs can also be changed to operate
on filenames instead of tape locations.
\l and all low-level stuff could be done in LIB0, so transition path to unix

\subsection*{[[KERNEL2-ASMs]], [[LIB1-ASMs]]}
\n could put KERNEL2-ASMs with LIB0-ASMs and be very basic
\n and leap frog advanced os feature to the one in C directly, 
\n but historically the first unix was in assembly

%trans:
[[KERNEL1-ASM]] has a convenient filesystem
\l actually LIB0-ASMs too
but its program loader is still rudimentary.
%
You can load
only one program at a time in memory (at [[0x1000]] by convention).
You can load the editor, edit a file, quit, then load the assembler,
wait it finishes, then run back the editor again, and so on.
%
It would be nice to run the assembler command from the editor, 
\l M-!
or at least being able to switch between the editor and assembler without
losing the state of the editor, or even better being able to edit files
while a time-consuming assembling or linking {job} is running.
\l also need multitask for multiple users. But meh, plan9 does not have.

To do so requires a {\em multi-tasking} kernel
where multiple programs loaded in memory at the same time
called {\em processes} can execute {concurrently}.
There are multiple ways to implement multi-tasking
\l e.g., cooperative vs preempty, in single address space vs vm, relocatable?
but the now dominant way is to write a {\em preemptive scheduler}
using a {\em timer} interrupt and to leverage {\em virtual memory}.
I call [[KERNEL2-ASMs]] a new kernel written in assembly
using this technique. The first edition of \unix
was actually written in 
assembly\furl{https://github.com/c3x04/Unix-1st-Edition-jun72}.
\l https://en.wikipedia.org/wiki/Research_Unix
\l show example of code? of ls.s? So can imagine how to hand-translate
\l yourself and imagine that's it's actually doable

Note that the timer and virtual memory requires hardware extensions
to the basic computer I described in Section~\ref{sec:basic-computer}.
Thanks to virtual memory, multiple programs can still be loaded
at the same ({virtual}) address, [[0x1000]], as long as their {physical}
addresses are different. This helps for the transition to the
new kernel as old programs would still work with the new kernel.
Those programs could also still call code from [[LIB0-ASMs]]
to interact with devices, the filesystem, etc. 
\l figure new memory image

The [[LIB0-ASMs]] library could be gradually extended to offer
process related services, e.g., [[fork()]], [[exec()]], which would allow
to program the scenario I mentioned before with the
concurrent use of the editor and assembler. 
%We call [[LIB1-ASMs]] this new core library.
The program loader in the kernel could also be extended to provide advanced
{shell} features such as pipes on the command line, job control, etc.
\l e.g., have find | grep.
\l windowing system help even more for concurrent scenario

%\subsection*{[[KERNEL3-ASMs]], [[LIB2-ASMs]]}

{\em Protection rings} are another hardware extension,
often associated with virtual memory, which is very useful to
have in a computer.
\n Multics introduced that
%
They provide the mean to protect the kernel
from regular programs and also to protect programs
from each other when combined with virtual memory.
%
In practice two rings are enough. The processor then
can operate in two different modes:
\begin{itemize}

\item {\em Kernel mode}.
\n or privilege mode, or supervisor mode
In this mode all instructions are allowed, including
the ones used to modify the virtual memory mapping, or the ones
to interact with devices. Those instructions are called
{\em privileged instructions}. Moreover, in kernel mode
all memory accesses are allowed.

\item {\em User mode}. In this mode many privileged instructions 
and access to certain memory area are disallowed.
Such accesses cause {\em traps} in the kernel when those operations happens.
\end{itemize}

The only way to go from user mode to kernel mode (other than by
causing a trap) is through a special instruction:
the {\em software interrupt}. 
\l and preset interrupt table, so safe
Calling this instruction is also known as performing a {\em system call}
or {\em syscall}.

Thanks to protection rings and virtual memory, certain
memory area can be marked as protected, e.g., the high memory
area where resides the kernel, in which case programs can not
mess anymore with the code of the kernel by overwriting its code.
% 
Moreover, we can gradually forbid programs to access directly
devices such as the screen or the keyboard so that the kernel
instead can mediate and control the use of those devices.

To transition to this safer model, we can gradually
modify [[LIB0-ASMs]] and move routines using privileged instructions
(dealing with
memory, 
devices, 
the filesystem, 
processes,
etc)
to the kernel
\l [[KERNEL3-ASMs]] ?
and give access to those routines via a system
call (instead of a library call) to user programs.
%
I call [[LIB1-ASMs]] this new library, which should be 
significantly smaller than [[LIB0-ASMs]]. 
Once all the privileged code has been migrated to the kernel,
we can turn on the protection rings and forbid the execution
of any privileged instructions in user programs.


\subsection*{[[COMPILERC0-ASMs]]}

The final program to write in assembly will enable
the transition to a higher-level language: C.
It is a compiler written in assembly for a simple subset of C.
I call this program [[COMPILERC0-ASMs]].

\section{Phase 3: C}

%trans:
Thanks to the previous programs,
we now have a basic \unix environment with a simple C compiler,
all written in assembly. 
It is now time to rewrite this environment in a more
succinct and readable way using C.

\subsection*{[[COMPILERC1-C0]], [[COMPILERC2-C1]], etc}

%trans:
Thanks to [[COMPILERC0-M]] we can now {{bootstrap}}
a C compiler written in itself: [[COMPILERC1-C0]].
\l bootstrap because start self-sustain here, in itself!
%
In fact, just like for the assembler and editor,
we can write a series of increasingly powerful compilers:
each compiler can add features, be compiled by
the previous generation compiler
generating a new binary compiler, e.g., [[COMPILERC1-M]],
enabling now programmers to write C programs using those new features,
including the compiler program itself.
\l T diagram, can rewrite C compiler in C!
\l cite multi-steps bcompiler?
Hopefully at some point there will be no need for more features
and we will reach a {fixpoint} with a C language
I call [[C]] from now on.
\l maybe can put cpp here, separate program?
\l need to talk about header file

\subsection*{[[KERNEL3-C/ASM]], [[LIB2-C/ASM]]}

We can rewrite lots of the core library in C. 
Indeed, the C language being very flexible and powerful, many of the
assembly code idioms can be expressed in C, and usually in a 
clearer and shorter way.
\l which is why it's rewritten in C in the first place
%
But, some code has to be kept in assembly, which is why
I call this new library [[LIB2-C/ASM]].
Some low-level code, such as the system call
instructions that allows to jump in the kernel, can not be expressed in C.
Moreover, it can be preferable sometimes for heavily used routines 
to keep code written in highly-optimized assembly.
%
Those routines though can be exposed to other programs
with a C interface, e.g., in a [[libc.h]] header file.
Those other programs can then be written entirely in C.

The kernel itself can also be rewritten in C and assembly in
a program I call [[KERNEL3-C/ASM]].
%
The fourth edition of \unix
\furl{http://minnie.tuhs.org/cgi-bin/utree.pl?file=V4}
was the edition where most programs, 
including the kernel, were rewritten in C.
At the time, writing a kernel in a high-level language such a C
was controversial.
\l was it? maybe original but not controversial?

\l thanks to powerful language, easier implement powerful features.
\l lots of stuff here? see Research unix history wikipedia?

\l put mkfile that builds plan9? ROOT? kernel with sh first? see comment in tex
%then show how can compile under itself!
%(even kernel? yes if can make a floppy under qemu or rpi ...)
%ROOT/ a distribution from scratch!
%explain gradual things, kernel with sh on a floppy,
%kernel with hdd vfat, kernel with rc, kernel with graphics,
%kernel with rio, etc.

\subsection*{[[EDITOR3-C]], [[LINKER2-C]], [[ASSEMBLER5-C]]}

We can rewrite most of the previous assembly programs entirely in C:
the editor ([[EDITOR3-C]]), 
the linker ([[LINKER2-C]]),
and even the assembler can be rewritten in C ([[ASSEMBLER5-C]]).
\l fun to have language implemented in a higher level language than yourself
% can use cpp!
\n could be done before LIB-C/ASM 

%\subsection*{[[XXX-C]]}

\subsection*{[[INTERPRETERBC-C]], [[COMPILERML0-C]]}

We can now prepare the transition to an even higher-level
language: OCaml.
%
I call [[COMPILERML0-C]] a rudimentary compiler written in C
for a subset of OCaml. Instead of generating object code,
or assembly, like the C compiler does,
\l could also generate C. There are also ocamlopt which generates asm
the OCaml compiler uses a different approach.
It generates instead {\em bytecode} instructions for a {\em virtual machine}.
\l the zinc
Then, a bytecode interpreter, which I call [[INTERPRETERBC-C]],
written in C, implements this virtual machine to run 
the bytecode generated by the compiler.
This interpreter includes also the code for the OCaml
runtime system: the {\em garbage collector} and the core OCaml libraries.
\l why this approach? P-code of Wirth does similar. Easier to port. 

\section{Phase 4: OCaml}
\l and DSL? OCaml, Lex, and Yacc?

%trans:
Thanks to the previous programs
we now have a nice \unix environment with a simple OCaml compiler,
all written in C (with a bit of assembly).
It is now time to write or rewrite when it fits
certain programs in OCaml.

\subsection*{[[COMPILERML1-ML0]], [[COMPILERML2-ML1]], etc}

Thanks to [[COMPILERML0-M]] (and [[INTERPRETERBC-M]]) we can now {{bootstrap}}
an OCaml compiler written in itself: [[COMPILERML1-ML0]].
\l bootstrap because start self-sustain here, in itself!
%
In fact, just like for the assembler and C compiler,
we can write a series of increasingly powerful OCaml compilers:
each compiler can add features, compile itself generating
a new binary compiler, e.g., [[COMPILERML1-BC]],
enabling now programmers to write OCaml programs using those new features.
\l T diagram, can rewrite ocaml compiler in ocaml!
\l cite multi-steps bcompiler?
Hopefully at some point there will be no need for more features
and we will reach a {fixpoint} with an OCaml language
I call [[ML]] from now on.

\subsection*{[[EDITOR4-ML]]}

Thanks to OCaml features such as 
{garbage collection},
{closures}, 
{parametric polymorphism},
{algebraic data types},
{pattern matching},
{type inference}, 
and {exceptions},
some code can be easier to write in OCaml than in C.
I call [[EDITOR4-ML]] a new editor written in OCaml.

We could also rewrite the assembler, linker, and even C compiler
in OCaml. It would make less sense though to rewrite
the kernel or the core library in OCaml. C is better suited for
low-level programs with strong constraints on speed, memory,
and latency.
\l after all if kernel is slow, everything on top is slow. gc is a no go.

\subsection*{[[LEX-ML/LY]], [[YACC-ML/LY]]}

Algebraic data types and pattern matching are very useful
OCaml features to implement algorithms working on trees
such as the {\em abstract syntax trees} (ASTs) of programs.
This makes OCaml a great language for writing compilers
or more generally tools taking as input other programs.
%
Lex and Yacc are two of those tools. They provide both
domain specific languages to assist in writing respectively
a scanner and a parser.
We could implement Lex and Yacc using just OCaml,
but because the Lex and Yacc programs require themselves a scanner
and a parser, we can also use Lex and Yacc to program
parts of Lex and Yacc. 
I call [[LEX-ML/LY]] and [[YACC-ML/LY]] 
a Lex and Yacc written in OCaml with a few parts (the scanner
and parser) written in Lex and Yacc.

To solve the chicken and egg problem
I usually wrote in the past first a compiler using 
a lower-level language. This compiler can then be used to bootstrap
a compiler written in itself.
For Lex and Yacc we can use a more direct strategy though.
Indeed, because the lexical specifications and grammars of Lex and
Yacc are pretty small, we can fairly easily run manually their
algorithms to generate manually the resulting code: 
[[LEX0-ML]] and [[YACC0-ML]]. Those first versions, manually generated,
do not have to be optimized. They can then be used
on the original [[LEX-ML/LY]] and [[YACC-ML/LY]]
to generate the optimized versions.
\t FIGURE

\subsection*{[[ASSEMBLER6-C/LY]], [[COMPILERC3-C/LY]], [[COMPILERML3-ML/LY]]}

Lex and Yacc are fairly language independent; only
the parts between curly braces, representing {\em actions},
are language dependent. It is fairly easy then
to create a generic Lex and Yacc that can generate
scanners and parsers for different languages, e.g.
OCaml or C.
Using this new Lex and Yacc we can rewrite parts of our
past programs to finally get [[ASSEMBLER6-C/LY]], [[COMPILERC3-C/LY]],
and [[COMPILERML3-ML/LY]].

%\subsection*{[[XXX-ML]]}

\section{Summary of \plan ancestor programs}

I summarize here the list of programs that can be viewed
as ancestors to programs explained in the \principia series.
Those programs would be necessary to bootstrap \plan from scratch:
\l t diagrams with missing and included pieces in different colors?

\begin{itemize}

\item Boot loader: 
[[BOOT-ASM]] (translated in [[BOOT-M-CARD]] by a human assembler).
\l need new one only if different machine actually, or different devices.
The \plan kernel [[9]] described in Section~\ref{sec:kernel} 
includes a boot loader fairly similar to [[BOOT-ASM]].
\l 9 includes one, but we use qemu -kernel special trick for now

\item Kernels: 
[[KERNEL0-ASM]] (translated in [[KERNEL0-M-CARD]] by a human assembler), 
[[KERNEL1-ASM]],
[[KERNEL2-ASMs]].
\l also a bit LIB0-ASMs, and FSINIT-ASM, CP-ASM
%
This led finally to [[KERNEL3-C/ASM]], which can be a close ancestor to [[9]]
described in Section~\ref{sec:kernel}. 
%
We could derive [[KERNEL3-C/ASM]] from the source code of [[9]] by 
just removing many features.

\item Core library: 
[[LIB0-ASMs]],
[[LIB1-ASMs]].
%
This led to [[LIB2-C/ASM]], which can be a close ancestor to [[libc]]
described in Section~\ref{sec:libc}.

\item C compilers: 
[[COMPILERC0-ASMs]],
[[COMPILERC1-C0]],
[[COMPILERC2-C1]].
%
This led finally to [[COMPILERC3-C/LY]], which can be a close ancestor to [[5c]]
described in Section~\ref{sec:compiler}.
\n actually 5c does not use lex.


\item Assemblers: 
[[ASSEMBLER0-ASM]] (translated in [[ASSEMBLER0-M-CARD]] by a human assembler),
[[ASSEMBLER1-ASM0]], 
[[ASSEMBLER2-ASM1]], 
% \ldots,
[[ASSEMBLER3-ASM-OBJ]], 
[[ASSEMBLER4-ASMs]],
[[ASSEMBLER5-C]].
%
This led finally to [[ASSEMBLER6-C/LY]], which can be a close ancestor to [[5a]]
described in Section~\ref{sec:assembler}.
\n actually 5a does not use lex

\item Linkers: 
[[LINKER0-ASM]],
[[LINKER1-ASMs]],
%
This led finally to [[LINKER2-C]], which can be a close ancestor to [[5l]]
described in Section~\ref{sec:linker}.

\item OCaml compilers: 
[[COMPILERML0-C]],
[[COMPILERML1-ML0]],
[[COMPILERML2-ML1]].
%
This led finally to [[COMPILERML3-ML/LY]], which can be a close ancestor to [[ocamlc]] described in Section~\ref{sec:ocaml}.
[[INTERPRETERBC-C]] can be a close ancestor to [[ocamlrun]].

%\item lex and yacc, no missing intermediate :)

\item Editors: 
[[EDITOR0-ASM0]], 
[[EDITOR1-ASM]], 
[[EDITOR2-ASMs]],
[[EDITOR3-C]].
This led finally to [[EDITOR4-ML]], which can be a close ancestor to [[efuns]]
described in Section~\ref{sec:editor}.

% not covered yet: rc, 5i, mk, db, prof, graphics, network, most utilities
\end{itemize}

%%\chapter{Turing Machines}

% universal turing machine embody many important ideas:
% - processor = interpreter of stored program, with encoding of a program
% - memory = tape
% - binary format = encoding of 6-tuplet for universal turing machine
% - mov = read/write
% - jump = transition state
% - arith, logic = basic mathematics that you can write in program
% but differences with Von Newman which is closer
% - RAM so no need notion of head, can address anywhere directly
% - interactivity with keyboard and screen so flexible data
%%
%%\begin{verbatim}
%%
%%Fundamental of math is logic language, axioms, inference rules.
%%Fundamental of computer science is Turing machine.
%%so tempting to start from Turing machine.
%%good for theoretical, but not practical. no interactivity for instance.
%%no keyboard, no really notion of screen. Input data on tape.
%%%Normal of course, and great for theoretical, but still.
%%infinite tape also. And also program can use any math. math builtin :)
%% arith and logic.
%%
%% actually better is Von Newman machine!
%%
%%\end{verbatim}
%%
%%The simplest computer is the {Turing machine}~\cite{turing-wikipedia}.
%%It is a mathematical model representing the essence of computers.
%%It can simulate any computer algorithm.
%%
%%A Turing machine is a kind of automata with an {alphabet}, {states}
%%and {transition} rules, but with also an infinite {tape}
%%divided into {cells} as well as a {head} to read or write on this
%%tape at a certain location.
%%\l like human with access to lots of papers :) we are computers
%%\l it recognizes a language, like other automatons (regular, pushdown)
%%\l tape is very similar to memory. Also alphabet? conventions? ascii?
%%%
%%The transition rules, which can be seen as the program,
%%describe given a certain state, and what is currently on the tape
%%under the {head}, to which state to transition, and possibly
%%what to write on the tape under the head or how to move 
%%the head on the tape.
%%\l FIGURE
%%
%%Turing machines are good models for theoretical computer science 
%%questions, but they are not very practical as actual computers.
%%For one thing, they assume an infinite tape (it is hard to find
%%infinite memory).
%%Moreover, the program is hardcoded with the machine, which is not
%%very flexible.
%%\l and how enter program? and how build? what about alphabet?
%%One can design theoretically a {universal Turing machine} though, where
%%the program of this universal machine would act as a kind of {interpreter}. 
%%Indeed, given a certain {encoding} of the rules of another Turing machine
%%written on the tape, one could write transition rules which simulates
%%the execution of this other machine. 
%%
%%The universal Turing machine actually embodies many of the ideas
%%of actual computers.
%%
%%\begin{verbatim}
%%
%%This ultimately led
%%to the discovery of the {stored-program} concept.
%%
%% important idea is indeed though interpreter and encoding,
%% and stored-program concept. Can use binary logic for all of that.
%% See Shanon. Nand!
%%
%% state, transition, read/write tape. Can add interactive with keyboard
%% and screen!
%%
%% tecs can do simple interpreter with same core idea, via 
%% binary logic.
%%
%%\end{verbatim}




%%\chapter{Literate Programming Tutorial}

% noweb tutorial in appendix (alok suggested that too)

%% from tex.web:
%In this way, we are able
%to define each individual global variable when we are prepared to
%understand what it means; we do not have to define all of the globals at
%once.

% explain principles of chunk, chunk in pieces, chunk
% with ? because boilerplate not shown.

% also explain 
% - the type annotations: list<>, ref_own, shapes (next=, from=, owner=)
% - the context DSL: main-> <>, etc. x | y | z -> w
% - ??


\addcontentsline{toc}{chapter}{References}

%\chapter{Changelog}
%\label{sec:changelog}
% 0.1 first pretty complete draft
% 0.2 bootstrapping appendix (long)
% 0.3 ascii diagrams for software architecture and memory layouts (in appendix)
% 0.4 read by Knuth! he read it! he think it's a wonderful idea!
% Turing machine appendix??

%\chapter{Index}
%alok proposed ideas of index of all algos or DS. But they are few algos ...

\bibliography{docs/latex/Principia}
\bibliographystyle{alpha}

%******************************************************************************
% Postlude
%******************************************************************************

\end{document}
