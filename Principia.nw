\documentclass[]{article}

%******************************************************************************
% Prelude
%******************************************************************************
\newif\iffinal
\newif\ifverbose
\finalfalse\verbosefalse % see also other newif in Macros.tex

%------------------------------------------------------------------------------
% Packages
%------------------------------------------------------------------------------
\usepackage{verbatim}
%note: required by noweblatexpad for the \t \l \n in this file
\usepackage{fancyvrb}

\usepackage{url}
\usepackage{hyperref}
 \hypersetup{colorlinks=true}
\usepackage{cite}
\usepackage[pageref]{backref}
 \def\backref{{\footnotesize cited page(s)}~}
%\usepackage{cleveref} %\cref

%\usepackage{multirow}
\usepackage{booktabs} 
 \newcommand{\otoprule}{\midrule[\heavyrulewidth]}
\usepackage{graphicx}

%\usepackage{docs/latex/noweb}
% \noweboptions{footnotesizecode,nomargintag}
% %note: allow chunk on different pages, less white space at bottom of pages
% \def\nwendcode{\endtrivlist \endgroup}
% \let\nwdocspar=\par

\usepackage{xspace}

%------------------------------------------------------------------------------
% Macros
%------------------------------------------------------------------------------
\input{docs/latex/Macros}

%------------------------------------------------------------------------------
% Config
%------------------------------------------------------------------------------
%\usepackage[margin=1in]{geometry}
%\setcounter{tocdepth}{1}

\begin{document}
%******************************************************************************
% Title
%******************************************************************************
\title{
{\Huge 
Principia Softwarica
}\\
Literate System Programs for the New Millenium\\
version 0.2
}
\author{
Yoann Padioleau\\
\texttt{yoann.padioleau@gmail.com}\\
\\
with code from\\
Ken Thompson, 
Rob Pike, 
Dave Presotto, 
Phil Winterbottom,\\
Tom Duff,
Andrew Hume,
Russ Cox,\\
Xavier Leroy,
Fabrice Le Fessant,
Francois Rouaix,
Yoann Padioleau
}
% Other great programmers:
%# academics
% - Donald Knuth (tex, mmix)
% - Niklaus Wirth (pascal, oberon)
% - Andrew Tanenbaum (minix, amsterdam compiler kit?)
%# open source movement
% - Richard Stallman (emacs, gcc, ...)
% - Linux Torvalds (linux, git)
% - Bruce Evans (dev86)
% - Larry Wall (perl, patch)
%# exotic
% - Fabrice Bellard (qemu, fbcc, ffmpeg)
%# industry
% - Jeff dean
% - Ole agesen
% - Guy Steele (scheme, Java)


\maketitle 
\l The Elements of ...?
\l Literate Programs of an Entire Operating System (Plan 9 graphics & network)
\l Foundational literate programs

\hrule
\input{docs/latex/Copyright}
\hrule

\begingroup
\hypersetup{linkcolor=blue}
\tableofcontents
\endgroup

%******************************************************************************
% Body
%******************************************************************************

\section{Introduction}
\n What. Better to start with the golden Why? It has some implicit Why.
\n machine/computer, system/software/program/source code/code, process
\l software -> programs?

\principia is a serie of books explaining how things work in a
{computer} by describing with full details all the {source code} of all
the essential {software} used by a {programmer}.

Among those essential software are 
the {kernel},
the {shell},
the {windowing system},
the {compiler},
the {linker},
the {editor},
the {debugger}.
Each software will be covered by a separate book. 

Each software in \principia comes from a {literate program}, which is a document
containing both source code and documentation and where the code
is organized and presented in a way which is more adapted
for human comprehension than compiler constraints.
The final code and the book are derived both automatically from this literate
program. 
\l The code and its documentation are strongly connected.
\l \footnote{Section~\ref{sec:lp} more information about literate programming}
\l should I talk about plan9 already here?

The sections below will give a quick overview of the \principia programs.
They form together the foundation on top of which all applications can
be built.
Similar to {\em Principia Mathematica}~\cite{principia}, 
which is a serie of books
covering the foundations of mathematics, the goal of \principia
is to cover the foundational software.
\n principia informatica  = foundation of cs
\n principia algorithmica = taocp
\n goal: http://www.nytimes.com/library/books/042999best-nonfiction-list.html
Those software are mostly all {meta} programs, which are
programs in which the input and/or output are other programs.
For instance the kernel is a program that {manages} other programs,
the compiler is a program that {generates} other programs.

Those programs are also sometimes referred as {system software}
in opposition to {application software} (e.g. spreadsheets,
word processors, email clients), which we will not cover in \principia.


\subsection{Motivations}
\n Why

Why writing those books?
%
The main reason is that I, Yoann, have always been curious and always
wanted to understand how things work under the hood, fully, to the 
smallest detail.
Software is now running the world; it is thus important to
understand those software, to be computer literate,
and source code is what defines those software.

There are already lots of books explaining how computers work, explaining
the {concepts}, {theories}, and {algorithms} behind software such as
kernels, compilers, a few also about debuggers, but
all those books rarely explain everything with full details, which
is what source code is all about. 
%
There are a few books which
include the whole source code of the software
described, e.g. the books about Minix~\cite{minix}, XINU~\cite{xinu}, 
lcc~\cite{lcc}, 
but those books cover only
a few essential software, mostly always either
the kernel or the compiler, and they do not form a coherent set.

Enter \principia, a set of books covering all essential
software, in a coherent way. 
In addition to the kernel and compiler, \principia covers also
the graphics stack, 
the networking stack, 
the windowing system, 
the assembler, 
the linker, 
and many other foundational software
that have never been fully explained before to the best of our knowledge.
\l with full details, as deep as possible, with all the source code.

We want to demistify those software by showing their code
and by showing that they are actually not that complicated.
%
We hope to remove some of the mental barriers people have which
prevent them to extend the tools they use every day: text editors,
compilers, even kernels.
%
As a side effect it will also maybe help people imagine better systems.
Indeed it can be very intimidating to invent something completely new
if you have no clue that it is actually not that hard to build from
scratch a complete operating system.

Another motivation for those books, in addition to satisfy our curiosity,
as well as your curiosity,
is that we think you are a better programmer if you understand
how things work under the hood. 
%
In our opinion, you become a better C programmer
when you understand roughly what code generates the C compiler.
A good way to write more efficient code, or to avoid
writing really slow code, is to have some ideas of the
assembly code that will be generated for your code by
the compiler. 
%
In the same way, you can better use resources, e.g. memory, if you
have some ideas about how the kernel manages for you
these resources. You can better fix latency issues if
you understand how the networking stack works.
The \principia books can complete the excellent book
{\em Computer systems: a programmer's pespective}~\cite{cs-bryant}
by illustrating the many concepts this book introduces with concrete code.
\n but principia is builder perspective!

We also think it is easier to debug
programs if you better understand the environment in which
those programs evolve, if you understand the whole
software stack, and how things interact with each other.
%
Indeed, to fully understand certain error messages, e.g. from the kernel,
from the networking stack, or from the linker, it is very useful
to have some ideas about what those software do.
Moreover, even if in almost all situations the bug or the performance
issue is in your program,
it could also be sometimes in a core library, in the kernel,
in the compiler, or in the linker. 
It is very rare, but when
those situations happen, if you have no clue about
the environment in which you program runs, you will
never been able to fix your problem.
\l it happens at facebook, and those people like maurer, paul saab are valued

Finally, by showing code written by great programmers, we hope
you will learn how to write better programs. In other engineering
fields, it is quite common for the students to learn from the
work of the masters of their fields.
\l Hoare wanted that! (see Oberon book intro)


Here are a few questions we hope the \principia books will answer:
\begin{itemize}
\item What exactly happens when the user type [[ls]] in a terminal?
What is the set of software involved in such a command? What
is the trace of this command through the different layers
of the software stack, from the keyboard interrupt to
the display of text glyphs on the screen?

\item How a debugger works? Does it rely on special
services from the processor or from the kernel to implement
breakpoints?

\item How source code gets compiled, assembled, linked,
and finally loaded in memory? What is the memory image
of a program? How it relates to the original source code?
How a debugger can display debugging information on a binary?

\item Which software does memory allocation? The kernel?
The C library? How [[malloc()]] is implemented?
How it relates to the [[brk()]] system call?

\item How certain graphical elements, rectangles, ellipses, fonts, are
rendered on the screen? How the graphics card helps? How
the kernel helps? How things are intercepted
by the windowing system to make sure applications can not
draw in other windows?

\item What happens when you open a connection to another machine?
When you type a URL in your web browser? How one can
achieve a reliable communication on an unreliable physical network?

\item What does it take to port an entire operating system
on a new machine, e.g. on 
the Raspberry Pi\furl{https://www.raspberrypi.org/}?

\end{itemize}

\n list of courses that are never taught but should be!
%https://news.ycombinator.com/item?id=10201611

\subsection{The ideal teaching operating system}
\n How

%%Very few programmers actually fully understand how everything work, 
%%how things interact with each other. It is easy to find
%%programming languages specialists, or kernel specialists, but
%%it is very rare to find people who can write compilers and
%%at the same time have a very good knowledge of the kernel
%%or networking stack and can optimize things also at this level.
%%%
%%Such programmers though are tremendously useful in companies
%%because they can think at multiple levels and optimize things globally.
%%\l saab, maurer at facebook
%%It is even more difficult those days 
%%to find those knowledgable programmers as

The question now is which actual source code to present?
The code of the major operating systems, GNU/Linux, MacOS, and Windows
is gigantic, with hundreds of millions lines of code (LOC).
Here by operating system we mean not only the kernel but also
the programs which provide the platform for running application software
(the windowing system, the compiler, etc).
It is impossible to understand such large codebase.
\l even less to make books out of it
\l too bad because under all those layers are beautiful code, beautiful algos

We think there is hope though, that it is possible to understand fairly
well everything, that it is possible to answer all of the questions
mentioned in the previous section,
by focusing on the {essence} of those major operating systems.
We think it is possible to design a teaching operating system
with capabilities similar to the mainstream operating systems,
but with a fraction of their code size.

Here are the requirements for the ideal teaching
operating system we want to use as the basis for our
\principia books:

\begin{itemize}
\item {open source}: we want to show (lots of) code, 
and we want people to be able
to play with such code, to modify it, so our ideal operating
system must be open source

\item {small}: we want software which can be described in
books of reasonable size. The software needs to be a bit minimalistic.

\item {as simple as possible, but not simpler}: we want
small code, but we don't want to show toy code.
%
For instance 
ARM~\cite{arm-refman} 
is probably a simpler architecture than 
x86~\cite{x86-refman}
so it makes sense to present the code of an ARM assembler
rather than an x86 assembler,
\l still show same essence
but we don't want to show a toy assembler for a toy architecture.
%
In the same way we can present the code of a C compiler
without certain advanced state of the art
optimisations, to simplify the presentation,
but we don't want to show a toy compiler for a toy language.

\item {real}: it has to run on real machines, e.g. an x86 desktop,
a Mac laptop, a Raspberry Pi.

\item {complete}: it needs to not only have a kernel and a compiler
but also a graphics stack, a networking stack, a windowing system, etc.

\item {coherent}: the whole set of software must form a coherent
set, so that the interactions between those software can also be
described, succintely.

\item {self hosting}: we want to be able to improve,
to recompile, and to run everything under the system itself.
\l bootstrapping! you improve yourself from yourself!

\end{itemize}

\l jean louis gasse nostalgia about the time solo programmers could do huge
%  things, when things were still manageable (os, compiler, etc)
%http://www.mondaynote.com/2015/08/24/a-salute-to-solo-programmers/

We think this ideal teaching operating system actually already exists.
We think it is \plan.

\subsection{\plan}
\n =~ Where

\plan
\furl{plan9.bell-labs.com/plan9/}
is the successor of \unix. It was designed, from scratch,
by a small team of great programmers (Rob Pike, Dave Presotto, 
Phil Winterbottom) including the original creator of \unix (Ken Thompson).
Their goal was to redesign \unix to better integrate 
graphics and networking, which both became popular
after \unix was originally invented.
\l "Why is X so big" Ken thompson (http://c2.com/cgi/wiki?PlanNineFromBellLabs)
In some sense \plan is a kind of \unix 2.0.

The choice of \plan as the basis for our \principia books
may not be obvious, but it is in our opinion
the simplest and at the same time fairly complete operating system.
%
If you look at the screenshot of \plan in action in Figure~\ref{fig:plan9},
you'll see many features:
\begin{itemize}
\item a screen with basic graphics and 
multiple windows

\item multiple shells running independent commands at the 
same time

\item a simple clock graphical application

\item a simple program communicating through 
the network

\end{itemize}


\begin{figure*}
\includegraphics[angle=90, height=18cm]{plan9}
\caption{\plan in action}
\label{fig:plan9}
\end{figure*}
\t one showing efuns! with nice colors! one showing mmm would be nice too :)
\l WMP (not WIMP)

By comparison, if you look at 
\l screenshots of 
other systems,
you'll see that \plan in essence provides the same core services
than MacOS, GNU/Linux/X11, or Windows,
without certain bells and whistles,
\l multi tasking, graphics, windows, IO, etc
but with a codebase which is significantly smaller.
Indeed, our fork of \plan\footnote{\urlprincipia}, which includes all the
essential software described in all our \principia books, and even a 
few more programs, 
is less than 400 000 LOC.
\n via make loc from the root directory

The \plan programs are minimalistic but powerful, simple, and small.
The creators of \plan were not afraid to rethink everything,
not just the kernel, even if it meant not being backward compatible 
with \unix.
This led to software with more elegant designs, to the removal
of many ugly corners in \unix, 
\l russ cox: if switch from windows -> linux, see inconsistencies and
\l inelegance in windows, same when do linux -> plan9
and ultimately to less source code while still providing more services.

For instance, thanks to a few novel ideas in the kernel, namely
{per-process namespace}, 
{file servers} and
{union mount},
every application who wish to interact with the {console}
can just use the uniform [[/dev/cons]] device file, whether
this console is attached directly to the terminal, 
to a remote machine, or wether it is one of the terminal window
of the graphical user interface.
Thanks to this design, the \plan windowing system, [[rio]], 
could be implemented with only 10 500 LOC, including the code for
terminal windows.

By comparison, Linux and X Window introduced the separate concepts 
of teletype [[tty]] devices and pseudo [[pty]] terminals, 
and the code of [[xterm]], which is just a terminal 
for X Window, not X Window itself, has already 88 000 LOC.
This is partly because [[xterm]] carries the historical baggage of
standards invented in the 70's, e.g. special control sequences
for VT100 terminals. 

\plan contains all the essential software used by a programmer.
They form a coherent set because they were all designed from scratch
by a small team of programmers.
\n With the exception of the web browser? hmm they have one, just not complete

\subsection{The C language and ARM architecture}
\l Our basis, Our starting point, Our axioms? Our language?
\l ocaml for a few books
\n hopefully later a bit Rust, safer than C, 

The goal of books such as 
{\em The Elements}~\cite{elements} or
{\em Principia Mathematica}~\cite{principia} 
is to describe the foundations of a field starting from
a very small basis, e.g. a logic {language} with a small set of {axioms}
and inference {rules}, from which all the rest can be derived
or built.
\n Hobbes (Law), Spinoza (Ethics?), Newton (Physics) did their own Elements
%
The book {\em The Elements of Computing Systems}~\cite{tecs}
attempts a similar thing for computers,
starting only from the [[nand]] logic gate and building gradually
the [[and]], [[or]], and [[not]] logic gates, a multiplexer,
a flip-flop, memory banks, an additioner, an arithmetic 
and logic unit, and finally a simple processor.
\n ALU, CPU
\n nand2tetris has bootstrapping issues too. my hack assembler was in ocaml :)

In \principia we are interested in software though, not hardware.
Because most of the software described in the \principia books
are written in C, our basis in some sense
is the C programming language~\cite{k-r}.
C is a fairly large language, with non trivial semantics, and so 
our basis is unfortunately fairly large too.

Note that one of the \principia book describes the
C compiler, which targets the ARM~\cite{arm-refman} architecture. 
So, in principle
our basis could be reduced to the ARM machine language, which
is fairly simple. 
\l conceptually you could run the compiler in your head, a known bootstrapping
\l technique, because you are a human computer and C interpreter,
\l and so manually translate all the code, which reduced to ARM assembly
Another \principia book describes actually
an ARM emulator. But both the C compiler and the ARM emulator
are written in C itself, which brings us back to C as our basis.
\l also ocaml is written in ocaml lex/yacc too ... so big basis

A more elegant alternative, avoiding self-reference,
would be to start from a simple machine and
build a tower of increasingly powerful languages. Starting
from raw binary machine code, one could gradually build more 
sophisticated languages through a series of {bootstrapping} steps.
In fact such a project partially
exists\furl{http://homepage.ntlworld.com/edmund.grimley-evans/bcompiler.html},
but with already six steps its author was still far away from a language
as expressive as C.
\l hmm expressive is dangerous, turing tarpit
\l see appendix for possible bootstrapping of principia from scratch?

Another alternative, chosen by Donald Knuth
for his encyclopedic books {\em The Art of Computer Programming}~\cite{taocp}
was to choose as a basis a very simple computer he invented
called MIX, and a very simple assembly language, MIXAL.
Using assembly is maybe ok for describing algorithms,
but we think it would not be productive for writing entire
programs, and this would lead to very long \principia books.

We think that starting directly from the ARM machine, 
a real but fairly simple machine,
and the C language, a higher level language than assembly, 
is more practical for \principia.
\l more practical basis
\l Raspberry Pi target machine? Qemu?

\t in fact need to also add ocaml, and lex, and yacc as the basis ... hmmm

\subsection{Literate programming}
\n How

We want to show source code because code is the ultimate
explanation for what a software does.
%
We think though 
that showing pages and pages of listings in an appendix, 
as done for instance in the Minix book~\cite{minix},
\n also the case for oberon, xv6, unix v6
even when this appendix is preceded by documentation chapters, is not
the best way to explain code. We think the code and its documentation
should be mixed together, as done for instance in the Xinu book~\cite{xinu},
so one does not have to switch back and forth between
an appendix and multiple chapters.

Literate programming~\cite{lp-book} is a technique invented by
Donald Knuth to make it easy to mix code and documentation
in a document in order to better develop and better explain programs. 
Such documents are called {literate programs}. All \principia
programs are literate programs.

Note that literate programming is different from using 
{API documentation generators} such as 
javadoc\furl{http://www.oracle.com/technetwork/articles/java/index-jsp-135444.html} or 
doxygen\furl{http://www.stack.nl/~dimitri/doxygen/}. 
Noweb\furl{http://www.cs.tufts.edu/~nr/noweb/}, the literate
programming tool we will be using, does not provide the same kind
of services. 
\l cf sexp_int.mli, too many functions, given equal importance. 
\l could reorder, but then that's the point, use LP. 
\l Moreover 2 views will always be better than just one view.
%
Indeed, literate programming allows programmers to explain their
code in the order they think the flow of their thoughts and their code would
be best understood, rather than the order imposed by the compiler.
It allows among other things to explain code piece by piece, with the
possibility to present a high-level view first of the code,
to switch between {top-down} and {bottom-up} explanations, and to
separate {concerns}.
\l AOP a bit

For instance, the [[Proc]] data structure which we will
see in the \book{Kernel}, 
represents some information about a process. It is a huge structure
with more than 90 fields. Many of those fields are used only
for advanced features of the kernel. The C compiler imposes
to define this structure in one place. Noweb, which
can be seen essentially as a macro-processing language, allows
to present this structure piece by piece, gradually, in different
chapters. One can show first the code of the structure with 
the most important fields,
and delay the exposition of other fields to advanced topics chapters.
This greatly facilitates the understanding of the code, by not
submerging the reader with too much details first.

In the same way, the [[main()]] function in most
software is rather large and mixes together many concerns:
command line processing, error managment, debugging output,
optimisations, and usually a call to the main algorithm.
Showing in one listing the whole function would hide behind
noise this call to the main algorithm. The main flow
of the program though is arguably the most important thing to understand
first. Using literate programming
one can show code where the most important parts are kept, and where
other concerns are hidden and presented later. 

In fact, lots of the effort in writing the \principia books
has been in transforming the \plan software in literate
programs and in reorganizing again and again the \plan code to find
the best way, the best order, the best separation of concerns
in which we think the reader will more easily understand the code.


\subsection{Getting started}

To play with the different software described in the \principia books,
we recommend to use our fork of \plan. See \urlinstall.
\l what is special about this fork? less arch (but still 2), better orga, etc
Section~\ref{sec:code-orga} explains the directory structure of our
\plan repository.

To install our fork of \plan you'll need a machine with
a C compiler, a \unix-like operating system, and a kernel 
which can write on a VFAT filesystem. So, GNU/Linux and MacOS
are possible host operating systems you can use to
compile from scratch and then run \plan. \plan
is also a valid host as one can build \plan under \plan.
\l if want from scratch, see bootstrapping appendix

The installation consists first in cloning our fork of \plan
and cloning our fork of Ken Thompson's C (cross) compilers called [[kencc]].
\t why? because plan source code use special C, and also special assembler
\l =~ one included, but support unix/macos/plan9, but noise for principia
You can then compile [[kencc]] using a regular C compiler,
e.g. [[gcc]], from your host operating system, e.g. from MacOS or Linux.
Then you can {cross compile} the \plan kernel,
the \plan C standard library, and then the whole \plan operating
system, with all its libraries and software,
using the compiler ([[kencc]]) installed in the previous step.
\t install bin and lib and few other files on disk or virtual disk
This will build from scratch your own \plan{} {distribution}.
\l also need plan9port

Finally, you can run this \plan distribution either under 
Qemu\furl{http://www.qemu.org}, which makes it
easy to experiment, or you can also install it on a
real machine such as a Raspberry Pi.


\subsection{Requirements}

The \principia books are not introductions to programming,
computer science, or to any of its subfields.
%
For instance our book about the kernel is not an introduction
to operating systems. Indeed, we will assume the reader 
has already a vague idea of how an operating system
works and so is already familiar 
with concepts such as 
virtual memory, 
critical regions, 
interrupts, 
system calls, etc.

We will present with full details the source
code of different software, but we assume you already know
most of the {concepts}, {theories}, and {algorithms} behind those software.
The \principia books are there to cover the {practice}.
\n Software Practice and Experience style!
We assume our readers are mainly students in computer science with
a bachelor or PhD, who desire to consolidate their knowledge
by reading the ultimate computer science explanations: source code.

As said earlier, because most of the books are made of C source code, 
you'll need to have a good knowledge of 
the C programming language~\cite{k-r} to understand them.
\l also ocaml for a few books
%
Because most of the software we describe are \plan software, 
and because \plan has a lot in common with its ancestor \unix,
you'll need to be familiar with those systems.
We recommend to read~\cite{unix-pike} for \unix
and to read the tutorials [[docs/articles/9.ps]] and
[[docs/articles/names.ps]] available in our \plan repository
for \plan.

\subsection{Copyright}

Most of the software in \principia are \plan software with copyrights
from Lucent Technologies Inc. 
\t or ocaml software from INRIA
They are open source though; permission
is granted to copy, distribute and/or modify the source code.

\subsection{Acknowledgments}

We would like to acknowledge of course \plan's authors who wrote
in some sense most of the content of the \principia books: 
Ken Thompson, Rob Pike, Dave Presotto, Phil Winterbottom, Russ Cox, 
and many other people from
Bell Labs.

We would like also to thanks 
Xavier Leroy, Fabrice Le Fessant, and Francoix Rouaix, 
and many other people from 
INRIA Rocquencourt 
who wrote the OCaml programs we describe in \principia.



\section{Overview}

Before delving in the description of the different \principia
programs in the next sections,
we first give an overview of how the code is organized
and how the different programs depend on each other.

\subsection{Code organisation}
\label{sec:code-orga}

Table~\ref{tab:code-orga} presents a short description
of the main directories in our \plan fork
as well as the corresponding sections in this document
in which the software associated with the directory is discussed.
%
Table~\ref{tab:other-dirs} presents the remaining, less important,
directories.

A few \plan software have some architecture specific parts,
with support for x86 and ARM in our \plan fork.
The LOC column in Table~\ref{tab:code-orga} accounts only for 
the code to support one of the architecture: the ARM.
This is the only code we will show in the \principia books.
\t except for the kernel for now

\begin{table*}[tbh!]
\begin{tabular}{lllr}
\toprule
{\bf Directory} & {\bf Description} & {\bf Section} & {\bf LOC} \\
\otoprule
[[kernel/]] & The \plan basic kernel (for ARM and x86) & \ref{sec:kernel} & 60 000 \\
            & $+$ graphics stack (kernel code) & \ref{sec:graphics}  & 10 000 \\
            & $+$ network stack (kernel code) & \ref{sec:network} & 23 000 \\
[[include/]] & The header files (e.g. [[core/libc.h]]) & 
   \ref{sec:libc} & 5 500 \\
% no x86, just ARM
[[lib_core/]] & The core C library (for ARM and x86) & \ref{sec:libc} & 21 500 \\
% no x86, just ARM
[[shells/]] & The shell & \ref{sec:shell} & 7 500 \\

\midrule
[[compilers/]] & The C compiler (for ARM and x86) & \ref{sec:compiler} &25 000 \\
% no x86, just ARM
[[assemblers/]] & The ARM and x86 assemblers & \ref{sec:assembler} & 4 100 \\
% includes also include/arm/5.out.h, no x86 just ARM
[[linkers/]] & The ARM and x86 linkers & \ref{sec:linker} & 9 600 \\
% includes also include/debug/a.out.h and a few other header files, no x86
[[machine/]] & The ARM emulator & \ref{sec:emulator} & 4 400\\
[[languages/]] & The OCaml bytecode compiler/interpreter & \ref{sec:ocaml} & 30 000\\
% in ocaml, but not the native code compiler, nor external/libs nor debugger

[[generators/]] & The code generators Lex and Yacc & \ref{sec:generators} & 3 500\\
% in ocaml

\midrule
[[editors/]] & The editor & \ref{sec:editor} & 10 000\\
% in ocaml, but not the pfff major modes
[[builders/]] & The [[mk]] tool & \ref{sec:builder} & 4 800 \\
[[debuggers/]] & The debuggers [[db]] and [[acid]] & \ref{sec:debugger} & 17 000\\
% no x86 just ARM
[[profilers/]] & The profilers & \ref{sec:profiler} & 4 900 \\

\midrule
[[lib_graphics/]] & A large part of the graphics stack & \ref{sec:graphics} & 16 000 \\
% does not include kernel/, but include include/graphics/
[[windows/]] & The windowing system & \ref{sec:windowing} & 10 500\\

\midrule
[[lib_networking/]] & A small part of the network stack & \ref{sec:network} & 1 000\\
[[networking/]] & Networking applications, clients and servers & \ref{sec:network} & 75 000\\
[[browsers/]] & The web browser & \ref{sec:browser} & 24 000\\
%ocaml

%\midrule
%[[interpreters/]] & The Scheme interpreter & \ref{sec:scheme} & 4 600\\

\midrule
[[utilities/]] & Utilities such as [[ls]], [[cp]], [[mv]] & \ref{sec:utilities} & 16 000\\
               & [[grep]], [[gzip]], [[tar]], [[sed]], [[diff]], & \\
               & [[xargs]], [[ps]],  etc & \\
\otoprule
Total & & & 359 300 \\
\bottomrule
\end{tabular}
\caption{Main source code directories of our \plan repository}
\label{tab:code-orga}
\end{table*}
\n for the LOC it's a mix of cm -test_loc CROSS xxx/, or make loc in the dir


\begin{table*}[tbh!]
\begin{tabular}{llll}
\toprule
{\bf Directory} & {\bf Description} & \\
\otoprule
[[docs/]] & Articles and man pages  \\
[[ROOT/]] & The target directory where compiled binaries and libraries will \\
          & be installed which will form the \plan{} {distribution} \\
[[sys/]] & Backward compatible directory containing \\
         & mostly symbolic links and build scripts  \\
[[ape/]] & ANSI and POSIX environment, to ease \\
         & the compilation of legacy \unix applications & \\
[[CROSS/]] & Scripts to support cross compilation  \\

%\midrule
[[security/]] & Security, authentification, cryptography \\
[[lib_math/]] & Arbitrary precision mathematics library \\
[[lib_misc/]] & String, regexps, and compression libraries  \\
[[typesetting/]] & Tools to generate documents [[troff]], [[tbl]], [[pic]], [[grap]], [[eqn]] and [[man]] \\
[[database/]] & Simple key-value database, mostly used for storing network information \\
[[games/]] & Video games \\
[[applications/]] & Small applications \\
[[BIG/]] & software of interest not yet included \\
[[version_control/]] &  \\
[[lib_audio/]] & \\
\bottomrule
\end{tabular}
\caption{Other directories of our \plan repository}
\label{tab:other-dirs}
\end{table*}


\subsection{Software architecture}
%old: was Architecture overview but oxymoron, archi is already an overview

Many of the \principia programs are mutually dependent
on each other. Indeed, to {run} a compiler or an editor you need
a kernel (and a shell), but to {create} this kernel in the first
place you need an editor and a compiler.
In the same way, the C compiler uses code from the 
core C library, but to create this library you need a C compiler.
In fact the C compiler is written in C itself, so there are even
self dependencies.
\l Figure with all deps, split binary/source so partially solve mutual deps
\l bootstrapping appendix link?

It is possible though to layer things when one looks more
at how things are organized in memory.
\l rather than what depends on what codewise or runtimewise
\t Big figure with everything
A first separation to be made is 
code running in {kernel space} versus 
code running in {user space}.
%
Most of the code running
in kernel space is in [[kernel/]], as well as some code
in [[lib_graphics/]] and [[lib_networking/]]. Some of the
code in the C library, the memory pool library
\footnote{[[lib_core/libc/port/pool.c]]}
and a few utility functions and globals, are used both in the kernel and in user
programs but they are the exceptions.
The rest of the codebase runs in user space.

The boundary between user programs and the kernel is provided
by the system calls API. The functions of this API, as well
as many utility functions, are declared 
in [[include/core/libc.h]].
The user space parts of the system calls are implemented in 
[[lib_core/libc/9syscall/]]. This last directory
contains one assembly file per system call and each of those files contain
mostly the software interrupt instruction with a special code to
be dispatched to the appropriate code in the kernel.
The dispatcher kernel code is in [[kernel/syscalls/]].
\l figure with content of one such file for ARM?
Some of those system calls are process related,
e.g. [[rfork()]], [[exec()]], [[exit()]]
\footnote{with implementations in [[kernel/processes/]]},
some are memory related, e.g. [[brk()]]
\footnote{with implementation in [[kernel/memory]]},
and other are used for file input and output (IO),
e.g. [[open()]], [[close()]], [[read()]], [[write()]]
\footnote{with implementation in [[kernel/files/]]}.
\l table with all syscalls and a description?

In fact, those IO system calls provide an extended way
for programs to request services from the kernel
via the filesystem hierarchy. Indeed, under \unix everything
is a file, including devices. 
A process using
\l opening and reading or writing
the [[/dev/cons]] file will trigger code in the kernel
in [[kernel/console/devcons.c]] which itself will trigger
code which handles the keyboard device in [[kernel/devices/keyboard/]]
(when reading from [[/dev/cons]])
or the screen device in [[kernel/devices/screen/]]
(when writing to [[/dev/cons]]).


The concept was pushed even further under \plan where everything is 
a {file server}.
\l hard to understand now, cite?
The graphics API is accessible though files under [[/dev/draw/]]
and triggers code in [[kernel/devices/screen/devdraw.c]] and
[[lib_graphics/]].
In a similar way the networking API is accessible through files
under [[/net/]] and triggers code in [[kernel/network/]].

A second separation to be made is
library code versus
application code.
All the user programs in \plan rely first on the core C library
in [[lib_core/libc/]], exposed via the [[include/core/libc.h]] header
file. All programs are linked with [[libc.a]]. 
The other [[lib_xxx/]] directories contain other general purpose functions
and data structures which are used by different programs. By
using libraries, source code can be reused more easily.

The shell is a user program using the C library.
The assembler, linker, and compiler also rely on the C library
as well as other header files, e.g. [[include/arch/arm/5.out.h]]
which declares the set of ARM opcodes.

All the graphical applications, e.g. the clock but also the windowing
system rely also on the [[lib_graphics/lib_draw/]] library, which
is exposed in the [[include/graphics/draw.h]] header file.
The graphics library is essentially a thin wrapper over the protocol
used by the [[/dev/draw/]] device files.

\l like in cs illuminated, show the different layers. information/.../...

% \subsection{Books structures? Serie structure?}


\section{The Core System}

We now switch to quick descriptions of the \nbbooks software, and
so \nbbooks books, composing the \principia serie.
We grouped those software in 6 different sections.

The first group, which we call the {core system}, is made of the minimal
set of software which are needed to reach the point where the programmer can 
interactively launch other programs.
In the case of \plan this minimal set is made of 
the kernel, 
the shell, and the
standard C library.

The C library is necessary because it is used by the shell.
Indeed, the library provides the necessary {bridge} to call the kernel 
from user programs.
The library also provides memory allocation routines and a few
general utility functions. In fact, a small part of the C library
is also used internally by the kernel.

\l figure? showing relation between kernel/shell/libc?

\subsection{The kernel, [[9]]}
\label{sec:kernel}

The kernel is the program that {manages} all the other programs.
%
It is arguably the most important program; without the kernel
no other program can run. 
\l with the compiler
%
The kernel provides
the main abstractions of the computer and
{demultiplex} its resources (the processor,
the memory, the input and output devices)
to multiple programs at the same time.

The kernel is the biggest program and so the biggest book in the serie.
The \plan kernel is called simply [[9]].

\subsection{The core library, [[libc]]}
\label{sec:libc}

The C library 
contains code which is {used} by all the other 
programs (including the kernel).
%
In the case of \plan this library contains
the memory allocation routines ([[malloc()]], [[free()]]),
the bridge to call the kernel (the system calls),
the unicode functions, and many other
utility functions.
\l also describe thread library

\subsection{The shell, [[rc]]}
\label{sec:shell}

The shell is the program that allows users to {launch} other programs.
It is the primary user interface of \plan,
the so called {command line interface} 
(we will see later another interface).
\l a graphical user interface

In \plan, and also in its ancestor \unix, the shell is a regular
user program; it does not have to be included in the kernel.
The user can actually change shell without changing the kernel.

The shell will be the first user program we will describe in the
\principia serie. It illustrates many features
provided by the kernel because it is using internally many
system calls ([[rfork()]], [[exec()]], [[chdir()]], [[pipe()]], etc).

The \plan shell is called [[rc]] (for [[r]]un [[c]]ommand).
\l requiring compiler/assembler/linker (and actually relying also on lex/yacc)


\section{The Development Toolchain}

Once you have a terminal where you can launch programs,
you'll need tools to produce
{binary} programs from 
{source} code. 
%
The {development toolchain}
is a set of tools working together to produce
{executables}.
This toolchain is made essentially of
a compiler,
an assembler,
and a linker.

\n could be part of core system because need compiler to compile the kernel

\l figure? showing the compilation pipeline?

\subsection{The C compiler, [[5c]]}
\label{sec:compiler}

A compiler is a program that transforms source code
written in one high-level language (e.g. C), into another 
lower level language (e.g. assembly).
%
The C compiler is probably the most important program 
in \plan after the kernel. Indeed, the \plan kernel itself is written
in C and so needs a C compiler to become executable by a machine.

We will describe [[5c]], the C compiler targeting
ARM assembly. The [[5]] comes from the \plan convention to name architecture
with a number or single letter (0 is MIPS, 5 is ARM, 8 is x86, etc).
This is why the ARM assembler and linker we will
describe later are called respectively [[5a]] and [[5l]].
\l written in C, looping issue, bootstrapping.

%\subsection{The macro processor, [[cpp]]}

\subsection{The assembler, [[5a]]}
\label{sec:assembler}

An {assembler} is a kind of compiler;
it translates source code written in an {assembly} programming language
into {machine code}, or into an {object code} close to machine code.
%
Some of the kernel code, as well as code in the C library, is written
in assembly.

To be consistent with the compiler, we will 
describe [[5a]], the \plan ARM assembler.
\l 5a is actually written in C :) and also use yacc

\subsection{The linker, [[5l]]}
\label{sec:linker}

A linker is a program taking as input multiple files
containing object code, called {object files},
generated either by the assembler or compiler, and which creates
the final executable.
%
In theory, compiling, assembling and linking could be done by
a single program. The C compiler could take as input multiple C source
files and produce directly an executable. 
\n ocamlc kinda does that.
But from a software engineering
point of view, it is better to separate concerns and have three separate
tools for those three separate tasks.
\n and indeed ocamlc has separate code for the compiler, interpreter, linker

We will describe [[5l]], the \plan ARM linker.

\subsection{The processor emulator, [[5i]]}
\label{sec:emulator}

An emulator is a program that 
act like a computer and can {execute} another (binary) program.
%
An emulator is not really a part of the development toolchain but
because the assembler, linker, and compiler
target an architecture, it is useful to describe
the {instruction set} (ISA) of an architecture via
its emulator.

We will describe [[5i]], an ARM emulator, which can
be seen as a kind of interpreter for a low-level
language, hence the [[i]].
In fact, [[5i]] can also act as a (slow) debugger and profiler.


\subsection{The OCaml bytecode compiler/ interpreter, [[ocamlc/ocamlrun]]}
\label{sec:ocaml}

C is a great programming language. 
\l huge improvment over assembly
It is arguably the best language to implement system programs
such as kernels, virtual machines, just-in-time compilers, etc.
\l maybe Rust is better now, or modula-3, but not C plus plus for sure
It can also be used as a portable assembler, which makes it
a great language to implement efficiently core libraries
(which can even be used by other programming languages).
%
For many applications though, especially applications
without strong constraints on memory or speed, it can
be far more productive for the programmer to use higher-level
languages. Programming in C is indeed very error-prone, with
recurring bugs such as buffer overflows, segmentation faults,
or security holes.

This is why a few software in the \principia serie 
\n lex, yacc,  efuns, mmm (and ocamlc itself)
are written in the OCaml programming language
\furl{http://ocaml.org/}.
OCaml is a statically typed functional language, more expressive
and less error-prone than C. It is almost impossible to
have many of the bugs mentioned above while programming
in it.
%
Just like we prefer, when possible, to describe programs written in C
rather than assembly, because the resulting code is smaller
and easier to undersand, we also prefer, when possible, to describe programs
written in OCaml rather than C, because the resulting code is also smaller
and easier to undersand.
\l some stuff has to be in assembly, some has to be in C, for the rest: ocaml
\l also turns out were lucky and I liked efuns and mmm
\l great for compiler stuff actually :) in fact lex and yacc easier in ocaml
\l ultimate lang IMHO, even in 2015!

For completeness, we need then to present also the code of an OCaml compiler
or interpreter. Interestingly, OCaml uses a mixed approach. Indeed,
the OCaml system
includes first a compiler, [[ocamlc]],
written in OCaml itself,
\l hmm bootstrapping again, bigger basis now
generating {bytecode} for a virtual machine,
instead of object code for a real machine. Then, a bytecode
interpreter, [[ocamlrun]], written in C, implements this
virtual machine and so can run the bytecode generated by the compiler.
Any machine with a C compiler can then by transitivity execute
OCaml code, making OCaml as portable as C.

\subsection{The code generators, [[ocamllex]] and [[ocamlyacc]]}
\label{sec:generators}

Code generators are programs that {generate} other programs.
%
We will describe two such generators, Lex and Yacc.
They provide both a domain specific language (DSL) to
help respectively scan and parse languages.
They are used by a few other programs in \principia:
the assembler, the C compiler, the shell, the build system,
which is why we consider Lex and Yacc essential software.
\n actually lex is not used that much
The full understanding of the C compiler for instance would
be incomplete if one could not understand the code generated
from the C grammar specification by Yacc.

There are many clones of Lex and Yacc written in many
different languages. The [[lex]] and [[yacc]] programs
included in \plan are actually not really \plan software;
they are the original [[lex]] and [[yacc]], written in C,
copied without much modification directly from \unix.
Because they were written a long time ago, their C source code
is not as clear as the code of the other \plan software.
%
This is why we decided to present instead [[ocamllex]]
and [[ocamlyacc]], some Lex and Yacc clones written in OCaml.
\l and part of ocaml distrib for lex, and written by me for yacc
\l also self-reference, bootstrapping too, lex and yacc use lex and yacc :)


\section{The Developer Tools}

The {developer tools} are a set of tools which are not
strictly necessary to produce programs, like the software
development toolchain we've seen in the previous section, but which 
are really useful in the software development process.
%
Those tools are
the text editor,
the build system,
the debugger,
and the profiler.

\l figure? showing the source code at the center and the tools around?

\subsection{The text editor, [[efuns]]}
\label{sec:editor}

The text editor is a program to help {write} programs.
\l actually also to help read
%
It is perhaps the most important tool for a programmer.
Indeed, even if the kernel and compiler are essential
to run and produce software, their source code was first
entered in a computer by a programmer using a text editor. In fact, 
text editors are used to enter the code of text editors, leading to
bootstrapping issues, just like for a compiler.
\l link to appendix again?
\l Without an editor you would have to read or modify source code with
\l tools like [[cat]], [[ed]], etc. or even worse use punch cards.

The text editor is certainly also the program to which programmers are the
most emotionally attached to. 
\l editor wars
In fact, this is the main reason we chose
%, for the first time in the \principia serie,
to not present a \plan program,
even though \plan has a few text editors ([[ed]], [[sam]], and [[acme]]),
but instead to present an Emacs clone called [[efuns]]
\furl{https://github.com/aryx/fork-efuns}.
%
Another deviation is that this program is not written in C but in OCaml.
%%%a language arguably more expressive and convenient than C to code
%%%such programs. 
%%%Note that one of the previous \principia book describes an OCaml compiler. 

The text editor will be the first graphical program in the \principia serie.
Indeed, until now all the programs we described were command line programs.
The graphics stack will be described later.


\subsection{The build system, [[mk]]}
\label{sec:builder}

The build system is a program to help automate the {compilation} of programs
by calling appropriately the tools from the development toolchain.
\l such as the compiler, the assembler, the linker, etc.
%
The whole \plan operating system can be built from scratch
with one simple command, [[mk all]], thanks to the build program
called [[mk]] and a few configuration files (the [[mkfile]]s).

\subsection{The debugger, [[db]]}
\label{sec:debugger}

The debugger is a program that {commands} and {inspects} another program.
%
Programming is so difficult that invariably we make mistakes
when writing code. Having tools to help find those bugs is essential
and the debugger is the most important of those tools.
Even if many programmers, including great programmers\cite{coders-at-work},
use just [[printf()]] tracing commands to debug their programs, 
we think a debugger can greatly accelerate the time it takes to find bugs. 
%
Debuggers are also great tools to help understand programs, especially programs
written by other people.

The \plan debugger is called simply [[db]].
\l also acid, and strace.

\subsection{The profiler, [[prof]]}
\label{sec:profiler}

The profiler is a program that {generates statistics} about another program.
%
It is mainly used to optimize code by finding where the program
spends most of its time. It can be useful also as a tool to find bugs
because unexpected statistics can sometimes be good hints to
fix code where the programmer was expecting different results.
\l cf jon bentley

\t will see prof? tprof? 

%subsection{The version control system, [[gitocaml]]}

\section{Graphics}

Up until now we have mostly seen {command line} tools
interacting with the programmer through simple text-based terminals.
One of the most important invention in computer science though is the 
{graphical user interface} (GUI). Even regular
command line tools benefit from a graphical user interface
as one can run multiple tools in different {windows}
at the same time.
\l alternative is screen/tmux/virtual-terminals under linux/...

\l figure? showing screenshot and what the graphics stack does and what rio do?

\subsection{The graphics stack, [[/dev/draw]]}
\label{sec:graphics}

The graphics stack provides the basis on top of which
graphical applications can be built.
GUI elements such as menus,
windows, cursors, lines, or texts are ultimately rendered
on the screen using simple graphic operations provided
by the graphics stack.

Under \plan the graphics services are accessible through
the [[/dev/draw/]] device directory which is connected
to the screen device driver in the kernel. 
\l a bit ugly design, lots of graphic code in the kernel

\subsection{The windowing system, [[rio]]}
\label{sec:windowing}

One of the most important graphical application is
the windowing system. In some sense it
is an extension of the kernel and the shell; it is a
program that also {manages and launches} other programs,
represented visually by separate {windows}.

The \plan windowing system is called [[rio]]. It is essentially
a demultiplexer of the [[/dev/cons]], [[/dev/mouse]], 
[[/dev/keyboard]] and [[/dev/draw]] device files.
[[rio]] is a {file server} that internally uses 
the mouse, keyboard, and screen {physical} devices,
and provides a {virtual} mouse, virtual keyboard,
and virtual screen to its windows using
{views} of those same device files
(thanks to the per-process namespace feature in the kernel).
In fact, an unusual feature of [[rio]] is that it can be run inside itself.
\l small xterm, beautiful, also as powerful as X11

\section{Networking}

Up until now we have described software which can run on 
isolated machines. We'll now switch to programs
that can communicate with each other on different machines.
%
Just like the graphical user interface, networking
(and internetworking a.k.a Internet)
has been one of the most important invention in computer science.
it has led ultimately to the creation of the {Web} where networking
programs (web browsers and servers) made possible great things,
e.g. the worldwide collaborative effort Wikipedia.

\l figure? ???

\subsection{The network stack, [[/net]]}
\label{sec:network}

The network stack is a part of the kernel, fairly
large, which provides the necessary abstractions
for programs to communicate with each other on
different machines, using different protocols.

Under \plan the network services are accessible through
the [[/net/]] device directory.
\l small NFS, beautiful

\subsection{The web browser, [[mmm]]}
\label{sec:browser}

There are too many networking protocols 
and too many networking programs (clients and servers)
to present. UUCP, Email, News, FTP, Telnet, Gopher, are all
very useful networking programs. We decided though to focus
on what we think is the most important one: the web
browser. One can actually use Email, News, FTP, and
many other things using just a web browser. It has become
so versatile that it can be considered almost an operating
system on its own.
\l in fact OS papers about web browsers now, and can run linux in it

One of the reason \plan didn't get a wider adoption was that
it didn't include a real web browser. A few prototypes
were written but they were too limited. Moreover, because \plan
is not compatible with \unix, it was difficult to port
web browsers such as Firefox to \plan.

We will describe [[mmm]]
\furl{http://pauillac.inria.fr/mmm/}, 
a web browser written in OCaml.
It is also more limited than popular web browsers such
as Firefox or Chrome, but we think its codebase is far
easier to understand and present.

%%%\section{Advanced Languages}
%%%
%%%We will present below two languages more expressive
%%%and less error-prone than C. It is almost impossible to
%%%have the bugs mentioned above while programming
%%%in those higher-level languages.
%%%
%%%\subsection{The toy frontend compiler, [[tigerc]]}
%%%\subsection{The backend compiler, [[c--]]}

%%%\subsection{The Scheme interpreter, [[s9]]}
%%%\label{sec:scheme}
%%%
%%%Scheme, a dialect of Lisp,
%%%is a minimalistic but very powerful dynamic functional language. It has
%%%a very small set of orthogonal features which can be combined in many
%%%different  ways. In fact, its 
%%%full specificiation~\cite{r4rs} has 55 pages.
%%%This fits well the minimalistic approach of \principia.
%%%
%%%We will describe [[s9]]
%%%\furl{http://www.t3x.org/s9book/}
%%%a scheme interpreter written in C which was written for education purpose.
%%%In fact, a literate programming book~\cite{s9book} has
%%%already been written about it.

%%%\subsection{The OCaml compiler}

%%%\section{Video games}

%%%\subsection{[[soldat]]}
%%%\subsection{[[quake]]}



\section{Utilities}
\label{sec:utilities}

In addition to all the software mentioned above,
a programmer very often use small utilities to perform or automate
certain tasks. 
The code of those utilities is usually pretty small as those utilities
have often a single and simple function.
One of the \principia books will
be dedicated to describe the code of a few of those utilities,
the most important one for the programmer.

\subsection{File and directory utilities, [[ls]], [[cp]], etc}

The shell has very few builtins. To create, delete, or modify
files or directories a programmer needs to use special
programs. Those programs are essentially small wrappers
around the file and directory system calls provided by the kernel.

Here are the file and directory utilities we will describe:
\begin{itemize}
\item [[touch]] and [[mkdir]]
\item [[cat]] and [[ls]]
\item [[rm]], [[cp]], [[mv]]
\item [[chmod]], [[chgrp]]
\l [[mtime]] ??
\l find?
\end{itemize}

\subsection{String processing utilities, [[grep]], [[sed]], etc}

Source code is stored as plain text, which are a set of lines, which
are a set of strings.
It is thus normal that string processing utilities
are very often used by programmers to either search, modify,
or compare text.

Here are the string processing utilities we will describe:
\begin{itemize}
\item [[grep]], 
\l g re p,  global regexp p?
one of the most versatile and useful tool for a programmer.
It can be used to find code using {regular expressions}.
\item [[sed]] can be used to help refactor code
\l stream editor
\l and [[tr]]
\item [[diff]] can be used to compare different versions
of the same codebase
\l very useful programmer tool, also dual of patch
\end{itemize}
\l awk?

\subsection{Process utilities, [[ps]], [[kill]], etc}

Source code ultimately is transformed into 
binary programs which ultimately become 
processes when run. 
%
A programmer needs a set of utilities
to manipulate those processes. Under \plan the [[/proc]]
directory can be used to inspect and manipulate those processes.
An alternative is to use command line programs which
are often small wrappers over [[/proc]].

Here are a few process utilities we will describe:
\begin{itemize}
\item [[ps]] and [[pstree]]
\item [[kill]]
\l actually it's a script
\l \item [[sleep]]
\end{itemize}

\subsection{Archive utilities, [[tar]], [[gzip]]}

Once a program has been written, a programmer often wants
to share it with other people.
In this case, he can count on a few utilities to package
and compress code.

Here are a few utilities used to help disseminate software:
\begin{itemize}
\item [[tar]]
\item [[gzip]]
\end{itemize}


%\subsection{Misc utilities}

%\subsection{Date utilities}

%\begin{itemize}
%\item [[date]]
%\item [[cal]]
%\end{itemize}

%\subsection{Byte utilities}

%\begin{itemize}
%\item [[dd]]
%\item [[split]]
%\item [[xd]]
%\end{itemize}

\section{Applications}

We decided to limit ourselves in \principia to system programs
which are the most relevant to the programmer.
We do not cover application software. We 
do not describe the code of applications such
as spreadsheets, word processors, calendars, email clients, 
or video games.

We encourage though other people to find such
applications with a small codebase (using 
a minimalistic approach), and to write and publish 
their literate programs.


\section{Related Works}

Here are a few operating systems that were considered for \principia
but which were ultimately discarded:
\n (helps to appreciate even more how good our plan9 choice is):
\n dup: readme.txt

\begin{itemize}

\item \unix V6 (Ken Thompson et al.)
\furl{http://minnie.tuhs.org/cgi-bin/utree.pl},
fully commented in the classic book by John Lions~\cite{lions},
\l tuhs = the unix heritage society
or its modern incarnation xv6
\furl{http://pdos.csail.mit.edu/6.828/2014/xv6.html},
are great resources to fully understand a \unix kernel.
But the kernel is too simple: there is no support for graphics
or networking.

\item XINU (Douglas Comer)
\furl{http://minnie.tuhs.org/cgi-bin/utree.pl?file=Xinu7},
fully documented in two books~\cite{xinu,xinu2}
has a network stack but the kernel
is still too simple with no virtual memory for instance.
\l and has multi processor support?

\item Minix (Andrew Tannenbaum et al.)
\furl{http://minnie.tuhs.org/cgi-bin/utree.pl?file=Minix1.1},
also fully documented~\cite{minix},
is fairly small but it is just a kernel. Minix does not provide
for instance its own windowing system; it relies on X Window instead
which is far more complicated than \plan windowing system.


\item GNU/Linux/X11 (Richard Stallman et al.)
\furl{http://www.gnu.org/}
\furl{http://www.kernel.org/}
\furl{http://www.freedesktop.org}
is far bigger than \plan.
If you take the source code of 
the Linux kernel, 
the GNU C library (glibc),
the bash shell,
the GNU assembler ([[gas]]) and linker ([[ld]]) part of the binutils package,
the GNU C compiler ([[gcc]]),
the Emacs editor, 
GNU make,
the GNU debugger ([[gdb]]), 
the GNU profiler ([[gprof]]),
the GNU Lex and Yacc clones ([[flex]] and [[bison]]),
and X11, you'll get
orders of magnitude more source code than \plan, even though
\plan provides in essence the same core services. 
Of course the Linux kernel contains thousands of specific device drivers,
gcc handles a multitude of different architectures, and X11
can support lots
of graphic cards; all of those things could be discarded when
presenting the core of those software. But their core is still far
bigger than the equivalent core in \plan.
\n Linux 1991, Plan9 1992 (but open source only in 2000)



\item Hack (Noam Nisan and Shimon Shocken)
\furl{http://www.nand2tetris.org/},
is a toy computer introduced in the excellent
book {\em The Elements of Computing Systems}~\cite{tecs}.
This book is great for understanding processors, assemblers,
and even compilers, but the operating system part is really too simple.


\item MMIX (Donald Knuth)
\furl{http://www-cs-faculty.stanford.edu/~uno/mmix-news.html}
and its ancestor MIX
are computers designed by Donald Knuth
and used in his classic book serie 
{\em The Art of Computer Programming}~\cite{taocp}.
A book using literate programming, {\em MMIXware}~\cite{mmixware},
has even been written
to explain the full code of the MMIX simulator and assembler.
But, similar to Hack, very few software has been written for
this machine. For instance the book assumes the presence of an operating
system called NNIX, but nobody has ever written it.


\item STEPS (Alan Kay et al.)
\furl{http://vpri.org/html/writings.php}
is a project to reinvent from scratch programming.
It has a far more ambitious goal than \principia:
write a full operating system in 20 000 LOC. It is 
unfortunately not finished yet.
\l the kernel book is 30 000 Lines, and it's just the kernel (and actually not all of it), so hmmm

\item Oberon (Niklaus Wirth)
\furl{http://www.projectoberon.com/} 
is a kernel,
compiler, 
and windowing system 
designed from scratch. It is
a great operating system, very compact, and fully documented in a 
book~\cite{project-oberon}.
It imposes though strong restrictions on the programmer:
only applications written in Oberon can be run. This simplifies
lots of things. But, operating systems like
\unix (and \plan) are more universal: they
can run any program, as long as your program was compiled into a binary.
\l  javascript emulator: http://schierlm.github.io/OberonEmulator/

\item TempleOS (Terry A. Davis)
\furl{http://www.templeos.org/} is an operating system
single handedly created over a decade. It contains a kernel,
a windowing system, a compiler for a dialect of C, and
even some games.
It has graphics capabilities but there is no network support.

\end{itemize}

\l too many other to list here, see the comment in the .tex
%\item Squeak (Alan Kay et al.),
%\furl{http://squeak.org/}
% - nuttx, ils ont meme une graphic stack, widget, window manager
% - http://www.homebrewcpu.com/, full stack too, LCC, minix 2, tcp/ip, etc
% - RECC collection (Robert Elder)
%   http://recc.robertelder.org/ 
% - short canonical programs:
%   http://aosabook.org/blog/2014/01/call-for-technical-reviewers-for-500-lines-or-less/
%   https://github.com/aosabook/500lines
% - A tiny hand crafted CPU emulator, C compiler, and Operating System
%   https://github.com/rswier/swieros
% - http://menuetos.net/, written in assembly
% - https://moonforth.github.io/
% - boyer moore special machine, assembler, etc

\section{Conclusion}

We hope the \principia books will greatly consolidate
your computer science knowledge and give you a better and
more complete picture of what is going on in your computer.

We think the \principia programs form together the minimal foundation
on top of which all applications can be built. Even though there are
\nbbooks books in the serie, we still think it is the
minimal foundation. It is hard to remove any of those software
because they all depend on each other.
%
Of course, you need to rely first on a kernel (hence the name), but
the shell and the C library are also essential. 
% => 3
But those software
are coded in C and assembly so you need a C compiler and
an assembler, and because source code is usually split in many
files you also need a linker.
\l But to run those tools you need a kernel :) interdependent
The C compiler itself usually uses DSLs like lex and yacc.
%(which are themselves written in C and actually use also lex and yacc themselves).
% => 7
To write all this code in the first place you need an editor.
\l but btw editor run in an OS and coded in a (another) language
With so many source files you need
a build system to automate and optimize the compilation process.
Because the software we just mentioned inevitably have bugs
or non optimial parts, you will need a debugger and a profiler.
% => 11
Finally, nowadays
it is inconceivable to not use a graphical user interface
and to not work with multiple windows opened at the same time.
In the same way it is also not conceivable to work
in isolation; programmers collaborate with each other
especially via the web.
This means you need a graphical and networking stack as well
as a windowing system and a web browser.
% => 15
% (miss 1 language, 1 utility, 1 emulator => 18)
As we said earlier, it is hard to remove any of the software
from the serie.


We hope those books will answer many of your questions, 
even those that seems very simple at first,
such as what happens
when the user type [[ls]] in a terminal window, but which
involves as you will see many software layers (the shell,
the C library, the kernel, the graphics stack, the windowing system)
and quite a lot of code.

The books in the serie can be read in any order. You don't have
to read them all. We recommend to pick the software you are 
the most interested in, for instance
the one you are more curious about because you have 
only a fuzzy idea of how they are implemented, and 
read the corresponding book in the serie.
\l In our case, the graphics stack, windowing systems 

Enjoy!


\t appendix
\t can start from turing machine? enter initial prog via punch card or switches?


\addcontentsline{toc}{section}{References}

\bibliography{docs/latex/Principia}
\bibliographystyle{plain}

%******************************************************************************
% Postlude
%******************************************************************************

\end{document}
