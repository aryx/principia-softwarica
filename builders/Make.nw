\documentclass[twocolumn, landscape]{report}

%******************************************************************************
% Prelude
%******************************************************************************
\newif\iffinal
\newif\ifverbose
\finaltrue\verbosefalse % see also other newif in Macros.tex

%------------------------------------------------------------------------------
%history: 
%------------------------------------------------------------------------------

%thx to LP, I changed for the better a few things:
% - removed dead code:
%    dead namespaces (S_PID, S_MAKEFILE, S_EXPORTED, S_MAKEVAR),
%    dead rule attributes (UNUSED, UPD),
%    dead constants (IWS)
%    dead fields (Job.nproc)
%    TODO useless code (head.n=node, ...)
% - cleaned up code:
%    SEMI renamed variables, was abusing head/tail for too many things 
%    (target vs prerequisites, but also head vs tail of lists of arcs, 
%    head and tail of rule/assignement, tail for last word, ...)
% - reorganized the files (removed arc.c, job.c, introduced dumpers.c, mv
%    around a few functions)

%thx to codemap/codegraph/scheck:
% - use cg to reduce backward deps (only 8: 0.76%), introduce globals.c/utils.c,
%   removed job.c, arc.c, moved stow(),
%   (harder to understand non layered code)
% - use scheck to remove deadcode, dead prototypes, useless export
%   or mv as forward decl
%   (harder to understand big interface files)
% - use cg to reduce number of globals by moving them closer to the
%   relevant file (or even function), better cluster the code
%   (harder to understand non functional code using lots of globals)

%thx to mk in ocaml: (see also the %ocaml-found: tag in this file)
%  - parsing code can be simpler, and also better see the
%    different lexing contexts and subtelities
%  - can do X=a and $X=42, hmm
%  - understand need for ${name}ici
%  - understand check for recipe pointer in ambiguous(), cos can have 
%    many arcs but no ambiguity cos all have the same recipe
%  - understand vacuous check and interaction with ambiguous

%thx to this manual, I better understand mk/make:
% - minimal-syntax design leads to need for escaping-newline because newline is
%   (ab)used as a terminator for many things
% - escaping rules, tricky, but needed because space is used too for separator
% - need for ${} in mk (no pb in make though) so can do ${name}here
% - finally will remember the syntax for ${name:%=%} :) 
% - also the syntax for rules, do I need empty lines between rules, what
%   is exactly a rule boundary
% - need for multiple targets, and multiple rules with same target 
% - SEMI how to debug issues in your own Makefile
% - SEMI mk is actually a job scheduler! with dependent tasks!
% - to not use ';' but instead && for eflag -e to work
% - scope and life of variables (still dont understand in GNU make, = vs := )
%   whether can use $xx in target, in prereqs, in includes, etc

%history LP-ization:
% - skeleton, mostly copy paste of Template.nw skeleton
% - put all content of files in the Extra section, via 'pfff -lpize'
%   which also now split in chunks!
%    * function, global, struct, enum, constant, macro(actually function)
%    * TODO ctor/dtor, dumper
%    * [[xxx]] other fields, [[xxx]] extra fields
% - read Extra section, identify concepts, first TOC
% - distribute parts of the Extra section in the main file
% - understand main(), LP split main, improve TOC
% - understand main functions, LP split, cluster, improve TOC
% - LP split the structures, use datalog for flow to field info
% - nullify, boolify, errorify, enumify,  typeify,    scheckify, plan9ify
% - aspecify advanced features! remove useless features
% - SEMI add figures
% - SEMI add explanations

%------------------------------------------------------------------------------
% Packages
%------------------------------------------------------------------------------

\usepackage{../docs/latex/noweb}
 \noweboptions{footnotesizecode,nomargintag}
 %note: allow chunk on different pages, less white space at bottom of pages
 \def\nwendcode{\endtrivlist \endgroup}
 \let\nwdocspar=\par
\usepackage{xspace}
\usepackage{verbatim}
%note: required by noweblatexpad for the \t \l \n in this file
\usepackage{fancyvrb}
\usepackage{url}
\usepackage{hyperref}
 \hypersetup{colorlinks=true}
\usepackage[pageref]{backref}
 \def\backref{{\footnotesize cited page(s)}~}
\usepackage{booktabs} 
 \newcommand{\otoprule}{\midrule[\heavyrulewidth]}
\usepackage{graphicx}

%------------------------------------------------------------------------------
% Macros
%------------------------------------------------------------------------------
\input{../docs/latex/Macros}

%------------------------------------------------------------------------------
% Config
%------------------------------------------------------------------------------
\allcodefalse
% ifallcode is used for:
%  - forward decl, func decl, extern decl, #ifdef, pragmas (stuff in Extra.nw)

\addtolength{\topmargin}{-.850in}
\addtolength{\textheight}{1.70in}

\begin{document}
%******************************************************************************
% Title
%******************************************************************************
\title{
{\Huge 
Principia Softwarica: The Build System [[mk]]
}\\
{version 0.1}
}

\author{
Yoann Padioleau\\
\texttt{yoann.padioleau@gmail.com}\\
\\
with code from\\
Andrew Hume
}

\maketitle 
\onecolumn
\hrule
\input{../docs/latex/Copyright}
\input{../docs/latex/CopyrightPlan9}
\hrule
\twocolumn

\begingroup
\hypersetup{linkcolor=blue}
% need to s/onecolumn/twocolumn in report.cls :) for \tableofcontents
\tableofcontents
\endgroup

%******************************************************************************
% Body
%******************************************************************************

\chapter{Introduction}

The goal of this book is to explain with full details the source code of
a {build system}.

\section{Motivations}

Why a build system? 
Because I think you are a better programmer if
you fully understand how things work under the hood,
and a build system is one of the tools a programmer uses the most.
%
Indeed, it allows programmers to 
assemble, 
compile, 
link,
test,
package, and 
distribute 
software with one simple command, ``the one command that rules them all'',
from very simple programs to entire operating systems.
\n check, deploy


\n def so I can use dependency/rule/Makefile/make/... in questions below
A {build system} allows to {describe} and {maintain} {dependencies} 
between files.
\n could have better definition (said later)
%
Those dependencies are usually represented by {rules}, which are
stored in a special {configuration file},
for instance, a [[Makefile]] with the build system Make~\cite{make}
(one of the most popular build systems).



Even though a build system is not as interesting as 
a kernel or 
a compiler,
it is a necessary piece in the programmer's toolbox.
Indeed, all programs, including kernels and compilers, rely on a 
build system to be built.
%bootstrap:
In fact, a build system usually also relies on itself to be built, leading to
bootstrapping issues similar to the ones found in compilers.
%
Moreover, build systems contain components that are useful in many contexts
such as a job scheduler.
\l even if may seem boring at first
\l contain only simple algorithms? no neep concept?
\l like kernel, paralellize tasks, one of the first program to do (after kernel)
\l parallelization is even more important now
\t spend lots of time in front of terminal running mk. Important program.
\t in the end Google and Facebook spend quite some time making build systems.
%
Finally, build systems such as Make use an original approach 
to solve problems.
Indeed, to describe dependencies, Make provides a 
{domain-specific language} (DSL).
\n not a XML/JSON/... configuration file (said later)
%
The author of Make carefully designed this language to require
as less syntax as possible, in order to remove as much overhead as possible
for the programmer when writing rules.
\n actually some bad design with TAB, but not with mk (said later)
Moreover, this specific language, because it is restricted, 
because it is not as powerful as a general-purpose programming language, 
allows in counterpart special checks that would be impossible in a 
general language.
\n actually not make, but with mk (said later)

Here are a few questions I hope this book will answer:

\begin{itemize}

\item What are the fundamental concepts of a build system?
\n graph of dependencies, DAG, target, prereq, rules, cycle, ambiguous, job

\item What are the kinds of dependencies a build system needs to represent
in order to cover all the use-cases?
Can a file depend only on one other file (one-to-one dependency),
or on many files (one-to-many)? 
Can many files depend on the same single file (many-to-one), 
or on many files (many-to-many)?
\l all dependencies concerning a file can be described by a single rule?
\l  or more convenient to have multiple rules describing dependency of same file?
\n Do you need multiple rules for same target? what about ambiguities?
\l can have mutual dependencies? self dependency? tree? graph? DAG?

%dup: above
\item What is the minimal syntax one needs to describe dependencies,
and to describe the {commands} to maintain those dependencies?
\n :, space, newline
If this syntax uses special symbols, how do you reference
filenames containing those symbols?
\n escaping mechanism, quoting

\item What are the mistakes a build system can detect?
Can it detect ambiguous rules? 
Infinite rules? 
Can it detect cycles in dependencies?

\item What kind of help can a build system offer to help debug a [[Makefile]]?
How can you visualize the dependencies? 
\n mk -dump_graph with mk.byte

\item How does the build system run different tasks in parallel, and 
how does it coordinate them? How do you write a simple job scheduler?
\t how achieve efficiency? incremental? more? can parallel? how know when safe?

\end{itemize}

\t put in conclusion non-trivial adv algo and data structures seen? see comment
%data-structures (beyond list/hashtbl): (use list of words, symbol hash table)
% - queue (jobs)
% - graph (via pointers, thanks to an hashtbl with node references first)

%algorithms (beyond search/sort): (actually use neither search nor sort)
% - graph DFS and visited flag, cycle detection
% - matching and substituting (poor's man regexp)


%tags used in this file for different recurring themes:
 %compiler: similar techno found in compiler (or assembler, or cpp)
 %
 %real-world: to relate to other build systems
 %ocaml: to give a hint on how rewriting C in OCaml could improve things
 %ocaml-found: found subtle behavior of mk because I ported it to ocaml
 %alt: alternative technique
 %old: for original code I changed to be clearer
 %pad: for code I introduced to be clearer
 %dead: dead code removed
 %toc: %trans: %dup: %example: %chunks:


\section{The \plan build system: [[mk]]}

I will explain in this book the code of the \plan build system
[[mk]]\furl{http://plan9.bell-labs.com/magic/man2html/1/mk},
which contains about 5300 lines of code (LOC).
%
[[mk]] is written entirely in C.
\l used also outside plan9 by a few open source software.


%dup: from Assembler.nw
Like for most books in \principia, I chose a \plan program because
those programs are simple, small, elegant, open source, and they form together
a coherent set.
%history:
Moreover, [[mk]] is the ``spiritual successor''~\cite{mk-successor} 
to Make~\cite{make}.
\l not as popular as make, but better variant
\n even shorter to type (said later)
Indeed, it was designed in the same lab (Bell Labs), 
and even Stuart Feldman, the original author of Make,
recommends [[mk]] as a better system.
\t cite?
\l mk is to plan9 what make was to unix, and plan9 successor to unix

[[mk]] is modeled after Make, like many other build systems,
but [[mk]] is both simpler and more powerful than Make.
%
For instance, [[mk]] does not suffer from 
the infamous [[TAB]] requirement in the first column
from Make~\footnote{
See \url{http://www.catb.org/esr/writings/taoup/html/ch15s04.html}
for the history behind the [[TAB]] requirement.
\l See \cite{TAOUP} Page X, or \url{}
\l also http://stackoverflow.com/questions/1755550/what-is-the-reasoning-behind-the-makefile-whitespace-syntax 
\n GNU make has .RECIPEPREFIX to allow spaces too
};
with [[mk]], the programmer can use spaces and tabs interchangeably.
%
Moreover, [[mk]] executes shell commands in parallel,
an important improvement over the original Make as
this speeds up greatly the building process on machines
with multiple processors.


With one single command ([[mk]]) executed from the top
directory of the \plan fork used in \principia, 
you can build all 
the \plan libraries, 
the \plan programs,
the \plan kernel,
and build a disk image containing a \plan distribution
that can be installed on a Raspberry Pi or booted through QEMU;
all of this with one single command.
\n actually I use make right now :)

\section{Other build systems}

Here are a few build systems that I considered for this book,
but which I ultimately discarded:
\begin{itemize}

\item Make~\cite{make}, which from now on I will call \unix Make,
to differentiate it from its other variants, 
was the original Make part of early versions of \unix.
%
Stuart Feldman, its author, won the ACM System Software Award in 2003 for it: 
``there is probably no large software system in the world today that
has not been processed by a version or offspring of MAKE.''
\furl{http://awards.acm.org/award_winners/feldman_1240498.cfm}.
%
Indeed, \unix Make has many variants, which 
are often confusingly called also Make (e.g., BSD Make, GNU Make), 
and which often use the same command-line program name: [[make]].
\n actually there is pmake, bmake, gmake, but often aliased as make


One of the latest version of \unix Make,
Make 2.58\footnote{See \url{http://minnie.tuhs.org/cgi-bin/utree.pl?file=V7/usr/src/cmd/make/ident.c}.}
in the seventh edition of \unix in 1979,
contains less than 2500 LOC.
\n also in ape/cmd/make/ (V8 2.78 3000 LOC) and in unix-history-repo
\n maybe good candidate in the end. even use yacc! but ugly yacc.
%
This is smaller than [[mk]], but this version contains also far less features
than [[mk]]. For instance, it lacks the 
ability to execute shell commands in parallel.
\l need for parallel probably because was done for plan9 (said later?)
%
This ability requires more than just a few lines of code.
In fact, it led to the complete redesign in [[mk]] of the 
approach used by Make to compute dependencies.
%
Indeed, [[mk]] computes a graph of dependencies statically when it starts,
and then uses this graph to guide the building process.
\t this allows many more things? really? 
\t  can run in // from very different parts of the tree? make cant?
\t make does not have a graph? no transitive closure so what?
%dup:
\l Stuart himself considers mk a better solution 




\item GNU Make~\furl{https://www.gnu.org/software/make/} is probably
the most popular variant of Make.
%
It is used by almost every open source applications under Linux.
\n Linux requires gmake; it uses many GNU make extensions and has a 
\n  very fancy Makefile (make V=1 ...)
\n I think even FreeBSD switched to GNU Make
%
GNU Make contains many extensions to the original \unix Make, 
including the ability to run shell commands in parallel as in [[mk]]\footnote{
With [[make -j]].}.
\l also conditional, but can be emulated in mk
\t but as good as mk for the parallel? since mk has full graph? can do more?
It supports also many platforms, including old platforms
such as DOS, VMS, or Amiga.
%
However its codebase is also significantly larger: 37~000 LOC, which
is almost one order of magnitude more code than [[mk]].
\l main.c 3200 LOC, almost as big as whole codebase of mk


GNU Make follows closely the design of \unix Make, and so
inherits also its major flaws, for instance, the requirement
to use [[TAB]]s in the first column for shell commands.
%
Where [[mk]] tries to generalize, unify, and reuse the services
and syntax of other tools, GNU Make specializes and adds an extra layer
of complexity.
%
For instance, use of variables in GNU Make (and \unix Make) requires
an extra set of parenthesis (e.g., [[$(OBJS)]])
\n except if single character or number
compared to the use of variables in a shell (e.g., [[$OBJS]]).
Moreover, the use of shell variables inside a [[Makefile]] requires an
extra dollar (e.g., [[$$i]]).
%
In [[mk]], the syntax for variables is the same than in
the shell (e.g., [[$OBJS]], [[$i]]).%$
\n actually there is the subst variable, and leaky abstraction
%
Finally, [[mk]] replaces cryptic Make variables such as [[$@]] or [[$^]]
with clearer variables such as [[$target]] or [[$prereq]].
\l for more comparisons, see comments in .tex
% - regexp rules and no suffix rules (but meta % rules were already in
%   unix make, not an invention of mk) (said later)
% - mk better written, nice use of assert!
% - more complex escaping rules? if use " in recipe in GNU Make?
% see mk.ps section 4
% see also https://github.com/dcjones/mk for list of improvements


\item Ant~\furl{http://ant.apache.org/} is a build system used by
many Java projects.
\l ex? everyone switched to Maven and now Gradle? Android uses Gradle?
Instead of using a DSL to express dependencies, and of relying on a
shell and command-line programs to maintain those dependencies, 
an Ant user writes dependency rules in an XML [[build.xml]] 
configuration file.
%
For instance, here is a rule to clean files:
\begin{verbatim}
<target name="clean" <delete dir="classes"></target>
\end{verbatim}


Ant supports many XML tags to perform various tasks such as
deleting files ([[<delete>]]), creating directories
([[<mkdir>]]), or calling the Java compiler ([[<javac>]]).


The main advantage of Ant over Make is {portability}.
%
Indeed, with Make, the command-line programs used
in a [[Makefile]] may be specific to an operating system.
%
For instance, the default C compiler, linker, and file utilities
under Microsoft Windows are not the same than in Linux or macOS.
\n but now cygwin, mac port, brew, portable GNU utilities (said later)
%
In fact, even the shell and [[make]] programs are different
under Linux, macOS, and Microsoft Windows.
\n actually Microsoft put also Bash as standard now
\n https://blogs.msdn.microsoft.com/powershell/2016/04/01/bash-for-windows-why-its-awesome-and-what-it-means-for-powershell/
%
However, the portability of Ant comes at a price; its
codebase is very large with more than 200~000 LOC (without
the tests), which is almost 40 times more code than [[mk]].
\n every utility needs its own wrapper


The main advantage of Make (and [[mk]]) over Ant is {generality}.
Indeed, the shell is an expressive language. Moreover, you
can call any command-line programs from the [[Makefile]].
\l does not need to look at API of Ant either, can reuse past knowledge
%
With Ant, if there is no XML tag for a certain task,
you need to extend Ant itself.
%
Finally, XML is an extremely verbose language; writing
XML dependency rules by hand is long and tedious.
\l ok if generated by IDE.
\l also Ant has Ivy (dependency manager, equivalent of Cargo?)


\item CMake~\furl{https://cmake.org/} is a cross-platform build system
used in many large open source projects (e.g., LLVM).
\l cite LLVM?
%
It was designed, like Ant, to overcome the lack of portability of [[Makefile]]s.
%
Unlike Ant, CMake acts as a frontend to Make (and other build systems).
Indeed, CMake is a {\em meta} build system.
%
Instead of writing [[Makefile]]s, the user of CMake 
creates [[CMakeLists.txt]] files containing builtin (portable) commands 
to compile source code.
%
CMake then generates from those configuration files regular
[[Makefile]]s that can be processed by Make.
%
CMake can also generate instead files for IDEs such as Apple's Xcode or
Microsoft's Visual Studio.
\n also can generate files for ninja
\l like automake, gyp, etc. mix actually configure/automate feature I think.

CMake contains thousands of builtin commands,
\l really?
offers a graphical user interface, and supports many IDEs.
However, its codebase is enormous with more than 250~000 LOC 
(without the tests). 
\n also without Utilities which is used but seems to contained forked libs
This is extremely large, especially
considering the fact that CMake is just a frontend to other build systems.
\l   has an article in OASA book 1 or 2



% one using python, EDSL, but more syntax boilerplate "", [;], etc

\end{itemize}
\t there are tons more build systems; Many language specific, many meta.
\t  but ask yourself do I really need it? make/mk not enough?

Note that the portability issue of Make has been 
partially fixed in the last ten years.
%
Indeed, with the availabiltiy of GNU utilities 
for operating systems other than Linux (e.g., 
through cygwin~\furl{https://www.cygwin.com/} for Microsoft Windows, 
and macports~\furl{https://www.macports.org/} or 
Homebrew~\furl{http://brew.sh/} for macOS),
[[Makefile]]s are now more portable.
\n actually for macOS file utilities are based on BSD


\l a few other, see the comment in this file:
%make-clones:
% - pmake, freebsd https://www.freebsd.org/doc/en_US.ISO8859-1/books/pmake/
%   parallel make
% - nmake (replaced by msbuild later)
% - dmake
% - omake, apparently has listen-to-fs features too, so daemon
%   and rerun when save in editor?
%   http://blog.camlcity.org/blog/omake1.html
% - https://github.com/dcjones/mk is mk port in Go! 2285 LOC but some files
%   looks almost like an automatic C->Go translation (identical logic)
%other:
% - ninja, seems interesting, the assembler of build system, 19~000 LOC
%   has an article in OASA book 3
% - tup, 31 000 LOC (just src/tup/), does not look that much better than mk
%   but looks pretty nice: *.c |> gcc %f -p %o |> %B.o
%   http://gittup.org/tup/make_vs_tup.html benchmark
%   http://gittup.org/tup/build_system_rules_and_algorithms.pdf
%   But the coolest thing is that it automatically handle dependencies!
%   and it uses fuses and inotify probably so it's automatic!
% - bazel?? latest from google:
%   http://google-opensource.blogspot.com/2015/09/building-build-system-bazel-reaches-beta.html
% - buck, huge, python-based like many other google-inspired build systems.
%   the ocaml-specific stuff is already 4000 LOC, hmmm
% - cons/scons, Perl-based and python-based build systems???
% - redo, from dan bernstein, seems minimalist, not sure it's better.
%   maybe better for autodeps.
% - msbuild, 452 000 LOC, wow, XML-based too, recently open sourced
%meta build systems:
% - imake (for X, but then superseded by autotools)
% - automake and autotools (for GNU)
% - gyp, from Google, python-inspired, simple, 28~000 LOC
%   support many backends (make, cmake, ninja, eclipse, xcode, ...),
% - fbmake, very similar to gyp
%wrappers:
% - qake, just a big makefile, so relies on GNU make
% - OCamlMakefile
% - premake?
%language specific:
% - java: 
%    * maven, first to provide ability to download dependency over the network?
%    * ant+ivy (to catchup with maven)
%    * gradle seems best one, use of DSL based on Groovy. huge codebase.
%    comparison: https://technologyconversations.com/2014/06/18/build-tools/
%    but all 3 are bad in my opinion.
% - javascript: gulp, grunt
% - ruby: rake, can write rules in Ruby. EDSL again. more verbose than make,
%   need of do, end, need '' around filenames, need of many builtins.
%   maybe inspiration for brew
% - haskell: shake, EDSL in haskell, seems too verbose, 
%   cabal
% - scala: sbt (scala build tool), even a book about this
% - rust: cargo, integrate in build system a dependency manager
% - ocaml: omake, opam (just dependency manager though),
%   jenga from jane street?
% - lisp: asdf
% - clojure: leiningen
% - go: realize (with watchers?)


% see https://en.wikipedia.org/wiki/List_of_build_automation_software

%http://www.catb.org/esr/writings/taoup/html/ch15s04.html
% discussions of a few meta build systems, and nice history of make
%http://www.lihaoyi.com/post/WhatsinaBuildTool.html
% nice comparisons of a few tools
%http://hadihariri.com/2014/04/21/build-make-no-more/
% nice bashing of all make competitors, a la "evolution of a programmer"
%the essence of make, not too bad, nice relation between ninja and make
% http://bentnib.org/posts/2015-04-17-propositions-as-filenames-essence-of-make.html
%make advocacy :)
% http://bost.ocks.org/mike/make/
%http://aosabook.org/en/500L/contingent-a-fully-dynamic-build-system.html
% meh, EDSL
%make theory and practice
% http://www.ploxiln.net/make.html  meh
%how we use make:
% https://segment.com/blog/how-we-use-make/ list of targets

% complimentary tool: 
%  - https://github.com/cespare/reflex to rebuild automatically after a
%    file changed
%  - the one from facebook? 


\section{Getting started}
\label{sec:getting-started}

%dup: (and adapted) Assembler.mw
To play with [[mk]], you will first need to install
the \plan fork used in \principia. See \urlinstall.
Once installed you can test [[mk]] under \plan with:

\begin{verbatim}
1   $ cd /tests/mk
2   $ mk -f hello.mk
3   5c -c hello.c
4   5c -c world.c
5   5l -o hello hello.5 world.5

6   $ mk
7   mk: 'hello' is up to date
8   $ touch world.c
9   $ mk
10  5c -c world.c
11  5l -o hello hello.5 world.5
12  $
\end{verbatim}
\n could get one from kencc or plan9port. issues though with rc vs sh ...

Line~2 runs [[mk]] with the [[-f hello.mk]] parameter
to tell [[mk]] to use the rules in the [[hello.mk]] file
instead of the default [[mkfile]] 
([[mk]]'s equivalent of a [[Makefile]]).
%
Line~3 through 5 then output the shell commands run by
[[mk]] given the rules contained in [[hello.mk]]. 
%
Those commands compile and link a simple program called [[hello]]
using the ARM C Compiler [[5c]] (see the \book{Compiler})
and ARM linker [[5l]] (see the \book{Linker}).
%
Remember that [[.5]] is the filename extension of ARM object files
under \plan, hence the use of [[hello.5]] and [[world.5]] in the
linking command above.


Line~6 reruns [[mk]], which should not recompile 
or relink anything since nothing has changed. Instead,
[[mk]] should indicate that [[hello]] is already up to date.
\n I added the already in mk-ocaml
%
Line~8 modifies the date of [[world.c]]. 
\l need to explain touch? modification time?
This time, reruning [[mk]] at Line~9 should trigger the recompilation
of [[world.c]] and relinking of [[hello]]. Note that
[[mk]] should not recompile [[hello.c]] since
[[hello.5]] depends only on [[hello.c]], not [[world.c]].
\l incremental



\section{Requirements}

%dup: from Assembler.nw
Because most of this book is made of C source code, 
you will need a good knowledge of 
the C programming language~\cite{k-r} to understand it.
%
I also assume you are already familiar with at least 
one build system, for instance, a variant of Make,
\l also a shell
%
and so are familiar with concepts such as
a [[Makefile]],
a {rule},
a {target}, 
a {prerequisite}, or
a {shell command}.
\n called recipe in mk, by shell command easier (said later)
%
If not, I suggest you to read one of the book about
GNU Make~\cite{managing-make, programming-with-gnu-software, gnu-make-manual}.

%dup: (and adapted) from Windows.nw
If, while reading this book,
you have specific questions on the interface of [[mk]],
I suggest you to consult the man page of [[mk]]
at [[docs/man/1/mk]] in my \plan repository.

Finally, the [[builders/docs/]] directory in my \plan repository
contains documents describing either [[mk]]~\cite{mk-successor},
or the [[mkfile]]s used in \plan~\cite{mk-plan9}.
%
Those documents are useful to understand some of the 
design decisions presented in this book, especially how and why
[[mk]] differs from Make.




\section{About this document}
#include "../docs/latex/About.nw"

\section{Copyright}
\input{../docs/latex/CopyrightPlan9Text}

\section{Acknowledgments}

I would like to acknowledge of course the author of [[mk]],
Andrew Hume,
who wrote in some sense most of this book.
\n and Bob Flandera? more for plan9 mkfiles than mk itself, 
\n apparently Hume did stuff also on optimizing Boyer Moore,
\n  cited by author of GNU grep https://lists.freebsd.org/pipermail/freebsd-current/2010-August/019310.html




\chapter{Overview}
\label{chap:overview}

%trans: %dup: Assembler.nw
Before showing the source code of [[mk]] in the following chapters, 
%toc:
I first give an overview in this chapter
of the general principles of a build system.
%
I also quickly describe the command-line interface of [[mk]]
and show a simple [[mkfile]] for a toy application.
\n actually hello.mk
%dup: from Assembler.nw
Finally, I define terms, explain how the code is organized, 
and more generally give the background necessary
to understand the code I will show later.

\section{Build system principles}
\n I try to not use too much [[mk]] here, and use "build system", to generalize

%trans:
To understand the goal of a build system, it is useful to remember
how programmers were compiling projects before Make was invented.
\l remember -> imagine? compiling -> maintaining? building?
%history:
Before Make, programmers were using {shell scripts} to {record} the
compiling and linking commands for a project. For instance, here
is one such script called [[make.rc]]\footnote{This script
is using the \plan shell, which is called [[rc]] (see the \book{Shell}), 
hence the use of the [[.rc]] filename extension.
\l not sh or bash
}
to build the toy program mentioned in Section~\ref{sec:getting-started}:
\l can be used to build {from scratch}, or after modified

<<tests/mk/make.rc>>=
#!/bin/rc

5c -c hello.c
5c -c world.c
5l -o hello hello.5 world.5
@

However, as programs grow larger, and the number of files grows,
the shell-script approach becomes too {inefficient}\footnote{
%bootstrap:
This approach can still be useful to bootstrap the build system itself.
}.
\l inefficient, and also a bit long, but could factorize with shell variables
Indeed, in the previous example, if only
[[hello.c]] is modified, there is no need to recompile also [[world.c]],
but that is what the script will do when it will be run again to rebuild
the program.
\l not incremental. rebuild from scratch.
%
An alternative for the programmer is to keep track in his head of
all the modifications, and to recompile manually only what is necessary.
%
However, again, as programs grow larger, and dependencies between files
become more complex, 
\l make depend, gcc -MM (said later?)
it becomes difficult to remember
which files need to be recompiled and what are the precise flags 
used to recompile or link those files. 
\n actually as programs become really really big, ninja is better


%dup: intro/motiv (but longer version)
Thus, the goal of a build system is to 
describe {\em concisely} and 
maintain {\em efficiently}
{dependencies} between files.
\l shell is concise but inefficient
\l could write special programs to check date (including in a shell lang)
\l  which would be efficient, but not concise so tedious
%
For a programmer, those files are C or Assembly programs, object files, 
and executables.
For a writer, those files are Troff or TeX documents, pictures, and
PDFs. 
A build system can be used in many contexts, not just for programming.

%toc:
Note that every word in the first sentence of the previous paragraph 
is important:
``concisely'' led in [[mk]] to the creation of a domain-specific language,
``dependencies'' led to the use of a {graph} to represent the relationships
between files, and
``efficiently'' led to the use of a job scheduler to run in
parallel multiple tasks, as 
explained in the following sections.
\n there are different kinds of deps actually many-to-many

In the next sections, I will use examples using the syntax of [[mk]],
which is very close to the syntax of Make,
but the principles apply to most build systems.

\subsection{A domain-specific language}
\label{sec:dsl}

A {\em domain-specific language} (DSL), as its name suggests, is a language
specialized for one particular task. 
%dup: intro/motivations overview/principe
In the case of a build system, the task is 
(1) to describe file dependencies, and 
(2) to describe the commands to maintain those dependencies.
%
A DSL uses a special syntax to describe more concisely than
with a {general-purpose language} the solution to a particular problem.
%dup: intro/motivations
\l Usually not as expressive as full PL, but trade expressivity for compactness
\l and also special checks (will see later)
%trans:
The shell script [[make.rc]] above is already a good solution for (2).
Indeed, a shell can almost be considered a DSL for running commands;
\l before grew as full language
it uses a special syntax for launching programs (by just typing
the name of a command, without the need to call [[fork()]] or [[exec()]]),
for creating pipes (with [[|]]), and for file redirection (with [[>]] or [[<]]).
%
Thus, the idea behind the DSL of [[mk]] (and Make before) 
\l give a name to this DSL? Mk-lang?
was to extend slightly the shell syntax to accomodate also (1), 
with special constructs to express dependencies between files.
\l small superset actually. very small language.
%toc:
Those constructs are 
the rule, 
the pattern,
the variable,
and the file inclusion,
as explained in the next sections.

\subsubsection{The rule}
\label{sec:rule-simple-mkfile}

%trans:
The most important construct in 
\n [[mk]]'s DSL 
the DSL of a build system
is the rule.
%
A {\em rule} describes a {relation} between two or more files,
for instance, the relation between an object file [[hello.5]]
and its source [[hello.c]].
\l essence is state dependencies/graph between files and how to achieve it.
%
In [[mk]]'s terminology, the object file is called the {\em target},
and the source the {\em prerequisite}. Moreover, the
command to maintain the dependency between those two files,
that is the shell command to generate the target from the prerequisite,
is called the {\em recipe}.
\l recipe also called body and start of rule called head (said later?)
%\paragraph{Syntax}
Here is the syntax of a rule in [[mk]]'s DSL:

\begin{verbatim}
target : prerequisites
    recipe
\end{verbatim}
\l syntax of mk and Make, except TAB vs space
\l in fact target_s_: prerequisites
% {target}: {prerequisites}
%     {recipe}


Here are the rules corresponding to the script [[make.rc]] shown above
for the toy program mentioned in Section~\ref{sec:getting-started}:
\l ported to mk, ``program''

<<tests/mk/mkfile version 1>>=
# rule 1
hello.5: hello.c
    5c -c hello.c
# rule 2
world.5: world.c
    5c -c world.c
# rule 3
hello: hello.5 world.5
    5l -o hello hello.5 world.5
@
\n would be better to have hello first rule so that default rule but ok.
\l use of comment here

The rules are usually stored in a special {\em configuration file}:
an [[mkfile]] for [[mk]] (and a [[Makefile]] for Make).
\l ``program''
%
Note that a rule can have multiple prerequisites, like in the third
rule above with the multiple object files.
%
In fact, a rule can also have multiple targets, as you will see 
in Section~\ref{sec:graph-many-to-one}.
%
Moreover, the same file can be a target in one rule
and a prerequisite in another rule,
for instance, [[hello.5]] in respectively the first and third rule.


The syntax of an [[mkfile]] is very similar to the syntax of a shell script.
\t Syntax for comment, same as shell, again.
\l could be extracted automatically ... except hello.5 not visible from 
\l  5c command, would need -o hello.5 then.
The only syntactical addition is the use of [[":"]] to separate the target from
the prerequisites, and the newline and indentation
\l any indentation is fine, tab, space
to separate the prerequisites from the recipe.
\l x : y \n z1 \n z2. could also x : y ; z1; z2
%
This syntax is {\em minimalist}. Indeed, there is no
quote around filenames or around commands. Moreover, the different
elements in the list of prerequisites are simply separated by space.
\l no keyword, no braces, nothing!
\l same spirit than shell actually. no need () for calling
\l no need remember order of options (but then lead to -xxx named arguments)
\l actually sometimes need quote, for escaping (said later)

%\paragraph{Semantic}
%trans: syntax, now semantic. just like syntax rule is simple, semantic trivial
The semantic of a rule is also very simple. [[mk]] will check
\t recursive: mk will check if target up-to-date with prereqs
\t  by first checking recursively if prereqs up-to-date with their own prereqs
\t  and then check the modif time of target is more recent than ...
if the {\em modification time} of a target is more recent than {all} its
prerequisites.
\n actually more recent or equal in mk-in-ocaml, cos second not precise enough
\n actually DFS first, so semantic more subtle than what it looks (said later)
\n  check if prerequires are up to date, recursively, and then check if tgt ...
If not, it will run the recipe, which hopefully will update the 
modification time of the target.
\l can do the check? mk does? mk-in-ocaml does!

%\paragraph{Recipe}
As I said before, the recipe is simply a shell command.
Even though the commands in the [[mkfile]] above are simple, 
[[mk]] allows to use the full language of the \plan shell [[rc]]
(see the \book{Shell}), with loops, conditionals, functions, etc.
%
Indeed, one of the design principles of [[mk]] was to leverage existing tools.
\n and syntax (said later)


The shell language is {\em embedded} inside [[mk]]'s DSL.
\l Not even interpreted. As is. simpler than in make (said later?)
%
This is similar to other \unix DSLs such as Lex~\cite{lex} or Yacc~\cite{yacc}
where the programmer can use special syntax to define respectively
{regexps} and {grammar rules}, but where he can also use the full C language 
for the {actions} triggered when a regexp or grammar rule is recognized
(See the \book{CompilerGenerator}).
\n actually in Make weird mix for vars, but not unlike Yacc with dollar1
\n Similar to PHP too, with HTML and where embed dynamic ocde
%
\label{sec:mk-is-a-graph}
In [[mk]], the actions are not written in C but in the shell language 
of [[rc]], and those actions are embedded not inside the definition of 
a regexp or of a grammar, but inside the definition of a {\em graph}.
%
Indeed, the targets and prerequisites are similar to the sources and
destinations of {\em arcs} in a graph. The recipe corresponds to the 
{\em label} on an arc.
%
In fact, as you will see in Section~\ref{sec:graph}, [[mk]] internally
uses a graph to represent the dependencies between files.



%\paragraph{Dependency arities}
%What types of dependencies we need? can have multiple targets:
%    x.ml x.mly: x.mly
%  (but then can be confusing when making graph of deps. dont want
%  run recipe twice, so tricky, so need mark multiple nodes from one job done)
\n  also can be used to factorize simple rules like a.o b.o: x.h

% can have same target and multiple rules (but only if one recipe)
%    x.o: x.c  recipe     x.o: y.h
% (also with metarules below, when instantiate can add more rules??)
%  but maybe this one is really sugar, could put all prerequisites at once!

% lead to optional recipe. space/tab first char cos support multiline 
% for shell command (otherwise would need escaped newline there too, ugly).



\subsubsection{The pattern}
\label{sec:pattern}

%trans:
The [[mkfile]] in the previous section allows to maintain
efficiently dependencies between files; 
if only [[hello.c]] is modified, then [[world.c]] will not be 
recompiled by [[mk]].
However, the [[mkfile]] is not very concise.
%
Note that the first two rules are very similar;
they differ only by the name of the file.
%
This is why to factorize rules, 
most build systems provide a way 
\n [[mk]] allows 
to use {patterns} inside a rule. 

In [[mk]],
a {\em pattern} is a sequence of characters including 
the special character [[%]]
that matches any sequence of characters.
\l separated by space? but then can also have quote.
\l longest match?
Here is an example of a pattern matching any C source files:

\begin{verbatim}
%.c
\end{verbatim}

A rule using a pattern for his target
is called a {\em meta rule} in [[mk]]'s terminology.
\l one of the target
\n can also be in prerequisite
\l rule generating rules
\l describe a set of arcs in a graph in a generic way.
%
Here is a better version of the [[mkfile]] for the same
toy program:


<<tests/mk/mkfile version 2>>=
# simple rule
hello: hello.5 world.5
    5l -o hello hello.5 world.5

# meta rule
%.5: %.c
    5c -c $stem.c
@
%$

During the processing of the first rule above, [[mk]] will
recognize that [[hello.5]] and [[world.5]],
which are mentioned in the prerequisites of the rule,
{\em match} the target in the {second rule} if the percent is 
set to [[hello]] or [[world]].
\l first rule called simple rule, second meta rule.
%
[[mk]] will then {\em instantiate} the meta rule to generate
specific rules for [[hello.5]] and [[world.5]].
\l actually done on the fly while building the graph
\n but sometimes instantiate too many, possibly ambiguous, (see vacuous remove)
%
The percent in the prerequisite is then {\em substituted} by
the matched string ([[hello]] or [[world]]), and the 
{special variable} [[$stem]] %$
can be used from the shell command to access the matched string.
\t footnote could use percent in recipe, but then not shell recipe
\t  goal is be as less intrusive as possible (as opposed to Make,
\t  which does processing on recipe such as variable expansion)
\t maybe just say explain later why different mechanism
\t  if choose percent, then need process string, need escape mechanism
\t  complicated.

The use of [[%]] to match any sequence of characters
is similar to the use of [[.*]] in a {\em regular expression}.
\l cite book on regexps?
%
In fact, [[mk]] supports also meta rules using 
regular expressions, as explained in Section~\ref{sec:regexp}.
%
However, in most cases, patterns using [[%]] are expressive enough
and simpler to write.
\l regexp notoriously difficult, and . is very common in filename but
\l  needs to be escaped in regexp

%real-world: 
Make supports meta rules with percents, but not with regular expressions.
It also supports {\em suffix rules} (e.g., [[.5.o: ...]]).
However, suffix rules are less intuitive to write and less expressive 
than meta rules. This is why they are not supported by [[mk]].
\l .SUFFIXES
\l generalize, simplify
\n can also use pattern in variable (said later)

\subsubsection{The variable}

%trans:
In the previous section, I have shown the use in a recipe 
of a variable ([[$stem]]) %$
set by [[mk]]. 
\l special var, internal var
%
Most build systems offer a way 
to define and use your own variables to factorize tings.
\n  in an [[mkfile]]
\l more uses than factorizing? readability?
\l actually also on the command line, and even override
%
In [[mk]],
those variables can contain a list of {\em words}, which
can correspond to anything: filenames, compiler flags, etc.
Here is an example of a variable containing two filenames:

\begin{verbatim}
OBJS=hello.5 world.5
\end{verbatim}

Here is a slightly different version 
% shorter (in numbers of characters) % actually not shorter
of the previous [[mkfile]] using a variable:

<<tests/mk/mkfile version 3>>=
OBJS=hello.5 world.5

hello: $OBJS
    5l -o hello $OBJS

%.5: %.c
    5c -c $stem.c

@
%$
\l CFLAGS=-g -x -...and then can use in many rules CFLAGS. easier to modify too.
\l but if use metarule then less useful

One of the design principles of [[mk]] was to leverage existing tools,
but also existing syntax.
Thus, the syntax to define and use variables in [[mk]] is
exactly the same than in the shell.
\n define and use! Make sucks here.
\l but not same exactly! handled by mk itself this time!
\l reuse syntax and some machinery (but not all machinery)
This syntax is again minimalist.
To {define} a variable, type a variable name, followed by an equal sign, 
followed by a list of words simply separated by space.
There is no braces, brackets, commas, or semicolons
\l or a type
like in other languages 
(e.g., [[ char* OBJS[] = {"hello.5", "world.5"}; ]] in C).
%
The next {newline} marks the end of the variable definition.
\l  (unless it is {escaped} to scale to larger list, as explained in Section X)
\t could add footnote on escaping newline and ref to later
To {use} a variable, prefix the variable name with the dollar sign.
\n also {} for special cases (said later)
\l history of this dollar sign?


Note that [[mk]], like the shell [[rc]], treats the content
of a variable as a list.
\l even if contains only one element
[[mk]] offers also a special syntax to concatenate lists
by simply juxtaposing a variable with other elements or other
variables, as in the following example:

\begin{verbatim}
X= b c d
# Y will contain a b c d e f
Y=a $X d e f
\end{verbatim}
%ocaml-found: scalar vs list context! can be stricter actually

[[mk]] offers also a special syntax to transform lists,
as explained in Section~\ref{sec:transform-list}
\l use pattern! orthogonal power


Once a variable is defined, you can use it in other variable
definitions, or in a rule (in the target, in the prerequisites, 
or even in the recipe). 
You can also use variables in patterns in meta rules.
\t can use before defined? empty then? and when define the can see new value?
\n also in file inclusion (said later)
%
Moreover, [[mk]] imports the variables from the environment,
so you can also use variables such as [[$HOME]] in your [[mkfile]]. %$
\l like the shell, also import and prefix with dollar
\n or objtype (said later)


Note that the term ``variable'' in the context of [[mk]] is slightly 
misleading.
Indeed, in [[mk]], variables are constants.
\n actually mk-in-ocaml check if override
\n even though can be overriden on commandline
Variable definitions are more {binding definitions}.
\t expanded in target and prerequiste and def as parse the file
\t  (but not in recipe, cos use different mechanism, export env, 
\t   again to not be intrusive on recipe, embeded DSL, so no extra escaping)
Indeed, [[mk]] needs to know statically the value of a variable to be
able to compute the graph of dependencies.
\t why? need explain more, at least footnote and ref to later
%real-world:
\t actually in Make vars are different and binding and scope is complex
\l Also clearer then diff between variables like OBJS=..
% and variables generated by mk such as target, stem etc.
\l also leaky abstraction, use of var in target/prereqs is diff than in recipe

%trans:
% more generic stuff when use modularity/inclusion

\subsubsection{File inclusion}

%trans:
The last construct 
\n of [[mk]]'s DSL 
found in most build systems
is the file inclusion. 
\l better term? the include?
%
In [[mk]],
a {\em file inclusion} is an {instruction} in the [[mkfile]],
starting with [[<]], used to load the rules and variables defined 
in another file.
This file can itself includes other files, recursively.
Here is an example of a file inclusion:

\begin{verbatim}
</$objtype/mkfile
\end{verbatim}

The effect is similar to a [[#include]] in C.
%
Note that the filename can contain variables defined
previously in the [[mkfile]] (or in the environment).
\l use variable! orthogonal power, can use variables everywhere
%
Here, [[$objtype]] %$
is a special \plan environment variable containing the
type of architecture of the current machine (e.g., [[arm]], [[386]]).
\t cite? plan9 article? one of my book?
%
You can then define for each architecture a specific [[mkfile]] with
variables such as [[CC]] or [[LD]]
containing the name of the native compiler and linker.
%
Section~\ref{sec:mkfile-objtype} presents such an [[mkfile]]
for the ARM architecture.
%
It is good practice to include [[/$objtype/mkfile]] %$
at the beginning of an [[mkfile]] for better portability.
%
Note that again [[mk]] reuses the syntax of other tools by
using the [[<]] symbol used for input redirection in the shell. 
\n not so convincing this one


Just like the rule, the pattern, and the variable,
a file inclusion allows to factorize things.
%
Indeed, you can store a library of meta rules
in a separate file (e.g., [[/shared/mkgeneric]]) and reuse this
file in multiple projects.
\l [[<]] so modularity and can factorize in different files
In fact, by combining variables and file inclusion, you can
also have a library of simple rules shared by multiple projects,
as shown by the example below:

<<tests/mk/mkfile version 4>>=
OBJS=hello.5 world.5
PROG=hello

</shared/mkone
@
%$
\n actually /sys/src/cmd/mkone, and it is OFILES and TARG not OBJS and PROG

<</shared/mkone>>=
$PROG: $OBJS
    5l -o $PROG $OBJS
%.5: %.c
    5c -c $stem.c
@
%$
%real-world: include (or -include)

% with variables above, offers already some kind of genericity!
% in fact many mkfiles are just  < /sys/src/cmd/mkone !
% (but still need metarules for that)

%\subsection{Generating some dependencies automatically}

% another use for file inclusion is include dependencies information generated
% automatically by other tools.
% Can do manually, but can be tedious.
% Many dependencies are implicit in files. if foo.c include
% foo.h, should recompile. Can declare deps in mkfile, but
% redundant, hard to maintain. 
% So can be good to auto generate some dependencies. But 
% different for each programming language.

% enter .depend 
% (and so also need for multiple rules for same targets (but with empty recipe))

% related is library of variables and rules for different languages.

% object level deps. libs.
% But also semantic level.

% question is can we generate every dependencies automatically?
% IDE tries to do that. But requires helps, AddFile, AddDir.
% what about when have multiple binaries in one distrib?
% hh_server nice, but works because generate a giant PHP library.

% in the end, could be job of compiler? like javac does? but then
% impose constrain on how to organize classes in files and path
% (or use hh_server, but again, impose constraint of one giant
% single binary/library)

%\subsubsection{Generality}

% works for all PLs. not adhoc to one PL like IDEs

%related: generate from mkfile .project, like Buck does.

%\subsection{Different types of dependencies}
% before? after?

\bigskip
[[mk]] offers a few more constructs
beyond 
the rule, 
the pattern,
the variable, and 
the file inclusion, 
but those are the main constructs of a build system.
%
See Chapter~\ref{chap:advanced} for the list of advanced
constructs supported by [[mk]].


\subsection{A graph of dependencies}
\label{sec:graph}
\label{sec:rule-essence}

%trans: 
The pattern, the variable, and the file inclusion are nice features,
but they are not the essence of a build system;
%
rules are the essence of a build system.
\l rest is sugar
%
Indeed, once the build system has 
loaded included files,
expanded variables, and 
substituted patterns,
what remains is a set of rules with {concrete} filenames 
as targets and prerequistes.
%
Moreover, as I mentioned briefly in Section~\ref{sec:mk-is-a-graph},
the rules in a build system define essentially the 
{nodes} and {arcs} of a {graph}.
Thus, the essence of a build system is also this {\em graph of dependencies}.


%toc:
In the next sections, I will present different examples of graph
of dependencies, with increasing complexity.
\l Each example, each graph, different shapes, see subtelities.
\l Also easier explain algo on this graph than on rules. core DS.

\subsubsection{A simple tree}
% | and /\  one-to-one and one-to-many

\begin{figure}[!]\centering
\begin{verbatim}
           +-------+
           | hello |
           +-------+
               X
              / \
       /-----/   \-----\
      /                 \
     v                   v
+--------+          +--------+
|hello.5 |          |world.5 |
+--------+          +--------+
     |                   |
     |                   |
     v                   v
+--------+          +--------+
|hello.c |          |world.c |
+--------+          +--------+
\end{verbatim}
\caption{Graph of dependencies for [[hello]].}\label{fig:graph-hello}
\end{figure}
\n maybe good step to explain later // scheduler

Figure~\ref{fig:graph-hello} presents the graph of dependencies
for the [[hello]] program of Section~\ref{sec:getting-started}.
\n when target is hello for mk
\l when x : y then arc. when x : y z then two arcs. too simple? (said later)
\t if mkfile use ver1 or ver2 of tests/mkfile, with pattern or not
\t  it does not matter. Ultimately
\t  you will get this graph (see Section for algo to build graph).
%
In this example, the graph is simply a {tree}.
%
The {\em nodes} in the graph of dependencies correspond to concrete filenames.
\l pattern and vars has been substituted (said before a bit)
%
The {\em arcs} represent dependencies between files.
For instance, [[world.5]] depends on [[world.c]], hence the arc between
the two nodes in Figure~\ref{fig:graph-hello}.
\l arc and direction
%
Note that a rule with two prerequistes leads to the creation
of two arcs in the graph of dependencies.
\l same for two targets (said later?)
\l one-to-one, one-to-many

\l target vs prerequistes, parent vs child, src vs dest.

%trans: nice graph, but not enough to explain algo.

\subsubsection{A labeled tree}
\label{sec:labeled-tree}

\begin{figure}[!]\centering
\begin{verbatim}
                     +-------+
                     | hello | 08:00:13
                     +-------+
         5l -o hello     X     5l -o hello
       hello.5 world.5  / \  hello.5 world.5
                 /-----/   \-----\
                /                 \
               v                   v
          +--------+          +--------+
 08:00:10 |hello.5 |          |world.5 | 08:00:11
          +--------+          +--------+
               |                   |
          5c -c hello.c        5c -c world.c
               v                   v
          +--------+          +--------+
 08:00:00 |hello.c |          |world.c | 11:30:00
          +--------+          +--------+
\end{verbatim}
\caption{Graph of dependencies for [[hello]], with labels.}
\label{fig:graph-hello-labels}
\end{figure}
\n better with labels on arcs, see duplicated recipe on multiple arcs.
\n better with labels for time. see core algorithm.
\l could put same time for the objects cos in //
%alt: graph, but with set of files in nodes? so no pb ambiguous arcs?
\t why not put recipe on the node itself? anyway there will be
\t  a master rule no? could, but could also be more flexible and
\t  have different recipe on different arcs. But then if
\t  two dependent are modified, which command to run? ...


Figure~\ref{fig:graph-hello-labels} presents also the graph of
dependencies for the [[hello]] program, but where nodes
ands arcs are annotated with {\em labels}.
%
Arcs are {labeled} with a {recipe} while
nodes are labeled with the {modification time} of the file they represent.
%
If the file does not exist, the modification time is set to zero.
\t footnote 1970 unix time
\l zero, so idea is similar to very very old, so will need to be updated.
%
In Figure~\ref{fig:graph-hello-labels}, the day, month, and year
of the modification time of the file are omitted for simplification purpose;
only the hours, minutes, and seconds are shown.
I assume all the files were modified in the same day.


The scenario that led to the dates in Figure~\ref{fig:graph-hello-labels}
is as follows: 
The programmer of the [[hello]] program finished modifying 
[[hello.c]] and [[world.c]] at 8am;
he then ran [[mk]] to build the program; [[mk]] finished compiling
[[hello.5]] at 8am and 10 seconds, and 
[[world.5]] at 8am and 11 seconds;
[[mk]] finished linking 
[[hello]] at 8am and 13 seconds;
\n actually seconds granularity not good enough (said later?), need nanosecond
finally, the programmer modified [[world.c]] at 11:30am
and stopped.
%
At this point, running [[mk]] should recompile [[world.c]]
(and relink [[hello]]), but should not recompile [[hello.c]]. 

%trans: Thx to labels, ready to talk about algo.

\subsubsection{Depth-first search on a tree}
\label{sec:algo-dfs}

\begin{figure}[!]\centering
\begin{verbatim}
               +-------+
           (1) | hello | (8)
               +--X--^-+
                   X  \
                / / \  \
          / - -  /   \  \----\
           /----/     \----\  \
        / /                 \  \
       v v                   v  \
    +--------+          +--------+
(2) |hello.5 + - - - - >|world.5 | (7)
    +--+-----+(4)    (5)+--+-----+
         | ^                 | ^
       | | |               | | |
       v v                 v v
    +------+-+          +------+-+
(3) |hello.c |      (6) |world.c |
    +--------+          +--------+
\end{verbatim}
\caption{Depth-first search traversal of the graph dependencies for [[hello]].}
\label{fig:graph-hello-dfs}
\end{figure}


Once the graph of dependencies is built,
\l and not so trivial
the main algorithm behind [[mk]] is to perform a 
\l and other build systems?
{\em depth-first search} (DFS) traversal on this graph.
\l cite cormen?
%
Figure~\ref{fig:graph-hello-dfs} presents the order in which 
the DFS visits the nodes on the previous graph.


The DFS starts from the {\em root} 
(step 1 in Figure~\ref{fig:graph-hello-dfs})
and goes as deep as possible along a {\em branch} (steps 2 and 3).
%
When it reaches a {\em leaf}, for instance [[hello.c]] (step 3),
the algorithm just checks whether the file exists.
If the file does not exist, then [[mk]] should report an error
since there is no instruction on how to generate this file
(there is no prerequiste connected to the node and so no recipe).
%
If the file exists, then the algorithm can continue and the
DFS can {backtrack} by going back up in the tree (step 4).
%
At this point, the algorithm checks whether the modification time 
of the node is more recent than all its prerequistes 
(which have all been already visited by the DFS by now). 
\t this is important. need recurse first on children. Indeed
\t  in example hello is more recent than both objet files, but
\t  still need to relink it at some point cos very deep there is a 
\t  a modified file
%
If the node is more recent, for instance, [[hello.5]]
is more recent than [[hello.c]] in Figure~\ref{fig:graph-hello-labels},
then there is nothing to be done but to continue the DFS 
(steps 5 and 6).
%
If the node is older than one or more of its prerequistes,
for instance, [[world.5]] is older than [[world.c]] 
in Figure~\ref{fig:graph-hello-labels},
then the algorithm should run the associated recipe
\l which recipe? recipe of arc?
in a separate shell process.
\l more on this later
Hopefully, running this process will modify the time of the node.
\l need to wait finish
%
This will in turn trigger the recompilation of all the
ancestors of the node while going back up to the root of
the graph (step 8 in Figure~\ref{fig:graph-hello-dfs}), since
the ancestors should now be older than this newly-generated file.


% But subtle, if modify hello.c and world.c, then 
% recompile both, but then dont want to run 2 times the 
% linking program. 2 arcs, but one command!
% ambiguous? no, same command here (see Section X for ambiguous check).
% (but maybe should really put recipe in node)

\subsubsection{A direct acyclic graph}
% /\  many-to-many
% \/

%trans:
In the previous examples, the graph was simply a tree. However,
%
most build systems support a more general form of graphs: 
{\em direct acyclic graphs} (DAGs), where
the same node can be referenced multiple times from different branches.
%
Figure~\ref{fig:graph-hello-dag} presents such as graph
for the same [[hello]] program, but where an additional
header file, [[common.h]], is shared and included by the 
two source files.


\begin{figure}[!]\centering
\begin{verbatim}
           +-------+
           | hello |
           +-------+
               X
              / \
       /-----/   \-------\
      /                   \
     v                     v
+--------+            +--------+
|hello.5 |            |world.5 |
+--------+            +--------+
     |                     |
     | \                  /|
     |  \------\ /-------/ |
     |          v          |
     v     +--------+      v
+--------+ |common.h| +--------+
|hello.c | +--------+ |world.c |
+--------+            +--------+
\end{verbatim}
\caption{Graph of dependencies for [[hello]] with [[common.h]].}
\label{fig:graph-hello-dag}
\end{figure}

Note that even though [[common.h]] is included by
[[hello.c]] and [[world.c]], there is no arc between those files
in the graph of dependencies.
%
Indeed, modifying [[common.h]] should not
cause the regeneration of [[hello.c]] or [[world.c]].
%
However, the modification of [[common.h]] should cause
the regeneration of the object files. Indeed, the header
file may define data structures that have an impact
on the object code generation, hence the two arcs
from the object files to the header file.
\t will talk later on how to automatically generate those dependencies

There are multiple situations where the same file can be
referenced multiple times in the graph of dependencies:
multiple executables may depend on and reuse the same library, 
multiple libraries may use the same object file,
multiple object files may depend on the same header file, etc.
\l Figure present a few of those examples.
Those shared files add arcs in the graph. However,
the graph must remain acyclic.
Indeed, a file can not depend on itself, directly or indirectly
\l would make no sense.
(Section~\ref{sec:cycle-check} presents the code to check for
{cycles} in the graph of dependencies).
\t Moreover, because DAG, DFS needs record if visited already a node
\t  to not update multiple times the same file
\l even though here cannot see that cos common.h is a leaf


%\subsubsection{Multiple rules with the same target}
\label{sec:master-rule} 

There are multiple ways to add the dependency
to [[common.h]] from [[hello.5]] (and [[world.5]]) in the
[[mkfile]]s of Section~\ref{sec:dsl}:

\begin{enumerate}
\item You can add [[common.h]] in the list of prerequistes
in the rule for [[hello.5]]:

\begin{verbatim}
hello.5: hello.c   common.h
    5c -c hello.c
\end{verbatim}

However, this approach does not work well when the rule
to compile [[hello.c]] is a meta-rule such as [[%.5: %.c]].
%
Indeed, each source file has its own header file dependencies,
which are impossible to factorize in a single meta-rule.


\item You can add a separate rule using the same target
and the same recipe:

\begin{verbatim}
hello.5: common.h
    5c -c hello.c
\end{verbatim}

It is important to impose to have the same recipe. 
If the recipe was different, [[mk]] would be confronted with an 
{\em ambiguity}
when both the source file [[hello.c]] and the header [[common.h]] are modified:
which recipe to choose to update the target [[hello.5]]?
\l see Section X for ambiguity check?
%
However, it is difficult for [[mk]] to check whether
the recipe of a meta-rule such as [[5c -c $stem.c]] is %$
equivalent to the recipe of a simple rule such as [[5c -c hello.c]].
\t really? difficult? for stem it is known at compile time

\item You can add a separate rule using the same target but
without any recipe:

\begin{verbatim}
hello.5: common.h
\end{verbatim}

This would work if the build system
imposes that there must be another {single} rule,
called the {\em master rule}, with the same target and including
a recipe. In that case, there would be no ambiguity and no need
to check if two recipes are equivalent.

\end{enumerate}

[[mk]] supports (1) and (3),
\l I dont think mk supports 2. it is using pointer equality, not string equal
but (3) is more convenient for the programmer as it works well with meta-rules.
%\subsubsection{Automatic dependencies}
Moreover, when combined with file inclusion, (3) allows
to leverage programs that automatically extract header
dependencies from source files 
(e.g., [[gcc -MM]] for C, [[ocamldep]] for OCaml).
\l other? makedepend? furl?
%
Indeed, the output of such programs can simply be redirected
in a [[.depend]] file that can be included from the [[mkfile]].
\l would be hard to give recipe, cos flags, but gcc -MM
\l   could generate a template using metavariable though
\l So then need to find master recipe again.
\l example of .depend? .depend for mk ?
\n in \plan they dont use tools like gcc -MM because they hate header files
\n  they use very few header files

%\subsubsection{Depth-first search on a DAG}

% So have to take care when go through graph.
\l Because DAG, in code we will see a few times to set a flag on a node marked 
\l as done, to avoid repeat multiple times the same thing.
\n actually not that much
% but here DAG too simple, nothing after common.h, so a leaf is simple
% implications for graph update?

\subsubsection{Many-to-one dependencies}
\label{sec:graph-many-to-one}
%\/ many-to-one

\begin{figure}[!]\centering
\begin{verbatim}
                   +-------+
                   | prog  |
                   +-------+
      5l -o prog       X      5l -o prog
   lexer.5 parser.5   / \  lexer.5 parser.5
               /-----/   \-----\
              /                 \
             v                   v
        +--------+          +--------+
        |lexer.5 |          |parser.5|
        +--------+          +--------+
   5c -c   | \                   /  |   5c -c
  lexer.c  |  \--------\ /------/   | parser.c
           v            v           v
        +--------+ +--------+  +--------+
        |lexer.c | |parser.h|  |parser.c|
        +--------+ +--------+  +--------+
      lex    |    yacc  \           /   yacc
    lexer.l  |  parser.y \---\ /---/  parser.y
             v                v
        +--------+       +--------+
        |lexer.l |       |parser.y|
        +--------+       +--------+
\end{verbatim}
\caption{Graph with many-to-one dependencies.}\label{fig:graph-many-to-one}
\end{figure}

\l have shown one-to-one, one-to-many, and some kind of many-to-one.
%trans:
In Figure~\ref{fig:graph-many-to-one}, two object files,
[[lexer.5]] and [[parser.5]],
depend on the same file, [[parser.h]], but 
they also depend on other files ([[lexer.c]] and [[parser.c]])
and have different recipes ([[5c -c lexer.c]] and [[5c -c parser.c]]).
This is similar to the situation depicted by Figure~\ref{fig:graph-hello-dag}
with the shared file [[common.h]].
%
However, in Figure~\ref{fig:graph-many-to-one}, 
two files, [[parser.h]] and [[parser.c]], depend {exclusively}
on the same file, [[parser.y]], with the same recipe ([[yacc parser.y]]).
%
Indeed, this last file is a Yacc~\cite{yacc} grammar file.
The [[yacc]] program generates
both a header file ([[.h]]) and a source file ([[.c]]) from a
single grammar file ([[.y]]).
%Those are real many-to-one dependencies.
%
There are multiple ways to express this {\em many-to-one} dependency:

\begin{enumerate}
\item You could create two separate rules for each target:

\begin{verbatim}
parser.h: parser.y
    yacc parser.y
parser.c: parser.y
    yacc parser.y
\end{verbatim}

\item You could create a single rule with {\em multiple targets}:

\begin{verbatim}
parser.h parser.c: parser.y
    yacc parser.y
\end{verbatim}
\end{enumerate}

However, the semantics for (1) and (2) are different. Indeed,
with (1), if you modify [[parser.y]], then [[mk]] will
create two shell processes and execute two times the
[[yacc]] command, which is useless (and could even be incorrect
if the two commands are run in parallel and the writes on
the same file are intertwined).
%
With (2), it will create a single process 
and execute only one time the [[yacc]] command.


The use of multiple targets in one rule has implications on the
DFS traversal of the graph of dependencies.
%
Indeed, in Figure~\ref{fig:graph-many-to-one}, once the DFS has processed
[[parser.y]], backtracked on [[parser.h]], and ran the recipe
to update [[parser.h]], 
\n actually done later and indirectly through job queue
it is important that the algorithm adjusts the 
modification time of both the [[parser.h]] and [[parser.c]] nodes.
%
If only the [[parser.h]] node is updated, [[mk]]
would run another time the [[yacc]] command when the DFS
reaches the [[parser.c]] node with an obsolete modification time.
\l could also each time reread mtime
This is why, as you will see later in Section~\ref{sec:multi-targets},
the arc from [[parser.h]] to [[parser.y]] contains also
a reference to the [[parser.c]] node.
\t so mk put as BEINGMADE alltargets ?

%trans: this conclude examples of graphs? 


\subsection{A job scheduler}
\label{sec:job}

%trans: 
I have described the main features of the DSL of a build system,
the underlying representation of dependencies in a build system,
as well as the basic algorithm behind a build system (the DFS); 
I will now focus on the efficiency of a build system.

%trans:
A build system maintains dependencies between files efficiently firstly
by being an {\em incremental} program. Indeed, if you modify
only one source file, the build system will recompile and 
relink only what is necessary.
%
This is made possible by comparing the modification times of nodes 
in the graph of dependencies.
%
In fact, this graph enables also the build system 
to be more efficient by running recipes in {\em parallel}.
%
Indeed, with a graph, it is easy to detect whether two commands
can be run in parallel when they belong to two independent branches 
in the graph.
\l independent is too informal?
\l In fact if 2 separate successors, can, even if at different level.
\l but algo does not do those branch-check? it uses the BEINGMADE thing
%
For instance, in Figure~\ref{fig:graph-many-to-one}, 
the recipes with the [[lex]] and [[yacc]] commands can be run in parallel.
%
However, if only the [[lex]] recipe finished, it is not possible
to run [[5c -c lexer.c]] in parallel with [[yacc]] since
there is an arc between [[lexer.5]] and [[parser.h]] in the graph.
You must also wait for the [[yacc]] recipe to finish.


To run recipes in parallel, a build system should not 
wait during the DFS that a shell process finishes executing a recipe.
%
This is why, during the DFS, [[mk]] adds instead the recipe in a {\em queue}
and continues the DFS.
%
Each element of this queue contains, in addition to the recipe,
a pointer to the target node (or target nodes) associated with
the recipe.
%
[[mk]] can add multiple recipes in the queue during the DFS, 
and can then execute in parallel those recipes once the DFS finished.
\n actually done during DFS via sched() (see later)
%alt: do DFS in parallel? but then concurrency issue? if DAG?
The recipe and associated target node(s) stored in the queue is called a
{\em job} in [[mk]]'s terminology. The queue is also called
the {\em job queue}. Thus, [[mk]] is also a {\em job scheduler}.


The use of a job queue has implications on the graph and the DFS.
%
Indeed, in Figure~\ref{fig:graph-hello-dfs}, as well as in
Figure~\ref{fig:graph-being-made}, if the job to regenerate
[[world.5]] from [[world.c]] is enqueud at step~7 (instead of being
executed {synchronously}), the modification time of 
[[world.5]] node will not be updated directly. 
%
Then, when the DFS backtracks on the root node at step~8, 
the modification time of the root node may still be more
recent than all its prerequistes 
(as shown in Figure~\ref{fig:graph-hello-labels}).
However, [[mk]] should not stop there and declares
that [[hello]] is up-to-date. Once the job to
generate [[world.5]] has finished, [[hello]] will not be up-to-date.
\l but can detect that situation so, as job queue not empty
This is why the modification time of nodes associated with a job
should be marked specially in the graph.

\begin{figure}[!]\centering
\begin{verbatim}
                +-------+ NotMade
            (1) | hello | (8)
                +--X--^-+
                    X  \
                 / / \  \
           / - -  /   \  \----\
            /----/     \----\  \
         / /                 \  \
        v v                   v  \
     +--------+          +--------+ BeingMade
 (2) |hello.5 + - - - - >|world.5 | (7)
     +--+-----+(4)    (5)+--+-----+
          | ^ Made            | ^
        | | |               | | |
        v v                 v v
Made +------+-+     Made +------+-+
 (3) |hello.c |      (6) |world.c |
     +--------+          +--------+
\end{verbatim}
\caption{Graph of dependencies with progress-status labels.}
\label{fig:graph-being-made}
\end{figure}

\label{sec:build-status}
To keep track of the nodes involved in a job, [[mk]] uses
an extra label on each node to indicate the progress status
of the node: [[NotMade]], [[Made]], and [[BeingMade]],
as shown in Figure~\ref{fig:graph-being-made}.
%
The algorithm behind [[mk]], exposed previously in Section~\ref{sec:algo-dfs},
is modified as follows. After the graph is built, every status labels in 
every nodes is set to [[NotMade]].
%
During the DFS, if the algorithm reaches a leaf containing an existing
file, the node is marked as [[Made]] (e.g., [[hello.c]] at step 3, and
[[world.c]] at step 6 in Figure~\ref{fig:graph-being-made}).
%
During backtracking, the algorithm will use different marks
depending on the situation:

\begin{itemize}
\item If the node is more recent than all its prerequiste nodes,
and all those nodes are marked as [[Made]], then this node is also
marked as [[Made]] 
(e.g., [[hello.5]] at step 4 in Figure~\ref{fig:graph-being-made}).

\item If the node is older than one or more of its prerequistes,
and all the prerequistes are marked as [[Made]], then a new
job will be enqueued and this node is marked as [[BeingMade]]
(e.g., [[world.5]] at step 7 in Figure~\ref{fig:graph-being-made}).

\item If the node is older or more recent, but one of its prerequistes
is marked as [[BeingMade]], then the status should be kept as [[NotMade]].
\l or BeingMade? then status should be unchanged (NotMade or BeingMade)

\end{itemize}

[[mk]] will run in a loop multiple times the DFS until
the root node is marked as [[Made]]. 
During each of those loops, the DFS will find new jobs 
to run in parallel.
\l mk needs also to wait for process to finish in this loop.
\l can have multiple waves before first wait if lots of processors!
\l FIGURE with waves?

% need just made vs notmade? why need also beingmade if have 
% also generic visited? and check on empty queue? because of the DAG?
%real-world: Make has the need for BeingMade? it kinda wait
% that all child are done still? 
\t then less opportunity for //ization? Example?

% mk is actually kind of a scheduler because run independent jobs in //, 
% and need to manage dependencies between those jobs
% then need coordinate. wait for finish, etc.
% Master/Workers?

% Note that in Figure with yacc, if BeingMade on parser.h,
% should also mark BeingMade for parser.c!

% Note that in second wave, can have more efficient DFS as no need
% go down in branch marked as Made
\l mk does that? does not because bottleneck is not there?
\t does DFS leverage the Made? Should have no need to go down on a node
\t  with Made!

%trans: ?

\section{[[mk]] command-line interface}
\label{sec:mk-interface}

The command-line interface of [[mk]] is very simple: just go
in a directory and type [[mk]]\footnote{
%real-world:
This interface is similar to the one in Make, 
except [[mk]] is even shorter to type than [[make]],
which is useful as [[mk]] is a command you will type a lot
(the two letters are even next to each other on a QWERTY keyboard).
}.
%dup: intro/getting-started
However, this assumes the directory contains a file named [[mkfile]]:

<<constant MKFILE>>=
#define	MKFILE		"mkfile"
@
%ocaml: used only once, so could be expanded where it's used
%real-world: make look for Makefile (said before)

Moreover, it assumes the first target in this [[mkfile]] is the file 
you want to build.


To change the default behavior, you can use the [[-f]] flag, as shown in
Section~\ref{sec:getting-started}, to specify another configuration file.
%
Moreover, you can change the default target by specifying a target
from the command line (e.g., [[mk hello.5]]). 
In fact, you can even give a list of targets on the command line.
\l great way to use mk is have list of one-liner (said later?)

[[mk]] supports also a few extra options to help debug [[mk]] itself
or to provide advanced features.
%chunks:
I will present gradually those options in this book.
%
Here is the full command-line interface of [[mk]]:

<<function badusage>>=
void
badusage(void)
{

    fprint(STDERR, 
           "Usage: mk [-f file] [-(n|a|e|t|k|i)] [-d[egp]] [targets ...]\n");
    Exit();
}
@
%old: fprint(STDERR, "Usage: mk [-f file] [-n] [-a] [-e] [-t] [-k] [-i] [-d[egp]] [targets ...]\n");
\l also have NPROC, NREP global variables
\t actually -I now, not -i
%ocaml: use Arg so cleaner --help (but true that redundant with man page)
\l also xxx=yyy

\ifallcode
%dead? used by some tools?
<<global version>>=
static char *version = "@(#)mk general release 4 (plan 9)";
@
\fi


\section{[[hello.mk]]}
\n [[hello.mk]] so consistent with my other books.
\l A simple [[mkfile]]

Here is finally the content of the [[hello.mk]] file
mentioned in Section~\ref{sec:getting-started}:

<<tests/mk/hello.mk>>=
OBJS=hello.5 world.5
CFLAGS=
LDFLAGS=

hello: $OBJS
 5l $LDFLAGS -o $target $prereq

%.5: %.c
 5c $CFLAGS -c $stem.c
@
%$
\n minimum spirit, no <objfile, no factorize with mkone
\n  no objtype/mkfile, cos then cant use hello.5, should use hello.dollarO
\n simple var, simple rule, metarule, special vars; the essence of mk is there.

This file is named [[hello.mk]] to illustrate the [[-f]]
command-line flag of [[mk]], but a common practice is to name
[[mk]]'s configuration file [[mkfile]] instead.


I have described most of the features used in [[hello.mk]]
in Section~\ref{sec:dsl}, so I will not repeat the explanations here.
%
The only new feature is the use of the {\em special variables}
[[$target]] and [[$prereq]]. Those variables are set by
[[mk]] in the environment of the shell process executing the recipe.
\l that way recipe can access them
%
As their names suggest, they contain respectively the name of 
the target and the list of prerequistes of the rule in which
they occur.
\l note that prereq not always work, for instance for meta rule better use stem
\l  cos prereq will also match the .h in the .depend

\n show also simple output of running mk on a toy project (said before)
% VERBATIM 5c ...
% if modify hello.c
% VERBATIM 5c hello.c; 5l ...
% if nothing
% VERBATIM already done
\l maybe even include mk -e output?

The compilation and linking flags ([[CFLAGS]] and [[LDFLAGS]])
are set to an empty list in the example above, 
but those flags can be {\em overriden} from the command-line. 
%
Indeed, [[mk]] can take a list of variable definitions
as arguments (e.g., [[mk CFLAGS=-g]]). 
Those definitions override any definition contained in the [[mkfile]]
(or in files included from the [[mkfile]]).
\l not needed in plan9? by default compile info? or -a for acid
This is convenient because you can simply cross-compile
a project by overriding the definition of [[objtype]]
from the command line (e.g., [[mk objtype=arm]] on a machine
where [[objtype]] is by default set to [[386]]).

For more examples of [[mkfile]]s, 
%bootstrap:
notably the [[mkfile]] of the [[mk]] project itself,
see Appendix~\ref{chap:examples}.
The goal of the examples in this chapter was just to illustrate
the main features of [[mk]]. 



%\section{Input [[mkfile]] language}

% precise syntax? can have OBJ = xxx or need OBJ=xxx ?
% need space before recipe? can have comments?


\section{Code organization}

%dup: (and adapted) from Assembler.nw
Table~\ref{tab:code-orga} presents short descriptions
of the source files used by [[mk]], together with
the main entities (e.g., structures, functions, globals) the file defines,
and the corresponding chapter in this document in which the code
contained in the file is primarily discussed.
\n sorted by chapters, make more sense than sorted by dir
\t so? what this table is useful for/

\begin{table*}[tbh!]
\begin{center}
\begin{tabular}{lcllr}
\toprule
{\bf Function}  & {\bf Chapter} & {\bf File} & {\bf Entities} & {\bf LOC} \\
\otoprule
data structures and constants     & \ref{chap:core-ds}           & [[mk.h]]       & [[Symtab]] [[Word]] [[Rule]] [[Arc]] [[Node]] [[Job]] & 0 \\ % Bufblock
symbol table and cache            & \ref{chap:core-ds}           & [[symtab.c]]   & [[hash]] [[symlook()]] [[symtraverse()]]& 0 \\ % symstat syminit?
variables                         & \ref{chap:core-ds}           & [[var.c]]      & [[setvar()]]                                          & 0 \\ % shname
list of strings (words)           & \ref{chap:core-ds}           & [[word.c]]     & [[newword()]]  [[wtos()]]     & 0 \\ % wdup delword addw
globals                           & \ref{chap:core-ds}           & [[globals.c]]  & [[rules]] [[metarules]] [[jobs]]                      & 0 \\ %pad: was in main.c
function prototypes               & \ref{chap:core-ds}           & [[fns.h]]      &                                                       & 0 \\

\midrule
entry point                       & \ref{chap:main}              & [[main.c]]     & [[main()]]                                            & 0 \\ % badusage

\midrule
lexer                             & \ref{chap:parsing}           & [[lex.c]]      & [[assline()]] [[nextrune()]]                          & 0 \\ % bquote 
parser                            & \ref{chap:parsing}           & [[parse.c]]    & [[parse()]] [[rhead()]] [[rbody()]] [[addrules()]]   & 0 \\ % ipush ipop 
parsing and escaping methods for [[rc]]   & \ref{chap:parsing}           & [[rc.c]]       & [[charin()]] [[squote()]] [[escapetoken()]]           & 0 \\ 
parsing and expanding variables   & \ref{chap:parsing}           & [[varsub.c]]   & [[stow()]] [[expandvar()]] [[varname()]] [[varsub()]]            & 0 \\ % subsub extractpat nextword
adding rules                      & \ref{chap:parsing}           & [[rule.c]]     & [[addrule()]]                                         & 0 \\ % rulecnt regerror

\midrule

building and checking the graph   & \ref{chap:graph}             & [[graph.c]]    & [[graph()]] [[applyrules()]] [[cyclechk()]]           &  0 \\ %newnode() newarc() nrep ambiguous vacuous
file and time management          & \ref{chap:graph}             & [[file.c]]     & [[timeof()]] [[touch()]] [[delete()]]                 & 0 \\
pattern matching and substituting & \ref{chap:graph}             & [[match.c]]    & [[match()]] [[subst()]]                               & 0 \\

\midrule

finding outdated files in the graph & \ref{chap:finding-outdated}  & [[mk.c]]       & [[mk()]] [[work()]] [[outofdate()]] [[update()]]      & 0 \\ % clrmade pretend update
building a job                    & \ref{chap:finding-outdated}  & [[recipe.c]]   & [[dorecipe()]] [[newjob()]]                                       & 0 \\ % 

scheduling jobs                   & \ref{chap:scheduling}        & [[run.c]]      & [[run()]] [[sched()]] [[waitup()]] [[RunEvent]]       & 0 \\ % Process pnew pdelete nproc killchildren usage

shell environment                 & \ref{chap:interacting}       & [[env.c]]      & [[buildenv()]] [[envy]] [[envinsert()]]               & 0 \\ %execinit initenv envupd
environment and process methods for \plan         & \ref{chap:interacting}       & [[plan9.c]]    & [[readenv()]] [[exportenv()]] [[execsh()]]            & 0 \\ %waitfor pipecmd Exit catchnotes

\midrule
printing shell commands           & \ref{chap:debugging-support} & [[shprint.c]]  & [[shprint()]]                                         & 0 \\ %mygetenv vexpand front?

handling archives (libraries)     & \ref{chap:advanced}          & [[archive.c]]  & [[atimeof()]]                                         & 0 \\ % atouch() atimes()

\midrule
dumpers                           & \ref{chap:debugging-appendix} & [[dumpers.c]] & [[dumpv()]] [[dumpn()]] [[dumpr()]] [[dumpj()]] & 0 \\ % was spread in other files before
error management                  & \ref{chap:error}             & [[utils.c]]    & [[Malloc()]] [[Realloc()]]                            & 0 \\
string buffer                     & \ref{chap:libc}              & [[bufblock.c]] & [[newbuf()]] [[insert()]]                             & 0 \\ % rinsert growbuf freebuf

\otoprule
Total                                    &                               &                 &                                                                           & 0 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Chapters and source files of [[mk]].}
\label{tab:code-orga}
\end{table*}

% still remains backward/mutual dependencies with codegraph on mk/ because:
%  - plan9.c and rc.c specific methods crosscut many files. They are called
%    from generic functions but also call themselves generic functions
%    (like an ugly framework). Examples:
%     * Lex.assline -> Rc.escapetoken -> Lex.nextrune
%     * Plan9.readenv -> Utils.Malloc -> Plan9.Exit
%     * Plan9.notifyf -> Run.killchildren -> Plan9.Exit
%  - parse.c and run.c are mixed (parsing call process for <|, for ``)
%    (should have separate eval.c)
%  - process are mixed (for ``, for varible expansion)
%  - mk.c, recipe.c, and run.c are mixed
%    (should merge mk.c and recipe.c, and put update() in graph.c)


\section{Software architecture}

%dup: (and adapted) from Assembler.nw
Figure~\ref{fig:controlflow} describes 
the main control flow of [[mk]], while
Figure~\ref{fig:dataflow} describes 
the main data flow of [[mk]].
%
The main steps of the building pipeline of [[mk]] are as follows:

\begin{enumerate}
\item {\em Parse} the [[mkfile]] (via [[parse()]]) to extract
the rules and meta rules in the file
\n chapter 5

\item {\em Build} the graph of dependencies (via [[graph()]])
for a specific target given the rules extracted previously 
\n chapter 6

\item {\em Find} outdated files in the graph (via [[work()]])
\n chapter 7

\item {\em Schedule} jobs (via [[sched()]]) that will run the shell
recipes (via [[execsh()]]) to update the outdated files
\n chapter 8
\n chapter 9 = To {\em interact} environment

\end{enumerate}

\begin{figure}[!]\centering
\begin{verbatim}
              +------+
              | main |
              +------+
                  |
     +------------+------------+
     v                         v
 +-------+                 +-------+
 | parse |                 |  mk   |
 +-------+                 +-------+
     |                         |
     v             +-----------+
+--------+         v           +----------------+
|assline |    +---------+      |                |
+--------+    |  graph  |      v                v
              +---------+ +---------+      +---------+
                   |      |  work   |      | waitup  |
             +-----+      +---------+      +---------+
             v                 |                |
       +----------+     +------+----+      +----+-+
       |applyrules|     |           |      |      v
       +----------+     v           v      | +---------+
                   +---------+ +---------+ | | update  |
                   |outofdate| |dorecipe | | +---------+
                   +---------+ +----+----+ |
                                    |      |
                               +----v----+ |
                               |   run   | |
                               +------+--+ |
                                      |    |
                                   +--v----v-+
                                   |  sched  |
                                   +----+----+
                                        |
                                   +----v----+
                                   | execsh  |
                                   +---------+
\end{verbatim}
\caption{Control flow diagram of [[mk]].}\label{fig:controlflow}
\end{figure}
\l graphviz of files? or just of the types?

\begin{figure}[!]\centering
\begin{verbatim}
                          /- simple rules -\
mkfile -> lines -> words -                  > graph -> jobs
                          \- meta rules   -/

             (use global symbol table)
\end{verbatim}
\caption{Data flow diagram of [[mk]].}\label{fig:dataflow}
\end{figure}

%trans:?
\l from top to bottom, and left to right.
After some basic command-line processing and initializations,
the function [[main()]] calls [[parse()]] with the file to {parse}
as an argument (by default [[mkfile]], unless you specified
another file with the [[-f]] command-line flag).
%
[[parse()]] first calls the lexer to assemble a {line} (via [[assline()]]),
which is then split in {words}, which are then analyzed
to populate important globals such as [[rules]] and [[metarules]],
which contain the lists of extracted {rules}.
%
[[parse()]] also sets the global [[target1]] with the name of the 
first {target} found in the [[mkfile]], unless you gave
a specific target on the command line (e.g., with [[mk hello.5]]).
%
Finally, [[parse()]] updates and uses a global {symbol table} containing
the values for the {variables} defined in the [[mkfile]]
and in the {environment}.
\n also process inclusion, so recursive function


After the rules have been extracted, [[main()]] calls [[mk()]] with
the name of a target as an argument (the name stored in [[target1]] by default).
\n actually target1 is a list (said lter)
%
[[mk()]] then calls [[graph()]] to build the {graph of dependencies}
for this target given the rules and metarules extracted during parsing.
%dup: overview/principles/graph
This graph contains nodes and arcs, as explained in Section~\ref{sec:graph}.
The {nodes} correspond to concrete files
(e.g., [[hello.5]], [[hello.c]]),
\n unless virtual targets (said later)
and the arcs connect two nodes when a node depends on another node 
(e.g., [[hello.5]] is connected to [[hello.c]]).
%
Those arcs are also labeled with the rule containing the recipe
to generate the target node.
\l also modification time, and build status.

[[graph()]] works by first creating a node for the target parameter,
called the {root} of the graph, and by then
calling [[applyrules()]] on this node.
%
[[applyrules()]] then finds a rule or meta rule with the node as a target,
\n can have amgiguity (said later)
and creates new nodes for all the prerequistes found in the rule. 
It then calls recursively [[applyrules()]] on those new nodes.
\l at some point leaf cos existing file with no matching rule for target
%
Note that as opposed to Make,
in [[mk]] the graph of dependency is computed statically once and for all
at the very beginning.
\t so? better?


[[mk()]] then calls [[work()]] to find outdated files in the graph.
%
Starting from the root, [[work()]] performs a depth-first search
\l as explained in Section X
and goes down recursively in
the graph to find nodes corresponding to inexistent files, or
to files that are older than the files in the nodes they are connected to.
\l depth first search
%
Once it found such a node,
[[work()]] calls [[dorecipe()]]
with the outdated node as a parameter. 
\t note that DFS, go deep first! even if node more recent than all
\t  connected nodes, those nodes may not be up to date, so call
\t  work on them first! and then compare the date. (or said before)
[[dorecipe()]] then finds the arc containing the master rule 
\l see Section X for master rule
with the recipe to regenerate the file in the node.
\l can have ambiguity, but always a master arc, a master rule (said later)
%
[[dorecipe()]] then calls [[run()]] to add in a queue the job to 
run the recipe. 
[[run()]] possibly calls [[sched()]] to schedule the job, if there was
a free processor to run the job in parallel.
%
[[sched()]] then calls [[execsh()]] to fork and execute in a shell the recipe.


[[mk()]] calls [[work()]] in a loop, to schedule jobs in
parallel until the root node is up-to-date ([[Made]]). 
However, during those loops,
[[work()]] may not be able to schedule any job. Indeed, all the processors
may already be in use, or certain jobs may not be able to start until
other jobs are finished.
This is why [[mk()]] also calls sometimes [[waitup()]] to wait for 
those jobs to finish.
Once a job is finished, [[waitup()]] calls [[update()]] to update 
the node in the graph associated with the job, and [[sched()]] 
to schedule another job.
\t as I said, ends when root node is up-to-date and no more process 
\t  to wait for.


%\section{Boostrapping}
% we use a mkfile and mk to compile mk :)
% can bootstrap simply using a shell script for the very first time,
% as mentioned before


\section{Book structure}

%trans: %dup: (and adapted) from Assembler.nw
You now have enough background to understand the source code of [[mk]].

%toc:
The rest of the book is organized as follows.
%
I will start by describing the core data structures of [[mk]]
in Chapter~\ref{chap:core-ds}. 
%
Then, I will use a top-down approach, starting with Chapter~\ref{chap:main}
with the description of [[main()]] and the initializations of [[mk]].
%
The following chapters will describe
the main components of the building pipeline:
Chapter~\ref{chap:parsing} will present the code to parse an [[mkfile]],
Chapter~\ref{chap:graph} the code to build the graph of dependencies,
Chapter~\ref{chap:finding-outdated} the code to find outdated files in the graph,
Chapter~\ref{chap:scheduling} the code to schedule jobs, and finally
Chapter~\ref{chap:interacting} the code to interact with the shell.
%
In Chapter~\ref{chap:debugging-support}, I will present code to help you
debug and profile your [[mkfile]].
%
Chapter~\ref{chap:advanced} presents advanced
features of [[mk]] that I did not present before to simplify the explanations,
for instance, {rule attributes}.
%
Finally, Chapter~\ref{chap:conclusion} concludes
and gives pointers to other books in the \principia series.

%toc:
Some appendices present the code of non-functional properties:
code to help debug [[mk]] itself in Appendix~\ref{chap:debugging-appendix},
code which profiles [[mk]] itself in Appendix~\ref{chap:profiling-appendix},
and code to manage errors in Appendix~\ref{chap:error}.
%
Appendix~\ref{chap:libc} contains the code of generic
utility functions used by [[mk]] but which are not specific to [[mk]].
%
Finally, Appendix~\ref{chap:examples} presents examples of [[mkfile]]s.


%###############################################################################

\chapter{Core Data Structures}
\label{chap:core-ds}

\begin{verse}
\begin{flushright}
  {\it Show me your code and conceal your data structures, and I shall
    continue to be mystified. Show me your data structures, and I
    won't usually need your code; it'll be obvious.\\
    ~\\
    Fred Brooks}
\end{flushright}
\end{verse}


%toc:
In this chapter, I will present the core data structures of [[mk]]:
the {symbol table} (containing among other things the value of variables), 
the {list of rules} and {meta rules},
the {graph of dependencies},
and the description of a {job}.
\l appendix X for string buffer Bufblock? put also words there?
All those data structures are defined in the [[mk.h]] header file.

% core types: (see mk.h)
% Bufblock, Word
% Sym (a bit generic, could be in libc?)

% Rule (mk -dp?), recipe? metarule, target, prerequiste (see mk.ps)
% Node, Arc (mk -dg)
% Job, (mk -de)
% Envy (bad name, but presented later)

% core globals (see also mk.h externs):
% symbol hashtbl global (symbtab.c)
% rules, metarules, patrule,    jobs (globals.c)
% bout (globals.c)
% envy (env.c)


\section{Symbol table}

[[mk]] uses internally a {\em symbol table} to keep track
of different {things}:
the value of variables,
\l need manage variable of user.
the set of rules associated with a target,
\l need quick access to rules for a specific target.
the modification time of a file in the graph of dependencies,
etc.
\l node, dir bulk time?, more?
\l to find node corresponding to certain file (DAG) when building the graph

\l so need generic hashtbl.

\subsection{[[Symtab]]}

The structure below represents a symbol 
(e.g., variable, target, file)
and its property. 
It essentially associates a {\em key} to a {\em value}:

<<struct Symtab>>=
struct Symtab
{
    // the key: (name x space)

    // ref_own<string>
    char		*name;
    // enum<Namespace>, the ``namespace''
    short		space;

    // the value (generic)

    union{
        void*	ptr;
        uintptr	value;
    } u;

    // Extra
    <<[[Symtab]] extra fields>>
};
@
\l rename Sym? more consistent with 5a, 5l?
%ocaml: reuse hashtbl and use a record of hashtbl for namespaces
% so better types instead of overly generic union
\n is it the "world" data structure? hmm rules and metarules is more the world
\n here it's more an index over the rules, metarules, and some cache
\n  (e.g. time of a file)

Because the same string can be used
to represent a target, a file, or a variable, the key
in [[Symtab]] is a {pair} made of a string and an enumeration
constant called a {\em namespace} (stored in [[Symtab.space]]).
%
The first namespace is the one for variables:

<<enum Namespace>>=
enum Namespace {
    S_VAR,	/* variable -> value */ // value is a list of words
    <<[[Sxxx]] cases>>
};
@
\l actually should be S_CONST? well some variables can be overriden
\l  when passed on the command line, so technically it is kinda a variable
\l  but maybe better to differentate still
% use for fast access and also for memoization, operates as a cache
%dead:
%S_PID,		/* pid -> products */
%S_MAKEFILE,	/* target -> node */

%ex:
To look for the value of the variable [[$OBJS]], %$
you must use the key [[("OBJS", S_VAR)]].
\l I will speak later how, with [[symlook()]] function
\l to look for rules for target x, use [[(x,S_TARGET)]]
%chunks: 
I will gradually describe the other namespaces in the following chapters.

The value associated to a key can also be different things:
a list of words for a variable,
an integer representing a time for the modification time of a file, etc.
%
Thus, the value for a key in [[Symtab]] is a {union} containing
either an integer (in [[Symtab.u.value]]) or a generic
pointer (in [[Symtab.u.ptr]]).


\subsection{[[hash]]}

%dup: from Assembler
The symbol table itself is represented by a global 
{\em hash table} called [[hash]].
It makes sense to use a global because the symbol table will be accessed
by different components of the building pipeline.
%
One way to implement a hash table in C is to use a big array
of lists, also known as an array of {\em buckets}:

<<global hash>>=
// hash<(string * enum<Namespace>), 'a> (next = Symtab.next in bucket)
static Symtab *hash[NHASH];
@
<<constant NHASH>>=
#define	NHASH	4099
@
%ocaml: does not have to be a global, can be passed around
% if split in different parts for each namespace
% (var and internal vars, hnodes, time cache)

%dup: Assembler.nw
One way to implement a list of something in C is to embed in
this something a [[next]] field pointing to the next
element in the list:

<<[[Symtab]] extra fields>>=
struct Symtab	*next;
@

%dup:
The end of the list is represented by the null pointer.
% but no S in mk

% FIGURE? or too simple? does not hurt maybe, so can also see
% different namespaces?

%dup: (and adapted) from Linker.nw
The main interface to the symbol table is the
function [[symlook()]], which internally uses the global [[hash]].
%
[[symlook()]] takes a symbol name and a namespace, forming a full key, and 
returns the [[Symtab]] in the symbol table [[hash]] associated with this key.

\label{sec:symlook}
% many stuff -> <>
% will create new entry if not there!
<<function symlook>>=
Symtab*
symlook(char *sym, int space, void *install)
{
    Symtab *s;
    long h;
    <<[[symlook()]] other locals>>

    <<[[symlook()]] compute hash value [[h]] of [[sym]]>>

    // hash_lookup((sym, space), h, hash)
    for(s = hash[h]; s; s = s->next)
        if((s->space == space) && (strcmp(s->name, sym) == 0))
            return s;
    // else
    <<[[symlook()]] if symbol not found>>
}
@
%ocaml: just use Hashtbl.find
%compiler: called lookup in 5c (and 5a)
\t define symexists() that uses install == nil, cleaner?
\t  and return bool



<<[[symlook()]] other locals>>=
char *p;
@
<<[[symlook()]] compute hash value [[h]] of [[sym]]>>=
//h = hash(sym, space)
for(p = sym, h = space; *p; h += *p++)
    h *= HASHMUL;
if(h < 0)
    h = ~h;
h %= NHASH;
@

<<constant HASHMUL>>=
#define	HASHMUL	79L	/* this is a good value */
@


If the key is not found and the install parameter is not
[[nil]], [[symlook()]] creates a new symbol:


<<[[symlook()]] if symbol not found>>=
if(install == nil)
    return nil;

s = (Symtab *)Malloc(sizeof(Symtab));
s->space = space;
s->name = sym;
s->u.ptr = install;

// add_list(s, hash)
s->next = hash[h];
hash[h] = s;

return s;
@


In addition to [[symlook()]], [[mk]] relies also on the
generic function [[symtraverse()]] below to apply a function [[fn]]
to all elements in a specific namespace:

% execinit -> <>
<<function symtraverse>>=
void
symtraverse(int space, void (*fn)(Symtab*))
{
    Symtab **s, *ss;

    for(s = hash; s < &hash[NHASH]; s++)
        for(ss = *s; ss; ss = ss->next)
            if(ss->space == space)
                (*fn)(ss);
}
@
%ocaml: used only for ecopy, so could use Hashtbl.copy?
\l used for instance for???



%dead:
%   main -> <>
%  <<function syminit>>=
%  void
%  syminit(void)
%  {
%      Symtab **s, *ss;
%  
%      for(s = hash; s < &hash[NHASH]; s++){
%          for(ss = *s; ss; ss = ss->next)
%              free((char *)ss);
%          *s = nil;
%      }
%  }
%  @
%   free necessary? should be nil no? syminit called at the start of main
%   but then no need for syminit at all!
%ocaml: use Hashtbl, which is initialized correctly by ocaml

%\subsection{[[setvar()]]}

Finally, because setting the value of a variable is a common operation
in [[mk]], the function below provides a shortcut:

% many stuff -> <>
<<function setvar>>=
void
setvar(char *name, void *value)
{
    symlook(name, S_VAR, value)->u.ptr = value;
}
@
%dead:    symlook(name, S_MAKEVAR, (void*)"");
%dead:
%<<[[Sxxx]] cases>>=
%S_MAKEVAR,	/* dumpable mk variable */
%@
%ocaml: since S_MAKEVAR is dead, can just use Hashtbl.set

\subsection{Namespaces}

%trans:
[[mk]] allows the user define variables. [[mk]] also defines
special variables such as [[$stem]] or [[$target]].
%
To clearly separate those two kinds of variables, [[mk]] stores
them in different {namespaces}:
[[S_VAR]] for the user (and environment) variables, and
[[S_INTERNAL]] for the special (internal) variables.
\l can be in conflict with user variable  so multiple namespaces?

<<[[Sxxx]] cases>>=
S_INTERNAL,	/* an internal mk variable (e.g., stem, target) */
@
%ocaml: use record
\l diff with S_VAR? not set by user! set by mk automatically
\l those one are really variables (maybe that's why they are in lowercase)

To access the value of the [[$stem]] variable, call %$
the [[symlook()]] function with the pair [[("stem", S_INTERNAL)]].


The {private global} [[myenv]] below stores the list of special variables:

<<global myenv>>=
static char	*myenv[] =
{
    "target",
    "prereq",
    "stem",

    <<[[myenv]] other array elements>>
    0,
};
@
\l $stem $prereq (better than $@, $^ )
\l but actually can be multiple targets, and prereqs!

%chunks:
I will gradually describe the other internal variables used by [[mk]]
in the following chapters.
%chunks:
I will also gradually describe more namespaces.

[[myenv]] is used to initialize entries in the symbol table
in [[initenv()]] called from [[main()]]:

\label{sec:initenv}
% main -> <>
<<function initenv>>=
void
initenv(void)
{
    char **p;

    for(p = myenv; *p; p++)
        symlook(*p, S_INTERNAL, (void *)"");

    readenv();				/* o.s. dependent */
}
@
\t call initsyms() or inithash() (or rename Envy to Shellenv)
%bug? it is supposed to be words no? so at least newword("") ?

Moreover, [[readenv()]] called above, and described in 
Section~\ref{sec:readenv}, initializes the symbol table
with the environment variables (e.g., [[$HOME]], [[$objtype]]),
which will be stored in the [[S_VAR]] namespace.





\section{[[Word]]s}
\label{sec:word}
% and Bufblock

%trans:
There are many places in the code of [[mk]] where [[mk]]
manipulates a {\em list of words}:
when it manipulates a list of prerequistes, 
a list of targets,
or the content of a variable.
%ex: HFILES=fns.h mk.h => list of 2 words
C does not have any builtin support for lists, so [[mk]] uses the following
structure to represent a list of words:

<<struct Word>>=
struct Word
{
    // ref_own<string>
    char 		*s;

    struct Word 	*next;
};
@
\n rename Words? but then less consistent with other use of C. 
\n You just need to better qualify the pointer each time in comment.
\n But has to be Words; no much point in having Word just be a char* wrapper
%ocaml: just use List, or even better W of word list

[[mk]] defines also a few convenient functions to manipulate those lists.
%
[[newword()]] below builds a list with a single element
from a string [[s]]:

<<constructor newword>>=
Word*
newword(char *s)
{
    Word *w;

    w = (Word *)Malloc(sizeof(Word));
    w->s = strdup(s);
    w->next = nil;
    return w;
}
@
\l rename newword_list? new_singleword?

[[freewords()]] frees a list of words:

<<destructor freewords>>=
void
freewords(Word *w)
{
    Word *v;

    while(v = w){
        w = w->next;
        if(v->s)
            free(v->s);
        free(v);
    }
}
@
%ocaml: just use the garbage collector
%old: was called [[delwords]]

[[addw()]] adds a word (a string) [[s]] in a list of words [[w]]:

% treat list as a set, dont add if already there
<<function addw>>=
void
addw(Word *w, char *s)
{
    Word *lw;

    for(lw = w; w = w->next; lw = w){
        if(strcmp(s, w->s) == 0)
            return;
    }
    lw->next = newword(s);
}
@
\t mv in words.c ?
%ocaml-found: but can not use Set, because then order
% may not be the order of insertion, and it is important
% since $prereq must be the same than the order in the prereq in the mkfile.

[[wdup()]] copies (duplicates) a list of words:

<<function wdup>>=
Word*
wdup(Word *w)
{
    Word *v, *new, *base;

    v = base = nil;
    while(w){
        new = newword(w->s);
        if(v)
            v->next = new;
        else
            base = new;
        v = new;
        w = w->next;
    }
    return base;
}
@
%ocaml: just use sharing done by ocaml. Pure DS.

Finally, [[wtos()]] (for ``words to string'') concatenates
together the words in a list of words with a special
character [[sep]] (for separator):

<<function wtos>>=
char *
wtos(Word *w, int sep)
{
    Bufblock *buf;
    char *cp;

    buf = newbuf();
    for(; w; w = w->next){
        for(cp = w->s; *cp; cp++)
            insert(buf, *cp);
        if(w->next)
            insert(buf, sep);
    }
    insert(buf, '\0');

    cp = strdup(buf->start);
    freebuf(buf);
    return cp;
}
@
%ocaml: Common.join (but using Buffer would be faster too)

[[wtos()]] relies on the [[Bufblock]] data structure
described in Section~\ref{sec:bufblock}.
%
[[Bufblock]] is an implementation of a {\em string buffer}.
It implements efficiently string concatenation, to avoid quadratic
complexity when contenating a set of strings together.
\l will use Bufblock many times. 
%ocaml: use Buffer.insert
The code for [[Bufblock]] is in [[mk/bufblock.c]], but 
this code could be put in a library and used by other projects
as it is a general-purpose data structure. This is why
its code is described in Appendix~\ref{chap:libc} and not here.
\l but then Word could be there too

\l stow() later, but complex because does variable expansion!



\section{Rules}

%trans:
As mentioned in Section~\ref{sec:rule-essence}, the {rule}
is one of the most important concepts in a build system, and
so one of the most important data structures in [[mk]].
%
It represents the content of an [[mkfile]] and it guides
the creation of the graph of dependencies.

\subsection{[[Rule]]}

The structure [[Rule]] below represents a {rule} in memory.
You can see that the first fields represent the major
elements of a rule I mentioned in Section~\ref{sec:rule-simple-mkfile}:
the [[target]], the [[prereq]]uiste[[s]], and the [[recipe]].

<<struct Rule>>=
struct Rule
{
    char 		*target;	/* one target */
    // option<ref_own<Words>>
    Word 		*prereqs;		/* constituents of targets */
    // ref_own<string>, never nil, but can be the empty string (just '\0')
    char 		*recipe;	/* do it ! */

    <<[[Rule]] other fields>>
    <<[[Rule]] debug fields>>

    // Extra
    <<[[Rule]] extra fields>>
};
@
%old: tail -> prereqs, because tail abused (and prereqs more precise)
\l seems like recipe can not be nil actually, cos do *a->r->recipe
\t recipe can also just be '\0' ? ugly.
\t  can be nil? sometimes seems assume cannot, and sometimes can. fix it!
\t  according to rbody(), it cant be nil, at least it is empty string

[[Rule.prereqs]] contains a list of words, hence the use of a pointer
to a [[Word]] ([[Word]] was described in Section~\ref{sec:word}).
%
[[Rule.target]] is a single string, not a list of words, 
even though some rules have multiple targets. 
I will explain later how rules with multiple targets are represented.
%
\l note that can have multiple rules with the same head, 
\l in which case you add prerequistes?
[[Rule.recipe]] is a string. This string can contain variables 
using the dollar sign.
However, the strings in [[Rule.target]] and [[Rule.prereqs]]
do not contain any variable. Indeed, as I will show in 
Section~\ref{sec:expand-vars}, variables used outside a recipe
are expanded at parsing time.
% from man page:
%  Variable substitution in a rule is done when
%  the rule is read; variable substitution in the recipe is done
%  when the recipe is executed.
\l why? probably because recipe/rc will do the subst itself, but for
\l the graph we need concrete values so faster to do it eagerly when parsing.
% Also because for recipe, some values are known later, when instantiate 
% the rule (e.g., for $stem)
% can also be metarule, %.$O: %.c (but $O would have been expanded)
\l can contain percent

In addition to the major fields mentioned above, [[Rule]] contains
also information about where a rule comes from:

<<[[Rule]] debug fields>>=
char* 		file;		/* source file */
short 		line;		/* source line */
@
\l put in Debugging support section?

This is useful when reporting errors in the [[mkfile]] to the user.



\subsection{Simple [[rules]]}

The list of all {\em simple rules}, that is all non-meta rules, is stored
in the global [[rules]]:

<<global rules>>=
// list<ref_own<Rule>> (next = Rule.next, end = lr)
Rule *rules;
@
%ocaml: return them instead of using globals

When [[mk]] parses an [[mkfile]] (and possibly some included files),
it populates this global (using the [[addrule()]] function
described below in Section~\ref{sec:addrule}).

Again, in C, you can embed a [[next]] field in a structure to make it a
list:

<<[[Rule]] extra fields>>=
// list<ref_own<Rule>> (head = rules | metarules)
struct Rule	*next;
@

To quickly add a rule to the end of the list [[rules]], [[mk]]
maintains another global [[lr]] pointing to the last rule:

<<global lr>>=
// option<ref<Rule>> (head = rules)
static Rule *lr;
@

\subsection{[[metarules]]}

The list of all {\em meta rules} is stored instead in an another global:

<<global metarules>>=
// list<ref_own<Rule>> (next = Rule.next, end = lmr)
Rule *metarules;
@
%ocaml: return them instead of using globals

[[mk]] relies also on a global pointing to the last meta rule:

<<global lmr>>=
// option<ref<Rule>> (head = metarules)
static Rule *lmr;
@

Remember that a meta rule is a rule using the {special character} [[%]]
to specify a {\em pattern} in the target or prerequistes of a rule
(see Section~\ref{sec:pattern}).
In fact, [[mk]] supports another special character to represent
a pattern: [[&]], hence the code in the macro below:

<<function PERCENT>>=
#define	PERCENT(ch)	(((ch) == '%') || ((ch) == '&'))
@

The difference between those two special characters
is explained in the manual page of [[mk]]:
\begin{itemize}

\item [['%']] matches a maximal length string of any characters

\item [['&']] matches a maximal length string of any characters 
except period or slash
\end{itemize}
 
In addition to being stored in [[metarules]] instead of [[rules]],
a meta rule contains also the [[META]] {\em rule attribute} in 
[[Rule.attr]]:

<<[[Rule]] other fields>>=
// bitset<Rule_attr>
short 		attr;		/* attributes */
@
\l contain visible rule attribute between :: and some implicit like META

<<enum Rule_attr>>=
enum Rule_attr {
    META   = 0x0001,
    <<[[Rule_attr]] cases>>
};
@
%dead: indeed, unused :)
% UNUSED = 0x0002,
%dead:
% UPD    = 0x0004,
% <<[[rhead()]] when parsing rule attributes, switch rune cases>>=
% case 'U':
%     *attr |= UPD;
%     break;
% @
% "the targets are considered to have been updated even if recipe did not do so"

\l META is for % metarule (and also regexp rules :R:)

%chunks: 
I will describe gradually the other rule attributes in the following chapters.
\l users can add explicit attributes also to a rule



\subsection{Adding rules}
\label{sec:addrule}
% and indexing

Now that I described the data structures and globals related to
the rules and meta rules, I can explain the code to add rules.

\subsubsection{One rule with one target, [[addrule()]]}

[[addrule()]] is used to add a rule with a single target.
%
I will explain later the code to support rules with multiple targets.


% kind of ctor
% (main -> parse | main) -> <>
<<function addrule>>=
void
addrule(char *head, Word *tail, char *body, 
        Word *ahead, int attr, int hline, char *prog)
{
    Rule *r = nil;
    <<[[addrule()]] other locals>>

    <<[[addrule()]] find if rule already exists, set reuse>>

    if(r == nil)
        r = (Rule *)Malloc(sizeof(Rule));

    r->target = head;
    r->prereqs = tail;
    r->recipe = body;

    r->line = hline;
    r->file = infile;

    r->attr = attr;
    <<[[addrule()]] set more fields>>

    <<[[addrule()]] indexing [[r]] by target in [[S_TARGET]]>>

    <<[[addrule()]] if meta rule>>
    else {
        <<[[addrule()]] return if reuse, to not add the rule in a list>>
        r->pat = nil;

        // add_list(r, rules, lr)
        if(rules == nil)
            rules = lr = r;
        else {
            lr->next = r;
            lr = r;
        }
    }
}
@
\t rename arguments! target, prereqs, alltargets, etc
\l r->pat ?? Malloc does not memset zero anyway?
%ocaml: use record, and no need wrapper constructor, construct directly with
% the fields

<<[[addrule()]] if meta rule>>=
if(charin(head, "%&") || (attr&REGEXP)){
    r->attr |= META;
    <<[[addrule()]] return if reuse, to not add the rule in a list>>
    <<[[addrule()]] if REGEXP attribute>>

    // add_list(r, metarules, lmr)
    if(metarules == nil)
        metarules = lmr = r;
    else {
        lmr->next = r;
        lmr = r;
    }
}
@

I will describe the function [[charin()]] later in Section~\ref{sec:charin}.
%
Note that [[charin()]] does not just search for a set of character 
in a string (here [[head]], the target). Indeed, [[charin()]] must
also handle {escaped characters}, for instance, when the target name
is put inside a quote as in [['myfile%has%weird%characters.doc']],
in which case the target should not be considered a pattern.

For the rule attribute [[REGEXP]] used below, see Section~\ref{sec:regexp}.


\subsubsection{One target with multiple rules, [[S_TARGET]]}

It is useful, when building the graph of dependencies, to quickly
know the rule associated to a specific target.
%
Thus, [[mk]] uses another namespace, [[S_TARGET]], to store
such information in the symbol table.

<<[[Sxxx]] cases>>=
S_TARGET,		/* target -> rules */ 
@
%old: rule,  but really it is -> rules
%ocaml: build hash in separate phase

In fact, as I mentioned in Section~\ref{sec:master-rule}, [[mk]]
allows the user to write multiple rules using the same target.
%
For instance, an [[mkfile]] can contain a {master rule} such as
[[foo.5: foo.c ...]], and a [[.depend]] file included by this [[mkfile]]
can contain another rule without any recipe but extra dependencies such as
[[foo.5: foo.h bar.h]].
\l need :: ? will get ambiguous rule?
Thus, the symbol table and the namespace [[S_TARGET]] map a target
to a set of rules chained together by an extra field in [[Rule]]:

<<[[Rule]] extra fields>>=
// list<ref<Rule>> (head = symlook(x, S_TARGET))
struct Rule	*chain;		/* hashed per target */
@
% FIGURE note that Rule.chain is different than Rule.next! And also Sym.next!
% Here in same list because same target.
% For Sym.next in same list because same hashed (name x namespace).
% Can even have symbol in different namespace in the same bucket in Sym.next.

Here is the code to update the symbol table when adding a rule:

<<[[addrule()]] other locals>>=
Symtab *sym;
Rule *rr;
@
<<[[addrule()]] indexing [[r]] by target in [[S_TARGET]]>>=
if(!reuse){
    sym = symlook(head, S_TARGET, r);
    rr = sym->u.ptr;
    if(rr != r){ // target had already a rule
        r->chain = rr->chain;
        rr->chain = r;
    } else 
        r->chain = nil;
}
@
%old: rr = symlook(head, S_TARGET, r)->u.ptr;

Remember from Section~\ref{sec:symlook} that the last parameter
of [[symlook()]], called [[install]] (and here set to the argument [[r]]),
is used to initialize a new symbol if the symbol was not already
in the symbol table.
%
Thus, if the test [[if (rr != r)]] above succeeds, this means
a symbol was already there, in which case [[mk]] needs to add
the rule [[r]] to the chain.


The guard using the variable [[reuse]] above will be explained in the next
section.


\subsubsection{Overwriting a previous rule}
\label{sec:overwrite-rule}

[[mk]] allows to overwrite the recipe of a
a rule when another rule uses exactly the same target and prerequistes.
%
This can be useful when a generic [[mkfile.generic]]
defines some default targets and recipes, but the user wants to overwrite
those defaults in his own [[mkfile]] (which can include [[mkfile.generic]]).
\l could be done with variables, but true that sometimes difficult

The code below detects whether a previous rule was using the
same target and prerequistes, in which case [[mk]] needs to
reuse and overwrite this previously allocated rule:

<<[[addrule()]] other locals>>=
bool reuse;
@
<<[[addrule()]] find if rule already exists, set reuse>>=
reuse = false;
if(sym = symlook(head, S_TARGET, nil)){
    for(r = sym->u.ptr; r; r = r->chain)
        if(rcmp(r, head, tail) == 0){
            reuse = true;
            break;
        }
}
@
%ocaml: return a warning at least, but overwrite otherwise it will
% be detected by ambiguous later.
%ocaml: can have a :O: attribute clearly saying you overwrite! @override spirit.

<<function rcmp>>=
static int
rcmp(Rule *r, char *target, Word *tail)
{
    Word *w;

    if(strcmp(r->target, target))
        return 1;
    for(w = r->prereqs; w && tail; w = w->next, tail = tail->next)
        if(strcmp(w->s, tail->s))
            return 1;
    return (w || tail);
}
@
\t strcmp sucks, use string_equal, and return bool that makes sense
\t  so less rcmp(...) == 0, instead rule_equal(...)
%bug? what about when different orders of prereqs? should compare as a set.
% it will be detected as ambiguous then


<<[[addrule()]] set more fields>>=
if(!reuse){
    r->next = nil;
}
@
%old: was after the indexing of S_TARGET, but seems unrelated,
% so I put it earlier, with the code to set misc fields.

<<[[addrule()]] return if reuse, to not add the rule in a list>>=
if(reuse)
    return;
@




\subsubsection{One rule with multiple targets, [[addrules()]]}
\label{sec:multi-targets}
\n was in advanced topics, but  maybe fundamental features then.
\n Also too much code doing stuff to support multiple targets.

%trans:
I can finally show the code to handle rules with multiple targets.
%
[[mk]] first adds separate rules for each target in the original rule:

<<function addrules>>=
void
addrules(Word *head, Word *tail, char *body, int attr, int hline, char *prog)
{
    Word *w;

    assert(/*addrules args*/ head && body);

    <<[[addrules()]] set [[target1]]>>
    for(w = head; w; w = w->next)
        addrule(w->s, tail, body, head, attr, hline, prog);
}
@
\t head -> targets!! tail -> prereqs

%trans: dup: overview/graph/
However, as I mentioned in Section~\ref{sec:graph-many-to-one}, 
the use of multiple targets in a rule has implications on the
DFS traversal of the graph of dependencies.
%
[[mk]] needs to remember the other targets associated with a rule.
This is why in addition to passing [[w->s]] above, [[addrules()]]
passes also set the of targets in the third argument to [[addrule()]].
%
This argument is then stored in a special field in the rule:

<<[[Rule]] other fields>>=
// ref<list<ref_own<string>>
Word 		*alltargets;	/* all the targets */
@
% as in  gram.c gram.h: gram.y ...
% (hmm but could be rewritten with 2 rules, it's just
% that with mk the file would be possibly rebuilt two times!
% because it could run the command in parallel!!!!
%ocaml: merge with Rule.target, and make it a list instead of char*

<<[[addrule()]] set more fields>>=
r->alltargets = ahead;
@
\t rename ahead -> alltargets

% thus even if add a rule with one target (in Rule.target),
% mk remembers other targets in the rule:

% Is there a few places where use Word* targets
% where in most cases it should really be simply char* target; ??

\l why target and alltargets? why not just have alltargets?
\l because we index per single target so in applyrules we can be
\l faster (but still slow on metarules)


%\subsubsection{Extracting the first target}
%target1

\section{Graph}

%trans:
% Rule and metarules are general static descriptions, patterns.
% Node and Arc are concrete instantiation with existing files.

The graph of dependencies is represented in [[mk]] essentially
as a set of {nodes} linked together through pointers.
%
[[mk]] does not use a matrix or an array of adjacent lists
to represent a graph; it just uses pointers.

%toc: 
\l nodes and arcs, toc

\subsection{[[Node]]}

A node represents a file in the graph of dependencies.
%
As I mentioned in Section~\ref{sec:labeled-tree},
a node is also {labeled} with the modification time of the file.
That way, the DFS can find out-of-date files by comparing
the [[Node.time]] field in different nodes.

<<struct Node>>=
struct Node
{
    // ref_own<string>, usually a filename, or a virtual target like 'clean'
    char*		name; 
    // option<Time> (None = 0, for nonexistent files and virtual targets)
    ulong		time; // last mtime of file 

    <<[[Node]] arcs field>>
    <<[[Node]] other fields>>

    // Extra
    <<[[Node]] extra fields>>
};
@
\l name can also maybe be an archive member? like lib.a(foo.o) ?

The function below builds a new node:

%ctor. dtor = ?
% main -> mk -> graph -> applyrules -> <>
<<constructor newnode>>=
static Node*
newnode(char *name)
{
    Node *node;

    node = (Node *)Malloc(sizeof(Node));
    <<[[newnode()]] update node cache>>

    node->name = name;
    // call to timeof()! 
    node->time = timeof(name, false);
    <<[[newnode()]] set flags of node>>

    node->arcs = nil;
    node->next = nil;
    <<[[newnode()]] debug>>
    return node;
}
@
\n owning name after call! => call strdup in caller

% pass false, cos ok to use cache. nothing has been generated so nothing
%  can have invalidate the cache.
\t rename false to USE_CACHE_IS_OK, vs FORCE_READ_NO_CACHE
\l node cache? put now?

A node is also labeled with a set of {\em node attributes}:
\n seen rule attributes before

<<[[Node]] other fields>>=
// bitset<enum<Node_flag>>
ushort		flags;
@
<<enum Node_flag>>=
enum Node_flag {
    <<[[Node_flag]] cases>>
};
@
%old: used to be a set of #define

The build status of a node ([[Made]], [[NotMade]], and [[BeingMade]]),
which I mentioned in Section~\ref{sec:build-status}, 
is stored in [[Node.flags]], as well
as other information used for advanced features of [[mk]].

<<[[Node_flag]] cases>>=
NOTMADE    = 0x0020,
BEINGMADE  = 0x0040,
MADE       = 0x0080,
@
%ocaml: should be different field than other node flags


%chunks: 
I will gradually describe the other {node attributes} in the following chapters.
\l mostly used during analysis of graph.
\l often also an inline of the union of the flags of the rule 
\l  linking to the prerequistes nodes

\subsection{[[Arc]]}

A [[Node]] contains also a set of arcs where each arc
contains a pointer to another node (a prerequiste):

<<[[Node]] arcs field>>=
// list<ref_own<Arc>> (next = Arc.next)
Arc		*arcs;
@
%old: prereqs -> arcs; better call it arcs because
% not always a prereq as for instance virtual rule have an arc
% from the target without any destination node

<<struct Arc>>=
struct Arc
{
    // option<ref<Node>>, the other node in the arc (the dependency)
    struct Node *n;
    // ref<Rule>, to gen the target node from the dependent node
    Rule *r;

    <<[[Arc]] other fields>>
    
    //Extra
    <<[[Arc]] extra fields>>
};
@
\n option Rule? I dont think so, every call to newarc have a defined rule arg
%ocaml: I think you just need the recipe and some flags of the rule,
% so separate rule vs rule_exec

As I mentioned in Section~\ref{sec:labeled-tree}, an arc is labeled
with a rule, hence the field [[Arc.r]] above.
%
Note that [[Arc.n]] can sometimes be [[nil]] when a rule does not have
any prerequiste (for instance, because it is a virtual target,
as explained in Section~\ref{sec:virtual}). In that case,
we still want the node corresponding to the target of the rule
to be connected to a rule, especially its recipe.
%alt: could have rule in Node, actually master rule in Node.
\l seen Rule before.

The head of the list of arcs of a node is stored in [[Node.arcs]],
and the arcs are chained together with the following field:

<<[[Arc]] extra fields>>=
// list<ref_own<arc> (head = Node.prereqs)
struct Arc	*next;
@


Some nodes and arcs are derived from meta rules. 
For instance, in Figure~\ref{fig:graph-hello},
the nodes [[hello.5]] and [[hello.c]] could come from
a meta rule such as [[%.5: %.c ...]].
%
In that case, [[mk]] needs to remember in the arc 
connecting [[hello.5]] to [[hello.c]] the {stem}
that was used to instantiate the meta rule (here [[hello]]):

% so can set $stem for the actual value for the recipe to run
<<[[Arc]] other fields>>=
// option<ref_own<string>>, what '%' matched?
char		*stem;
@
\l why need that? Node.name is not expanded already?
%  yes, in graph, but in recipe still need to set dollar stem in env
\l here not in Rule because Rule is generic template; Here it is instantiated.
% Also used because in dorecipe we also use alltargets, which is not
%  expanded.
%ocaml: do a rule_exec containing all necessary information

The function below builds a new arc to connect 
to a node [[n]], with the rule [[r]], possibly instantiated
with the stem [[stem]] if the rule was a meta rule
(the last parameter [[match]] is used for regexp rules,
as explained in Section~\ref{sec:regexp}):

% ctor.  dtor = ?
% main -> mk -> graph -> applyrules -> <>
<<constructor newarc>>=
Arc*
newarc(Node *n, Rule *r, char *stem, Resub *match)
{
    Arc *a;

    a = (Arc *)Malloc(sizeof(Arc));
    a->n = n;
    a->r = r;
    a->stem = strdup(stem);

    a->next = nil;
    a->flag = 0;
    <<[[newarc()]] set other fields>>
    return a;
}
@
\l assert r ? (but not assert n)




\section{[[Job]] and [[jobs]]}

%trans:
Finally, the last core data structure of [[mk]] is the
%
description of a job.
%
As I mentioned in Section~\ref{sec:job}, a job must contain
all the information needed to run a recipe and update
the graph of dependencies: a rule (and its recipe), 
a list of nodes to update, and the value of special
variables such as [[$target]], [[$prereq]], or
[[$stem]]:

<<struct Job>>=
struct Job
{
    // ref<Rule>
    Rule		*r;	/* master rule for job */

    // list<ref<Node>> (next = Node.next)
    Node		*n;	/* list of node targets */

    // $target and $prereq
    Word		*t;	/* targets */
    Word		*p;	/* prerequisites */
    // $stem
    char		*stem;

    <<[[Job]] other fields>>

    // Extra
    <<[[Job]] extra fields>>
};
@
%ocaml: I think you just need the recipe and some flags of the rule.
\l why need targets? info is already in node. To ease envupd?
\l why so short field names suddenly? avoid confusion with the one in Rule? meh


% reference rule, node (so Arc) (and Word but less important)

% have list of nodes because finishing a job will modify
% the graph! put some MADE

% targets! prerequisites! plural!

% stem again? propagated from Arc? to pass to shell for dollar stem setting
%ocaml: factorize in rule_exec

The list of target nodes of a job are chained together through
an extra field in [[Node]]:

<<[[Node]] extra fields>>=
// list<ref<Node>> (head = Job.n)
struct Node	*next;		/* list for a rule */
@
% list of all targets for a job (with a master rule), as when done
%  will need to mark as done multiple files (and check they are there)
% usually have set of nodes via Arc, but here, no need target (parent) node.

The function below builds a new job:

% ctor
<<constructor newjob>>=
Job*
newjob(Rule *r, Node *nlist, char *stem, char **match, 
       Word *pre, Word *npre, 
       Word *targets, Word *atar)
{
    Job *j;

    j = (Job *)Malloc(sizeof(Job));
    j->r = r;
    j->n = nlist;
    j->p = pre;
    j->t = targets;

    j->stem = stem;
    j->match = match;

    j->np = npre;
    j->at = atar;

    j->next = nil;
    return j;
}
@
%old: tar -> targets
\t rename fields, aspectize setting certain fields


This job can then be added in the job queue stored in the
global [[jobs]]:

<<global jobs>>=
// list<ref_own<jobs>> (next = Job.next)
Job *jobs;
@
\n here cos used by mk(). Cant delayed.
\t actually aspectized now, so maybe could delay? or maybe useful
\t  to know there is a list of jobs
%ocaml: not sure you need a global, can be a local in run.ml?
% also use a Queue

<<[[Job]] extra fields>>=
struct Job	*next;
@

%trans:?
% draw diagram showing relations to each other, what includes what?









\chapter{[[main()]]}
\label{chap:main}

%trans: %dup: (and adapted) from Assembler.nw
I now switch from a bottom-up approach in Chapter~\ref{chap:core-ds},
to a top-down approach in this chapter.
%toc:
Indeed, I will describe now the main functions of [[mk]], 
starting in this chapter with [[main()]], the entry point of [[mk]].

\section{[[main()]] skeleton}

The main components of the building pipeline are illustrated in the
[[main()]] skeleton below:

<<function main>>=
void
main(int argc, char **argv)
{
    <<[[main()]] locals>>

    // Initializing

    <<[[main()]] initializations>>

    // Parsing the mkfile

    <<[[main()]] parsing mkfile, call [[parse()]]>>

    // Building the graph, finding out-of-date files

    <<[[main()]] initializations before building>>
    <<[[main()]] setting the targets, call [[mk()]]>>

    // Reporting (optional)

    <<[[main()]] print profiling stats if uflag>>

    // Exiting

    exits(nil);
}
@

%trans:
The next chapters will detail those different components.
%toc:
In this chapter, I will focus mostly on the initializations
and the processing of command-line arguments.

An important global set by [[main()]] is [[bout]]:

<<global bout>>=
Biobuf bout;
@
%ocaml: stdlib already buffer output

[[mk]] uses this global to print message to the user (errors, job progress,
profiling information, etc). 
[[bout]] is a buffer connected to the standard output:

<<[[main()]] initializations>>=
Binit(&bout, STDOUT, OWRITE);
@

[[Biobuf]], the type of  [[bout]], is a data structure defined
in the [[libbio]] (for buffered IO) library, which
extends the C library (see the \book{Libcore}).
\l  compared to scanf/printf? No big win vs printf I think.
\l  actually use fprintf(STDERR,) for other stuff


[[mk]] processes the command-line arguments in three steps,
as hinted in the code below, and as explained in the following sections.

<<[[main()]] initializations>>=
<<[[main()]] argv processing part 1, -xxx>>
<<[[main()]] setup profiling>>
initenv();
<<[[main()]] argv processing part 2, xxx=yyy>>
<<[[main()]] set variables for recursive mk>>
<<[[main()]] argv processing part 3, skip xxx=yyy>>
<<[[main()]] profile initializations>>
@
%old: 
% - the processing part 3 was before catchnotes() before but I think better here
% - calls to second usage was earlier
%dead: syminit();
\l also profiling stuff and recursive mk, explained later.

%/*
% *  start with a copy of the current environment variables
% *  instead of sharing them
% */
\t for execinit?? for initenv/

As I explained in Section~\ref{sec:initenv}, [[initenv()]] called above
initializes certain variables in the symbol table and {imports}
variables from the environment in the symbol table (e.g., [[$objtype]],
[[$HOME]]).
%$

\section{[[mk -]]{\em flag} arguments processing}

%trans:
The first step in the processing of command-line arguments
is an iteration over [[argv]]:

<<[[main()]] argv processing part 1, -xxx>>=
USED(argc);
for(argv++; *argv && (**argv == '-'); argv++)
{
    <<[[main()]] add [[argv[0]]] in [[buf]]>>

    switch(argv[0][1]) {
    <<[[main()]] -xxx switch cases>>
    default:
        badusage();
    }
}
@
%ocaml: use Arg module

This iteration looks for command-line arguments prefixed by [[-]]
(e.g., [[-f]]).
\l or debug flags.
%chunks:
I will gradually described the cases of the [[switch]] above 
in the following chapters.
\l note that modify argv!
\l also special code for buf? explained later

\section{[[mk ]]{\em var}[[=]]{\em values} arguments processing}

%trans:
The second step in the processing of command-line arguments
is also an iteration over [[argv]], but this time looking
for arguments containing an equal sign.
%
Indeed, [[mk]] allows the user to overwrite variables defined
in the [[mkfile]] by adding a command-line argument
in the form [[x=y]] before the target, as in [[mk objtype=arm all]].


Because the parser of [[mkfile]] I will describe in Chapter~\ref{chap:parsing}
contains already code to process variable definitions, [[mk]]
reuses this code to deal with command-line definitions.
%
Indeed, after storing those definitions in a temporary file, [[mk]] can
then simply call [[parse()]] on this temporary file to load those definitions.


The temporary file is first 
a filename ([[temp]]), then 
a file descriptor ([[tfd]]) once opened, and finally 
an output buffer ([[tb]]) once initialized:

<<[[main()]] locals>>=
int i;
char *temp = nil;
fdt tfd = -1;
Biobuf tb;
@

<<[[main()]] argv processing part 2, xxx=yyy>>=
for(i = 0; argv[i]; i++) 
  if(utfrune(argv[i], '=')){
    <<[[main()]] add [[argv[i]]] in [[buf]]>>

    <<[[main()]] create temporary file if not exist yet and set [[tb]]>>
    Bprint(&tb, "%s\n", argv[i]);
    <<[[main()]] mark [[argv[i]] for skipping>>
  }

if(tfd >= 0){
    Bflush(&tb);
    seek(tfd, 0L, SEEK__START);
    parse("<command line args>", tfd, true);
    remove(temp);
}
@
% abuse parse(), pass true so will get varoverride to true
%  so those variables will be marked specially S_OVERRIDE
%old: seek() used to be LSEEK() a macro expanding ultimately to seek

[[utfrune()]] called above is a function looking for a certain character
in a string. This string can contain unicode characters
(see the \book{Libcore} for more information on unicode and [[utfrune()]]).


The last argument to [[parse()]] above is a boolean indicating
whether [[parse()]] should accept definitions overwriting
previous definitions. Obviously, here [[true]] is passed to [[parse()]].


<<[[main()]] create temporary file if not exist yet and set [[tb]]>>=
if(tfd < 0){
    temp = maketmp();
    if(temp == nil) {
        perror("temp file");
        Exit();
    }
    tfd = create(temp, ORDWR, 0600);
    if(tfd < 0){
        perror(temp);
        Exit();
    }
    Binit(&tb, tfd, OWRITE);
}
@
\l perror?

The last step in the processing of command-line arguments
is to skip variable definitions:

<<[[main()]] mark [[argv[i]] for skipping>>=
/*
 *   assignment args become null strings
 */
*argv[i] = '\0';
@

<<[[main()]] argv processing part 3, skip xxx=yyy>>=
/* skip assignment args */
while(*argv && (**argv == '\0'))
    argv++;
@
% but what if mix target and assignments? then will get an error?
\t test it  mk XX=1 default YY=2,  get good error message?

Once mk has cleaned up [[argv]], the strings remaining
in [[argv]] are the targets the user wants to build.


\section{Parsing the [[mkfile]] or [[mk -f]]{\em file}}

%trans:
I described before in Section~\ref{sec:getting-started}
the use of [[-f]] to change the default file used by [[mk]]:

<<[[main()]] locals>>=
char *f = nil;
@
% can have more than one file? can use multiple -f, but meh.
%old: char *files[256];
%old: char **f = files;
%ocaml: one -f is enough

<<[[main()]] -xxx switch cases>>=
case 'f':
    if(*++argv == nil)
        badusage();
    f = *argv;
    <<[[main()]] add [[argv[0]]] in [[buf]]>>
    break;
@
% saw -xxx processing before.
%old:    *f++ = *argv;
%old: <<[[main()]] locals>>=
%old: char **ff;
%old: @


<<[[main()]] parsing mkfile, call [[parse()]]>>=
if(f == nil){
    if(access(MKFILE, AREAD) == OK_0)
        parse(MKFILE, open(MKFILE, OREAD), false);
} else
    parse(f, open(f, OREAD), false);
<<[[main()]] if DEBUG(D_PARSE)>>
@
%old: use AREAD and OREAD instead of 4 and 0 argument
%old:
%  if(f == files){
%      if(access(MKFILE, AREAD) == OK_0)
%          parse(MKFILE, open(MKFILE, OREAD), false);
%  } else
%      for(ff = files; ff < f; ff++)
%          parse(*ff, open(*ff, OREAD), false);

%bug? if mkfile not there, no error? it displays "mk: nothing to mk"
% because target will be empty

The [[parse()]] function called above will process the [[mkfile]]
(or another file if [[-f]] was used), modify the globals
[[rules]] and [[metarules]] as well as other globals.
%
Note that this time [[false]] is passed to [[parse()]], so overwriting
variable definitions is forbidden.
\l is it actually? or it is just not processed and silently passed over?

\section{Building the target(s)}

%trans:
Once [[parse()]] processed the [[mkfile]] and modified some globals,
[[mk]] is ready to build a target.
%toc:
There are multiple ways to specify the target to build and how
to build it, as explained in the following sections.

<<[[main()]] setting the targets, call [[mk()]]>>=
if(*argv == nil){
    <<[[main()]] when no target arguments>>
} else {
    <<[[main()]] if sequential mode and target arguments given>>
    else {
       <<[[main()]] parallel mode and target arguments given>>
    }
}
@

\subsection{Default target, [[target1]]}
\label{sec:default-target}

%dup: overview/interface overview/soft-archi
As I mentioned in Section~\ref{sec:mk-interface}, 
if the user did not specify any target on the command-line,
[[mk]] uses the target of the first simple rule found 
in the [[mkfile]] as the default target.
%
This default target is stored in the following global:

<<global target1>>=
Word *target1;
@
\l e.g. 'all, 

Section~\ref{sec:set-target1} contains the code in [[parse()]]
modifying [[target1]].
%
If [[target1]] was set, and the user did not provide a target
on the command-line, then [[mk]] builds this target by calling [[mk()]]:

<<[[main()]] locals>>=
Word *w;
@
<<[[main()]] when no target arguments>>=
if(target1)
    for(w = target1; w; w = w->next)
        // The call!
        mk(w->s);
else {
    fprint(STDERR, "mk: nothing to mk\n");
    Exit();
}
@

Note that first simple rule can contain multiple targets, which
is why the code above iterate over the list of words in [[target1]].
\l The function [[Exit]] called above is
% will see later that Exit is special, but here could be just exits I think.
\n actually you could call exit here 

\subsection{Sequential mode}

%trans:
The second way to build a target, or multiple targets, is 
to specify a set of targets on the command-line.
%
Moreover, [[mk]] supports a special flag, [[-s]] (for sequential mode),
to build in sequence those targets:

% sequential not parallel command line arguments
<<[[main()]] locals>>=
bool sflag = false;
@

<<[[main()]] -xxx switch cases>>=
case 's':
    sflag = true;
    break;
@
\t why do you want that? to debug?

<<[[main()]] if sequential mode and target arguments given>>=
if(sflag){
    for(; *argv; argv++)
        if(**argv)
            mk(*argv);
}
@
\l but actually for that target it will do things in parallel.
% (unless you put nproclimit to 1), so maybe not that useful.
\l put in advanced feature?

\subsection{Parallel mode}

%trans:
The last way to build one or more targets is to specify
them on the command-line without the  [[-s]] flag. 
%
In that case, [[mk]] builds the targets in parallel.
%
To do so, [[mk]] creates a new rule
with the command-line targets as the prerequistes of the new rule,
and an arbitrary string for its target.
%
[[mk]] then calls [[mk()]] with this arbitrary string, which will
engage the DFS and machinery to build its prerequistes in parallel.

<<[[main()]] parallel mode and target arguments given>>=
Word *head, *tail;
Word *t;

/* fake a new rule with all the args as prereqs */
tail = nil;
t = nil;
for(; *argv; argv++)
    if(**argv){
        // add_list(newword(*argv), t)
        if(tail == nil)
            tail = t = newword(*argv);
        else {
            t->next = newword(*argv);
            t = t->next;
        }
    }
if(tail->next == nil)
    // a single target argument
    mk(tail->s);
else {
    head = newword("<command line arguments>");
    addrules(head, tail, strdup(""), VIR, mkinline, nil);
    mk(head->s);
}
@
\l why not call addrule() directly? why addrules? because addrules set target1
\l confusing that tail is actually the head of the prereqs
\t tail -> prereqs, t -> eprereqs, head -> targets

You can see a few functions I described in Chapter~\ref{chap:core-ds}
such as [[newword()]] and [[addrules()]]. I will describe [[mk()]],
the most important function of [[mk]], in Chapter~\ref{chap:finding-outdated}.

The [[VIR]] argument above indicates that the target is a 
{\em virtual target}. [[VIR]] is a rule attribute I will
explain fully in Section~\ref{sec:virtual}.
%
The arbitrary string used in the call to [[newword()]]
is a virtual target because it does not correspond
to a file. It is ok if the target does not exist after the recipe has run.
\l Virtual targets are often used for targets such as [[mk clean]] (said later)


The last two arguments to [[addrules()]] above
are the line and file location of the rule. This is used
for error reporting. Here, because this rule was created artificially
by [[mk]], the file location is set to [[nil]].
\l mkinline, meh. could pass -1 or 0 instead




\chapter{Parsing the [[mkfile]]}
\label{chap:parsing}

%trans:
Now that you have seen [[main()]], I can explain
the different components in the building pipeline,
starting in this chapter with the parsing functions.

%dup?
I mentioned before [[parse()]], which 
takes a path to an [[mkfile]] as an argument, 
parses this [[mkfile]] to identify rules, meta rules, and definitions, and 
stores those entities in different globals
([[rules]], [[metarules]], and the symbol table [[hash]]).
\n also patrules and target1 (said later)
%ocaml: return those instead of modifying globals
%toc:
[[parse()]] is a complex function that relies on many other
functions to 
scan a file,
identify rules,
expand variables,
process included files,
or define variables,
as explained in the following sections.

\l mk has a very particular way to parse. Partly because mimimal-syntax.
% But maybe grew out with more escaping rules and in the end it's
% a bit complicated.
%real-world: Unix Make was actually using yacc.
%ocaml: cleaner to use Yacc. But, in recipe the tokens
% have a different meaning so you need a lexer with different states/modes 
% (a la Perl/PHP). Also escaping is tricky. Unicode also is tricky.

\section{[[parse()]]}

%trans:
Before showing the code of [[parse()]],
%
I describe here a few globals used by [[parse()]] (and a few other
functions) to report errors to the user.

[[infile]] below contains the name of the file currently 
processed by [[parse()]] (an [[mkfile]] or one of its included files):

<<global infile>>=
char *infile;
@

[[mkinline]] contains the line number of the line currently processed
by [[parse()]]:

<<global mkinline>>=
int mkinline;
@
\t rename just inline? conflicting keyword? rename lineno.
%ocaml: so many code in this chapter that updates mkinline, ugly; a bit better 
% in mk-in-ocaml where it is more centralized

Both globals are used in the following macro to report syntax 
errors to the user:

<<function SYNERR>>=
#define	SYNERR(l)	(fprint(STDERR, "mk: %s:%d: syntax error; ", \
                            infile, ((l)>=0)? (l) : mkinline))
@
\l when call SYNERR with explicit line? when dont want default of using
%  mkinline because error was actually before.
\l note that useful to put mk: before, to indicate errors comes from mk,
%  not one of its subprocess (and there may be many)

Here is finally the code of [[parse()]]:

<<function parse>>=
void
parse(char *f, fdt fd, bool varoverride)
{
    Biobuf in;
    Bufblock *buf;
    char c; // one of : = <
    Word *head, *tail;
    int hline; // head line number
    <<[[parse()]] other locals>>

    <<[[parse()]] sanity check fd>>
    <<[[parse()]] start, push>>

    // Initialization
    infile = strdup(f);
    mkinline = 1;
    Binit(&in, fd, OREAD);
    buf = newbuf();

    // Lexing
    while(assline(&in, buf)){
        hline = mkinline;

        // Parsing
        c = rhead(buf->start, &head, &tail,     &attr, &prog);

        // Semantic actions (they may read more lines)
        switch(c)
        {
        <<[[parse()]] switch rhead cases>>
        }
    }
    close(fd);
    freebuf(buf);
    <<[[parse()]] end, pop>>
}
@
\l leak strdup for infile? when free?

The code of [[parse()]] operates in 4 steps:

\begin{enumerate}

\item Initialization of the globals [[infile]] and [[mkinline]]
mentioned above, as well as two local buffers:
[[in]] is an input buffer using the [[libbio]] library (see the \book{Libcore})
and is connected to the file descriptor of the opened [[mkfile]];
\n use Biobuf because will sometimes unget a character (said later)
[[buf]] is a string buffer (see Section~\ref{sec:bufblock}) that will
be used to store one line of the [[mkfile]].

%compiler:
\item Reading and assembling of a line from the [[mkfile]] in [[buf]]
(via the function [[assline()]]).
This is similar to the lexing phase in a compiler (see the \book{Compiler}).

\item Parsing of a line to extract its elements:
the target and prerequistes around the special character [[:]] in a rule,
or the variable name and values around the special character [[=]] in
a variable definition.
%
[[rhead()]] uses the buffer containing a line from the [[mkfile]]
as an argument and returns the special character [[c]] used in the line
([[:]] for a rule, [[=]] for a definition, [[<]] for an inclusion).
%
It also modifies the [[head]] and [[tail]] arguments passed by address
to contain respectively the left and right parts around
the special character (for [[<]], the left part [[head]] is empty).
\l head because head vs body. for body just for rule. def have no body.
\l also hline for head line, so report error first line

\item Acting based on the special character read in the previous step. 
This is where [[mk]] populates [[rules]] and [[metarules]],
as well as the symbol table.
%chunk:
I will describe in the next sections the cases of the [[switch]] above.

\end{enumerate}

\t FIGURE? where start from file and then group in line and
\t  then expand var? summary of the process.

%dup: Windows.nw
The skeleton of [[parse()]], above, omits the error management
code shown below:

<<[[parse()]] sanity check fd>>=
if(fd < 0){
    perror(f);
    Exit();
}
@

<<[[parse()]] switch rhead cases>>=
default:
    SYNERR(hline);
    fprint(STDERR, "expected one of :<=\n");
    Exit();
    break;
@

%dup: Windows.nw
In the rest of this book, I will usually not comment the 
error-management code. Such code is often trivial (but necessary).
\l explain Exit?

There are two other locals in [[parse()]] that are passed by address
to [[rhead()]]: 
[[attr]], which will contain possibly a rule attribute, 
\l or variable attribute
and [[prog]].
Both locals are used for advanced features of [[mk]] I will describe later.


\subsection{Assembling a line, [[assline()]]}

The [[assline()]] function below reads characters from the 
input buffer [[bp]], and fills the string buffer [[buf]] with
those characters until it finds a newline:

% nice function name
<<function assline>>=
/*
 *	Assemble a line skipping blank lines, comments, and eliding
 *	escaped newlines
 */
bool
assline(Biobuf *bp, Bufblock *buf)
{
    int c;
    <<[[assline()]] other locals>>

    // reset buf
    buf->current = buf->start;

    while ((c = nextrune(bp, true)) >= 0){
        switch(c)
        {
        case '\n':
            if (buf->current != buf->start) {
                insert(buf, '\0');
                return true;
            }
            break;		/* skip empty lines */
        <<[[assline()]] switch character cases>>
        default:
            rinsert(buf, c);
            break;
        }
    }
eof:
    insert(buf, '\0');
    return *buf->start != '\0';
}
@
\l int -> Rune?
\t use reset(), isempty(), and content()?
%ocaml: use Buffer too
\l superior or equal zero here? why diff with escapetoken later?

[[insert()]], [[rinsert]] (for ``Rune insert''), [[reset()]], [[content()]],
and [[isempty()]], called above, are all functions (or macros) 
operating on a string buffer and are described in Section~\ref{sec:bufblock}.
\l what is a rune? (said later, with nextrune)


The code of [[assline]] may look trivial, but as its 
(inappropriate) name suggests, [[assline()]] does not just
{read} a line: it {assembles} a line.
%
First, if the current line is a {comment},
[[assline()]] will
skip the line to return the next meaningful line to [[parse()]].
%
Indeed, as I said in Section~\ref{sec:rule-simple-mkfile},
[[mk]] allows the user to add comments in his [[mkfile]] by prefixing
a line with the special character [[#]]. 
\l like shell, cos reuse syntax
%
Moreover, [[assline()]] handles also blank lines, escaped newlines,
and certain quoted characters, as explained in the following sections.

%compiler: %dup: parsing/parse
[[assline()]] is similar to the lexing function in a
compiler (see the \book{Compiler} or \book{Assembler}).
\l return false when EOF after line parsed
\l pass true to nextrune so elide

%ocaml-found:
% skip only empty lines
% if have line with only spaces, it generate an error because not one of :=<

\subsubsection{Escaped newline, [[nextrune()]]}

[[assline()]] relies on the function [[nextrune()]] below to read
the next character from the input buffer [[bp]].
%
In \plan, a {\em rune} is the term used to denote a {unicode}
character (see the \book{Libcore}).
%
[[nextrune()]] is essentially a wrapper over [[Bgetrune()]] from
the [[libbio]] library.

%Again, this may sound trivial, but [[nextrune()]] is not. 
%Indeed, 
[[nextrune()]] must also handle {escaped newlines}.
%
An {\em escaped newline} is a newline character prefixed by
the special {\em escape character} [[\]].

%ex: with escaped newline? after all, new feature needs more introduction.

%dup: overview/principles/dsl/rule
As I mentioned in Section~\ref{sec:rule-simple-mkfile}, the syntax
of [[mk]] (and Make) is minimalist. For example, a variable definition
consists simply of a name followed by an equal sign and a set
of values separated by space and terminated by a newline.
%compiler:
Thus, spaces and newlines have a meaning in [[mk]]  (as opposed
to most programming languages).
\l TAB used too,  but ugly cos dont see diff in editor and can do without
%
However, if the list of values is very long, it would be convenient
to split the list over multiple lines.
%
[[mk]] allows to split such definitions on multiple lines if each
newline is preceded by the special character [[\]];
the newline is then said to be {\em escaped}.
%alt: have syntax for lists, with terminator different than newline,
%  for instance ']' in ocaml
%compiler:
This is similar to what the C preprocessor [[cpp]] provides
for definining long macros over multiple lines (see the \book{Compiler}).
\l space and newlines have meaning 
Thus, when [[nextrune()]] reads an escaped newline, it does not
return the newline character to the caller [[assline()]], which 
will cause [[assline()]] to read more characters 
(until the next true non-escaped newline).

<<function nextrune>>=
/*
 *	get next character stripping escaped newlines
 *	the flag specifies whether escaped newlines are to be elided or
 *	replaced with a blank.
 */
int
nextrune(Biobuf *bp, bool elide)
{
    int c;

    for (;;) {
        c = Bgetrune(bp);
        if (c == '\\') {
            if (Bgetrune(bp) == '\n') {
                // an escaped newline!
                mkinline++;
                if (elide)
                    continue;
                // else
                return ' ';
            }
            // else, it was just \
            Bungetrune(bp);
        }
        <<[[nextrune()]] handle mkinline>>
        return c;
    }
}
@
\l int -> Rune?? also in bio.h?
\t when do you not elide? for escapetoken() and bquote()
%ocaml: ugly to have to handle mkinline here again.

<<[[nextrune()]] handle mkinline>>=
if (c == '\n')
    mkinline++;
@

Note that if [[\]] is not followed by a newline, [[nextrune()]]
must just return the [[\]] character. However, [[nextrune()]]
already went too far in the input buffer by reading an extra
character (to check whether this character was a newline).
%
This is why [[libbio]] provides the function [[Bungetrune()]] called above
to go back in the input buffer.
%
This is one of the reason [[mk]] uses [[libbio]] instead
of the reading and seeking functions of the C library.

% FIGURE? with going to far and so going back?


\subsubsection{Comments}

As mentioned before, [[assline()]] recognizes and skips
comments:

%ex: with comment? after all just showed briefly before.

<<[[assline()]] other locals>>=
int lastc;
@
\t rename lastc to prevc


<<[[assline()]] switch character cases>>=
case '#':
    lastc = '#';
    // skip all characters in comment until newline
    while ((c = Bgetc(bp)) != '\n') {
        if (c < 0)
            goto eof;
        lastc = c;
    }
    mkinline++;
   <<[[assline()]] when processing comments, if escaped newline>>
   <<[[assline()]] when processing comments, if not only comment on the line>>
    // else, skip lines with only a comment
    break;
@
%old:        if(c != '\r')
%            lastc = c;
% seems windows-only stuff
\l handle unicode here? can have end of unicode char finishing with [[\n]]?

A comment can be alone on its line. I can also be used at the end
of a variable definition or rule as in [[FOO=1 # true]].
%
When a comment is not alone on its line, the characters
before the comments are not skipped but returned instead by [[assline()]]:

<<[[assline()]] when processing comments, if not only comment on the line>>=
if (buf->current != buf->start) {
     insert(buf, '\0');
     return true;
 }
@
\n like '\n' case 
\t use empty()


The [[lastc]] local variable above handles escaped newlines 
in a comment as in:

<<example of escaped newline>>=
A=foo # this is a long definition mixed with a comment\
  bar 
@

In that case, the definition will be parsed as the single line
[[A=foo bar]].
\l convenient for
% X=foo\
%  bar\
%  #notthisone\
%  foobar
% ...

<<[[assline()]] when processing comments, if escaped newline>>=
if (lastc == '\\')
    break;		/* propagate escaped newlines??*/
@

\subsubsection{Quoted characters}
% part 1

%trans: %dup: parsing/parse/assline
Assembling a line sounds like an easy ask, but as you have just
seen [[assline()]] is not trivial: it must handle escaped newlines
(through [[nextrune()]]), blank lines, and comments.


The use of a special character to denote comments, [[#]], introduces
in turn another complication for [[assline()]].
%
Indeed, what if the target or prerequiste in a rule contains
a [[#]] in its filename?
\l not that it can not contain a newline
We do not want [[assline()]] to skip all the characters following 
that [[#]] in the rule.
%
In fact, certain filenames in a project may contain
other special characters used by [[mk]] such as [[:]], [[=]], [[<]],
or even space.
\l but not newline


To reference filenames using special characters in an [[mkfile]], the user must
{\em quote} them in order for [[mk]] to not {interpret} them.
%compiler: 
This is a feature found in most programming languages.
\l Lisp. Statchey introduced it (Bourne said it). used in shell also a lot.
%
For [[mk]], the character used to quote a filename
is the single quote character [[']].
\l hence the name of the technique? in Lisp it is also (quote)
\l similar to escape, but for set of chars instead of just one
%dup: (but rearranged)
When [[assline()]] reads a line thats contains a quote,
the [[#]] inside the quote has a different meaning; it is not
a comment anymore.

%ex: with quoted chars? after all, new feature needs more introduction.


<<[[assline()]] switch character cases>>=
case '\'':
case '"':
case '\\':
    rinsert(buf, c);
    if (escapetoken(bp, buf, true, c) == ERROR_0)
        Exit();
    break;
@
\n meta! here use of double \, and single \ to escape quote itself (said later)

[[mk]] can be configured to use either the \plan shell [[rc]]
(see the \book{Shell}) or a Bourne-alike shell.
%
However, each shell has its own escaping rules: sometimes a single quote,
sometimes double quotes, or sometimes the antislash character.
Moreover, as I mentioned before, [[mk]] tries to reuse as 
much as possible the syntax of the shell.
%
This is why the cases of [[assline()]] above are as generic
as possible and delegate instead the shell escaping policy to
the shell-specific [[escapetoken()]] function.
\l method.

The function below is the [[escapetoken()]] for
[[rc]] in [[mk/rc.c]].
%
Like [[assline()]], it reads characters from the 
input buffer [[bp]], and fills the string buffer [[buf]] with
those characters, but this time not until the next newline
but until the next quote.
\n indeed, assline and escapetoken are very similar


% rc.c
<<function escapetoken>>=
/*
 *	Input an escaped token.  Possible escape chars are single-quote,
 *	double-quote and backslash.  Only the first is a valid escape for
 *	rc; the others are just inserted into the receiving buffer.
 */
error0
escapetoken(Biobuf *bp, Bufblock *buf, bool preserve, int esc)
{
    int c;
    int line = mkinline;

    if(esc != '\'')
        return OK_1;

    while((c = nextrune(bp, false)) > 0){
        if(c == '\''){
            if(preserve)
                rinsert(buf, c);
            <<[[escapetoken()]] return, unless double quote>>
        }
        // else
        rinsert(buf, c);
    }
    // must have reached EOF
    SYNERR(line); 
    fprint(STDERR, "missing closing %c\n", esc);
    return ERROR_0;
}
@
%old: initialization of line was done after first if (useless opti).
\l this time it is strictly superior to 0 test for nextrune? why the diff?
\l rename? input_quote?
\l could lpize the first if

Of course, since [[']] is now a special character {reserved} to quote
special characters, how do you quote [[']] itself?
%compiler:
A common technique found in most programming languages is to {double}
the escaping or quoting character
(as shown for C in the code of [[assline()]] above, where the antislash
character is doubled).
%
Thus, in [[rc]] and [[mk]], two [[']] inside a quoted
string are interpreted as a single [[']].
%
For example, [[mk]] interprets [['foo''bar']] as a filename
containing a single quote between the strings [[foo]] and [[bar]]
([[foo'bar]]).
%
Here is the code to handle double quotes:

<<[[escapetoken()]] return, unless double quote>>=
c = Bgetrune(bp);
if (c < 0)
    break; // eof
if(c != '\''){
    Bungetrune(bp);
    return OK_1;
}
// else, '', so continue the while loop
@

%real-world: antislash quote, and double antislash


\l use nextrune so handle escaped newline also in quoted string
% pass false this time to nextrune, so transform escaped newline in space
% escaped newline also has a different meaning, it is then a space.

\l escapetoken keep the quote when preserve is true

%\subsubsection{Quote and escaping}
% similar thing? why 2 mechanisms?

\subsection{Parsing the head of a line, [[rhead()]]}

%trans:
Once [[parse()]] assembled a line,
%
it can analyze the line with [[rhead()]] to return the 
special character {separator} involved in the line.
\l rhead for rule head, or maybe read head because used not only for rule.
%dup: parsing/parse
[[rhead()]] also sets the second and third arguments passed by address
to contain the list of words on the left (the {head} [[h]]) 
and right (the {tail} [[t]]) of the separator:

% read head
<<function rhead>>=
static int
rhead(char *line, Word **h, Word **t,    int *attr, char **prog)
{
    char *p;
    int sep; // one of : = < 
    <<[[rhead()]] other locals>>

    p = charin(line, ":=<");
    if(p == nil)
        return '?';

    sep = *p;
    *p++ = '\0';
    <<[[rhead()]] adjust sep if dynamic mkfile [[<|]]>>
    <<[[rhead()]] adjust [[attr]] and [[prog]]>>

    // potentially expand variables in head
    *h = stow(line);
    <<[[rhead()]] sanity check h>>
    // potentially expand variables in tail
    *t = stow(p);

    return sep;
}
@
%old:     *h = w = stow(line);
% but why needed intermediate w? so sanity check test code was simpler, 
% with less indirection? *(*h)->s instead of just *w->s
\l int -> Rune for return? or even char!

[[rhead()]] relies on the function [[charin()]] to find one of
the characters mentioned in its second argument in the string
passed in its first argument; I will describe [[charin()]] below.
\l complex actually, handle head which can contain a :=< (said later)

The address of the special character separator is stored first in the local
variable [[p]]. The content of [[p]] is saved in the other local
variable [[sep]], before being overwritten by the end-of-string
null character as illustrated in the middle of
Figure~\ref{fig:mk-rhead-overwrite}.
%
At this point, [[line]] and [[p]] point to the start of two
independent strings. Both strings are then processed by [[stow()]]
(for ``string to words''), which I will explain in Section~\ref{sec:stow}.
%
[[stow()]] splits the content of a string in a list of words, as shown
at the bottom of Figure~\ref{fig:mk-rhead-overwrite}.
\l again, more complex, need expand variables (said later)


\begin{figure}[!]\centering
\begin{verbatim}
 line
   |
   v-------------------+          start of
   |foo.5 : foo.c foo.h|          rhead()
   +-------------------+

 line     p
   |      |                         after
   v------v------------+         *p++ = '\0'
   |foo.5 | foo.c foo.h|
   +------+------------+
 h          t
 |          |
 v--+       v--+
 |. |       |. |
 ++-+       ++-+
  |          |
  v-----+    v-----+   +-----+     end of
  |foo.5|    |foo.c|-->|foo.h|     rhead()
  +-----+    +-----+   +-----+
\end{verbatim}
\caption{Evolution of local variables in [[rhead()]].}\label{fig:mk-rhead-overwrite}
\end{figure}


%compiler:
\l equivalent to  parsing in compiler

% FIGURE where before long string,
% then string split in 2 because of \0 put at place of special char


Note that if the separator in the line is [[<]] 
(for an inclusion instruction), it is normal for [[h]] to be empty;
%
otherwise, [[mk]] should report an error to the user:

<<[[rhead()]] sanity check h>>=
if(*((*h)->s) == '\0' && sep != '<' && sep != '|') {
    SYNERR(mkinline-1);
    fprint(STDERR, "no var/target on left side of assignment/rule\n");
    Exit();
}
@
%pad: I added the /target in error message and replaced w by *h
\l explain pipe, advanced feature





\subsubsection{Finding special characters, [[charin()]]}
\label{sec:charin}

[[charin()]] below essentially iterates over the characters
\l actually rune
in [[cp]] by incrementing [[cp]] until the start of [[cp]] points
to one of the character in [[pat]]:

% addrule | rhead -> <>
<<function charin>>=
/*
 *	Search a string for characters in a pattern set.
 *	Characters in quotes and variable generators are escaped.
 */
char*
charin(char *cp, char *pat)
{
    Rune r;
    int n;
    bool vargen = false;

    while(*cp){
        n = chartorune(&r, cp);
        switch(r){
        <<[[charin()]] switch rune cases>>
        default:
            if(utfrune(pat, r) && !vargen)
                return cp;
            break;
        }
        cp += n;
    }
    <<[[charin()]] sanity check vargen>>
    return nil;
}
@
%$
\t rename vargen to  invargenerator (in code and below also)
\n unicode stuff makes the code hard to read. Regexps or lex are much cleaner.
%ocaml: redo a bit some of the lexing work done in assline, sad.
% I use lex so work less repeated
%old: I reversed utfrune() with !vargen, but then less efficient

[[chartorune()]] called above is a function from the C library (see
the \book{Libcore}).
It is similar to [[Bgetrune()]] used in [[nextrune()]] before, but
operates on a plain string instead of a string buffer.
\l return chars read to form a rune in n, correspond to advanced in buffer

[[utfrune()]] is another function from the C library. It checks
whether a rune is equal to one of the characters in a 
a set of characters (the ``pattern'').
\l but confusing with mk pattern

The local variable [[vargen]] is used to handle variable generator,
an advanced feature of [[mk]] I will explain later in 
Section~\ref{sec:var-generator}.

\subsubsection{Skipping quoted characters}
% part 2

Just like [[assline()]] needs special code to handle quoted strings, because
they may contain a [[#]] that needs to be treated differently, 
[[charin()]] needs also special code to handle quoted strings,
because they may contain one of the special characters
[[rhead()]] is looking for ([[:]], [[=]], or [[<]]).


<<[[charin()]] switch rune cases>>=
case '\'':			/* skip quoted string */
    cp = squote(cp+1);	/* n must = 1 */
    if(!cp)
        return nil;
    break;
@

% charin -> <>
<<function squote>>=
/*
 *	skip a token in single quotes.
 */
static char *
squote(char *cp)
{
    Rune r;
    int n;

    while(*cp){
        n = chartorune(&r, cp);
        if(r == '\'') {
            <<[[squote()]] return, unless double quote>>
        }
        cp += n;
    }
    SYNERR(-1);		/* should never occur */
    fprint(STDERR, "missing closing '\n");
    return nil;
}
@

<<[[squote()]] return, unless double quote>>=
n += chartorune(&r, cp+n);
if(r != '\'')
    return cp;
// else, double '', continue while loop
@
\n simpler that escapetoken because no need to update mkinline, or
\n to modify a buffer. We just need to skip it.
\l newline should never occur because escapetoken already handles the case?

%\subsubsection{Skipping variables generators}
% now in advanced topics


\subsection{Splitting a string in words, [[stow()]]}
\label{sec:stow}

%trans:
After [[rhead()]] found the position of the special character in 
the line assembled by [[assline()]],
%dup: parsing/parse parsing/parse/rhead
[[rhead()]] calls [[stow()]] to split in multiple words the strings
on the left and right parts of the special character.
\l so used for target, prereqs, but also varname, var defs, and also file in <
%
The code of [[stow()]] below is very simple because it 
delegates most of the complexity to [[nextword()]], which I will
explain after.

% string to word
<<function stow>>=
Word *
stow(char *s)
{
    // list<ref_own<Word>>
    Word *head, *new;
    // option<ref<Word>>
    Word *lastw;

    head = nil;
    lastw = nil;
    while(*s){
        new = nextword(&s);
        if(new == nil)
            break;

        // head = concat_list(head, new)
        if (lastw)
            lastw->next = new;
        else
            head = lastw = new;

        while(lastw->next)
            lastw = lastw->next;
        
    }
    if (!head)
        head = newword("");
    return head;
}
@
%old: I renamed w to lastw
\t rename newword -> new_singleword
\t rename nextword to nextwords?
%ocaml: expanding is done later in eval.ml instead, but 
% the splitting is done at lexing/parsing time.

Note that [[nextword()]] does not return a string but a list
of words, for reasons I will explain soon.
%
This is why [[stow()]] must concatenate the lists in
[[head]] and [[new]], not just adding one word to a list of words.


\subsubsection{Assembling the next words, [[nextword()]]}

The code for [[nextword()]] sounds trivial again: look for
the next space character 
\l or tab. or newline?
in the string as the separation marker between two words.
%
However, again, [[nextword()]] must also handle quoted strings, because
they may contain a space that must not be treated as a {word separator}.
%
In fact, [[nextword()]] must also handle variables and other features
of [[mk]], as explained in the following sections.

% stow -> <>
<<function nextword>>=
/*
 *	break out a word from a string handling quotes, executions,
 *	and variable expansions.
 */
static Word*
nextword(char **s)
{
    char *cp = *s;
    Bufblock *b;
    Rune r;
    // list<ref_own<Word>>
    Word *head;
    // option<ref<Word>>
    Word  *lastw;
    bool empty;
    <<[[nextword()]] other locals>>

    b = newbuf();

restart:
    head = nil;
    lastw = nil;
    empty = true;
    <<[[nextword()]] skipping leading white space>>

    while(*cp){
        cp += chartorune(&r, cp);
        switch(r)
        {
        case ' ':
        case '\t':
        case '\n':
            goto out;
        <<[[nextword()]] switch rune cases>>
        default:
            empty = false;
            rinsert(b, r);
            break;
        }
    }
out:
    *s = cp;
    if(b->current != b->start){
        <<[[nextword()]] when buffer not empty, if there was already an head>>
         else {
            insert(b, '\0');
            head = newword(b->start);
        }
    }
    freebuf(b);
    return head;
}
@
%$
\t rename b to buf
%old: I renamed tail to lastw
\t use empty()


The presence of a label [[restart]] above, as well as the local
variables [[empty]] and [[lastw]] will become clear later.
%
They are needed because certain strings can expand in other strings
that need to be reprocessed by [[nextword()]].
\l those cases are contain in hidden cases in switch above
%chunks: will see those cases gradually.
% advanced variables and advanced words gradually

Note that [[nextword()]] skips the leading white spaces in the string:

<<[[nextword()]] skipping leading white space>>=
while(*cp == ' ' || *cp == '\t')		/* leading white space */
    cp++;
@

This is why there is no difference between writing a rule like
[[foo.5:foo.c]] and [[foo.5: foo.c]], or between a definition like
[[FOO=a b]] and [[FOO = a b]].

\subsubsection{Expanding quoted characters}
% part 3

As I just mentioned, [[nextword()]] must handle quoted strings.
%
As opposed to [[assline()]], which just inputs a quoted string,
or [[charin()]], which skips over a quoted string, [[nextword()]]
{expands} a quoted string and stores the expansion in the buffer [[b]].
%
Indeed, [[stow()]] and [[nextword()]] are the last steps in the
parsing phase; there is no need to keep the quotes (or double
quotes inside those quotes) anymore. 

<<[[nextword()]] switch rune cases>>=
case '\'':
case '"':
case '\\':
    empty = false;
    cp = expandquote(cp, r, b);
    if(cp == nil){
        fprint(STDERR, "missing closing quote: %s\n", *s);
        Exit();
    }
    break;
@
\l but missing closing quote should never happen
\l what if empty quoted string? empty is not false then

<<function expandquote>>=
/*
 *	extract an escaped token.  Possible escape chars are single-quote,
 *	double-quote, and backslash.  Only the first is valid for rc. The
 *	others are just inserted into the receiving buffer.
 */
char*
expandquote(char *s, Rune r, Bufblock *b)
{
    if (r != '\'') {
        rinsert(b, r);
        return s;
    }

    while(*s){
        s += chartorune(&r, s);
        if(r == '\'') {
            <<[[expandquote()]] return, unless double quote>>
        }
        rinsert(b, r);
    }
    return nil;
}
@
\l could lpize again the first if

<<[[expandquote()]] return, unless double quote>>=
if(*s == '\'')
    s++; // skip one of the double quote
else
    return s;
@

\l '' to handle escaping of ' itself? but then handled by

\subsubsection{Expanding variables}
\label{sec:expand-vars}

The expansion of variables in rules, variable definitions, and inclusions
\l but not recipe (said later)
is done at parsing-time by [[nextword()]], just like the expansion
of quotes (and backquotes, as explained in Section~\ref{sec:backquotes}).
\l why parsing-time? why doing it at all?


Note that a single variable can expand to multiple words because a variable
holds a list of words in [[mk]].

<<[[nextword()]] other locals>>=
// list<ref_own<Word>
Word *w;
@

This is why [[nextword()]] returns a list of words, 
and [[stow()]] concatenate a list of words;
what may seem like a single word (a variable) can expand to multiple words.


The code of [[nextword()]] below relies on [[varsub()]] to return 
the value of a variable. 
%
[[varsub()]] can also substitute variables, as explained 
in Section~\ref{sec:subst-var}, hence its name.

<<[[nextword()]] switch rune cases>>=
case '$':
    w = varsub(&cp);
    <<[[nextword()]] when in variable case, if w is nil>>
    empty = false;

    <<[[nextword()]] when in variable case, if non-space chars before var>>
    <<[[nextword()]] when in variable case, if head is not empty>>
    else
        head = lastw = w;

    while(lastw->next)
        lastw = lastw->next;
    break;
@
%$

\l expand vars in rules target, prerequisites, in variable uses.
% but not recipe!
% this is done% by shell, and so it allows late binding of variables, use
% before define, etc.

\l will expand not just variable set in mkfile but also
% plan9 variables, because of [[readenv()]] at the beginning
% of mk.


[[varsub()]] in turn relies on 
[[varname()]] to extract the name of the variable from the string 
pointed by [[s]] ([[varname()]] also increments [[s]] by side effect), and
[[varmatch()]] to grab the value of the variable from the symbol table:

% nextword -> <>
<<function varsub>>=
Word*
varsub(char **s)
{
    Bufblock *b;
    Word *w;

    <<[[varsub()]] if variable starts with open brace>>
    // else

    b = varname(s);
    <<[[varsub()]] sanity check b>>
    w = varmatch(b->start);

    freebuf(b);
    return w;
}
@
\n not just [[$V]], can have complex pattern there too.
\n sometimes ${xx:%.y=%.z} ! but later in adv topics
%$

Note that if the user mentions a variable not defined anywhere
(neither in the [[mkfile]] nor in the environment), [[nextword()]]
will skip over this variable:

<<[[varsub()]] sanity check b>>=
if(b == nil)
    return nil;
@

<<[[nextword()]] when in variable case, if w is nil>>=
if(w == nil){
   <<[[nextword()]] when in variable case, if w is nil and no char before>>
    break;
}
@

In fact, if the variable is not defined and there was no character
before the variable, as in [[FOO= $bar abc]], %$
[[nextword()]] must skip over the undefined variable and also
skip again the possible whitespaces after the variable, hence the jump to
[[restart]] below:

<<[[nextword()]] when in variable case, if w is nil and no char before>>=
if(empty)
    goto restart;
@
\n but no reprocessing of words already parsed here, cp is still empty
\n reprocessing is done for backquote only I think


%\subsection{Simple variables, }
%$

[[varname()]] below
returns a buffer containing the name of a variable in [[s]], and 
increments [[s]] to point after the variable name:

<<function varname>>=
/*
 *	extract a variable name
 */
static Bufblock*
varname(char **s)
{
    Bufblock *b;
    char *cp = *s;
    Rune r;
    int n;

    b = newbuf();

    for(;;){
        n = chartorune(&r, cp);
        if (!WORDCHR(r))
            break;
        rinsert(b, r);
        cp += n;
    }
    <<[[varname()]] sanity check b>>
    *s = cp;
    insert(b, '\0');
    return b;
}
@

What constitutes a variable name is mostly anything except
the set of special characters in the macro below:

<<function WORDCHR>>=
#define WORDCHR(r)	((r) > ' ' && !utfrune("!\"#$%&'()*+,-./:;<=>?@[\\]^`{|}~", (r)))
@
%$
\l  maybe specific to shell too?


<<[[varname()]] sanity check b>>=
if (b->current == b->start){
    SYNERR(-1);
    fprint(STDERR, "missing variable name <%s>\n", *s);
    freebuf(b);
    return nil;
}
@




<<function varmatch>>=
static Word*
varmatch(char *name)
{
    Word *w;
    Symtab *sym;
    
    sym = symlook(name, S_VAR, nil);
    if(sym){
        /* check for at least one non-NULL value */
        for (w = sym->u.ptr; w; w = w->next)
            if(w->s && *w->s)
                return wdup(w);
    }
    return nil;
}
@
\l rename? bad name


Note that if a variable is preceded directly by non-space characters,
as at the bottom of Figure~\ref{fig:mk-nextword1}, the first word in the list of
words in the value of the variable must be {adjusted}:

<<[[nextword()]] when in variable case, if non-space chars before var>>=
if(b->current != b->start){
    bufcpy(b, w->s, strlen(w->s));
    insert(b, '\0');
    free(w->s);
    // adjust the first word
    w->s = strdup(b->start);
    // reset buf
    b->current = b->start;
}
@
\t use isempty()
%ocaml: I actually forbid that if variable is not a scalar

\begin{figure}[!]\centering
\begin{verbatim}
  cp           where A=foo bar
   |                 B=bar foo
  +v------+
s:|@$A$B: |                                   start of
  +-------+                                  nextword()


             start
   cp          | current
    |          | |
  +-v-----+    v-v---------+                 processing
s:|@$A$B: |  b:|@          |                    '@'
  +-------+    +-----------+

            start           start
               |current     current
     cp        | |             |
      |        v-v---------+   v----+------+
  +---v---+  b:|@          | b:|@foo|      |
s:|@$A$B: |    +-----------+   +----+------+ processing
  +-------+    +---+  +---+    +----+  +---+    '$A'
             w:|foo|->|bar|  w:|@foo|->|bar|
               +---+  +---+    +----+  +---+

               (before)          (after)

                  adjusting the first
                         word
\end{verbatim}
\caption{[[nextword()]] edge cases (part 1).}\label{fig:mk-nextword1}
\end{figure}

Moreover, if a variable is followed by another variable, 
as at the top of Figure~\ref{fig:mk-nextword2}, the last
word of the first variable must be {merged} with the first word
of the second variable:

<<[[nextword()]] when in variable case, if head is not empty>>=
if(head){
    // merge the last and first words
    bufcpy(b, lastw->s, strlen(lastw->s));
    bufcpy(b, w->s, strlen(w->s));
    insert(b, '\0');
    free(lastw->s);
    lastw->s = strdup(b->start);

    lastw->next = w->next;
    free(w->s);
    free(w);
    // reset buf
    b->current = b->start;
}
@
%ocaml: I actually forbid that if variable is not a scalar


Finally, if a variable is followed directly by non-space
characters, as at the bottom of Figure~\ref{fig:mk-nextword2}, the last word
must be adjusted:

<<[[nextword()]] when buffer not empty, if there was already an head>>=
if(head){
    cp = b->current;
    bufcpy(b, lastw->s, strlen(lastw->s));
    bufcpy(b, b->start, cp - b->start);
    insert(b, '\0');
    free(lastw->s);
    // adjust the last word
    lastw->s = strdup(cp);
}
@
%ocaml: I actually forbid that if variable is not a scalar
\l ugly abuse cp here

\begin{figure}[!]\centering
\begin{verbatim}
                   where A=foo bar
                         B=bar foo

            start               start
            current             current
               |                   |
       cp      v-----------+       v------+----+
        |    b:|           |     b:|barbar|    |
  +-----v-+    +-----------+       +------+----+ processing
s:|@$A$B: |    +---+  +---+        +---+  +---+     '$B'
  +-------+  w:|bar|->|foo|      w:|bar|->|foo|
               +---+  +---+        +---+  +---+
               +----+  +---+       +----+  +------+  +---+
          head:|@foo|->|bar|  head:|@foo|->|barbar|->|foo|
               +----+  ^---+       +----+  ^------+  +---+
                       |                   |
                      lastw               lastw

                   (before)          (after)

                     merging the last and
                          first word

            start                    start
               |current                |cp   current
        cp     | |                     | |    |
         |     v-v---------+           v-v---+v----+ processing
  +------v+  b:|:          |         b:|:foo:|     |    ':'
s:|@$A$B: |    +-----------+           +-----+-----+
  +-------+
            +----+  +------+  +---+  +----+  +------+  +----+
      head: |@foo|->|barbar|->|foo|  |@foo|->|barbar|->|foo:|
            +----+  +------+  ^---+  +----+  +------+  ^----+
                              |                        |
                             lastw                    lastw
                (before)                   (after)

                       adjusting the last
                              word
\end{verbatim}
\caption{[[nextword()]] edge cases (part 2).}\label{fig:mk-nextword2}
\end{figure}
\n actually the buffer contain [[@foo]] at the beginning here

\t maybe say ok if variables are scalar, but could issue a warning otherwise

\n could put the code for dollar{name} here, important escaping mechanism
\n will see backquote later too


\section{Rules, {\em target}[[:]]{\em prereqs}}
\n actually targets [[:]] prereqs, not just one target

%trans:
Now that you have seen the generic parts of the code to
parse an [[mkfile]],
%
I can describe the specific parts with the {actions} in [[parse()]]
to process the rules, definitions, and inclusions.
%
Those actions are based on
the information returned by [[rhead()]]:
%dup: parsing/parse parsing/parse/rhead parsing/parse/stow
the special character in the line, as well as the list of words on the 
left and right parts of this special character.
%
I will start in this section with the action to manage rules,
when the special character returned by [[rhead()]] is [[:]].


<<[[parse()]] other locals>>=
char *body;
@
<<[[parse()]] switch rhead cases>>=
case ':':
    body = rbody(&in);
    addrules(head, tail, body,   attr, hline, prog);
    break;
@
\t mv hline as the last parameter, or fourth

The action above relies on [[rbody()]] to read the body
of a rule, that is its recipe.
%
I will explain [[rbody()]] in the next section.

%dup: core-ds/rule/addrules
I have described [[addrules()]] called above in Section~\ref{sec:multi-targets}.
[[addrules()]] can handle rules with multiple targets; it
internally calls [[addrule()]] for each target.


I mentioned also in Section~\ref{sec:default-target} 
that [[mk]], during parsing, sets the global [[target1]] to contain
the first target found in the mkfile.
%
Here is finally the code that sets [[target1]]:

\label{sec:set-target1}
<<[[addrules()]] set [[target1]]>>=
/* tuck away first non-meta rule as default target*/
if(target1 == nil && !(attr&REGEXP)){
    for(w = head; w; w = w->next)
        if(charin(w->s, "%&"))
            break;
    if(w == nil) // head does not contain any pattern
        target1 = wdup(head);
}
@
\n I could mv this before, but addrules is in Core DS and target1
\n mentioned in main. Also use charin below described in this chapter.

Note the use of [[charin()]] above to make sure the first target
does not contain any {pattern}. Indeed, [[mk]] would not
know how to instantiate this pattern to build a concrete target.
\l pattern can be [[%]] or [[&]] as described in X

For the rule attribute [[REGEXP]] used above, see Section~\ref{sec:regexp}.
\l why not also attr&META? because we didnt call yet addrule, and
\l addrule does not return the rule


\subsection{Parsing the recipe, [[rbody()]]}

[[rbody()]], like [[assline()]], takes as a parameter an input buffer [[in]].
%
The {cursor} in this buffer should now point to the character following
the newline of the line containing the target and prerequisites of the rule.
% FIGURE?
[[rbody()]] then fills its local string buffer [[buf]] until
a non-spacing character is found in the first column.
%
This character marks the end of the recipe
and the start of a new rule, definition, or inclusion.

% read body
<<function rbody>>=
static char *
rbody(Biobuf *in)
{
    Bufblock *buf;
    int r, lastr;
    char *p;

    lastr = '\n';
    buf = newbuf();

    for(;;){
        r = Bgetrune(in);
        if (r < 0)
            break; // eof
        // in first column
        if (lastr == '\n') {
            <<[[rbody()]] if comment in first column>>
            else 
              if (r != ' ' && r != '\t') {
                Bungetrune(in);
                break;
            }
        } else
            // not in first column
            rinsert(buf, r);

        lastr = r;
        <<[[rbody()]] handle mkinline>>
    }

    insert(buf, '\0');
    p = strdup(buf->start);
    freebuf(buf);

    return p;
}
@
\l int -> Rune

<<[[rbody()]] handle mkinline>>=
if (r == '\n')
    mkinline++;
@

If a comment is found in the first column, it can not be
the start of a rule, definition, or inclusion, so it is
added in the buffer for the recipe:

<<[[rbody()]] if comment in first column>>=
if (r == '#')
    rinsert(buf, r); // the shell recognize comments too
@

If a rule has no recipe, [[rbody()]] returns
the empty string to its caller [[parse()]]. 
%
Note that the empty string is not the same thing than [[nil]]. Indeed,
an empty string contains one byte: the end-of-string null character [[\0]].
\l why insist? because invariant used later

Note also that [[mk]] does not impose to use a [[TAB]] 
in the first column like Make. A space character is also valid.
Moreover, more than one spacing character can be used.
\l but then weird shprint?
You can also write multiple shell commands on multiple lines as long
as they all have a leading spacing character in the first column.
\l Make force to escape newline?

\t note that no call to  stow() here! no variable expansion


\subsection{Parsing rule attributes}

[[mk]] allows the user to customize certain rules by using
{\em rule attributes}.
%dup: main/build/parallel-mode (for VIR)
An attribute often used is the attribute to indicate that
the target in a rule does not correspond to a filename;
[[mk]] should then not expect from the recipe to generate such a target.
\l but in fact mk does not check that, so it is more to skip
\l the reading of time if by bad luck there is a file named clean in the dir
For instance, many [[mkfile]]s use the target [[clean]] to
cleanup a directory.
%dup: main/build/parallel-mode (for VIR)
In [[mk]]'s terminology, such a target is called a {\em virtual target}
(see Section~\ref{sec:virtual} for a full explanation).
%real-world:
In GNU Make, the use of [[.PHONY:]] followed by a string in a
[[Makefile]] indicates that the string is a virtual target.

In [[mk]], the syntax to add attributes to a rule is to add
non-spacing characters after the first [[:]] of a rule, and
to add another [[:]] after those characters, as in the following rule:

<<tests/mkfile/mkclean>>=
clean:V:
    rm -f *.5 $PROG $LIB
@

Each character between the two [[:]] correspond to a different
attribute.

Rule attributes are stored in [[Rule.attr]], but before, they
are stored in a local variable in [[parse()]]:

<<[[parse()]] other locals>>=
// bitset<Rule_attr>
int attr;
@
\l or var_attr?

This variable is passed by address to [[rhead()]]:

<<[[rhead()]] adjust [[attr]] and [[prog]]>>=
*attr = 0; // Nothing
*prog = nil;
// variable attributes
<<[[rhead()]] if sep is [[=]]>>
// rule attributes
<<[[rhead()]] if sep is [[:]]>>
@

[[rhead()]] then modifies [[attr]] in the cases of
the [[switch]] below:

<<[[rhead()]] other locals>>=
Rune r;
int n;
@

<<[[rhead()]] if sep is [[:]]>>=
if((sep == ':') && *p && (*p != ' ') && (*p != '\t')){
    while (*p) {
        n = chartorune(&r, p);
        if (r == ':')
            break;
        p += n;
        switch(r)
        {
        <<[[rhead()]] when parsing rule attributes, switch rune cases>>
        default:
            SYNERR(-1);
            fprint(STDERR, "unknown attribute '%c'\n", p[-1]);
            Exit();
        }
    }
    if (*p++ != ':') {
eos:
        SYNERR(-1);
        fprint(STDERR, "missing trailing :\n");
        Exit();
    }
}
@
%old: switch cases originally sorted alphabetically, but after
% LP distribution, they might not be anymore

%chunks: 
I will describe gradually the cases above and the different rule
attributes.



\section{Included files, [[<]]{\em file}}

%trans:
I will now describe the action to manage inclusions,
when the special character returned by [[rhead()]] is [[<]].

A file inclusion in an [[mkfile]] will result in the opening of
a new file, hence the following additional variables in [[parse()]]:

<<[[parse()]] other locals>>=
char *p;
fdt newfd;
@


<<[[parse()]] switch rhead cases>>=
case '<':
    p = wtos(tail, ' ');
    <<[[parse()]] when parsing included file, sanity check p>>
    newfd = open(p, OREAD);
    <<[[parse()]] when parsing included file, sanity check newfd>>
    else
        // recurse
        parse(p, newfd, false);
    break;
@

<<[[parse()]] when parsing included file, sanity check p>>=
if(*p == '\0'){
    SYNERR(-1);
    fprint(STDERR, "missing include file name\n");
    Exit();
}
@
<<[[parse()]] when parsing included file, sanity check newfd>>=
if(newfd < 0){
    fprint(STDERR, "warning: skipping missing include file: ");
    perror(p);
}
@


Note that [[tail]] above, contains the list of words on the right
of [[<]] ([[head]], which contains the list of words on the left,
should be empty).
%
Just like for the rules, this list of words is set in [[rhead()]]
and is the result of a call to [[stow()]], which performs
quote and variable expansions.
%
Thus, you can also use variables in the filename to include, 
as in:

<<tests/mkfile/mkincludearc>>=
</$objtype/mkfile
@
%$

I described [[wtos()]] called above in Section~\ref{sec:word}.
It converts a list of words back to a string. In practice, this
list should contain only one element.
\l handle filename with spaces now ... no need escaping
%ocaml: be more consistent and stricter



To include a file, [[mk]] simply calls recursively [[parse()]].
However, a the globals [[infile]] and [[mkinline]] must
be saved before the call and restored after, hence the following
calls in [[parse()]]:

<<[[parse()]] start, push>>=
ipush();
@

<<[[parse()]] end, pop>>=
ipop();
@
%ocaml: do that in eval, and use call stack to handle recursivity

Both functions use the following structure to remember the list
of {parent} files:

<<struct input>>=
struct Input
{
    char *file;
    int line;

    // Extra
    struct Input *next;
};
@

<<global inputs>>=
// list<ref_own<Input>> (next = Input.next)
static struct Input *inputs = nil;
@


<<function ipush>>=
void
ipush(void)
{
    struct Input *in, *me;

    me = (struct Input *)Malloc(sizeof(*me));
    me->file = infile;
    me->line = mkinline;
    me->next = nil;

    // add_list(me, inputs)
    if(inputs == nil)
        inputs = me;
    else {
        for(in = inputs; in->next; )
            in = in->next;
        in->next = me;
    }
}
@
%ocaml: do not use globals for infile and inline? actually I do,
% so my code is very similar here.

<<function ipop>>=
void
ipop(void)
{
    struct Input *in, *me;

    assert(/*pop input list*/ inputs != 0);
    // me = pop_list(inputs)
    if(inputs->next == nil){
        me = inputs;
        inputs = nil;
    } else {
        for(in = inputs; in->next->next; )
            in = in->next;
        me = in->next;
        in->next = nil;
    }
    infile = me->file;
    mkinline = me->line;
    free((char *)me);
}
@




\section{Variable definitions, {\em var}[[=]]{\em values}}
\n really Constant definitions, not variable.
\n ninja calls them bindings

%trans: finally, last construct variable def.
% left is variable name, right is set of values (possibly containing
% variables, which are expanded by nextword())


<<[[parse()]] other locals>>=
bool set = true;
@
%old: not set initially, but clearer that way

<<[[parse()]] switch rhead cases>>=
case '=':
    <<[[parse()]] when parsing variable definitions, sanity check head>>
    <<[[parse()]] when parsing variable definitions, override handling>>
    if(set){
        setvar(head->s, (void *) tail);
        <<[[parse()]] when parsing variable definitions, extra setting>>
    }
    <<[[parse()]] when parsing variable definitions, if variable with attr>>
    break;
@
%dead: was already commented in the original code
%    /*
%    char *cp;
%    dumpw("tail", tail);
%    cp = wtos(tail, ' '); print("assign %s to %s\n", head->s, cp); free(cp);
%    */


<<[[parse()]] when parsing variable definitions, sanity check head>>=
if(head->next){
    SYNERR(-1);
    fprint(STDERR, "multiple vars on left side of assignment\n");
    Exit();
}
@

%ocaml-found:
% note that calls wtos() on head before, so can have vars in head!
% can do crazy stuff like
% A=B
% $A=42
% and now B is 42! pointers :) 

\subsection{Overriding variable definitions}

% saw calling parse with true for override for the xxx=yyy argument parsing.
% remember parse() signature is varoverride parameter.

<<[[parse()]] when parsing variable definitions, override handling>>=
if(symlook(head->s, S_OVERRIDE, nil)){
    set = varoverride;
} else {
    set = true;
    if(varoverride)
        symlook(head->s, S_OVERRIDE, (void *)"");
}
@
\t could simplify maybe, if there, then set set=false; clearer.
\t also generate warning when set again same var? 
\t mk allows it? value is used until next def since evaluate vars as we parse.

<<[[Sxxx]] cases>>=
S_OVERRIDE,	/* can't override */
@
% when do mk xxx=yyy, it's overriding! and it's ok.



\subsection{Parsing variable attributes}

% rc.c
<<global termchars>>=
char	*termchars = "'= \t";	/*used in parse.c to isolate assignment attribute*/
@
% what we really want is to find first =, but if there is
% a quote before, then the = should not be interpreted as the = before
% attribute.

<<[[rhead()]] if sep is [[=]]>>=
if(sep == '='){
    pp = charin(p, termchars);	/* termchars is shell-dependent */
    if (pp && *pp == '=') {
        while (p != pp) {
            n = chartorune(&r, p);
            switch(r)
            {
            <<[[rhead()]] when parsing variable attributes, switch rune cases>>
            default:
                SYNERR(-1);
                fprint(STDERR, "unknown attribute '%c'\n",*p);
                Exit();
            }
            p += n;
        }
        p++;		/* skip trailing '=' */
    }
}
@
% can chain? X=V=foo ? or it is more C=U=bla




%\section{Variable uses (expanding variables part 2), [[$]]{\em C} }
%$




\chapter{Building the Dependency Graph}
\label{chap:graph}

% Will see [[mk()]] later. But will call [[graph(target)]]
% which goes down building tree using [[rules]] and [[metarules]].

\section{[[graph()]] and [[applyrules()]]}

% mk -> <>
<<function graph>>=
Node*
graph(char *target)
{
    Node *root;
    <<[[graph()]] other locals>>

    <<[[graph()]] set cnt for infinite rule detection>>
    root = applyrules(target, cnt);
    <<[[graph()]] free cnt>>

    <<[[graph()]] checking the graph>>

    // propagate attributes in rules to their node
    attribute(root);

    return root;
}
@
%pad: node -> root, clearer I think

% need attribute? anyway can access rule of node each time
% or probably because it is a union of attr for all matching rules.
%  not just the master rule.
% But in which situation it matters? for Virtual! because timeof
% may have been wrong and considered a file with the same name of
% a target. When we find out it is a virtual node, we need to reset
% the time of this node.



<<function applyrules>>=
static Node*
applyrules(char *target, char *cnt)
{
    Node *node;
    Arc head;
    Arc *a = &head;
    <<[[applyrules]] other locals>>

    <<[[applyrules]] debug>>
    <<[[applyrules]] check node cache if target is already there>>
    // else

    target = strdup(target);
    // calls timeof()
    node = newnode(target);

    head.n = nil;
    head.next = nil;

    <<[[applyrules]] set rmatch>>

    // apply regular rules with target as a head (modify node, head, a)
    <<[[applyrules()]] apply regular rules>>

    // apply meta rules
    <<[[applyrules()]] apply meta rules>>

    // ???
    a->next = node->arcs;
    node->arcs = head.next;

    return node;
}
@
\t node->prereqs should be nil no? assert?
\t confusing again to use head, this time for head of arcs.

% call to newnode() which will call timeof()!

%old: in comment in the original code!
%/*		if(r->attr&VIR)
% *			node->flags |= VIRTUAL;
% *		if(r->attr&NOREC)
% *			node->flags |= NORECIPE;
% *		if(r->attr&DEL)
% *			node->flags |= DELETE;
% */
% now done in attribute() anyway.


\section{Finding the simple rules for a target}

% built index while parsing in hash

<<[[applyrules]] other locals>>=
Symtab *sym;
Rule *r;
Word *w;
@
% rename w to prerequiste?

<<[[applyrules()]] apply regular rules>>=
sym = symlook(target, S_TARGET, nil);
for(r = (sym? sym->u.ptr : nil); r; r = r->chain){
    <<[[applyrules()]] skip this rule and continue if some conditions>>
    <<[[applyrules()]] infinite rule detection part1>>
    <<[[applyrules()]] when found a regular rule for target [[node]], set flags>>
    <<[[applyrules()]] if no prerequistes in rule r>>
    else
        for(w = r->prereqs; w; w = w->next){
            // recursive call!
            a->next = newarc(applyrules(w->s, cnt), r, "", rmatch);
            a = a->next;
    }
    <<[[applyrules()]] infinite rule detection part2>>
    head.n = node;
}
@
\t what is the point of setting head.n? it is not used anyway, and
\t  similar to node

% so for each prereq there is an arc! (but all of those arcs
% will share the same rule/recipe)

<<[[applyrules()]] skip this rule and continue if some conditions>>=
if(r->attr&META) continue;
if(strcmp(target, r->target)) continue; // how can happen??
if((!r->recipe || !*r->recipe)
   && (!r->prereqs || !r->prereqs->s || !*r->prereqs->s)) 
      continue;	/* no effect; ignore */
@
%TODO safe to continue when no recipe? should still have an arc
% so than can check if outofdate no?
\t how can be META and in S_TARGET?
\t ugly those chain of test on tail. Should assert always has a tail?
\t  or should forbid singleton empty word.
\t  maybe define  empty_prereqs() ? and empty_recipe() !!!
\t but no need !r->recipe, every rule has recipe (which can be empty)
\t  so do assert in empty_recipe()

% empty recipe and empty prereqs is useless, but tools like ocamldep
% generates such dependencies

<<[[applyrules()]] if no prerequistes in rule r>>=
// no prerequistes, a leaf, still create fake arc
if(!r->prereqs || !r->prereqs->s || !*r->prereqs->s) {
    a->next = newarc((Node *)nil, r, "", rmatch);
    a = a->next;
} 
@
% example: clean:, there will be no prereqs
% stem is "" here

% why need that? to at least have something propagate in attribute()?
%  so will pass the VIRT to the node?
% because there is a rule attached here!! a recipe for virtual targets.


\section{Finding matching metarules and substituting the stem}

<<[[applyrules]] other locals>>=
char buf[NAMEBLOCK];
char stem[NAMEBLOCK];
@

<<constant NAMEBLOCK>>=
#define	NAMEBLOCK	1000
@


% this time, do not use S_TARGET hash, go through all metarules
% and check if pattern can match target.

<<[[applyrules()]] apply meta rules>>=
for(r = metarules; r; r = r->next){
    <<[[applyrules()]] skip this meta rule and continue if some conditions>>
    <<[[applyrules()]] if regexp rule then continue if some conditions>>
    else {
        if(!match(node->name, r->target, stem)) continue;
    }
    <<[[applyrules()]] infinite rule detection part1>>

    <<[[applyrules()]] if no prerequistes in meta rule r>>
    else
        for(w = r->prereqs; w; w = w->next){
            <<[[applyrules()]] if regexp rule, adjust buf and rmatch>>
            else
                subst(stem, w->s, buf, sizeof(buf));
            // recursive call!
            a->next = newarc(applyrules(buf, cnt), r, stem, rmatch);
            a = a->next;
        }
    <<[[applyrules()]] infinite rule detection part2>>
}
@

<<[[applyrules()]] skip this meta rule and continue if some conditions>>=
if((!r->recipe || !*r->recipe) 
   && (!r->prereqs || !r->prereqs->s || !*r->prereqs->s)) 
    continue;	/* no effect; ignore */
@
\t useless !r->recipe, can assert instead
% could issue a warning, but some tools generate such dependencies with
%  no recipe and no prereqs
\l could factorize with previous one

<<[[applyrules()]] if no prerequistes in meta rule r>>=
if(!r->prereqs || !r->prereqs->s || !*r->prereqs->s) {
    a->next = newarc((Node *)nil, r, stem, rmatch);
    a = a->next;
} 
@
% almost identical to before, except pass stem here instead of ""
% (not sure it matters, maybe could factorize code)
\t use empty_prereqs() again

\subsection{Matching, [[match()]]}

% go through template until percent, and progress at the same time on name.

% stem is OUT. pointer to array to modify.



<<function match>>=
bool
match(char *name, char *template,    char *stem)
{
    Rune r;
    int n;

    while(*name && *template){
        n = chartorune(&r, template);
        if (PERCENT(r))
            break;
        while (n--)
            if(*name++ != *template++)
                return false;
    }
    if(!PERCENT(*template))
        return false;

    // how many characters % is matching
    n = strlen(name) - strlen(template+1);

    if (n < 0)
        return false;
    if (strcmp(template+1, name+n))
        return false;

    strncpy(stem, name, n);
    stem[n] = '\0';

    <<[[match()]] if ampersand template>>

    return true;
}
@
% simple pattern, single % or &
\t introduce a len_after_percent_in_template?

<<[[match()]] if ampersand template>>=
if(*template == '&')
    return !charin(stem, "./");
@
\l adv topics?

\subsection{Substituting, [[subst()]]}

<<function subst>>=
void
subst(char *stem, char *template,   char *dest, int dlen)
{
    Rune r;
    char *s, *e;
    int n;

    e = dest + dlen - 1;
    while(*template){
        n = chartorune(&r, template);
        if (PERCENT(r)) {
            template += n;
            for (s = stem; *s; s++)
                if(dest < e)
                    *dest++ = *s;
        } else
            while (n--){
                if(dest < e)
                    *dest++ = *template;
                template++;
            }
    }
    *dest = '\0';
}
@


\section{Node cache}
\l generalize to Cache? with node cache, time cache, etc?
% not really a cache

% S_NODE used at a few places so maybe cant delay that to opti section.
% Also it is a DAG, not a tree

% useful in practice? when reuse a target node? when same file
%  mentioned again and again, like .o or .a used in different targets?
% but then it's not really an opti, you want to have only one node, 
%  otherwise you will run multiple times the same recipe

<<[[applyrules]] check node cache if target is already there>>=
sym = symlook(target, S_NODE, nil);
if(sym)
    return sym->u.ptr;
@

<<[[Sxxx]] cases>>=
S_NODE,		/* target name -> node */
@

<<[[newnode()]] update node cache>>=
symlook(name, S_NODE, (void *)node);
@


\section{[[timeof()]]}
% for the leaves!

% force is here to say if want to bypass cache and redo
% the actual read! for instance after the recipe is done
% we want to update the time of the file and disable the cache for sure!

% newnode() -> <> -> mkmktime -> bulktime
%                             -> dirstat (libc)
<<function timeof>>=
ulong
timeof(char *name, bool force)
{
    <<[[timeof()]] locals>>

    <<[[timeof()]] if name archive member>>
    if(force)
        return mkmtime(name, true);
    // else
    <<[[timeof()]] if not force, use time cache>>
}
@
%old: ulong mtime(char *name) {  return mkmtime(name, true);}
%but simpler to remove, refactor caller sites

% Note that applyrules goes until the end, even if leaf is up to date
% already. it will compute also the dependencies for this up-to-date
% leaf. Because applyrules just does one thing! Build static graph,
% and then can do analysis on this graph. Better separate concerns
% (so can do tricky analysis like implicit dependencies, etc)

\subsection{[[mkmtime()]]}

<<function mkmtime>>=
ulong
mkmtime(char *name, bool force)
{
    Dir *d;
    char buf[4096];
    ulong t;
    <<[[mkmtime]] locals>>

    strecpy(buf, buf + sizeof buf - 1, name);
    cleanname(buf);
    name = buf;

    <<[[mkmtime()]] bulk dir optimisation>>

    if((d = dirstat(name)) == nil)
        return 0;

    t = d->mtime;
    free(d);

    return t;
}
@

% cleanname! so normalize! otherwise lost opportunity to realize 
%  same file
% but actually not put in node cache

\subsection{Time cache}
\t adv topics? with opti of node cache?


% multiple levels of cache ... first in cache, 
% then bulk so will get more hit cache.


<<[[Sxxx]] cases>>=
S_TIME,		/* file -> time */
@

<<[[timeof()]] locals>>=
ulong t;
@

<<[[timeof()]] if not force, use time cache>>=
<<[[timeof()]] check time cache>>
t = mkmtime(name, false);
<<[[timeof()]] update time cache>>
return t;
@


<<[[timeof()]] locals>>=
Symtab *sym;
@

<<[[timeof()]] check time cache>>=
sym = symlook(name, S_TIME, nil);
if (sym)
    return sym->u.value;		/* uggh */
@
% comment?

<<[[timeof()]] update time cache>>=
if(t == 0)
    return 0;
symlook(name, S_TIME, (void*)t);		/* install time in cache */
@

% other opti about bulktime later.



\section{Checking the rules and the graph}
\n was 'checking the rules', but really it is more checking the graph

% Also simplify the graph by removing some arcs (see togo()).
% Give priority for instance to specialized over meta rules. So
% important calls below.

<<[[graph()]] checking the graph>>=
cyclechk(root);
<<[[graph()]] set root flags before [[vacuous()]]>>
vacuous(root);
ambiguous(root);
@
% can do vacuous after ambiguous? NO! cos whole point of vacuous is
% to delete some nodes and arcs that would generate ambiguity!

\subsection{Cycle detection}
\label{sec:cycle-check}

% possible because constraints on variables.
% Need to be known at parsing-time. Static cycle detection check.
% (but dynamic could be ok too).

<<[[Node_flag]] cases>>=
CYCLE      = 0x0002,
@

<<function cyclechk>>=
static void
cyclechk(Node *n)
{
    Arc *a;

    if((n->flags&CYCLE) && n->arcs){
        fprint(STDERR, "mk: cycle in graph detected at target %s\n", n->name);
        Exit();
    }
    n->flags |= CYCLE;
    for(a = n->arcs; a; a = a->next)
        if(a->n)
            cyclechk(a->n);
    n->flags &= ~CYCLE;
}
@
% important to restore back flags as they were before?
% YES because graph is a DAG, so might encounter another time the same
%  note but that's ok.
% Could optimize and have both a visited field and cycle field
%  so dont explore twice the same already checked subtree?
%  NO, cos context might be different and we might have a loop.

% FIGURE

% sucks a bit that does not show trace, so hard to debug.
%ocaml: give a trace!

% static detection of cycle. possible only because restriction on
%  variables and time of evaluation of those variables.

%real-world:
% Make does detection dynamically I think.

\subsection{Infinite rule detection and [[$NREP]]}%$

% to manage bad rules like %: %.c that can be executed multiple times

<<[[graph()]] other locals>>=
// map<ruleid, int>
char *cnt;
@

<<[[Rule]] other fields>>=
int 		rule;		/* rule number */
@


% incremented by addrule()
<<global nrules>>=
static int nrules = 0;
@
% used by rulecnt() later
<<[[addrule()]] set more fields>>=
r->rule = nrules++;
@


<<[[graph()]] set cnt for infinite rule detection>>=
cnt = rulecnt();
@

<<[[graph()]] free cnt>>=
free(cnt);
@

% statistic on how much a rule was fired? for infinite detection?
<<function rulecnt>>=
char*
rulecnt(void)
{
    char *s;

    s = Malloc(nrules);
    memset(s, 0, nrules);
    return s;
}
@
\l mv Profiling? or used also for checking?
\l but hard to move profiling, passed around a lot
% nrules modified by addrule






% n reps, to manage bad rules like %: %.c that can be executed multiple times
<<global nreps>>=
int nreps = 1;
@
% n repetition?


<<[[applyrules()]] infinite rule detection part1>>=
if(cnt[r->rule] >= nreps) 
    continue;
cnt[r->rule]++;
@

<<[[applyrules()]] infinite rule detection part2>>=
cnt[r->rule]--;
@
% restore back because want to allow to apply the same metarules
% multiple time. We just dont want to forbid it when call
% applyrules recursively with the prereqs as the new targets.



<<[[mk()]] initialisation>>=
nrep();		/* it can be updated dynamically */
@

% mk -> nrep
<<function nrep>>=
void
nrep(void)
{
    Symtab *sym;
    Word *w;

    sym = symlook("NREP", S_VAR, nil);
    if(sym){
        w = sym->u.ptr;
        if (w && w->s && *w->s)
            nreps = atoi(w->s);
    }
    if(nreps < 1)
        nreps = 1;
    <<[[nrep()]] if DEBUG(D_GRAPH)>>
}
@
\t mv to graph.c? or check.c?


\subsection{Ambiguous rules detection}
% also modify graph to fix things.

% help to understand subtelities of multiple targets, multiple prereqs,
%  multiple rules ?

\l seems that rely on TOGO field, so assume vacuous has been called before?

%nice output:
%mk: ambiguous recipes for install:
%	install <-(/sys/src/cmd/mklib:28)- libthreads.a <-(/sys/src/cmd/mklib:12)- libthreads.a(scheduler.8)
%	install <-(mkfile:19)- 


<<function ambiguous>>=
static void
ambiguous(Node *n)
{
    Arc *a;
    Rule *r = nil;
    Arc *la = nil;
    bool bad = false;

    for(a = n->arcs; a; a = a->next){
        // recurse
        if(a->n)
            ambiguous(a->n);

        // rules without any recipe do not generate ambiguity
        if(*a->r->recipe == '\0') continue;
        // else

        // first rule with recipe (so no ambiguity)
        if(r == nil) {
            r = a->r;
            la = a;
        }
        else{
            <<[[ambiguous()]] give priority to simple rules over meta rules>>
            if(r->recipe != a->r->recipe){
                if(!bad){
                    fprint(STDERR, "mk: ambiguous recipes for %s:\n", n->name);
                    bad = true;
                    trace(n->name, la);
                }
                trace(n->name, a);
            }
        }
    }
    if(bad)
        Exit();
    <<[[ambiguous()]] get rid of all skipped arcs>>
}
@
\t rename r to master_rule ? and la to master_arc ?
\t use empty_recipe() (and assert != nil)
\l could optimize again and avoid visiting node already visited

%ocaml-found:
% use != to compare recipe? string address comparison? not strcmp?
%  when can have 2 arcs with same recipe pointer?
% all the time, because in foo.exe: foo.o bar.o\n gcc foo.o bar.o -o foo.exe
%  you will have multiple arcs with recipe, but always same recipe so it's ok.
%  When we build the graph we splitted such single rule in multiple arcs,
%  but we need to remember they come from the same rule after.
  

% why not mark TOGO for rule without recipe? because we still want this
% arc for outdated check! this arc does not contain a recipe, but it
% is still important, e.g. dependencies like foo.o: foo.h

\l can have target with no recipe arc at all?

<<function trace>>=
static void
trace(char *s, Arc *a)
{
    fprint(STDERR, "\t%s", s);
    while(a){
        fprint(STDERR, " <-(%s:%d)- %s", a->r->file, a->r->line,
            a->n? a->n->name:"");
        if(a->n){
            for(a = a->n->arcs; a; a = a->next)
                if(*a->r->recipe) break;
        } else
            a = nil;
    }
    fprint(STDERR, "\n");
}
@
\t what is the stuff looking in the prereqs of the prereq?

\subsubsection{Specialized versus generic rules}

% seems fair, can write general rules and then add specific ones.

<<[[ambiguous()]] give priority to simple rules over meta rules>>=
if(r->recipe != a->r->recipe){
    if((r->attr&META) && !(a->r->attr&META)){
        la->flag |= TOGO;
        r = a->r;
        la = a;
    } else if(!(r->attr&META) && (a->r->attr&META)){
        a->flag |= TOGO;
        continue;
    }
}
@

\subsubsection{Adjusting the graph, [[togo()]]}

<<[[Arc]] other fields>>=
// bool (TOGO)
short flag;
@
\t rename toremove?

<<constant TOGO>>=
/* Arc.flag */
#define		TOGO		true
@
% mark arc as to be removed
\t remove, just use better field name for arc.flag.

<<[[ambiguous()]] get rid of all skipped arcs>>=
togo(n);
@


<<function togo>>=
static void
togo(Node *node)
{
    Arc *a, *la;

    /* delete them now */
    la = nil;
    for(a = node->arcs; a; la = a, a = a->next)
        if(a->flag&TOGO){
            //remove_list(a, node->prereqs)
            if(a == node->arcs)
                node->arcs = a->next;
            else
                la->next = a->next, a = la;
        }
}
@
%pad: I've added the warning, BUT ACTUALLY I should not!
%            fprint(STDERR, "mk: vacuous arc found %s->%s\n", 
%                     node->name, a->n->name);
\l could do it for explain mode

% la = last a
% leak! 
% should we remove all children of this node? no because DAG so potentially
%  referenced elsewhere

%ocaml: useful only for ambiguous for specialized vs generic so do
% adjustments there by resetting the list of prereqs


\subsection{Vacuous node detection}
% def of vacuous =~ stupid

%ocaml-found:
% without this check and graph adjustment, generic rules such as
% %.o: %.c; CC ...  and %.o: %.s; AS ... would conflict with each other
% (as in plan9/sys/src/cmd/mkone)
% and generate ambiguity. Of course if an object foo.o is requested
% and the directory contain foo.c, obviously we do not want the
% rule foo.o: foo.s. 
% We can mark as PROBABLY node with files that already exist and
% detect VACUOUS node that would generate ambiguity by looking
% if the file in the node does not exist and the node was generated
% from a metarule!



% goal is to mark certain nodes as VACUOUS.
<<[[Node_flag]] cases>>=
VACUOUS    = 0x0200,
@
\l but not sure this flag is needed as it is used only inside
\l  vacuous. If got rid of READY, then no need for VACUOUS

% before, mark a few nodes as PROBABLE
<<[[Node_flag]] cases>>=
PROBABLE   = 0x0100,
@
% not very clear name, but means file exist so probably
% a good node


<<[[newnode()]] set flags of node>>=
node->flags = (node->time? PROBABLE : 0);
@
% so if file exists, then PROBABLE.

<<[[applyrules()]] when found a regular rule for target [[node]], set flags>>=
node->flags |= PROBABLE;
@
% make sense, even if file does not exist, probably a node we
% are interested in.

<<[[graph()]] set root flags before [[vacuous()]]>>=
root->flags |= PROBABLE;	/* make sure it doesn't get deleted */
@


<<[[Node_flag]] cases>>=
READY      = 0x0004,
@
% for what? not very clear name
% Seems just because graph is a DAG so avoid redo work.
% also marked node as vacuous_checked; used later in mk()



<<function vacuous>>=
static bool
vacuous(Node *node)
{
    Arc *la, *a;
    bool vac = !(node->flags&PROBABLE);

    if(node->flags&READY)
        return node->flags&VACUOUS;
    node->flags |= READY;

    for(a = node->arcs; a; a = a->next)
        if(a->n && vacuous(a->n) && (a->r->attr&META))
            a->flag |= TOGO;
        else
            vac = false;
    <<[[vacuous]] possibly undelete some arcs>>

    togo(node);
    if(vac) {
        node->flags |= VACUOUS;
    }
    return vac;
}
@
% pad: I've originally added the warning
%        fprint(STDERR, "mk: vacuous node found %s\n", node->name);
% but this is wrong. a vacuous node is ok.

<<[[vacuous]] possibly undelete some arcs>>=
/* if a rule generated arcs that DON'T go; no others from that rule go */
for(a = node->arcs; a; a = a->next)
    if(!(a->flag&TOGO))
        for(la = node->arcs; la; la = la->next)
            if((la->flag&TOGO) && (la->r == a->r)){
                la->flag &= ~TOGO;
            }
@
\l when this matter?

\section{Rule attributes propagation}

% propagate from rule to node
\t why? just for the union of virtual?

<<function attribute>>=
static void
attribute(Node *n)
{
    Arc *a;

    for(a = n->arcs; a; a = a->next){
        <<[[attribute()]] propagate rule attribute to node cases>>
        // recurse
        if(a->n)
            attribute(a->n);
    }
    <<[[attribute()]] if virtual node>>
}
@



\chapter{Finding Outdated Files}
\label{chap:finding-outdated}

% the core algorithm.
% main idea = 
%  construct dependency graph for target root (previous chapter)
%  set all nodes to NOTMADE.
%    Then ask to build root which will recurse. 
%    If leaf then node MADE if exist
%    Otherwise look at time 
%     if file > time of all children then MADE, 
%     otherwise run recipe job in parallel, return and wait it's MADE (so loop).
%    if error in any recipe, then Exit.

% while(root not finished) { run wave of jobs and update graph }

\section{[[mk()]]}

%<<[[Node_flag]] cases>>=
%NOTMADE    = 0x0020,
%BEINGMADE  = 0x0040,
%MADE       = 0x0080,
%@
%ocaml: should be different field than other node flags

% NOTMADE initially (see clrmade())
% BEINGMADE? when start recipe, but mk can run in // so it returns!
%   can go back to node with BEINGMADE? when
%   2 files with deps on same Node, because of the DAG
% MADE finally when recipe done! (or time was already good, so no need update)

% FIGURE displaying a tree of files and
% see how mk is running some waves of jobs, and how at each node
% there is a ready or outofdate boolean value.

\t put global did here, but then need to reset it each time before work
\t  and so need also to have a local did in mk
\t make did a global because was always threading it
% everywhere and doing did = foo () || did;   
% possible? change semantic?


% main -> <> -> graph() ; clrmade(); work()
<<function mk>>=
void
mk(char *target)
{
    Node *root;
    bool did = false;
    // enum<WaitupResult>
    int res;

    <<[[mk()]] initialisation>>

    root = graph(target);
    <<[[mk()]] if DEBUG(D_GRAPH)>>
    clrmade(root);

    while(root->flags&NOTMADE){
        if(work(root, (Node *)nil, (Arc *)nil))
            did = true;	/* found something to do */
        else {
            res = waitup(1, (int *)nil);
            <<[[mk()]] if no child to waitup and root not MADE, possibly break>>
        }
    }
    if(root->flags&BEINGMADE)
        waitup(-1, (int *)nil);

    <<[[mk()]] before returning, more [[waitup()]] if there was an error>>
    if(!did)
        Bprint(&bout, "mk: '%s' is up to date\n", root->name);
    return;
}
@
%old: was called node, but root is better.
%old: was if(waitup(1, (int *)nil) > 0) but hard to read
%pad: I added the return

% pass nil, nil to pass previous node and previous arc, but used only
% for pretending stuff.

%ocaml-found:
% waitup take 1 to say 'ok to not have children'.
% when can be the case? when top is virtual target with no recipe
% e.g., all: prog,
% and previous round built prog and updated to MADE the prog node.
% Still the root node is not made, and work will not trigger
% any new process; it will update though all to MADE but we should
% not waitup().

%ocaml-found:
% why extra waitup after? because last work round may have put
% root as BeingMade and so will go out of the loop,
% but we still need to wait!

%note: subtle, important while loop! because of mk // so run some jobs
% but then later another job might depend on the end of this one,
% so have BEINGMADE flag and waitup??

% can have runerrs only when use mk -k ? so could put that in assert too?
% NO, can have it also when C-c.

% When can have did==false so print 'xx is up to date'?
% when did nothing, because prog was _already_ up to date.
\l maybe change message.




\section{[[clrmade()]]}

<<function MADESET>>=
#define	MADESET(n,m)	n->flags = (n->flags&~(NOTMADE|BEINGMADE|MADE))|(m)
@
% keep flags as they were, except for the made stuff
%ocaml: use a separate field than the other node attributes

<<function clrmade>>=
void
clrmade(Node *n)
{
    Arc *a;

    <<[[clrmade()]] [[n->flags]] pretend adjustments>>
    MADESET(n, NOTMADE);
    for(a = n->arcs; a; a = a->next)
        if(a->n)
            // recurse
            clrmade(a->n);
}
@
%bug? DAG? should avoid repetitive work too there?
%ocaml: no need, do that in newnode() thx to another field.

\section{[[work()]]}

% have to do transitive closure of dependencies ... when
% "making" node, look at is prerequistes, and check if they need
% work too, because maybe a leaf was modified!


<<[[work()]] locals>>=
bool did = false;
@
\t move as a global! so no need return bool in work() also.


\t parent_node, prev_arc, TODO rename parameters?
% extra params for advanced functionality not needed

% main -> mk -> <> -> outofdate(); dorecipe()
<<function work>>=
bool
work(Node *node,   Node *p, Arc *parc)
{
    <<[[work()]] locals>>

    <<[[work()]] debug>>
    if(node->flags&BEINGMADE)
        return did;
    <<[[work()]] possibly unpretending node>>

    // Leaf case
    <<[[work()]] no prerequisite, a leaf>>
    // else
    // Node case
    <<[[work()]] some prerequisites, a node>>
}
@
% can refactor in did ||= foo(); ? hmm maybe not, we actually want the
% side effect!
\t maybe can rewrite with skeleton of if(node->prereqs == nil)
\t  and use else instead of ugly return shortcut.

\t can refactor did in a global? dangerous ... but would clarify I think

% BEINGMADE test because DAG.

\subsection{Leaf case}

% a leaf!
<<[[work()]] no prerequisite, a leaf>>=
/* consider no prerequisite case */
if(node->arcs == nil){
    if(node->time == 0){
        <<[[work()]] print error when inexistent file without prerequisites>>
    } else
        MADESET(node, MADE);
    return did;
}
@
% if no prereqs, then no arcs, then no rule with recipe
\t why not use update(false, node) here?

% remember that virtual rules have an arc

<<[[work()]] locals>>=
char cwd[256];
@

<<[[work()]] print error when inexistent file without prerequisites>>=
if(getwd(cwd, sizeof cwd))
    fprint(STDERR, "mk: don't know how to make '%s' in directory %s\n", node->name, cwd);
else
    fprint(STDERR, "mk: don't know how to make '%s'\n", node->name);

<<[[work()]] when inexistent target without prerequisites, if kflag>>
else
    Exit();
@
% the directory part is useful in recursive mk context
\l aspectize that part and put in recursive mk section?

\subsection{Node case}

<<[[work()]] locals>>=
bool weoutofdate = false;
bool ready = true;
Arc *a;
@
%old: used to be done later, and weoutofdate used to be aflag
%old: initialization was done later
\n called weoutofdate to avoid conflict with outofdate function

<<[[work()]] some prerequisites, a node>>=
<<[[work()]] adjust weoutofdate if aflag>>
/*
 *   now see if we are out of date or what
 */
for(a = node->arcs; a; a = a->next)
    if(a->n){
        // recursive call! go in depth
        did = work(a->n, node, a) || did;

        if(a->n->flags&(NOTMADE|BEINGMADE))
            ready = false;
        if(outofdate(node, a, false)){
            weoutofdate = true;
            <<[[work()]] update [[ra]] when outofdate [[node]] with arc [[a]]>>
        }
    } else {
        if(node->time == 0){
            weoutofdate = true;
            <<[[work()]] update [[ra]] when no dest in arc and no src>>
        }
    }

if(!ready)	/* can't do anything now */
    return did;
if(!weoutofdate){
    MADESET(node, MADE);
    return did;
}
<<[[work()]] possibly pretending node>>

// else, out of date

did = dorecipe(node) || did;
return did;
@

\t why not remove ready=false and just return earlier? do side effects
\t  in loop for other stuff? for pretending stuff?
\t  also could rewrite code with cleaner if then else instead of those
\t  ugly return shortcut

% when can happen have no node attached to an arc? virtual targets
%  or when file does not exist, so for sure we are out of date.


%\section{[[outofdate()]]}

<<function outofdate>>=
bool
outofdate(Node *node, Arc *arc, bool eval)
{
    <<[[outofdate()]] locals>>

    <<[[outofdate()]] if arc->prog>>
    else 
     <<[[outofdate()]] if arc node is an archive member>>
     else
        /*
         * Treat equal times as out-of-date.
         * It's a race, and the safer option is to do
         * extra building rather than not enough.
         */
        return node->time < arc->n->time;
}
@
% inexistent file has zero time (like very very old time)
%old: was <=, but annoying, so I ignored the comment

% eval argument? seems used only for :P: prog stuff

%note: on very fast machine the <= above test can be actually annoying.
% For instance in ocaml both the .cmo and .cmi may have
% been created in the same second by ocamlc, which means
% then that the .cmo file will always be recompiled because
% its .cmi has the same date (and recompiling the .cmo
% will generate a new .cmi, which then will trigger at the next
% round another compilation of the .cmo and so on).




\section{[[dorecipe()]]}

%bool dorecipe(Node *node)

% will iterate over arcs, node in args, words, and symbols.

% find right arc with recipe, find master rule

<<[[dorecipe()]] other locals>>=
Arc *aa = nil; // arc with recipe
Rule *r = nil; // rule with recipe
@
\t rename aa to arc or master_arc

% set list of targets (most of the time single one)
% and have alltargets and only-old-targets (outofdate)

<<[[dorecipe()]] other locals>>=
Word head, ahead; // out of date targets,   all targets
@
\t rename head ahead

% set list of prereqs (most of the time single one)
% and have allprereqs and only-new-prereqs (causing outofdate target)

<<[[dorecipe()]] other locals>>=
Word lp, ln; // all prereqs, prereqs making target out of date
@
\t rename lp ln
%old: l for list?

<<[[dorecipe()]] other locals>>=
bool did = false;
@
%dead, always false (maybe use [[did]] instead of false cos clearer)


% main -> mk -> work -> ... -> run(newjob())
<<function dorecipe>>=
bool
dorecipe(Node *node)
{
    // iterators
    Arc *a;
    Node *n;
    Word *w;
    Symtab *s;
    <<[[dorecipe()]] other locals>>

    /*
     *   pick up the rule
     */
    for(a = node->arcs; a; a = a->next)
        if(*a->r->recipe) {
            aa = a;
            r = a->r;
        }
    <<[[dorecipe()]] if no recipe found>>
    // else

    /*
     *   build the node list
     */
    node->next = nil;
    head.next = ahead.next = nil;
    <<[[dorecipe()]] build lists of targets and node list>>
    <<[[dorecipe()]] return if one target not READY>>

    /*
     *   gather the params for the job
     */
    lp.next = ln.next = nil;
    for(n = node; n; n = n->next){
        <<[[dorecipe()]] build lists of prerequisites>>
        MADESET(n, BEINGMADE);
    }

    // run the job
    run(newjob(r, node, aa->stem, aa->match, 
               lp.next, ln.next, 
               head.next, ahead.next));
    return true;
}
@
%/*print("lt=%s ln=%s lp=%s\n",wtos(head.next, ' '),wtos(ln.next, ' '),wtos(lp.next, ' '));
\t remove local did?
\t replace return true by did=true; return;

% if return error code, then should stop mk right?


<<[[dorecipe()]] other locals>>=
Word *ww, *aw;
char buf[BIGBLOCK];
@


<<[[dorecipe()]] build lists of targets and node list>>=
ww = &head;
aw = &ahead;
<<[[dorecipe()]] if regexp rule>>
else {
    for(w = r->alltargets; w; w = w->next){
        if(r->attr&META)
            subst(aa->stem, w->s, buf, sizeof(buf));
        else
            strecpy(buf, buf + sizeof buf - 1, w->s);

        aw->next = newword(buf);
        aw = aw->next;

        s = symlook(buf, S_NODE, nil);
        if(s == nil)
            continue;	/* not a node we are interested in */
        // else
        n = s->u.ptr;

        <<[[dorecipe()]] update list of outdated targets>>

        if(n == node) continue;

        n->next = node->next;
        node->next = n;
    }
}
@
%ocaml: can do the subst once and for all and have a better rule_exec!
\t but then need to have talked about S_NODE before? so cant aspectize
\t  node cache?

% when can have the case of node not found in S_NODE? because
%  when we build the graph we start from one target which may
%  be part of a multi-target rule, e.g. parser.ml parser.mli: parser.mly
%  and if we did mk parser.mli, then parser.ml will not be in
%  the graph even though we want to add it for the alltargets variable

<<[[dorecipe()]] update list of outdated targets>>=
if(!aflag && n->time) {
    for(a = n->arcs; a; a = a->next)
        if(a->n && outofdate(n, a, false))
            break;
    // no out of date arc, node does not need to be regenerated
    if(a == nil)
        continue; 
    // else, find an outdated arc for node of target
}
ww->next = newword(buf);
ww = ww->next;
@


<<[[dorecipe()]] return if one target not READY>>=
for(n = node; n; n = n->next)
    if(!(n->flags&READY))
        return did;
@
% did always false here no?
% and look vacuous(), READY is always set ...
\t ???? remove code?

<<[[dorecipe()]] build lists of prerequisites>>=
for(a = n->arcs; a; a = a->next){
    if(a->n){
        addw(&lp, a->n->name);
        if(outofdate(n, a, false)){
            addw(&ln, a->n->name);
            <<[[dorecipe()]] explain when found arc [[a]] making target [[n]] out of date>>
        }
    } else {
        <<[[dorecipe()]] explain when found target [[n]] with no prerequisite>>
    }
}
@
\l why arc without a node??

<<[[dorecipe()]] other locals>>=
char cwd[256];
@

<<[[dorecipe()]] if no recipe found>>=
/*
 *   no recipe? go to buggery!
 */
if(r == nil){
    if(!(node->flags&VIRTUAL) && !(node->flags&NORECIPE)){
        if(getwd(cwd, sizeof cwd))
            fprint(STDERR, "mk: no recipe to make '%s' in directory %s\n", node->name, cwd);
        else
            fprint(STDERR, "mk: no recipe to make '%s'\n", node->name);
        Exit();
    }
    // else
    <<[[dorecipe()]] when no recipe found, if archive name>>
    else
        update(false, node);

    <<[[dorecipe()]] when no recipe found, if tflag>>
    return did;
}
@
\t change !X && !Y  to X || Y   so easier to aspectize




\chapter{Scheduling Jobs}
\label{chap:scheduling}
\l Fixing outdated files

%trans: remaining things to see: run(), waitup()
% seen Job and jobs before.
% will see a few more globals: nproclimit, nevent, nrunning

% can do stuff in //. Make originally didn't (apparently).

\section{Enqueuing jobs}

\subsection{[[nrunning]] and [[nproclimit]]}

<<global nrunning>>=
static int nrunning;
@

<<global nproclimit>>=
static int nproclimit;
@
% can be set in env (readenv) $NPROC, see below
\l put code setting up nproclimit here?

\subsection{[[run()]]}

% main -> mk -> work -> dorecipe -> <> (-> sched())
<<function run>>=
void
run(Job *j)
{
    Job *jj;

    // enqueue(j, jobs)
    if(jobs){
        for(jj = jobs; jj->next; jj = jj->next)
            ;
        jj->next = j;
    } else 
        jobs = j;
    j->next = nil;

    /* this code also in waitup after parse redirect */
    if(nrunning < nproclimit)
        sched();
}
@
%ocaml: use Queue.add
\l j->next = nil  redundant since does it already in newjob

% if no more processor free, then just enqueued. next round
%  will pop this one.


\subsection{[[$NPROC]]}%$
% export NPROC=...

<<[[mk()]] initialisation>>=
nproc();	/* it can be updated dynamically */
@

% like nrep()

<<function nproc>>=
void
nproc(void)
{
    Symtab *sym;
    Word *w;

    if(sym = symlook("NPROC", S_VAR, nil)) {
        w = sym->u.ptr;
        if (w && w->s && w->s[0])
            nproclimit = atoi(w->s);
    }
    if(nproclimit < 1)
        nproclimit = 1;
    <<[[nproc()]] if DEBUG(D_EXEC)>>

    <<[[nproc()]] grow nevents if necessary>>
}
@


\section{Scheduling jobs}

%trans: if processor free, then run job, and keep track of it.

\subsection{[[RunEvent]] and [[events]]}

% to keep track of children process, what they were supposed to do.

<<struct RunEvent>>=
struct RunEvent {
    // option<Pid> (None = 0)
    int pid;

    // ref_own<Job>
    Job *job;
};
@
%bug? seems that job never freed

<<global events>>=
// growing_array<Runevent> (size = nevents (== nproclimit))
static RunEvent *events;
@
%ocaml: use instead map (e.g., hash) from pid to job.


<<global nevents>>=
static int nevents;
@
\t should be == nproclimit, so maybe can remove it
\l  or sometimes nproclimit can go down, in which case we dont really
\l  but useless opti
\t  just remember old_nproclimit in nproc() so know if need realloc

% the slaves processors! called a slot.

% sched -> <>
<<function nextslot>>=
int
nextslot(void)
{
    int i;

    for(i = 0; i < nproclimit; i++)
        if(events[i].pid <= 0) 
            return i;
    assert(/*out of slots!!*/ false);
    return 0;	/* cyntax */
}
@
%ocaml: no need growing_array, just use hash

% waitup -> <>
<<function pidslot>>=
int
pidslot(int pid)
{
    int i;

    for(i = 0; i < nevents; i++)
        if(events[i].pid == pid) 
            return i;
    <<[[pidslot()]] if DEBUG(D_EXEC)>>
    return -1;
}
@
% this one iterate on all nevents, not just nproclimit
% but should be the same between two runs of mk
% so maybe could simplify.
\t use nproclimit, not nevents above, more consistent
%ocaml: use Hashtbl.find


<<[[nproc()]] grow nevents if necessary>>=
if(nproclimit > nevents){
    if(nevents)
        events = (RunEvent *)Realloc((char *)events, nproclimit*sizeof(RunEvent));
    else
        events = (RunEvent *)Malloc(nproclimit*sizeof(RunEvent));

    while(nevents < nproclimit)
        events[nevents++].pid = 0;
}
@
\t replace nproclimit by != nevents
%ocaml: no need, Hashtbl.t grows as needed

% so nevents should be = to nproclimit at the end


\subsection{[[sched()]]}

% main -> mk -> work -> dorecipe -> run -> sched()
<<function sched>>=
static void
sched(void)
{
    Job *j;
    int slot;
    Envy *e;
    char *flags;
    <<[[sched()]] other locals>>

    <<[[sched()]] sanity check jobs>>

    // j = pop(jobs)
    j = jobs;
    jobs = j->next;
    <<[[sched()]] if DEBUG(D_EXEC)>>

    slot = nextslot();
    events[slot].job = j;

    e = buildenv(j, slot);
    <<[[sched()]] print recipe command on stdout>>

    <<[[sched()]] if dry mode or touch mode, alternate to execsh>>
    else {
       <<[[sched()]] if DEBUG(D_EXEC) print recipe>>
        flags = "-e";
       <<[[sched()]] reset flags if NOMINUSE rule>>

        // launching the job!
        events[slot].pid = execsh(flags, j->r->recipe, nil, e);

        usage();
        nrunning++;
       <<[[sched()]] if DEBUG(D_EXEC) print pid>>
    }
}
@
% -e is important!! want any error in the subshell command
% to abort the whole thing

<<[[sched()]] sanity check jobs>>=
if(jobs == nil){
    usage();
    return;
}
@
\t how can have jobs == nil?
% should issue an error here, assert!
\t actually usage is for profiling

% buildenv is important; it is the function using most of the fields in j;

\section{Executing Jobs}

\subsection{Shell configuration}

<<global shell>>=
char 	*shell =	"/bin/rc";
@
<<global shellname>>=
char 	*shellname =	"rc";
@

<<global shflags>>=
char	*shflags = "-I";	/* rc flag to force non-interactive mode */
@
\t need that ? does not repeat on stdout the command

% pad: should add also -e!! actually -e is added by default
%  unless you added :E: for NOMINUSE

% will fork, exec shell, and pass as stdin input in pipe 
% to the shell the recipe string.
%alt: sh -e '' ? but then need escape mess?

% can also pass a buf for bquote (backquote execution), to store
%  stdout instead of just displaying it gradually

% FIGURE? cos 2 forks below.
% 2 fork trick for what? because dont want current
% process to be blocked on feeding input to shell. Instead
% do that in another process.

\subsection{[[execsh()]]}

%plan9.c 
<<function execsh>>=
int
execsh(char *args, char *cmd, Bufblock *buf, Envy *e)
{
    int pid;
    fdt in[2];
    int err;
    <<[[execsh()]] other locals>>

    <<[[execsh()]] if buf then create pipe to save output>>

    pid = rfork(RFPROC|RFFDG|RFENVG);
    <<[[execsh()]] sanity check pid rfork>>
    // children
    if(pid == 0){
        <<[[execsh()]] in children, if buf, close one side of pipe>>
        err = pipe(in);
        <<[[execsh()]] sanity check err pipe>>
        pid = fork();
        <<[[execsh()]] sanity check pid fork>>
        // child 1, the shell interpreter
        if(pid != 0){
            // input now comes from the pipe
            dup(in[0], STDIN);
            <<[[execsh()]] in child 1, if buf, dup and close>>
            close(in[0]);
            close(in[1]);
            if (e)
                exportenv(e);
            if(shflags)
                execl(shell, shellname, shflags, args, nil);
            else
                execl(shell, shellname, args, nil);
            // should not be reached
            perror(shell);
            _exits("exec");
        }
        // child 2, feeding the shell with recipe, through a pipe
        <<[[execsh()]] in child 2, if buf, close other side of pipe>>
        close(in[0]);
        // feed the shell
        <<[[execsh()]] in child 2, write cmd in pipe>>
        close(in[1]); // will flush
        _exits(nil);
    }
    // parent
    <<[[execsh()]] in parent, if buf, close other side of pipe and read output>>
    return pid;
}
@
%pad: I introduced int err; so can better aspectize sanity checks
\n put flags after cmd in argument of execsh, more logical? no! cmd is input!
\t rename cmd to input ?

%ocaml-found:
% important for second fork to do the code of the shell in the case where
%  pid <> 0!!
% otherwise, would wait for wrong process! would wait for
% child2 that feeds the shell which returns very quickly!
% Also wait for could possibly capture child2.

% because feed stdin and then close, that means programs called from
% shell can not read from stdin :( for instance make sync works but
% mk sync does not.


% exportenv! so set in child the variables in environment

<<[[execsh()]] other locals>>=
char *p;
@
<<[[execsh()]] in child 2, write cmd in pipe>>=
p = cmd+strlen(cmd);
while(cmd < p){
    n = write(in[1], cmd, p-cmd);
    if(n < 0)
        break;
    cmd += n;
}
@



<<[[execsh()]] sanity check pid rfork>>=
if(pid < 0){
    perror("mk rfork");
    Exit();
}
@
<<[[execsh()]] sanity check pid fork>>=
if(pid < 0){
    perror("mk fork");
    Exit();
}
@
<<[[execsh()]] sanity check err pipe>>=
if(err < 0){
    perror("pipe");
    Exit();
}
@








\section{Waiting for jobs to finish}

\subsection{[[waitup()]]}

% wait user processes?

% retstatus param mostly not used. used for <| and :P: so can aspectize it
% INOUT. for IN it is a pid and OUT it is the return status (ugly)
% put here because can have only one call to wait. wait is waiting
%  indiscrimently, so have to centralize in one place.
% For more info, see Section X.

\t enum WaitupResult { WEmptyChild=1, WJobEnded=0, WNotJobProcess=-1 }
\t enum WaitupParam { EmptyChildOk, EmptyChildError1 =-1, EmptyChildError2 =-2}

% mk -> work; <>
<<function waitup>>=
int
waitup(int echildok, int *retstatus)
{
    // child process
    int pid;
    // return string of child process
    char buf[ERRMAX];
    // index in events[]
    int slot;
    Job *j;
    Symtab *sym;
    Word *w;
    bool fake = false;
    <<[[waitup()]] other locals>>

    <<[[waitup()]] if retstatus, check process list>>
again:		/* rogue processes */

    pid = waitfor(buf);
    <<[[waitup()]] if no more children>>
    <<[[waitup()]] if DEBUG(D_EXEC) print pid>>
    <<[[waitup()]] if retstatus, check if matching pid>>

    slot = pidslot(pid);
    <<[[waitup()]] if slot not found, not a job pid, update process list>>

    j = events[slot].job;
    usage();
    nrunning--;
    // free events[slot]
    events[slot].pid = -1;

    <<[[waitup()]] if error in child process, possibly set fake or exit>>
    // else

    for(w = j->t; w; w = w->next){
        sym = symlook(w->s, S_NODE, nil);
        <<[[waitup()]] skip if node not found>>
        update(fake, (Node*) sym->u.ptr);
    }

    if(nrunning < nproclimit)
        sched();
    return 0;
}
@
%bug? could free job in events[i] too
%old: fake was called [[uarg]] (for update arg)
\t return value used?

%plan9.c
<<function waitfor>>=
int
waitfor(char *msg)
{
    Waitmsg *w;
    int pid;

    // blocking call, wait for any children
    w = wait();
    // no more children
    if(w == nil)
        return -1;
    strecpy(msg, msg+ERRMAX, w->msg);
    pid = w->pid;
    free(w);
    return pid;
}
@
% strecpy set msg[0] to \0 if nothing?


% echildok -1, -2, -3?? just to help debug which one of the waitup?
%alt: could use __LINE__ instead
\t introduce enum? ERROR_CHILD_OK, ERROR_CHILD_NOTOK1, ERROR_CHILD_NOTOK2
\t  or is it EMPTY_CHILD_OK?

<<[[waitup()]] if no more children>>=
if(pid == -1){
    if(echildok > 0)
        return 1;
    else {
        fprint(STDERR, "mk: (waitup %d) ", echildok);
        perror("mk wait");
        Exit();
    }
}
@



<<[[waitup()]] other locals>>=
Envy *e;
Bufblock *bp;
@

<<[[waitup()]] if error in child process, possibly set fake or exit>>=
if(buf[0]){
    e = buildenv(j, slot);
    bp = newbuf();
    shprint(j->r->recipe, e, bp);
    front(bp->start);
    fprint(STDERR, "mk: %s: exit status=%s", bp->start, buf);
    freebuf(bp);

    <<[[waitup()]] when error in child process, delete if DELETE node>>
    fprint(STDERR, "\n");

    <<[[waitup()]] when error in child process, if kflag>>
    else {
        jobs = nil;
        Exit();
    }
}
@
% nice shprint! see final command with expanded variables!

<<[[waitup()]] skip if node not found>>=
if(sym == nil)
    continue;	/* not interested in this node */
@
\t should never happen I think, cos node list is made with
\t  node we are interested in

% because up to date target anyway?

<<function front>>=
void
front(char *s)
{
    char *t, *q;
    int i, j;
    char *flds[512];

    q = strdup(s);
    i = getfields(q, flds, nelem(flds), 0, " \t\n");
    if(i > 5){
        flds[4] = flds[i-1];
        flds[3] = "...";
        i = 5;
    }
    t = s;
    for(j = 0; j < i; j++){
        for(s = flds[j]; *s; *t++ = *s++);
        *t++ = ' ';
    }
    *t = 0;
    free(q);
}
@
\t ??? getfields?


\subsection{[[update()]]}

% mk -> waitup -> <>
<<function update>>=
void
update(bool fake, Node *node)
{
    Arc *a;

    MADESET(node, fake? BEINGMADE : MADE);
    <<[[update()]] debug>>


    if((!(node->flags&VIRTUAL)) && (access(node->name, AEXIST) == 0)){
        node->time = timeof(node->name, true);
        <<[[update()]] unpretend node>>
        <<[[update()]] set outofdate prereqs if arc prog>>
    } 
    else {
        // virtual target or target still does not exist (but marked as MADE)
        node->time = 1;
        for(a = node->arcs; a; a = a->next)
            if(a->n && outofdate(node, a, true))
                node->time = a->n->time;
    }
}
@
% pass true to outofdate here!!
% pass true to timeof, to force, to not use the cache because indeed
%  time probably changed
% what if it did not change? return an error? No! it can happen legitimely
%  in some cases, like when do x.tab.h: y.tab.h (see plan9/shell/rc/mkfile).
% why set time to 1 for virtual node? just to be different than None
%  and then anway later take latest time of one of the prereq
\t but then what if no prereq? then time is 1. important?

% so if target not exist, still put MADE for node
% because anyway, otherwise would loop infinitely.
% and set date of newest prereq.





\section{Process management}

% cant just exit with processes hanging out.
% wait at least until job finished.
% Because do things in parallel, if one command
%  returns an error, because syntax error in a file,
%  we do not want to abruptly kill and stop the other 
%  commands because we may endup with corrupted object file
%  (or source file if interrupt yacc or lex)

% See pb with mk-rc.byte when didnt implement that?
% got corrupted cmo all the time.

\subsection{[[Exit()]]}

<<function Exit>>=
void
Exit(void)
{
    while(waitpid() >= 0)
        ;
    exits("error");
}
@

% Malloc, Realloc, they call Exit in case of error.
% there should be no calls directly to exits

%trans:
% good to wait, but what if too long recipe? % enter C-c!


\subsection{Notes (signals) management}
% [[C-c]]

<<[[main()]] initializations before building>>=
catchnotes();
@
%old: argv processing part 3 was there before, in the middle


% main -> <>
<<function catchnotes>>=
void
catchnotes()
{
    atnotify(notifyf, 1);
}
@

<<function notifyf>>=
int
notifyf(void *a, char *msg)
{
    <<[[notifyf()]] sanity check not too many notes>>
    if(strcmp(msg, "interrupt")!=0 && strcmp(msg, "hangup")!=0)
        return 0;
    killchildren(msg);
    return -1;
}
@
% 0 -> IGNORE_NOTE?


<<[[notifyf()]] sanity check not too many notes>>=
static int nnote;

USED(a);
if(++nnote > 100){	/* until andrew fixes his program */
    fprint(STDERR, "mk: too many notes\n");
    notify(0);
    abort();
}
@

\subsection{Killing all children}

\subsubsection{[[killchildren()]]}

<<function killchildren>>=
void
killchildren(char *msg)
{
    <<[[killchildren()]] locals>>

    kflag = true;	/* to make sure waitup doesn't exit */
    jobs = nil;		/* make sure no more get scheduled */

    <<[[killchildren()]] expunge not-job processes>>

    while(waitup(1, (int *)nil) == 0)
        ;
    Bprint(&bout, "mk: %s\n", msg);
    Exit();
}
@
\t 0 -> WJobEnded
\t 1 -> EmptyChildOk

% so at some point should get WEmptyChild
% (or WNotJobProcessDone but would be weird)

\l why not send also postnote to job child process?

\subsubsection{[[kflag]] and [[runerrs]]}

% kflag also used for mk -k, but does not make much sense I think.

<<global kflag>>=
bool kflag = false;
@
% continue even if errors.
% not so useful in general, but useful for killchildren when get a C-c ???
<<[[main()]] -xxx switch cases>>=
case 'k':
    kflag = true;
    break;
@


<<global runerrs>>=
int runerrs;
@
<<[[mk()]] initialisation>>=
runerrs = 0;
@



\subsubsection{Adjusting [[mk()]], [[work()]], and [[waitup()]]}

% instead of Exit
<<[[work()]] when inexistent target without prerequisites, if kflag>>=
if(kflag){
    node->flags |= BEINGMADE;
    runerrs++;
}
@

% instead of Exit again
<<[[waitup()]] when error in child process, if kflag>>=
if(kflag){
    runerrs++;
    fake = true;
}
@
% fake so BEINGMADE instead of MADE


<<[[mk()]] if no child to waitup and root not MADE, possibly break>>=
if(res > 0){
    if(root->flags&(NOTMADE|BEINGMADE)){
        assert(/*must be run errors*/ runerrs);
        break;	/* nothing more waiting */
    }
}
@
\t res == WEmptyChild ?

% subtle
<<[[mk()]] before returning, more [[waitup()]] if there was an error>>=
while(jobs)
    waitup(-2, (int *)nil);

assert(/*target didnt get done*/ runerrs || (root->flags&MADE));
@
\t when do we have this? really need that?






\chapter{Interacting with the Shell}
\label{chap:interacting}

% main strategy of mk for interacting with shell is to
% pass variable value through environment! (simple)
% and to pass recipe through stdin (simpler than through -c, no
% need quote escaping)

%real-world:
% mk simpler than make on this. no Make variables vs shell variables.
% (but then stronger coupling with the shell language? portable?)


% The recipe is a shell command (or multiple commands).
% Seen execsh() above with global shell, shellname stuff. 
%  (already plan9 specific and some stuff rc specific)
% was taking an Envy because recipe can contain variables.
% buildenv() in sched() before execsh, passing an Envy.

%\section{Variables expansion (part 3)}
% not done actually! shell does it!  simpler! no $$i mess. simplify some things.


% variables, different occurences: in rule target, prerequistes,
% or in recipe. Different handling depending on context.
% See quote from mk manual.

\section{[[Envy]] and [[envy]]}
%\section{Environment Variables, [[Envy]] and [[envy]]}
% could be in core DS, but used just here, and outside S_VAR is good enough.

\t Not envy :) just environment variable stuff, rename? Env and env?
<<struct Envy>>=
struct Envy
{
    // ref<string>, the key
    char 		*name;

    // list<ref_own<string>>, the value
    Word 		*values;
};
@
\l why y?


<<global envy>>=
// growing_array<Envy> (endmarker = nil,nil)
Envy	*envy;
@
%ocaml: just use a hashtbl and reuse env (hash). useless I think to optimize.

%use special data structure with special marks because used in
% special way. Have first full static environment and then
% each time different value for special mk variables ($stem, $target).

<<constant ENVQUANTA>>=
#define ENVQUANTA 10
@

<<global nextv>>=
// idx for next free entry in envy array
static int nextv;
@

% envupd | ecopy | execinit -> <>
<<function envinsert>>=
static void
envinsert(char *name, Word *value)
{
    static int envsize;

    // grow array if necessary
    if (nextv >= envsize) {
        envsize += ENVQUANTA;
        envy = (Envy *) Realloc((char *) envy, envsize*sizeof(Envy));
    }

    envy[nextv].name = name;
    envy[nextv++].values = value;
}
@
%ocaml: just Hashtbl.add


<<function envupd>>=
static void
envupd(char *name, Word *value)
{
    Envy *e;

    for(e = envy; e->name; e++)
        if(strcmp(name, e->name) == 0){
            freewords(e->values);
            e->values = value;
            return;
        }
    e->name = name;
    e->values = value;
    envinsert(nil,nil); // ???
}
@
\t else if not found, generate error! 
\t could also just call simply envinsert(e->name, value) instead
\t of this envinsert(nil,nil), but then this would require
\t  to have access to nextv in envupd too.
%ocaml: just Hashtbl.replace

% what is this nil,nil? to make a special mark in envy.
%  because then use e->name termination condition.
%  because growing array and special end marker!

\section{Initializing the shell environment, [[execinit()]]}
% as opposed to make, no $(XX), no $$i, the string is passed
% as is to shell! less escaping need!

<<[[main()]] initializations before building>>=
execinit();
@
%ocaml: just reuse env, no need for another environment-like data structure.


% main -> parse; mk; <>
<<function execinit>>=
void
execinit(void)
{
    char **p;

    nextv = 0; // reset envy

    // internal mk variables
    for(p = myenv; *p; p++)
        envinsert(*p, stow(""));

    // user variables in mkfile or process shell environment
    symtraverse(S_VAR, ecopy);

    // end marker
    envinsert(nil, nil);
}
@
\l potential leak when reset on Envy.values ?
\t again nil,nil insert because nextv is static. Should be global!
\t rename initenv() for that!

%note that internal vars are put first in Envy because they are the one
% that will be updated! so want fast iteration there.

% use myenv also in initenv(inithash), but it is for the S_INTERNAL stuff and
%  used later for readenv

% execinit -> symtraverse -> <> (as x <- symtraverse(..., <>) <- execinit)
<<function ecopy>>=
static void
ecopy(Symtab *s)
{
    char **p;

    <<[[ecopy()]] return and do not copy if S_NOEXPORT symbol>>
    <<[[ecopy()]] return and do not copy if conflict with mk internal variable>>
     // else
     envinsert(s->name, s->u.ptr);
}
@
%ocaml: just Hashtbl.copy, but not even sure we need that

<<[[ecopy()]] return and do not copy if conflict with mk internal variable>>=
for(p = myenv; *p; p++)
    if(strcmp(*p, s->name) == 0)
        return;
@
% maybe should issue a warning? anyway it is only for user variables
%  because for readenv(), those variables are skipped.
%ocaml-found:
% when use mk recursively, you will get a $stem in the environment! and
% you need to filter it otherwise in exportenv you would need to take care
% of duplicate and give priority to the fresh $stem


\section{Importing the external environment, [[readenv()]]}
\label{sec:readenv}

% popupate symbol table, but by transitivity it populates
% also envy at some point because of execinit

% plan9 specific, in plan9.c
% initenv -> <>
<<function readenv>>=
void
readenv(void)
{
    fdt envf; // envdir
    fdt f; // envfile
    Dir *e;
    int i, n, len, len2;
    char *p;
    char nam[1024];
    Word *w;

    rfork(RFENVG);	/*  use copy of the current environment variables */

    envf = open("/env", OREAD);
    <<[[readenv()]] sanity check envf>>
    while((n = dirread(envf, &e)) > 0){
        for(i = 0; i < n; i++){
            len = e[i].length;
            <<[[readenv()]] skip some names>>

            snprint(nam, sizeof nam, "/env/%s", e[i].name);
            f = open(nam, OREAD);
            <<[[readenv()]] sanity check f>>
            p = Malloc(len+1);
            len2 = read(f, p, len);
            <<[[readenv()]] sanity check len2>>
            close(f);
            <<[[readenv()]] add null terminator character at end of [[p]]>>
            w = encodenulls(p, len);
            free(p);
            p = strdup(e[i].name);

            // populating symbol table
            setvar(p, (void *) w);
        }
        free(e);
    }
    close(envf);
}
@

%dead: 
% <<[[Sxxx]] cases>>=
% S_EXPORTED,	/* var -> current exported value */
% @
%  symlook(p, S_EXPORTED, (void*)"")->u.ptr = "";


<<[[readenv()]] skip some names>>=
/* don't import funny names, NULL values,
 * or internal mk variables
 */
if(len <= 0 || *shname(e[i].name) != '\0')
    continue;
if (symlook(e[i].name, S_INTERNAL, nil))
    continue;
@
%alt: could import variables with same name, just give priority to
% S_INTERNAL when symlook

<<[[readenv()]] add null terminator character at end of [[p]]>>=
if (p[len-1] == '\0')
    len--;
else
    p[len] = '\0';
@
% why? some variable have and some have not? not specified in plan9?

<<[[readenv()]] sanity check envf>>=
if(envf < 0)
    return;
@
<<[[readenv()]] sanity check f>>=
if(f < 0)
    continue;
@

<<[[readenv()]] sanity check len2>>=
if(len2 != len){
    perror(nam);
    close(f);
    continue;
}
@


<<function encodenulls>>=
/* break string of values into words at 01's or nulls*/
static Word *
encodenulls(char *s, int n)
{
    Word *w, *head;
    char *cp;

    head = w = nil;
    while (n-- > 0) {
        for (cp = s; *cp && *cp != '\0'; cp++)
                n--;
        *cp = '\0';

        // add_list(newword(s), w)
        if (w) {
            w->next = newword(s);
            w = w->next;
        } else
            head = w = newword(s);

        s = cp+1;
    }
    if (!head)
        head = newword("");
    return head;
}
@


\section{Adjusting the shell environment, [[buildenv()]]}

% ->  run -> sched -> <>
<<function buildenv>>=
Envy*
buildenv(Job *j, int slot)
{
    <<[[buildenv()]] locals>>

    // main variables 
    envupd("target", wdup(j->t));
    <<[[buildenv()]] if regexp rule>>
    else
        envupd("stem", newword(j->stem));
    envupd("prereq", wdup(j->p));

    // advanced variables 
    <<[[buildenv()]] envupd some variables>>

    return envy;
}
@
\t rename adjustenv? or instantiate_env?




\section{Exporting the shell environment, [[exportenv()]]}

% execsh -> <>
<<function exportenv>>=
/* as well as 01's, change blanks to nulls, so that rc will
 * treat the words as separate arguments
 */
void
exportenv(Envy *e)
{
    Symtab *sy;
    int n;
    fdt f;
    bool hasvalue;
    bool first;
    Word *w;
    char nam[256];

    for(;e->name; e++){
        sy = symlook(e->name, S_VAR, nil);
        if (e->values == nil || e->values->s == nil || e->values->s[0] == '\0')
            hasvalue = false;
        else
            hasvalue = true;
        if(sy == nil && !hasvalue)	/* non-existant null symbol */
            continue;
        // else

        snprint(nam, sizeof nam, "/env/%s", e->name);
        <<[[exportenv()]] if existing symbol but not value, remove from env>>
        // else
    
        f = create(nam, OWRITE, 0666L);
        <<[[exportenv()]] sanity check f>>
        first = true;
        for (w = e->values; w; w = w->next) {
            n = strlen(w->s);
            if (n) {
                <<[[exportenv()]] write null separator>>
                if (write(f, w->s, n) != n)
                    perror(nam);
            }
        }
        close(f);
    }
}
@
\t rename sy to sym, more consistent

<<[[exportenv()]] write null separator>>=
if(first)
    first = false;
else{
    if (write (f, "\000", 1) != 1)
        perror(nam);
}
@

<<[[exportenv()]] if existing symbol but not value, remove from env>>=
if (sy != nil && !hasvalue) {	/* Remove from environment */
    /* we could remove it from the symbol table
     * too, but we're in the child copy, and it
     * would still remain in the parent's table.
     */
    remove(nam);
    freewords(e->values);
    e->values = nil;		/* memory leak */
    continue;
}
@

<<[[exportenv()]] sanity check f>>=
if(f < 0) {
    fprint(STDERR, "can't create %s, f=%d\n", nam, f);
    perror(nam);
    continue;
}
@





\chapter{Debugging and Profiling Support}
\label{chap:debugging-support}


% seen file/line fields in Rule, so can report error accurately.

\section{Printing jobs, [[shprint()]]}
\l not sure it can be considered a debugging facility, more mandatory

% With use of variables, can be hard to know what was actually executed
% so when there is an error, useful to expand those vars.

% There are some bugs though, so do not trust completely.

<<[[sched()]] other locals>>=
Bufblock *buf;
@

<<[[sched()]] print recipe command on stdout>>=
buf = newbuf();
shprint(j->r->recipe, e, buf);
if(!tflag && (nflag || !(j->r->attr&QUIET)))
    Bwrite(&bout, buf->start, (long)strlen(buf->start));
freebuf(buf);
@
\t should do shprint inside the if, useless otherwise

%\subsection{[[shprint()]]}

% called to print what mk is executing, and also when error in a child.

% sched | update -> <> 
<<function shprint>>=
void
shprint(char *s, Envy *env, Bufblock *buf)
{
    Rune r;
    int n;

    while(*s) {
        n = chartorune(&r, s);
        if (r == '$')
            s = vexpand(s, env, buf);
        else {
            rinsert(buf, r);
            s += n;
            s = copyq(s, r, buf);	/*handle quoted strings*/
        }
    }
    insert(buf, 0);
}
@
%$
\l abuse s as argument and iterator. Maybe could do char *p = s; 

\subsection{Expanding and printing variables}
% part 4?

<<function vexpand>>=
static char*
vexpand(char *w, Envy *env, Bufblock *buf)
{
    char *s;
    char *p, *q;
    char carry;

    assert(/*vexpand no $*/ *w == '$');
    p = w+1;	/* skip dollar sign */
    if(*p == '{') {
        p++;
        q = utfrune(p, '}');
        if (!q)
            q = strchr(p, 0);
    } else
        q = shname(p);

    carry = *q;
    *q = '\0';
    s = mygetenv(p, env);
    *q = carry;

    if (carry == '}')
        q++;

    if (s) {
        bufcpy(buf, s, strlen(s));
        free(s);
    } else 		/* copy name intact*/
        bufcpy(buf, w, q-w);

    return q;
}
@
\l rename carry to savechar?

% either $xxx where xxx must be part of certain chars
% or ${...} and can be anything inside

<<function shname>>=
char *
shname(char *a)
{
    Rune r;
    int n;

    while (*a) {
        n = chartorune(&r, a);
        if (!WORDCHR(r))
            break;
        a += n;
    }
    return a;
}
@


% expand only "recognizable" ("interesting") variables

<<function mygetenv>>=
static char*
mygetenv(char *name, Envy *env)
{
    if (!env)
        return nil;
    if (!symlook(name, S_WESET, nil) && 
        !symlook(name, S_INTERNAL, nil))
        return nil;
    // else

    /* only resolve internal variables and variables we've set */
    for(; env->name; env++){
        if (strcmp(env->name, name) == 0)
            return wtos(env->values, ' ');
    }
    return nil;
}
@


<<[[Sxxx]] cases>>=
S_WESET,	/* variable; we set in the mkfile */
@
% set in mkfile or via mk xxx=yyy? when mk xxx=yyy, varoverride is true
%  so also S_WESET i guess.
% useful for shprint, to expand only weset variables.

<<[[parse()]] when parsing variable definitions, extra setting>>=
symlook(head->s, S_WESET, (void *)"");
@


\subsection{Printing quoted strings}
% part 4

% -> <>
<<function copyq>>=
/*
 *	check for quoted strings.  backquotes are handled here; single quotes above.
 *	s points to char after opening quote, q.
 */
char *
copyq(char *s, Rune q, Bufblock *buf)
{
    if(q == '\'')				/* copy quoted string */
        return copysingle(s, buf);

    if(q != '`')				/* not quoted */
        return s;
    // else

    while(*s){				/* copy backquoted string */
        s += chartorune(&q, s);
        rinsert(buf, q);
        if(q == '}')
            break;
        if(q == '\'')
            s = copysingle(s, buf);	/* copy quoted string */
    }
    return s;
}
@

<<function copysingle>>=
/*
 *	copy a single-quoted string; s points to char after opening quote
 */
static char *
copysingle(char *s, Bufblock *buf)
{
    Rune r;

    while(*s){
        s += chartorune(&r, s);
        rinsert(buf, r);
        if(r == '\'')
            break;
    }
    return s;
}
@



\section{Explain mode, [[mk -e]]}

<<global explain>>=
bool explain = false;
@
% used to a char*
<<[[main()]] -xxx switch cases>>=
case 'e':
    explain = true;
    break;
@


<<[[dorecipe()]] explain when found arc [[a]] making target [[n]] out of date>>=
if(explain)
    fprint(STDOUT, "%s(%ld) < %s(%ld)\n",
        n->name, n->time, a->n->name, a->n->time);
@

<<[[dorecipe()]] explain when found target [[n]] with no prerequisite>>=
if(explain)
    fprint(STDOUT, "%s has no prerequisites\n", n->name);
@

\section{Dry mode, [[mk -n]]}

% just print recipe, do not execute them
<<global nflag>>=
bool nflag = false;
@
<<[[main()]] -xxx switch cases>>=
case 'n':
    nflag = true;
    break;
@

<<[[sched()]] other locals>>=
Node *n;
@
<<[[sched()]] if dry mode or touch mode, alternate to execsh>>=
if(nflag || tflag){
    for(n = j->n; n; n = n->next){
        <<[[sched()]] if touch mode>>
        n->time = time((long *)nil);
        MADESET(n, MADE);
    }
}
@
% marked as MADE (and still print recipe)

\section{What-if mode, [[mk -w]]{\em file}}

% what if I change a file, what needs to be done?
% usually works with -n, mk -n -wprog.h

<<[[main()]] locals>>=
Bufblock *whatif = nil;
@

<<[[main()]] -xxx switch cases>>=
case 'w':
    if(whatif == nil)
        whatif = newbuf();
    else
        insert(whatif, ' ');
    if(argv[0][2])
        bufcpy(whatif, &argv[0][2], strlen(&argv[0][2]));
    else {
        if(*++argv == '\0')
            badusage();
        bufcpy(whatif, &argv[0][0], strlen(&argv[0][0]));
    }
    break;
@
\t why not use Words instead? since you join to later split

<<[[main()]] initializations before building>>=
if(whatif){
    insert(whatif, '\0');
    timeinit(whatif->start);
    freebuf(whatif);
}
@
%old: was before execinit, catchnotes before, but I dont think it matters

% fake time of file to be just modified now by modifying the time cache
<<function timeinit>>=
void
timeinit(char *s)
{
    ulong t;
    char *cp;
    Rune r;
    int c, n;

    t = time(nil);
    while (*s) {
        cp = s;
        do{
            n = chartorune(&r, s);
            if (r == ' ' || r == ',' || r == '\n')
                break;
            s += n;
        } while(*s);
        c = *s;
        *s = '\0';

        symlook(strdup(cp), S_TIME, (void *)t)->u.value = t;

        if (c)
            *s++ = c;
        while(*s){
            n = chartorune(&r, s);
            if(r != ' ' && r != ',' && r != '\n')
                break;
            s += n;
        }
    }
}
@

%related: buck query, but operates on hundreds of .buck files so can
% answer complex queries?

%\chapter{Profiling Support}

\section{Processor utilization, [[mk -u]]}

% display time spent in different nrunnning configuration
% also display time to parse and build graph since at that time
%  nrunning is zero.

<<global uflag>>=
bool uflag = false;
@
<<[[main()]] -xxx switch cases>>=
case 'u':
    uflag = true;
    break;
@


<<global tslot>>=
// map<nrunning, int>
static ulong tslot[1000];
@
% must be more than nproclimit

% could be made local static!
<<global tick>>=
static ulong tick;
@

<<[[main()]] setup profiling>>=
usage();
@

<<[[main()]] profile initializations>>=
usage();
@
%old: used to be just after initenv() (=~ inithash()), but I think ok 
% to do after.

% called from many places
<<function usage>>=
void
usage(void)
{
    ulong t;

    t = time(0);
    if(tick)
        tslot[nrunning] += t - tick;
    tick = t;
}
@
\t rename usage, to profile







<<[[main()]] print profiling stats if uflag>>=
if(uflag)
    prusage();
@

<<function prusage>>=
void
prusage(void)
{
    int i;

    usage();
    for(i = 0; i <= nevents; i++)
        fprint(STDOUT, "%d: %lud\n", i, tslot[i]);
}
@
% nevents = nproclimit



\chapter{Advanced Features TODO}
\label{chap:advanced}

\section{Regular-expression rules, [[:R:]]}
\label{sec:regexp}

% use libregexp/: regcomp(), regexec(), regsub()

%in sys/src/cmd/mkfile
%^([$OS])\.(.*):R:	\2.\1
%	$stem1^l -o $target $stem2.$stem1
%
%.*\.[$OS]:R:	$HFILES
%
%(.*)\.([$OS])'$':R:	\1.c
%	$stem2^c $CFLAGS $stem1.c


<<[[Rule_attr]] cases>>=
REGEXP = 0x0020,
@
<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'R':
    *attr |= REGEXP;
    break;
@

% need attribute, otherwise no way to infer that quoted regexp
% is a regexp and not a weird filename
% (for %& we look outside quoted strings with charin())

<<[[Rule]] other fields>>=
Reprog		*pat;		/* reg exp goo */
@


% this is for error managment, regcomp can probably trigger
% the regerror callback! subtle!
<<global patrule>>=
Rule *patrule;
@

<<[[addrule()]] if REGEXP attribute>>=
if(attr&REGEXP){
    patrule = r;
    r->pat = regcomp(head);
}
@


<<[[dorecipe()]] if regexp rule>>=
if(r->attr&REGEXP){
    ww->next = newword(node->name);
    aw->next = newword(node->name);
}
@

<<[[buildenv()]] if regexp rule>>=
if(j->r->attr&REGEXP)
    envupd("stem", newword(""));
@
% instead use stem1, stem2, etc

<<function regerror>>=
//@Scheck: not dead, called via regcomp() when have regexp syntax error
void regerror(char *s)
{
    if(patrule)
        fprint(STDERR, "mk: %s:%d: regular expression error; %s\n",
            patrule->file, patrule->line, s);
    else
        fprint(STDERR, "mk: %s:%d: regular expression error; %s\n",
            infile, mkinline, s);
    Exit();
}
@


<<constant NREGEXP>>=
#define		NREGEXP		10
@
% so stem1, stem2, stem9

% instead of stem
<<[[Arc]] other fields>>=
char		*match[NREGEXP];
@

<<[[newarc()]] set other fields>>=
rcopy(a->match, match, NREGEXP);
@

<<function rcopy>>=
void
rcopy(char **to, Resub *match, int n)
{
    int c;
    char *p;

    *to = match->sp;		/* stem0 matches complete target */
    for(to++, match++; --n > 0; to++, match++){
        if(match->sp && match->ep){
            p = match->ep;
            c = *p;
            *p = 0;
            *to = strdup(match->sp);
            *p = c;
        }
        else
            *to = 0;
    }
}
@


% instead of stem
<<[[Job]] other fields>>=
char		**match;
@

<<[[myenv]] other array elements>>=
"stem0",		/* must be in order from here */
"stem1",
"stem2",
"stem3",
"stem4",
"stem5",
"stem6",
"stem7",
"stem8",
"stem9",
@

<<[[buildenv()]] locals>>=
char **p;
int i;
@

<<[[buildenv()]] envupd some variables>>=
/* update stem0 -> stem9 */
for(p = myenv; *p; p++)
    if(strcmp(*p, "stem0") == 0)
        break;
for(i = 0; *p; i++, p++){
    if((j->r->attr&REGEXP) && j->match[i])
        envupd(*p, newword(j->match[i]));
    else 
        envupd(*p, newword(""));
}
@

<<[[applyrules]] other locals>>=
Resub rmatch[NREGEXP];
@
<<[[applyrules]] set rmatch>>=
memset((char*)rmatch, 0, sizeof(rmatch));
@
% rmatch??? for metarules? for newarc?

<<[[applyrules()]] if regexp rule then continue if some conditions>>=
if(r->attr&REGEXP){
    stem[0] = '\0';
    memset((char*)rmatch, 0, sizeof(rmatch));
    patrule = r;
    if(regexec(r->pat, node->name, rmatch, NREGEXP) == 0)
        continue;
}
@

<<[[applyrules()]] if regexp rule, adjust buf and rmatch>>=
if(r->attr&REGEXP)
    regsub(w->s, buf, sizeof(buf), rmatch, NREGEXP);
@

\section{Dynamic [[mkfile]], [[<|]]{\em prog}}

% How to do the ifdef I use in pfff's makefiles?

% Useful to provide an alternative to ifdef in Make.
% instead you put the configurable part in an external
% shell program that just does some echo FOO=X.

<<[[rhead()]] adjust sep if dynamic mkfile [[<|]]>>=
if(sep == '<' && *p == '|'){
    sep = '|';
    p++;
}
@

% mkfile content through pipe
% <|cmd  instead of <file

<<[[parse()]] other locals>>=
int pid;
@
% abused to store pid and later return status

<<[[parse()]] switch rhead cases>>=
case '|':
    p = wtos(tail, ' ');
    if(*p == '\0'){
        SYNERR(-1);
        fprint(STDERR, "missing include program name\n");
        Exit();
    }

    execinit();
    pid = pipecmd(p, envy, &newfd);
    if(newfd < 0){
        fprint(STDERR, "warning: skipping missing program file: ");
        perror(p);
    } else
        parse(p, newfd, 0);
    while(waitup(-3, &pid) >= 0)
        ;
    if(pid != 0){
        fprint(STDERR, "bad include program status\n");
        Exit();
    }
    break;
@
% why use waitup? it's a parsing time, so there should be no jobs
% yet so stupid to introduce Process machinery for that.

% execinit cos command string can contain variables?

%plan9.c
<<function pipecmd>>=
int
pipecmd(char *cmd, Envy *e, int *fd)
{
    int pid;
    fdt pfd[2];

    if(DEBUG(D_EXEC))
        fprint(STDOUT, "pipecmd='%s'\n", cmd);/**/

    if(fd && pipe(pfd) < 0){
        perror("pipe");
        Exit();
    }
    pid = rfork(RFPROC|RFFDG|RFENVG);
    if(pid < 0){
        perror("mk fork");
        Exit();
    }
    if(pid == 0){
        if(fd){
            close(pfd[0]);
            dup(pfd[1], 1);
            close(pfd[1]);
        }
        if(e)
            exportenv(e);
        if(shflags)
            execl(shell, shellname, shflags, "-c", cmd, nil);
        else
            execl(shell, shellname, "-c", cmd, nil);
        perror(shell);
        _exits("exec");
    }
    if(fd){
        close(pfd[1]);
        *fd = pfd[0];
    }
    return pid;
}
@



\section{Shell-command expansion, [[`]]{\em cmd}[[`]]}
\label{sec:backquotes}

% actually two styles, `{...} or `...`

% useful for `data`.
% also useful for globbing! SRC=`echo *.c`

<<[[assline()]] switch character cases>>=
case '`':
    if (bquote(bp, buf) == ERROR_0)
        Exit();
    break;
@

\subsection{[[bquote()]]}

<<function bquote>>=
/*
 *	assemble a back-quoted shell command into a buffer
 */
static error0
bquote(Biobuf *bp, Bufblock *buf)
{
    int c, line, term;
    int start;

    line = mkinline;
    while((c = Bgetrune(bp)) == ' ' || c == '\t')
            ;
    if(c == '{'){
        term = '}';		/* rc style */
        while((c = Bgetrune(bp)) == ' ' || c == '\t')
            ;
    } else
        term = '`';		/* sh style */

    start = buf->current - buf->start;
    for(;c > 0; c = nextrune(bp, false)){
        if(c == term){
            insert(buf, '\n');
            insert(buf, '\0');
            buf->current = buf->start + start;

            execinit();
            // running the command, passing a buf argument
            execsh(nil, buf->current, buf, envy);

            return OK_1;
        }
        if(c == '\n')
            break;
        if(c == '\'' || c == '"' || c == '\\'){
            insert(buf, c);
            if(!escapetoken(bp, buf, 1, c))
                return ERROR_0;
            continue;
        }
        rinsert(buf, c);
    }
    SYNERR(line);
    fprint(STDERR, "missing closing %c after `\n", term);
    return ERROR_0;
}
@

% pass false to nextrune, so do not elide

\subsection{Adjusting [[execsh()]]}

% but so different that maybe should write another function
% instead of trying to factorize too much.


<<[[execsh()]] other locals>>=
fdt out[2];
@
<<[[execsh()]] if buf then create pipe to save output>>=
if(buf && pipe(out) < 0){
    perror("pipe");
    Exit();
}
@

<<[[execsh()]] in children, if buf, close one side of pipe>>=
if(buf)
    close(out[0]);
@

<<[[execsh()]] in child 1, if buf, dup and close>>=
if(buf){
    // output now goes in the pipe
    dup(out[1], STDOUT);
    close(out[1]);
}
@

<<[[execsh()]] in child 2, if buf, close other side of pipe>>=
if(buf)
    close(out[1]);
@
%bug: %pad: was not guared by if(buf), but was bug I think



<<[[execsh()]] other locals>>=
int tot, n;
@

<<[[execsh()]] in parent, if buf, close other side of pipe and read output>>=
if(buf){
    close(out[1]);
    tot = 0;
    for(;;){
        if (buf->current >= buf->end)
            growbuf(buf);
        n = read(out[0], buf->current, buf->end-buf->current);
        if(n <= 0)
            break;
        buf->current += n;
        tot += n;
    }
    if (tot && buf->current[-1] == '\n')
        buf->current--;
    close(out[0]);
}
@

\section{Substitution variables, [[${]]{\em name}[[:]]{\em pattern}[[=]]{\em subst}[[}]]}
%$
\label{sec:transform-list}
\label{sec:var-generator}
\label{sec:subst-var}

% 2 uses for ${}. One for escaping, or for concatenation
% with things that would be interpreted as part of variable name.
% Other is variable "generator"

% note that can use ${x:%=%} only outside recipe :(
% they are not recognized by the shell and recipe are
% not processed by mk.
% So variables are a bit leaky abstraction. Vars in targets/prereqs
% are treated differently than in recipe.


%\subsubsection{Variables} 
% was in Parsing chapter before, after charin()
\subsection{Parsing adjustments}

% variables can contain ':' so need special handling here.
% Didnt do that in assline because variables dont influence the
% semantic of # or newline.

<<[[charin()]] switch rune cases>>=
case '$':
    if(*(cp+1) == '{')
        vargen = true;
    break;
case '}':
    if(vargen)
        vargen = false;
    else
       // same as default: case
       if(utfrune(pat, r))
          return cp;
    break;
@
%$
\l could goto default:

%ocaml-found:
% note that in ${name} name must also be simple, but need that
% so can contatenate a variable name with a letter, as in ${name}ici
% otherwise ambiguity with $nameici, and dont want put a space because
% space have a meaning.
%real-world: not a pb in make since impose $(xxx) format for every variables.

%rc deals with that with ^ so can do in a recipe $stem1^l to get
% 5l, 8l, etc.


<<[[charin()]] sanity check vargen>>=
if(vargen){
    SYNERR(-1);
    fprint(STDERR, "missing closing } in pattern generator\n");
}
@
% pattern, meh, actually not always a pattern




% charin -> <>
<<[[varsub()]] if variable starts with open brace>>=
if(**s == '{')		/* either ${name} or ${name: A%B==C%D}*/
    return expandvar(s);
@

<<function expandvar>>=
static Word*
expandvar(char **s)
{
    Word *w;
    Bufblock *buf;
    Symtab *sym;
    char *cp, *begin, *end;

    begin = *s;
    (*s)++;						/* skip the '{' */
    buf = varname(s);
    if (buf == nil)
        return nil;
    cp = *s;
    if (*cp == '}') {				/* ${name} variant*/ //$
        (*s)++;					/* skip the '}' */
        w = varmatch(buf->start);
        freebuf(buf);
        return w;
    }


    if (*cp != ':') {
        SYNERR(-1);
        fprint(STDERR, "bad variable name <%s>\n", buf->start);
        freebuf(buf);
        return nil;
    }
    cp++;
    end = charin(cp , "}");
    if(end == nil){
        SYNERR(-1);
        fprint(STDERR, "missing '}': %s\n", begin);
        Exit();
    }
    *end = '\0';
    *s = end+1;
    
    sym = symlook(buf->start, S_VAR, 0);
    if(sym == nil || sym->u.value == 0)
        w = newword(buf->start);
    else
        w = subsub(sym->u.ptr, cp, end);
    freebuf(buf);
    return w;
}
@
%$

\subsection{Substitutions, [[subsub()]]}

% note that does not support recursive vars! like ${name:${O}%.c=%.o}

<<function subsub>>=
static Word*
subsub(Word *v, char *s, char *end)
{
    int nmid;
    Word *head, *tail, *w, *h;
    Word *a, *b, *c, *d;
    Bufblock *buf;
    char *cp, *enda;

    a = extractpat(s, &cp, "=%&", end);
    b = c = d = nil;
    if(PERCENT(*cp))
        b = extractpat(cp+1, &cp, "=", end);
    if(*cp == '=')
        c = extractpat(cp+1, &cp, "&%", end);
    if(PERCENT(*cp))
        d = stow(cp+1);
    else if(*cp)
        d = stow(cp);

    head = tail = nil;
    buf = newbuf();
    for(; v; v = v->next){
        h = w = 0;
        if(submatch(v->s, a, b, &nmid, &enda)){
            /* enda points to end of A match in source;
             * nmid = number of chars between end of A and start of B
             */
            if(c){
                h = w = wdup(c);
                while(w->next)
                    w = w->next;
            }
            if(PERCENT(*cp) && nmid > 0){	
                if(w){
                    bufcpy(buf, w->s, strlen(w->s));
                    bufcpy(buf, enda, nmid);
                    insert(buf, '\0');
                    free(w->s);
                    w->s = strdup(buf->start);
                } else {
                    bufcpy(buf, enda, nmid);
                    insert(buf, '\0');
                    h = w = newword(buf->start);
                }
                buf->current = buf->start;
            }
            if(d && *d->s){
                if(w){

                    bufcpy(buf, w->s, strlen(w->s));
                    bufcpy(buf, d->s, strlen(d->s));
                    insert(buf, '\0');
                    free(w->s);
                    w->s = strdup(buf->start);
                    w->next = wdup(d->next);
                    while(w->next)
                        w = w->next;
                    buf->current = buf->start;
                } else
                    h = w = wdup(d);
            }
        }
        if(w == 0)
            h = w = newword(v->s);
    
        if(head == 0)
            head = h;
        else
            tail->next = h;
        tail = w;
    }
    freebuf(buf);
    freewords(a);
    freewords(b);
    freewords(c);
    freewords(d);
    return head;
}
@

% another parsing function ...
% subsub -> <>
<<function extractpat>>=
static Word*
extractpat(char *s, char **r, char *term, char *end)
{
    int save;
    char *cp;
    Word *w;

    cp = charin(s, term);
    if(cp){
        *r = cp;
        if(cp == s)
            return nil;
        save = *cp;
        *cp = '\0';
        w = stow(s);
        *cp = save;
    } else {
        *r = end;
        w = stow(s);
    }
    return w;
}
@

% subsub -> <>
<<function submatch>>=
static bool
submatch(char *s, Word *a, Word *b, int *nmid, char **enda)
{
    Word *w;
    int n;
    char *end;

    n = 0;
    for(w = a; w; w = w->next){
        n = strlen(w->s);
        if(strncmp(s, w->s, n) == 0)
            break;
    }
    if(a && w == nil)		/*  a == NULL matches everything*/
        return false;

    *enda = s+n;		/* pointer to end a A part match */
    *nmid = strlen(s)-n;	/* size of remainder of source */
    end = *enda+*nmid;
    for(w = b; w; w = w->next){
        n = strlen(w->s);
        if(strcmp(w->s, end-n) == 0){
            *nmid -= n;
            break;
        }
    }
    if(b && w == nil)		/* b == NULL matches everything */
        return false;
    return true;
}
@


\section{Advanced rule attributes}

\subsection{Virtual target, [[:V:]]}
\label{sec:virtual}
%real-world: .PHONY

% this is useful

% ex, clean:V: so even if there is a file called clean
% in the directory (and no prereqs),
%  mk clean   will still run the recipe
% or all:V: $PROG in mkone

<<[[Rule_attr]] cases>>=
VIR    = 0x0010,
@
<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'V':
    *attr |= VIR;
    break;
@

<<[[Node_flag]] cases>>=
VIRTUAL    = 0x0001,
@
<<[[attribute()]] propagate rule attribute to node cases>>=
if(a->r->attr&VIR)
    n->flags |= VIRTUAL;
@


<<[[attribute()]] if virtual node>>=
if(n->flags&VIRTUAL)
    n->time = 0;
@
%ocaml-found:
% need that because maybe there was a file with the same name
% than the virtual target in the directory
% with a date that timeof would have used! then maybe this
% date was very recent, more recent than the prereqs and
% so the recipe will not have been triggered.

\subsection{Deleting a target when the recipe returns an error, [[:D:]]}

% seems safe, but really the tool involved should not generate
% the target in the first place.
% Note that ocamllex has this problem. 

<<[[Rule_attr]] cases>>=
DEL    = 0x0080,
@
<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'D':
    *attr |= DEL;
    break;
@

<<[[Node_flag]] cases>>=
DELETE     = 0x0800,
@
<<[[attribute()]] propagate rule attribute to node cases>>=
if(a->r->attr&DEL)
    n->flags |= DELETE;
@
% why need to propagate this on the node? could not just
% look for info of the rule from the job? Maybe.




<<[[waitup()]] other locals>>=
Node *n;
bool done;
@

<<[[waitup()]] when error in child process, delete if DELETE node>>=
for(n = j->n, done = false; n; n = n->next)
    if(n->flags&DELETE){
        if(!done) {
            fprint(STDERR, ", deleting");
            done = true;
        }
        fprint(STDERR, " '%s'", n->name);
        delete(n->name);
    }
@
%old: if(done++ == 0)

% waitup -> <>
<<function delete>>=
void
delete(char *name)
{
    if(utfrune(name, '(') == 0) {		/* file */
        if(remove(name) < 0)
            perror(name);
    } else
        fprint(STDERR, "hoon off; mk can't delete archive members\n");
}
@
%ugly: bug if filename contain open parenthesis

\subsection{Not printing the recipe (quiet mode), [[:Q:]]}
% work with -n too? nflag || !QUIET?

<<[[Rule_attr]] cases>>=
QUIET  = 0x0008,
@

<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'Q':
    *attr |= QUIET;
    break;
@




\subsection{Running a shell script without [[-e]], [[:E:]]}
% "continue execution if the recipe draws errors"

% when want that?

<<[[Node_flag]] cases>>=
NOMINUSE   = 0x1000,
@

<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'E':
    *attr |= NOMINUSE;
    break;
@

<<[[sched()]] reset flags if NOMINUSE rule>>=
if (j->r->attr&NOMINUSE)
    flags = nil;
@
%old: flags = (j->r->attr&NOMINUSE)? nil : "-e" ; but was harder to aspectize


\subsection{Disabling the no-recipe warning, [[:N:]]}
%"if there is no recipe, the target has its time updated"

% when want that?

<<[[Rule_attr]] cases>>=
NOREC  = 0x0040,
@
<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'N':
    *attr |= NOREC;
    break;
@


<<[[Node_flag]] cases>>=
NORECIPE   = 0x0400,
@
<<[[attribute()]] propagate rule attribute to node cases>>=
if(a->r->attr&NOREC)
    n->flags |= NORECIPE;
@

% when no recipe found, print warning or not?


\subsection{Forbidding metarules to match virtual targets,  [[:n:]]}

%" the rule is a meta-rule that cannot be a target of a virtual rule,
% only files match the pattern in the target"

% when want that?

<<[[Rule_attr]] cases>>=
NOVIRT = 0x0100,
@
<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'n':
    *attr |= NOVIRT;
    break;
@

<<[[applyrules()]] skip this meta rule and continue if some conditions>>=
if ((r->attr&NOVIRT) && a != &head && (a->r->attr&VIR))
    continue;
@
% really useful?

\subsection{Custom dependency comparison program, [[:P:]]}
% big one

% "the characters after the P until terminating : are taken as a program name.
% It will be invoked as rc -c prog 'arg1' 'arg2' and should return a null
% exit status if and only if arg1 is up to date with respect to arg2 ..."

% it's a way to have a hook in the out-of-date and time of file system.
% If it is not good enough for you, you can give adhoc stuff.

% see mk man page.  use for x.tab and y.tab.h, but
% can do without
% x.tab.h:Pcmp -s: y.tab.h
%    cp y.tab.h x.tab.h

% but can do with
% x.tab.h: y.tab.h
%    cmp y.tab.h x.tab.h || cp y.tab.h x.tab.h

<<[[Rule]] other fields>>=
char		*prog;		/* to use in out of date */
@
% prog??


<<[[parse()]] other locals>>=
char *prog;
@
% passed as rhread(..., &prog) so can then
% be passed to addrules as an argument

<<[[addrule()]] set more fields>>=
r->prog = prog;
@



<<[[Arc]] other fields>>=
char		*prog;
@

<<[[newarc()]] set other fields>>=
a->prog = r->prog;
@




<<[[rhead()]] other locals>>=
char *pp;
@

<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'P':
    pp = utfrune(p, ':');
    if (pp == 0 || *pp == 0)
        goto eos;
    *pp = 0;
    *prog = strdup(p);
    *pp = ':';
    p = pp;
    break;
@


<<[[Sxxx]] cases>>=
S_OUTOFDATE,	/* n1\377n2 -> 2(outofdate) or 1(not outofdate) */
@
% abuse string ...


<<[[update()]] set outofdate prereqs if arc prog>>=
for(a = node->arcs; a; a = a->next)
    if(a->prog)
        outofdate(node, a, true);
@


<<[[outofdate()]] locals>>=
char buf[3*NAMEBLOCK];
char *str = nil;
Symtab *sym;
int ret;
@

<<[[outofdate()]] if arc->prog>>=
if(arc->prog){
    snprint(buf, sizeof buf, "%s%c%s", node->name, 0377,
        arc->n->name);
    sym = symlook(buf, S_OUTOFDATE, 0);
    if(sym == nil || eval){
        if(sym == nil)
            str = strdup(buf);
        ret = pcmp(arc->prog, node->name, arc->n->name);
        if(sym)
            sym->u.value = ret;
        else
            symlook(str, S_OUTOFDATE, (void *)ret);
    } else
        ret = sym->u.value;
    return (ret-1);
}
@

<<function pcmp>>=
static int
pcmp(char *prog, char *p, char *q)
{
    char buf[3*NAMEBLOCK];
    int pid;

    Bflush(&bout);
    snprint(buf, sizeof buf, "%s '%s' '%s'\n", prog, p, q);
    pid = pipecmd(buf, 0, 0);
    while(waitup(-3, &pid) >= 0)
        ;
    return (pid? 2:1);
}
@






\subsubsection{[[Process]]}

% with all those extensions (backquote, :P:, <|)
% you can run not just recipe jobs. However we need to wait for
% those process to finish too. wait() does not discriminate
% so we need to put more logic in waitup() and remember
% which process are job process and which one are not.

<<struct Process>>=
struct Process {
    int pid;
    int status;

    // Extra
    // double_list<ref_own<Process> backward, forward
    Process *b, *f;
};
@
%ocaml: another ex of redundant code. should be factorized in container library.
\t why need that? [[events]] not enough? seems to store results of process
\t  so rename  ProcessStatus?
\t  I think needed only for process that are not jobs! not in events!
\t  such as backquote, or :P: stuff.
\t rename status to bool error.
\t rename NotJobProcesses

<<global phead>>=
// double_list<ref_own<Process> (next = Process.f)
static Process *phead;
@

% classic, dynamic allocator
<<global pfree>>=
// double_list<ref_own<Process> (next = Process.f)
static Process *pfree;
@

% ctor
% waitup -> <>
<<function pnew>>=
static void
pnew(int pid, int status)
{
    Process *p;

    // p = pop_list(pfree)
    if(pfree){
        p = pfree;
        pfree = p->f;
    } else
        p = (Process *)Malloc(sizeof(Process));

    p->pid = pid;
    p->status = status;

    // add_list(p, phead)
    p->f = phead;
    phead = p;
    if(p->f)
        p->f->b = p;
    p->b = nil;
}
@

% dtor
<<function pdelete>>=
static void
pdelete(Process *p)
{
    // remove_double_list(p, phead, pfree)
    if(p->f)
        p->f->b = p->b;
    if(p->b)
        p->b->f = p->f;
    else
        phead = p->f;
    p->f = pfree;
    pfree = p;
}
@
\l rename pfree instead?

\subsubsection{[[waitup()]] adjustments}

<<[[waitup()]] if slot not found, not a job pid, update process list>>=
if(slot < 0){
   <<[[waitup()]] if DEBUG(D_EXEC) and slot < 0>>
    pnew(pid, buf[0]? 1:0);
    goto again;
}
@
% when can this happen? can waitfor child that was not in 
%  events? maybe child 2 in execsh?



% used only for <|, maybe move to later

<<[[waitup()]] other locals>>=
Process *p;
@

<<[[waitup()]] if retstatus, check process list>>=
/* first check against the process list */
if(retstatus)
    for(p = phead; p; p = p->f)
        if(p->pid == *retstatus){
            *retstatus = p->status;
            pdelete(p);
            return -1;
        }
@

<<[[waitup()]] if retstatus, check if matching pid>>=
if(retstatus && pid == *retstatus){
    *retstatus = buf[0]? 1:0;
    return -1;
}
@

\subsubsection{[[killchildren()]] adjustments}

<<[[killchildren()]] locals>>=
Process *p;
@
<<[[killchildren()]] expunge not-job processes>>=
for(p = phead; p; p = p->f)
    expunge(p->pid, msg);
@


<<function expunge>>=
void
expunge(int pid, char *msg)
{
    postnote(PNPROC, pid, msg);
}
@
% PNPROC?



\section{Advanced variable attributes}
% constant with attributes?? just private var
%  to not transmit to recipe process?

\subsection{Private variables, [[=U=]]}

% U for? unexport?

<<[[rhead()]] when parsing variable attributes, switch rune cases>>=
case 'U':
    *attr = 1;
    break;
@
<<[[parse()]] when parsing variable definitions, if variable with attr>>=
if(attr)
    symlook(head->s, S_NOEXPORT, (void *)"");
@

<<[[Sxxx]] cases>>=
S_NOEXPORT,	/* var -> noexport */ // set of noexport variables
@

% ecopy called from execinit
<<[[ecopy()]] return and do not copy if S_NOEXPORT symbol>>=
if(symlook(s->name, S_NOEXPORT, nil))
    return;
@


\section{Advanced [[mk]] variables}

\subsection{[[$target]] versus [[$alltargets]]}


<<[[myenv]] other array elements>>=
"alltarget",
@

<<[[Job]] other fields>>=
Word		*at;	/* all targets */
@

<<[[buildenv()]] envupd some variables>>=
envupd("alltarget", wdup(j->at));
@

\t code dealing with alltargets in work here.

\subsection{[[$prereq]] versus [[$newprereq]]}

% seems used only for aggregate rules to optimize library building.

<<[[myenv]] other array elements>>=
"newprereq",
@
% so does not put *.5 but only the .5 that were more recent than
% the archive.


<<[[Job]] other fields>>=
Word		*np;	/* new prerequistes */
@

<<[[buildenv()]] envupd some variables>>=
envupd("newprereq", wdup(j->np));
@

% related to aggregates?

\t code dealing with newprereq in work here.

\subsection{[[$pid]]}
%$


<<[[myenv]] other array elements>>=
"pid",
@

<<[[buildenv()]] locals>>=
char buf[256];
@

% note that buildenv is called in the master process, not the execsh one.

<<[[buildenv()]] envupd some variables>>=
snprint(buf, sizeof buf, "%d", getpid());
envupd("pid", newword(buf));
@
% use of pid? mk.ps say can be useful to communicate between rules
% meh

\subsection{[[$nproc]]}
%$

<<[[myenv]] other array elements>>=
"nproc",
@

<<[[buildenv()]] envupd some variables>>=
snprint(buf, sizeof buf, "%d", slot);
envupd("nproc", newword(buf));
@
% use of nproc??


%dead: (was used only in dumper but never set correctly)
%<<[[Job]] other fields>>
%// option<int> (None = -1)
%int		nproc;	/* slot number */
%@
% in newjob()
%    j->nproc = -1;

\section{Aggregates, archives}
% meh

<<[[myenv]] other array elements>>=
"newmember",
@

<<[[buildenv()]] locals>>=
Word *w, *v, **l;
char *cp, *qp;
@

<<[[buildenv()]] envupd some variables>>=
// newmember
l = &v;
v = w = wdup(j->np);
while(w){
    cp = strchr(w->s, '(');
    if(cp){
        qp = strchr(cp+1, ')');
        if(qp){
            *qp = 0;
            strcpy(w->s, cp+1);
            l = &w->next;
            w = w->next;
            continue;
        }
    }
    *l = w->next;
    free(w->s);
    free(w);
    w = *l;
}
envupd("newmember", v);
@


<<[[Sxxx]] cases>>=
S_AGG,		/* aggregate -> time */
@


<<[[dorecipe()]] when no recipe found, if archive name>>=
if(strchr(node->name, '(') && node->time == 0)
    MADESET(node, MADE);
@

<<[[timeof()]] if name archive member>>=
if(utfrune(name, '('))
    return atimeof(force, name);		/* archive */
@

<<function atimeof>>=
ulong
atimeof(int force, char *name)
{
    Symtab *sym;
    ulong t;
    char *archive, *member, buf[512];

    archive = split(name, &member);
    if(archive == 0)
        Exit();

    t = mkmtime(archive, true);
    sym = symlook(archive, S_AGG, 0);
    if(sym){
        if(force || t > sym->u.value){
            atimes(archive);
            sym->u.value = t;
        }
    }
    else{
        atimes(archive);
        /* mark the aggegate as having been done */
        symlook(strdup(archive), S_AGG, "")->u.value = t;
    }
        /* truncate long member name to sizeof of name field in archive header */
    snprint(buf, sizeof(buf), "%s(%.*s)", archive, utfnlen(member, SARNAME), member);
    sym = symlook(buf, S_TIME, 0);
    if (sym)
        return sym->u.value;
    return 0;
}
@

<<function atouch>>=
void
atouch(char *name)
{
    char *archive, *member;
    int fd, i;
    struct ar_hdr h;
    long t;

    archive = split(name, &member);
    if(archive == 0)
        Exit();

    fd = open(archive, ORDWR);
    if(fd < 0){
        fd = create(archive, OWRITE, 0666);
        if(fd < 0){
            perror(archive);
            Exit();
        }
        write(fd, ARMAG, SARMAG);
    }
    if(symlook(name, S_TIME, 0)){
        /* hoon off and change it in situ */
        seek(fd, SARMAG, 0);
        while(read(fd, (char *)&h, sizeof(h)) == sizeof(h)){
            for(i = SARNAME-1; i > 0 && h.name[i] == ' '; i--)
                    ;
            h.name[i+1]=0;
            if(strcmp(member, h.name) == 0){
                t = SARNAME-sizeof(h);	/* ughgghh */
                seek(fd, t, 1);
                fprint(fd, "%-12ld", time(0));
                break;
            }
            t = atol(h.size);
            if(t&01) t++;
            seek(fd, t, 1);
        }
    }
    close(fd);
}
@

<<function atimes>>=
static void
atimes(char *ar)
{
    struct ar_hdr h;
    ulong at, t;
    int fd, i;
    char buf[BIGBLOCK];
    Dir *d;
    
    fd = open(ar, OREAD);
    if(fd < 0)
        return;

    if(read(fd, buf, SARMAG) != SARMAG){
        close(fd);
        return;
    }
    if((d = dirfstat(fd)) == nil){
        close(fd);
        return;
    }
    at = d->mtime;
    free(d);
    while(read(fd, (char *)&h, SAR_HDR) == SAR_HDR){
        t = strtoul(h.date, nil, 0);
        if(t >= at)	/* new things in old archives confuses mk */
            t = at-1;
        if(t == 0)	/* as it sometimes happens; thanks ken */
            t = 1;
        for(i = sizeof(h.name)-1; i > 0 && h.name[i] == ' '; i--)
            ;
        if(h.name[i] == '/')		/* system V bug */
            i--;
        h.name[i+1]=0;		/* can stomp on date field */
        snprint(buf, sizeof buf, "%s(%s)", ar, h.name);
        symlook(strdup(buf), S_TIME, (void*)t)->u.value = t;
        t = atol(h.size);
        if(t&01) t++;
        seek(fd, t, 1);
    }
    close(fd);
}
@

<<[[Sxxx]] cases>>=
S_BITCH,	/* bitched about aggregate not there */
@

<<function type>>=
static int
type(char *file)
{
    int fd;
    char buf[SARMAG];

    fd = open(file, OREAD);
    if(fd < 0){
        if(symlook(file, S_BITCH, 0) == 0){
            Bprint(&bout, "%s doesn't exist: assuming it will be an archive\n", file);
            symlook(file, S_BITCH, (void *)file);
        }
        return 1;
    }
    if(read(fd, buf, SARMAG) != SARMAG){
        close(fd);
        return 0;
    }
    close(fd);
    return strncmp(ARMAG, buf, SARMAG) == 0;
}
@

<<function split>>=
static char*
split(char *name, char **member)
{
    char *p, *q;

    p = strdup(name);
    q = utfrune(p, '(');
    if(q){
        *q++ = 0;
        if(member)
            *member = q;
        q = utfrune(q, ')');
        if (q)
            *q = 0;
        if(type(p))
            return p;
        free(p);
        fprint(STDERR, "mk: '%s' is not an archive\n", name);
    }
    return 0;
}
@



<<[[outofdate()]] if arc node is an archive member>>=
if(strchr(arc->n->name, '(') && arc->n->time == 0)
   /* missing archive member */
   return true;
@

\section{Optimizations}

%\subsection{Node cache}
% not really opti, needed to build a graph

%\subsection{Time cache}

\subsection{Missing-intermediates optimization, [[mk -I]]}
% mk -i suppress this behavior
% pad: mk -I! new mode :) because I don't like the default mode!

% it is there to optimize disk space, to allow to remove .o
% the intermediate betweem the .out and the .c by pretending
% they are there if we know no .c files has actually changed.
% useful at a time where disk space was expensive, and so having
% both the .o and the .a were seen as redundant waste.

% from man page:
%Nonexistent targets that have prerequisites
%and are themselves prerequisites are treated specially.

\t put to true by default? and have 'I' instead to do the opti?

<<global iflag>>=
bool iflag = false;
@
<<[[main()]] -xxx switch cases>>=
case 'i':
    iflag = true;
    break;
@


<<[[Node_flag]] cases>>=
CANPRETEND = 0x0008,
PRETENDING = 0x0010,
@
% again, should be a different field, not agglomerated with the other node flags


<<[[clrmade()]] [[n->flags]] pretend adjustments>>=
n->flags &= ~(CANPRETEND|PRETENDING);
if(strchr(n->name, '(') == 0 || n->time)
    n->flags |= CANPRETEND;
@


<<[[work()]] possibly pretending node>>=
/*
 *   can we pretend to be made?
 */
if((!iflag) && (node->time == 0) 
        && (node->flags&(PRETENDING|CANPRETEND))
        && p && ra->n && !outofdate(p, ra, false)){
    node->flags &= ~CANPRETEND;
    MADESET(node, MADE);
    if(explain && ((node->flags&PRETENDING) == 0))
        fprint(STDOUT, "pretending %s has time %lud\n", node->name, node->time);
    node->flags |= PRETENDING;
    return did;
}
/*
 *   node is out of date and we REALLY do have to do something.
 *   quickly rescan for pretenders
 */
for(a = node->arcs; a; a = a->next)
    if(a->n && (a->n->flags&PRETENDING)){
        if(explain)
            Bprint(&bout, "unpretending %s because of %s because of %s\n",
            a->n->name, node->name, 
            ra->n? ra->n->name : "rule with no prerequisites");

        unpretend(a->n);
        did = work(a->n, node, a) || did;
        ready = false;
    }
if(!ready)/* try later unless nothing has happened for -k's sake */
    return did || work(node, p, parc);
@



<<[[work()]] possibly unpretending node>>=
if((node->flags&MADE) && (node->flags&PRETENDING) && p
    && outofdate(p, parc, false)){
    if(explain)
        fprint(STDOUT, "unpretending %s(%lud) because %s is out of date(%lud)\n",
            node->name, node->time, p->name, p->time);
    unpretend(node);
}
/*
 *   have a look if we are pretending in case
 *   someone has been unpretended out from underneath us
 */
if(node->flags&MADE){
    if(node->flags&PRETENDING){
        node->time = 0;
    }else
        return did;
}
@

<<function unpretend>>=
static void
unpretend(Node *n)
{
    MADESET(n, NOTMADE);
    n->flags &= ~(CANPRETEND|PRETENDING);
    n->time = 0;
}
@

<<[[work()]] locals>>=
Arc *ra = nil;
@
% need to save in ra for pretending stuff

<<[[work()]] update [[ra]] when outofdate [[node]] with arc [[a]]>>=
if((ra == nil) || (ra->n == nil) || (ra->n->time < a->n->time))
    ra = a;
@

<<[[work()]] update [[ra]] when no dest in arc and no src>>=
if(ra == nil)
    ra = a;
@


<<[[update()]] unpretend node>>=
node->flags &= ~(CANPRETEND|PRETENDING);
@

\subsection{Touching-mode optimization, [[mk -t]]}

% when the user knows what he is doing

<<global tflag>>=
bool tflag = false;
@
<<[[main()]] -xxx switch cases>>=
case 't':
    tflag = true;
    break;
@

<<[[dorecipe()]] when no recipe found, if tflag>>=
if(tflag){
    if(!(node->flags&VIRTUAL))
        touch(node->name);
    else if(explain)
        Bprint(&bout, "no touch of virtual '%s'\n", node->name);
}
@

<<[[sched()]] if touch mode>>=
if(tflag){
    if(!(n->flags&VIRTUAL))
        touch(n->name);
    else if(explain)
        Bprint(&bout, "no touch of virtual '%s'\n", n->name);
}
@

<<function touch>>=
void
touch(char *name)
{
    Bprint(&bout, "touch(%s)\n", name);
    if(nflag)
        return;

    if(utfrune(name, '('))
        atouch(name);		/* archive */
    else
     if(chgtime(name) < 0) {
        perror(name);
        Exit();
    }
}
@

<<function chgtime>>=
int
chgtime(char *name)
{
    Dir sbuf;

    if(access(name, AEXIST) >= 0) {
        nulldir(&sbuf);
        sbuf.mtime = time((long *)nil);
        return dirwstat(name, &sbuf);
    }
    return close(create(name, OWRITE, 0666));
}
@

\subsection{Bulk time optimisation}

% in addition to time cache, mk also batch the call to time
% by reading/caching the dir?

<<[[mkmtime]] locals>>=
//char *s, *ss;
//char carry;
//Symtab *sym;
@

<<[[mkmtime()]] bulk dir optimisation>>=
USED(force);
//TODO    s = utfrrune(name, '/');
//TODO    if(s == name)
//TODO        s++;
//TODO    if(s){
//TODO        ss = name;
//TODO        carry = *s;
//TODO        *s = '\0';
//TODO    }else{
//TODO        ss = nil;
//TODO        carry = 0;
//TODO    }
//TODO    if(carry)
//TODO        *s = carry;
//TODO
//TODO bulkmtime(ss);
//TODO if(!force){
//TODO     sym = symlook(name, S_TIME, 0);
//TODO     if(sym)
//TODO         return sym->u.value;
//TODO     return 0;
//TODO }
@
% subtle: this opti has subtle implications with weird filesystem like VFAT!!
% With this optimisation mk will not be able to run correctly
% in simple projects like tests/8c/! It's because the directory
% contains HELLOC.C not helloc.c and so even if dirstat("helloc.c")
% will work, once you read the directory you will create an
% entry for HELLOC.C in S_TIME but not for helloc.c and then when asking
% for helloc.c you will look in the cache and see nothing and so
% return 0.

<<[[Sxxx]] cases>>=
S_BULKED,	/* we have bulked this dir */
@

<<function bulkmtime>>=
void
bulkmtime(char *dir)
{
    char buf[4096];
    char *ss, *s, *sym;

    if(dir){
        sym = dir;
        s = dir;
        if(strcmp(dir, "/") == 0)
            strecpy(buf, buf + sizeof buf - 1, dir);
        else
            snprint(buf, sizeof buf, "%s/", dir);
    }else{
        s = ".";
        sym = "";
        buf[0] = 0;
    }
    if(symlook(sym, S_BULKED, 0))
        return;
    ss = strdup(sym);
    symlook(ss, S_BULKED, (void*)ss);
    dirtime(s, buf);
}
@

<<function dirtime>>=
void
dirtime(char *dir, char *path)
{
    int i, fd, n;
    ulong mtime;
    Dir *d;
    char buf[4096];

    fd = open(dir, OREAD);
    if(fd >= 0){
        while((n = dirread(fd, &d)) > 0){
            for(i=0; i<n; i++){
                mtime = d[i].mtime;
                /* defensive driving: this does happen */
                if(mtime == 0)
                    mtime = 1;
                snprint(buf, sizeof buf, "%s%s", path,
                    d[i].name);
                if(symlook(buf, S_TIME, 0) == nil)
                    symlook(strdup(buf), S_TIME,
                        (void*)mtime)->u.value = mtime;
            }
            free(d);
        }
        close(fd);
    }
}
@


\section{Recompiling everything, [[mk -a]]}

<<global aflag>>=
bool aflag = false;
@
<<[[main()]] -xxx switch cases>>=
case 'a':
    aflag = true;
    iflag = true;
    break;
@
% iflag = true; used to be after the switch, but same
% when -a, means we want to build _also_ the intermediate files when
% they are not there.

<<[[work()]] adjust weoutofdate if aflag>>=
if(aflag)
    weoutofdate = true;
@
%old: weoutofdate = aflag;

\section{Recursive [[mk]]}

% if one of the mk return error code, then should stop mk right?
% convenient then.
% TODO: this is not the case right now!!

%\section{Setting [[MKFLAGS]], [[MKARGS]]}
% used by? at least mkfile called recursively can expect those variables


<<[[main()]] locals>>=
Bufblock *buf = newbuf();
@

<<[[main()]] add [[argv[0]]] in [[buf]]>>=
bufcpy(buf, argv[0], strlen(argv[0]));
insert(buf, ' ');
@

<<[[main()]] add [[argv[i]]] in [[buf]]>>=
bufcpy(buf, argv[i], strlen(argv[i]));
insert(buf, ' ');
@



<<[[main()]] set variables for recursive mk>>=
<<[[main()]] set MKFLAGS variable>>
<<[[main()]] set MKARGS variable>>
@

<<[[main()]] set MKFLAGS variable>>=
if (buf->current != buf->start) {
    buf->current--;
    insert(buf, '\0');
}
symlook("MKFLAGS", S_VAR, (void*) stow(buf->start));
@

<<[[main()]] set MKARGS variable>>=
buf->current = buf->start;
for(i = 0; argv[i]; i++){
    if(*argv[i] == '\0') 
        continue;
    if(i)
        insert(buf, ' ');
    bufcpy(buf, argv[i], strlen(argv[i]));
}
insert(buf, '\0');
symlook("MKARGS", S_VAR, (void *) stow(buf->start));

freebuf(buf);
@






\chapter{Conclusion}
\label{chap:conclusion}

% could be nice to have a mk -d session at the end
% where see how all make sense, the flag values of nodes with 0x4
% because all are READY, etc.


% Note that did not cover much automatic dependencies.
% Also tricky when change flags, flag should also be source
% of dependencies (redo does that?)

% how to allow :: of GNU make? bad to allow :: in the end? can generate
%  ambiguity? just agglomerate recipe?

% next book: Shell.nw!! many of the features of mk, are features
%  of Shell! ability to do loops, calls programs easily, redirection,
%  pipe, etc. Reuse everything.

% related futur work? Cargo, Yarn, and other package-manager/build-system?

\appendix

\chapter{Debugging}
\label{chap:debugging-appendix}

%todo? http://bashdb.sourceforge.net/remake/
% apparently make with more debugging capabilities

%\section{Dumping internals, [[mk -d]]}

%mk -d[epg] EXEC PARSE GRAPH

<<enum Dxxx>>=
enum Dxxx {
    // for rules
    D_PARSE =		0x01,
    // for node and arcs
    D_GRAPH =		0x02,
    // for jobs
    D_EXEC  =		0x04,

    // tracing some calls
    D_TRACE  =		0x08,
};
@
%old: used to be a set of #define
%pad: I added D_TRACE

% -d flag
<<global debug>>=
// bitset<enum<dxxx>>
int debug;
@

<<function DEBUG>>=
#define	DEBUG(x)	(debug&(x))
@

<<[[main()]] locals>>=
char *s;
@
<<[[main()]] -xxx switch cases>>=
case 'd':
    if(*(s = &argv[0][2]))
        while(*s) 
         switch(*s++) {
         case 'p':	debug |= D_PARSE; break;
         case 'g':	debug |= D_GRAPH; break;
         case 'e':	debug |= D_EXEC; break;
        }
    else
        debug = 0xFFFF; // D_PARSE | D_GRAPH | D_EXEC
    break;
@
\t D_TRACE?


\section{The rules, [[mk -dp]]}
% p for parsing

<<[[main()]] if DEBUG(D_PARSE)>>=
if(DEBUG(D_PARSE)){
    dumpw("default targets", target1);
    dumpr("rules", rules);
    dumpr("metarules", metarules);
    dumpv("variables");
}
@


<<dumper dumpw>>=
void
dumpw(char *s, Word *w)
{
    Bprint(&bout, "%s", s);
    for(; w; w = w->next)
        Bprint(&bout, " '%s'", w->s);
    Bputc(&bout, '\n');
}
@

% todo: remove the pointer thing %p, not that useful I think
% note that the variable expansion has been done for the target and prerequestes
<<dumper dumpr>>=
void
dumpr(char *s, Rule *r)
{
    Bprint(&bout, "%s: start=%p\n", s, r);
    for(; r; r = r->next){
        Bprint(&bout, "\tRule %p: %s:%d attr=%x next=%p chain=%p alltarget='%s'",
            r, r->file, r->line, r->attr, r->next, r->chain, wtos(r->alltargets, ' '));
        if(r->prog)
            Bprint(&bout, " prog='%s'", r->prog);
        Bprint(&bout, "\n\ttarget=%s: %s\n", r->target, wtos(r->prereqs,' '));
        Bprint(&bout, "\trecipe@%p='%s'\n", r->recipe, r->recipe);
    }
}
@

<<dumper dumpv>>=
void
dumpv(char *s)
{
    Bprint(&bout, "%s:\n", s);
    symtraverse(S_VAR, print1);
}
@

<<function print1>>=
static void
print1(Symtab *s)
{
    Word *w;

    Bprint(&bout, "\t%s=", s->name);
    for (w = s->u.ptr; w; w = w->next)
        Bprint(&bout, "'%s'", w->s);
    Bprint(&bout, "\n");
}
@





\section{The graph, [[mk -dg]]}

% node = graph(target) from mk(target)
<<[[mk()]] if DEBUG(D_GRAPH)>>=
if(DEBUG(D_GRAPH)){
    dumpn("new target\n", root);
    Bflush(&bout);
}
@
% below the buf is here to compute indentation, depth of the graph in spaces

<<dumper dumpn>>=
void
dumpn(char *s, Node *n)
{
    char buf[1024];
    Arc *a;

    Bprint(&bout, "%s%s@%p: time=%ld flags=0x%x next=%p\n",
        s, n->name, n, n->time, n->flags, n->next);
    for(a = n->arcs; a; a = a->next){
        snprint(buf, sizeof buf, "%s   ", (*s == ' ')? s:"");
        dumpa(buf, a);
    }
}
@

<<dumper dumpa>>=
void
dumpa(char *s, Arc *a)
{
    char buf[1024];

    Bprint(&bout, "%sArc@%p: n=%p r=%p flag=0x%x stem='%s'",
        s, a, a->n, a->r, a->flag, a->stem);
    if(a->prog)
        Bprint(&bout, " prog='%s'", a->prog);
    Bprint(&bout, "\n");

    if(a->n){
        snprint(buf, sizeof(buf), "%s    ", (*s == ' ')? s:"");
        dumpn(buf, a->n);
    }
}
@

<<[[nrep()]] if DEBUG(D_GRAPH)>>=
if(DEBUG(D_GRAPH))
    Bprint(&bout, "nreps = %d\n", nreps);
@

\section{The jobs, [[mk -de]]}
% e for execution


<<[[sched()]] if DEBUG(D_EXEC)>>=
if(DEBUG(D_EXEC))
    fprint(STDOUT, "firing up job for target %s\n", wtos(j->t, ' '));
@

<<[[sched()]] if DEBUG(D_EXEC) print recipe>>=
if(DEBUG(D_EXEC))
    fprint(STDOUT, "recipe='%s'\n", j->r->recipe);
Bflush(&bout);
@

<<[[sched()]] if DEBUG(D_EXEC) print pid>>=
if(DEBUG(D_EXEC))
    fprint(STDOUT, "pid for target %s = %d\n", wtos(j->t, ' '), events[slot].pid);
@


<<[[waitup()]] if DEBUG(D_EXEC) print pid>>=
if(DEBUG(D_EXEC))
    fprint(STDOUT, "waitup got pid=%d, status='%s'\n", pid, buf);
@

<<[[waitup()]] if DEBUG(D_EXEC) and slot < 0>>=
 if(DEBUG(D_EXEC))
     fprint(STDERR, "mk: wait returned unexpected process %d\n", pid);
@

<<[[pidslot()]] if DEBUG(D_EXEC)>>=
if(DEBUG(D_EXEC))
    fprint(STDERR, "mk: wait returned unexpected process %d\n", pid);
@

<<[[nproc()]] if DEBUG(D_EXEC)>>=
if(DEBUG(D_EXEC))
    fprint(STDERR, "nprocs = %d\n", nproclimit);
@



% actually never used
<<dumper dumpj>>=
void
dumpj(char *s, Job *j, int all)
{
    Bprint(&bout, "%s\n", s);
    while(j){
        Bprint(&bout, "job@%p: r=%p n=%p stem='%s'\n",
            j, j->r, j->n, j->stem);
        Bprint(&bout, "\ttarget='%s' alltarget='%s' prereq='%s' nprereq='%s'\n",
            wtos(j->t, ' '), wtos(j->at, ' '), wtos(j->p, ' '), wtos(j->np, ' '));
        j = all? j->next : nil;
    }
}
@
%old:  nproc=%d ... , j->nproc

\section{The function calls, [[mk -dt]]}

<<[[applyrules]] debug>>=
if(DEBUG(D_TRACE)) 
    print("applyrules(%lux='%s')\n", target, target);
@

<<[[newnode()]] debug>>=
if(DEBUG(D_TRACE)) 
    print("newnode(%s), time = %d\n", name, node->time);
@


<<[[work()]] debug>>=
if(DEBUG(D_TRACE))
    print("work(%s) flags=0x%x time=%lud\n", node->name, node->flags, node->time);
@

<<[[update()]] debug>>=
if(DEBUG(D_TRACE))
    print("update(): node %s time=%lud flags=0x%x\n", node->name, node->time, node->flags);
@




\chapter{Profiling}
\label{chap:profiling-appendix}

<<global buf>>=
short buf[10000];
@


<<[[main()]] setup profiling>>=
#ifdef	PROF
    {
        extern int etext();
        monitor(main, etext, buf, sizeof buf, 300);
    }
#endif
@


% dead function actually
<<function symstat>>=
void
symstat(void)
{
    Symtab **s, *ss;
    int n;
    int l[1000];

    memset((char *)l, 0, sizeof(l));
    for(s = hash; s < &hash[NHASH]; s++){
        for(ss = *s, n = 0; ss; ss = ss->next)
            n++;
        l[n]++;
    }
    for(n = 0; n < 1000; n++)
        if(l[n]) 
            Bprint(&bout, "%d of length %d\n", l[n], n);
}
@


\chapter{Error Management}
\label{chap:error}

% related is mk -k, but not really error management in the end.

% automatic error handling

% Also call Exit, not exit! so carefully wait for children.

<<function Malloc>>=
void*
Malloc(int n)
{
    void *s;

    s = malloc(n);
    if(!s) {
        fprint(STDERR, "mk: cannot alloc %d bytes\n", n);
        Exit();
    }
    return s;
}
@
%old: used to have 'register void *s', but 8c does not use it I think

<<function Realloc>>=
void *
Realloc(void *s, int n)
{
    if(s)
        s = realloc(s, n);
    else
        s = malloc(n);
    if(!s) {
        fprint(STDERR, "mk: cannot alloc %d bytes\n", n);
        Exit();
    }
    return s;
}
@




\chapter{Libc}
\label{chap:libc}

%\section{Memory managment}
% Malloc and Realloc now in error management section

\section{Buffer managment}
\label{sec:bufblock}

<<struct Bufblock>>=
struct Bufblock
{
    char 		*start;
    char 		*end;

    char 		*current;

    // Extra
    struct Bufblock *next;
};
@

<<global freelist>>=
static Bufblock *freelist;
@



<<constant QUANTA>>=
#define	QUANTA	4096
@

% ctor
<<constructor newbuf>>=
Bufblock *
newbuf(void)
{
    Bufblock *p;

    if (freelist) {
        p = freelist;
        freelist = freelist->next;
    } else {
        p = (Bufblock *) Malloc(sizeof(Bufblock));
        p->start = Malloc(QUANTA*sizeof(*p->start));
        p->end = p->start+QUANTA;
    }
    p->current = p->start;
    *p->start = '\0';
    p->next = nil;
    return p;
}
@

% dtor
<<destructor freebuf>>=
void
freebuf(Bufblock *p)
{
    p->next = freelist;
    freelist = p;
}
@

<<function growbuf>>=
void
growbuf(Bufblock *p)
{
    int n;
    Bufblock *f;
    char *cp;

    n = p->end-p->start+QUANTA;
        /* search the free list for a big buffer */
    for (f = freelist; f; f = f->next) {
        if (f->end-f->start >= n) {
            memcpy(f->start, p->start, p->end-p->start);
            cp = f->start;
            f->start = p->start;
            p->start = cp;
            cp = f->end;
            f->end = p->end;
            p->end = cp;
            f->current = f->start;
            break;
        }
    }
    if (!f) {		/* not found - grow it */
        p->start = Realloc(p->start, n);
        p->end = p->start+n;
    }
    p->current = p->start+n-QUANTA;
}
@

<<function bufcpy>>=
void
bufcpy(Bufblock *buf, char *cp, int n)
{

    while (n--)
        insert(buf, *cp++);
}
@

<<function insert>>=
void
insert(Bufblock *buf, int c)
{

    if (buf->current >= buf->end)
        growbuf(buf);
    *buf->current++ = c;
}
@

<<function rinsert>>=
void
rinsert(Bufblock *buf, Rune r)
{
    int n;

    n = runelen(r);
    if (buf->current+n > buf->end)
        growbuf(buf);
    runetochar(buf->current, &r);
    buf->current += n;
}
@

\section{Utilities}

<<function maketmp>>=
char*
maketmp(void)
{
    static char temp[] = "/tmp/mkargXXXXXX";

    mktemp(temp);
    return temp;
}
@


\chapter{Examples of [[mkfile]]s TODO}
\label{chap:examples}


%How add to an existing variable? e.g. CFLAGS+= -p?
% easy, CFLAGS= -xxx $CFLAGS
%Can do the $(BLA:.cmo=.cmx)? this is convenient
% yes you can: OBJS=${SRC:%.ml=%.cmo}

\section{The [[mkfile]] of [[mk]]}
% relies on next section

%bootstrap:
% can also bootstrap simply using a shell script for the very first time.
% show the script? (just mk clean; mk > bootstrap.sh), could
% even be a target of the mkfile :)


\section{The [[mkfile]]s of \plan}
% mentioned in mk.ms ?

\subsection{[[/$objtype/mkfile]] for the ARM} %$
\label{sec:mkfile-objtype}

\subsection{[[/sys/src/mkfile.proto]]}


\subsection{[[/sys/src/cmd/mkone]]}
% also mkmany

\subsection{[[/sys/src/cmd/mklib]]}
% also mksyslib


% and then the instances
%\subsection{[[/sys/src/mkfile]]}
%\subsection{[[/sys/src/cmd/mkfile]]}

\chapter{Extra Code}

\ifallcode
#include "Make_extra.nw"
\fi

%\chapter{Changelog}
% code via make loc = 4763 LOC, after full lpized 5285 LOC
% orig Make.nw = 5400 LOC, after full lpized and comments in sections 8540
% now: =~ ?? LOC so added ?? LOE (Lines of explanations)
% mk in ocaml: 2300 LOC (but not all features: no archive, no :P, etc)


\chapter*{Glossary}
\addcontentsline{toc}{chapter}{Glossary}
\label{sec:glossary}

\begin{verbatim}
LOC = Lines Of Code
DSL = Domain Specific Language
IDE = Integrated Developement Environment
DAG = Directed Acyclic Graph
DFS = Depth-First Search
\end{verbatim}

\chapter*{Indexes}
\addcontentsline{toc}{chapter}{Index}

%\chapter{References} 
\addcontentsline{toc}{chapter}{References}

\bibliography{../docs/latex/Principia}
\bibliographystyle{alpha}

%******************************************************************************
% Postlude
%******************************************************************************

\end{document}
