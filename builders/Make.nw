\documentclass[12pt]{report}
%twocolumn, landscape

%******************************************************************************
% Prelude
%******************************************************************************
\newif\iffinal
\newif\ifverbose
\finaltrue\verbosefalse % see also other newif in Macros.tex

%------------------------------------------------------------------------------
%history: 
%------------------------------------------------------------------------------

%thx to LP, I changed for the better a few things:
% - removed dead code:
%    dead namespaces (S_PID, S_MAKEFILE, S_EXPORTED, S_MAKEVAR),
%    (apparently some were also removed in mk of kencc)
%    dead rule attributes (UNUSED, UPD),
%    dead constants (IWS)
%    dead fields (Job.nproc)
%    useless code (head.n = node, ...)
% - cleaned up code:
%    renamed variables, was abusing head/tail for too many things 
%    (target vs prerequisites, but also head vs tail of lists of arcs, 
%    head and tail of rule/assignment, tail for last word, ...)
% - reorganized the files (removed arc.c, job.c, introduced dumpers.c, mv
%    around a few functions)

%thx to codemap/codegraph/scheck:
% - use cg to reduce backward deps (only 8: 0.76%), introduce globals.c/utils.c
%   removed job.c, arc.c, moved stow(),
%   (harder to understand non layered code)
% - use scheck to remove deadcode, dead prototypes, useless export
%   or mv as forward decl
%   (harder to understand big interface files)
% - use cg to reduce number of globals by moving them closer to the
%   relevant file (or even function), better cluster the code
%   (harder to understand non functional code using lots of globals)

%thx to mk in ocaml: (see also the %ocaml-found: tag in this file)
%  - parsing code can be simpler, and also better see the
%    different lexing contexts and subtelities
%  - can do X=a and $X=42, hmm
%  - understand need for ${name}ici
%  - understand check for recipe pointer in ambiguous(), cos can have 
%    many arcs but no ambiguity cos all have the same recipe
%  - understand vacuous check and interaction with ambiguous

%thx to this manual, I better understand mk/make:
% - minimal-syntax design leads to need for escaping-newline because newline is
%   (ab)used as a terminator for many things
% - escaping rules, tricky, but needed because space is used too for separator
% - need for ${} in mk (no pb in make though) so can do ${name}here
% - finally will remember the syntax for ${name:%=%} :) 
% - also the syntax for rules, do I need empty lines between rules, what
%   is exactly a rule boundary
% - need for multiple targets, and multiple rules with same target 
% - SEMI how to debug issues in your own Makefile
% - mk is actually a job scheduler! with dependent tasks!
% - to not use ';' but instead && for eflag -e to work
% - scope and life of variables (still dont understand in GNU make, = vs := )
%   whether can use $xx in target, in prereqs, in includes, etc
% - wait and parent/children relation, wait reports also already finished child

%history LP-ization:
% - skeleton, mostly copy paste of Template.nw skeleton
% - put all content of files in the Extra section, via 'pfff -lpize'
%   which also now split in chunks!
%    * function, global, struct, enum, constant, macro(actually function)
%    * TODO ctor/dtor, dumper
%    * [[xxx]] other fields, [[xxx]] extra fields
% - read Extra section, identify concepts, first TOC
% - distribute parts of the Extra section in the main file
% - understand main(), LP split main, improve TOC
% - understand main functions, LP split, cluster, improve TOC
% - LP split the structures, use datalog for flow to field info
% - nullify, boolify, errorify, enumify,  typeify,    scheckify, plan9ify
% - aspecify advanced features! remove useless features
% - add figures
% - add explanations

%------------------------------------------------------------------------------
% Packages
%------------------------------------------------------------------------------

\usepackage{../docs/latex/noweb}
%% \noweboptions{footnotesizecode,nomargintag}
%% %note: allow chunk on different pages, less white space at bottom of pages
%% \def\nwendcode{\endtrivlist \endgroup}
%% \let\nwdocspar=\par
\usepackage{xspace}
\usepackage{verbatim}
%note: required by noweblatexpad for the \t \l \n in this file
\usepackage{fancyvrb}
\usepackage{url}
\usepackage{hyperref}
 \hypersetup{colorlinks=true}
\usepackage[pageref]{backref}
 \def\backref{{\footnotesize cited page(s)}~}
\usepackage{booktabs} 
 \newcommand{\otoprule}{\midrule[\heavyrulewidth]}
\usepackage{graphicx}

%------------------------------------------------------------------------------
% Macros
%------------------------------------------------------------------------------
\input{../docs/latex/Macros}

%------------------------------------------------------------------------------
% Config
%------------------------------------------------------------------------------
\allcodefalse
% ifallcode is used for:
%  - forward decl, func decl, extern decl, #ifdef, pragmas (stuff in Extra.nw)

\begin{document}
%******************************************************************************
% Title
%******************************************************************************
\title{
{\Huge 
Principia Softwarica: The Build System [[mk]]
}\\
{version 0.1}
}

\author{
Yoann Padioleau\\
\texttt{yoann.padioleau@gmail.com}\\
\\
with code from\\
Andrew Hume
}

\maketitle 
%\onecolumn
\hrule
\input{../docs/latex/Copyright}
\input{../docs/latex/CopyrightPlan9}
\hrule
%\twocolumn

\begingroup
\hypersetup{linkcolor=blue}
% need to s/onecolumn/twocolumn in report.cls :) for \tableofcontents
\tableofcontents
\endgroup

%******************************************************************************
% Body
%******************************************************************************

\chapter{Introduction}

The goal of this book is to explain with full details the source code of
a {build system}.

\section{Motivations}

Why a build system? 
Because I think you are a better programmer if
you fully understand how things work under the hood,
and a build system is one of the tools a programmer uses the most.
%
Indeed, it allows programmers to 
assemble, 
compile, 
link,
test,
package, and 
distribute 
software with one simple command, ``the one command that rules them all'',
from very simple programs to entire operating systems.
\n check, deploy


\n def so I can use dependency/rule/Makefile/make/... in questions below
A {build system} allows to {describe} and {maintain} {dependencies} 
between files.
\n could have better definition (said later)
%
Those dependencies are usually represented by {rules}, which are
stored in a special {configuration file},
for instance, a [[Makefile]] with the build system Make~\cite{make}
(one of the most popular build systems).



Even though a build system is not as interesting as 
a kernel or 
a compiler,
it is a necessary piece in the programmer's toolbox.
Indeed, all programs, including kernels and compilers, rely on a 
build system to be built.
%bootstrap:
In fact, a build system usually also relies on itself to be built, leading to
bootstrapping issues similar to the ones found in compilers.
%
Moreover, build systems contain components that are useful in many contexts,
for example a job scheduler.
\l even if may seem boring at first
\l contain only simple algorithms? no neep concept?
\l like kernel, paralellize tasks, one of the first program to do (after kernel)
\l parallelization is even more important now
%companies:
\t spend lots of time in front of terminal running mk. Important program.
\t in the end Google and Facebook spend quite some time making build systems.
%
Finally, build systems such as Make use an original approach 
to solve problems.
Indeed, to describe dependencies, Make provides a 
{domain-specific language} (DSL).
\n not a XML/JSON/... configuration file (said later)
%
The author of Make designed this language to require
as less syntax as possible, in order to remove as much overhead as possible
for the programmer when writing rules.
\n actually some bad design with TAB, but not with mk (said later)
Moreover, this specific language, because it is restricted, 
because it is not as powerful as a general-purpose programming language, 
allows in counterpart special checks that would be impossible in a 
general language.
\n actually not make, but with mk (said later)

Here are a few questions I hope this book will answer:

\begin{itemize}

\item What are the fundamental concepts of a build system?
What is the core algorithm behind a build system?
\n graph of dependencies, DAG, target, prereq, rules, cycle, ambiguous, job

\item What are the kinds of dependencies a build system needs to represent
in order to cover all the use-cases?
Can a file depend only on one other file (one-to-one dependency),
or on many files (one-to-many)? 
Can many files depend on the same single file (many-to-one), 
or on many files (many-to-many)?
\l all dependencies concerning a file can be described by a single rule?
\l  or more convenient to have multiple rules describing dependency of same file?
\n Do you need multiple rules for same target? what about ambiguities?
\l can have mutual dependencies? self dependency? tree? graph? DAG?

%dup: above
\item What is the minimal syntax you need to describe dependencies,
and to describe the {commands} to maintain those dependencies?
\n :',',  <space>, <newline>
If this syntax uses special symbols, how do you reference
filenames containing those symbols?
\n escaping mechanism, quoting

\item What are the mistakes a build system can detect?
Can it detect ambiguous rules? 
Infinite rules? 
Can it detect cycles in dependencies?

\item What kind of help can a build system offer to help debug a [[Makefile]]?
How can you visualize the dependencies? 
\n mk -dump_graph with mk.byte

\item How does a build system maintain dependencies efficiently?
Can it compile projects incrementally? Can different parts of
a project be compiled in parallel?

\item How does the build system run different tasks in parallel, and 
how does it coordinate them? How do you write a simple job scheduler?

\end{itemize}

\t put in conclusion non-trivial adv algo and data structures seen? see comment
%data-structures (beyond list/hashtbl): (use list of words, symbol hash table)
% - queue (jobs)
% - graph (via pointers, thanks to an hashtbl with node references first)

%algorithms (beyond search/sort): (actually use neither search nor sort)
% - graph DFS and visited flag, cycle detection
% - matching and substituting (poor's man regexp)


%tags used in this file for different recurring themes:
 %compiler: similar techno found in compiler (or assembler, or cpp)
 %shell? similar techno in shell?
 %bootstrap:
 %
 %real-world: to relate to other build systems
 %ocaml: to give a hint on how rewriting C in OCaml could improve things
 %ocaml-found: found subtle behavior of mk because I ported it to ocaml
 %alt: alternative technique
 %old: for original code I changed to be clearer
 %pad: for code I introduced to be clearer
 %dead: dead code removed
 %toc: %trans: %dup: %example: %chunks:


\section{The \plan build system: [[mk]]}

I will explain in this book the code of the \plan build system
[[mk]]\furl{http://plan9.bell-labs.com/magic/man2html/1/mk},
which contains about 5500 lines of code (LOC).
%
[[mk]] is written entirely in C.
\l used also outside plan9 by a few open source software.


%dup: from Assembler.nw
Like for most books in \principia, I chose a \plan program because
those programs are simple, small, elegant, open source, and they form together
a coherent set.
%history:
Moreover, [[mk]] is the ``spiritual successor''~\cite{mk-successor} 
to Make. % ~\cite{make}.
\l not as popular as make, but better variant
\n even shorter to type (said later)
Indeed, it was designed in the same lab (Bell Labs), 
and even Stuart Feldman, the original author of Make,
recommends [[mk]] as a better system.
\t cite?
\l mk is to plan9 what make was to unix, and plan9 successor to unix

[[mk]] is modeled after Make, like many other build systems,
but [[mk]] is both simpler and more powerful than Make.
%
For instance, [[mk]] does not suffer from 
the infamous [[TAB]] requirement in the first column
from Make~\footnote{
See \url{http://www.catb.org/esr/writings/taoup/html/ch15s04.html}
for the story behind the [[TAB]] requirement.
\l also http://stackoverflow.com/questions/1755550/what-is-the-reasoning-behind-the-makefile-whitespace-syntax 
\n GNU make has .RECIPEPREFIX to allow spaces too
};
with [[mk]], the programmer can use spaces and tabs interchangeably.
%
Moreover, [[mk]] executes shell commands in parallel,
an important improvement over the original Make as
this speeds up greatly the building process on machines
with multiple processors.


With one single command ([[mk]]) executed from the top
directory of the \plan fork used in \principia, 
you can build all 
the \plan libraries, 
the \plan programs,
the \plan kernel,
and build a disk image containing a \plan distribution
that can be installed on a Raspberry Pi or booted through QEMU;
all of this with one single command.
\n actually I use make right now :)

\section{Other build systems}

Here are a few build systems that I considered for this book,
but which I ultimately discarded:
\l which -> that?

\begin{itemize}

\item Make~\cite{make}, which from now on I will call \unix Make,
to differentiate it from its other variants, 
was the original Make part of early versions of \unix.
%
Stuart Feldman, its author, won the ACM System Software Award in 2003 for it: 
``there is probably no large software system in the world today that
has not been processed by a version or offspring of MAKE.''
\furl{http://awards.acm.org/award_winners/feldman_1240498.cfm}.
%
Indeed, \unix Make has many variants, which 
are often confusingly called also Make (e.g., BSD Make, GNU Make), 
and which often use the same command-line program name: [[make]].
\n actually there is pmake, bmake, gmake, but often aliased as make


One of the latest versions of \unix Make,
Make 2.58\footnote{See \url{http://minnie.tuhs.org/cgi-bin/utree.pl?file=V7/usr/src/cmd/make/ident.c}.}
for the seventh edition of \unix in 1979,
contains less than 2500 LOC.
\n also in ape/cmd/make/ (V8 2.78 3000 LOC) and in unix-history-repo
\n maybe good candidate in the end. even use yacc! but ugly yacc.
%
This is smaller than [[mk]], but this version contains also far less features
than [[mk]]. For instance, it lacks the 
ability to execute shell commands in parallel.
\l need for parallel probably because was done for plan9 (said later?)
%
This ability requires more than just a few lines of code.
In fact, it led in [[mk]] to the complete redesign of the 
approach used by Make to compute dependencies.
%
Indeed, [[mk]] computes a graph of dependencies statically when it starts,
and then uses this graph to guide the building process.
\t this allows many more things? really? 
\t  can run in // from very different parts of the tree? make cant?
\t make does not have a graph? no transitive closure so what?
%dup:
\l Stuart himself considers mk a better solution 




\item GNU Make\furl{https://www.gnu.org/software/make/} is probably
the most popular variant of Make.
%
It is used by almost every open source applications under Linux.
\n Linux requires gmake; it uses many GNU make extensions and has a 
\n  very fancy Makefile (make V=1 ...)
\n I think even FreeBSD switched to GNU Make
%
GNU Make contains many extensions to the original \unix Make, 
including the ability to run shell commands in parallel as in [[mk]]\footnote{
With [[make -j]].}.
\l also conditional, but can be emulated in mk
\t but as good as mk for the parallel? since mk has full graph? can do more?
It supports also many platforms, including old platforms
such as DOS, VMS, or Amiga.
%
However, its codebase is also significantly larger: 37~000 LOC, which
is almost one order of magnitude more code than [[mk]].
\l main.c 3200 LOC, almost as big as whole codebase of mk


GNU Make follows closely the design of \unix Make, and so
inherits also its major flaws, for instance, the requirement
to use [[TAB]] in the first column for shell commands.
%
Where [[mk]] tries to generalize, unify, and reuse the services
and syntax of other tools, GNU Make specializes and adds an extra layer
of complexity.
%
For example, use of variables in GNU Make (and \unix Make) requires
parenthesis (e.g., [[$(OBJS)]]) %$
\n except if single character or number
compared to the use of variables in a shell (e.g., [[$OBJS]]). %$
Moreover, the use of shell variables inside a [[Makefile]] requires an
extra dollar (e.g., [[$$i]]).
%
In [[mk]], the syntax for variables is the same than in
the shell (e.g., [[$OBJS]], [[$i]]).%$
\n actually there is the subst variable, and leaky abstraction
%
Finally, [[mk]] replaces cryptic Make variables such as [[$@]] or [[$^]]
with clearer variables such as [[$target]] or [[$prereq]].
\l for more comparisons, see comments in .tex
% - regexp rules and no suffix rules (but meta % rules were already in
%   unix make, not an invention of mk) (said later)
% - mk better written, nice use of assert!
% - more complex escaping rules? if use " in recipe in GNU Make?
% see mk.ps section 4
% see also https://github.com/dcjones/mk for list of improvements


\item Ant\furl{http://ant.apache.org/} is a build system used by
many Java projects.
\l ex? everyone switched to Maven and now Gradle? Android uses Gradle?
Instead of using a DSL to express dependencies, and of relying on a
shell and command-line programs to maintain those dependencies, 
an Ant user writes dependency rules in XML in a [[build.xml]] 
configuration file.
%
For instance, here is a rule to clean files:
\begin{verbatim}
<target name="clean" <delete dir="classes"></target>
\end{verbatim}
\l could put equivalent make? clean: rm -rf classes and footnote
\l  on .PHONY and virtual target?


Ant supports many XML tags to perform various tasks such as
deleting files ([[<delete>]]), creating directories
([[<mkdir>]]), or calling the Java compiler ([[<javac>]]).


The main advantage of Ant over Make is {portability}.
%
Indeed, with Make, the command-line programs used
in a [[Makefile]] may be specific to an operating system.
%
For instance, the default C compiler, linker, and file utilities
under Microsoft Windows are not the same than in Linux or macOS.
\n but now cygwin, mac port, brew, portable GNU utilities (said later)
%
In fact, even the shell and [[make]] programs are different
under Linux, macOS, and Microsoft Windows.
\n actually Microsoft put also Bash as standard now
\n https://blogs.msdn.microsoft.com/powershell/2016/04/01/bash-for-windows-why-its-awesome-and-what-it-means-for-powershell/
%
However, the portability of Ant comes at a price; its
codebase is very large with more than 200~000 LOC (without
the tests), which is almost 40 times more code than [[mk]].
\n every utility needs its own wrapper


The main advantage of Make (and [[mk]]) over Ant is {generality}.
Indeed, you can call any command-line programs from the [[Makefile]].
Moreover, the shell, which is the language used to write commands in
a [[Makefile]], is an expressive language.
\l does not need to look at API of Ant either, can reuse past knowledge
%
With Ant, if there is no XML tag for a certain task,
you need to extend Ant itself.
%
Finally, XML is an extremely verbose language; writing
XML dependency rules by hand is tedious.
\l ok if generated by IDE.
\l also Ant has Ivy (dependency manager, equivalent of Cargo?)


\item CMake\furl{https://cmake.org/} is a cross-platform build system
used in many large open source projects (e.g., LLVM).
\l cite LLVM?
%
It was designed, like Ant, to overcome the lack of portability of [[Makefile]]s.
%
Unlike Ant, CMake acts as a frontend to Make (and other build systems).
Indeed, CMake is a {\em meta build system}.
%
Instead of writing [[Makefile]]s, the user of CMake 
creates [[CMakeLists.txt]] files containing builtin (portable) commands 
to compile source code.
%
CMake then generates from those configuration files regular
[[Makefile]]s that can be processed by Make.
%
CMake can also generate instead files for IDEs such as Apple's Xcode or
Microsoft's Visual Studio.
\n also can generate files for ninja
\l like automake, gyp, etc. mix actually configure/automate feature I think.

CMake contains thousands of builtin commands,
\l really?
offers a graphical user interface, and supports many IDEs.
However, its codebase is enormous with more than 250~000 LOC 
(without the tests). 
\n also without Utilities which is used but seems to contained forked libs
This is extremely large, especially
considering the fact that CMake is just a frontend to other build systems.
\l   has an article in OASA book 1 or 2



% one using python, EDSL, but more syntax boilerplate "", [;], etc

\end{itemize}
\t there are tons more build systems; Many language specific, many meta.
\t  but ask yourself do I really need it? make/mk not enough?

Note that the lack of portability of [[Makefile]]s has been 
partially fixed in the last ten years.
%
Indeed, with the availability of GNU utilities 
for operating systems other than Linux (e.g., 
through cygwin\furl{https://www.cygwin.com/} for Microsoft Windows, 
and macports\furl{https://www.macports.org/} or 
Homebrew\furl{http://brew.sh/} for macOS),
[[Makefile]]s are now more portable because the same command-line
programs are available on more platforms.
\n actually for macOS file utilities are based on BSD


\l a few other, see the comment in this file:
%make-clones:
% - pmake, freebsd https://www.freebsd.org/doc/en_US.ISO8859-1/books/pmake/
%   parallel make
% - nmake (replaced by msbuild later)
% - dmake
% - omake, apparently has listen-to-fs features too, so daemon
%   and rerun when save in editor?
%   http://blog.camlcity.org/blog/omake1.html
% - https://github.com/dcjones/mk is mk port in Go! 2285 LOC but some files
%   looks almost like an automatic C->Go translation (identical logic)
% - https://github.com/sourcegraph/makex another clone in Go
%other:
% - ninja, seems interesting, the assembler of build system, 19~000 LOC
%   has an article in OASA book 3
% - tup, 31 000 LOC (just src/tup/), does not look that much better than mk
%   but looks pretty nice: *.c |> gcc %f -p %o |> %B.o
%   http://gittup.org/tup/make_vs_tup.html benchmark
%   http://gittup.org/tup/build_system_rules_and_algorithms.pdf
%   But the coolest thing is that it automatically handle dependencies!
%   and it uses fuses and inotify probably so it's automatic!
% - bazel?? latest from google:
%   http://google-opensource.blogspot.com/2015/09/building-build-system-bazel-reaches-beta.html
% - buck, huge, python-based like many other google-inspired build systems.
%   the ocaml-specific stuff is already 4000 LOC, hmmm
% - cons/scons, Perl-based and python-based build systems???
% - redo, from dan bernstein, seems minimalist, not sure it's better.
%   maybe better for autodeps.
% - msbuild, 452 000 LOC, wow, XML-based too, recently open sourced
%meta build systems:
% - imake (for X, but then superseded by autotools)
% - automake and autotools (for GNU)
% - gyp, from Google, python-inspired, simple, 28~000 LOC
%   support many backends (make, cmake, ninja, eclipse, xcode, ...),
% - fbmake, very similar to gyp
% - pants (from square), seems similar to fbmake
%wrappers:
% - qake, just a big makefile, so relies on GNU make
% - OCamlMakefile
% - premake?
%language specific:
% - java: 
%    * maven, first to provide ability to download dependency over the network?
%    * ant+ivy (to catchup with maven)
%    * gradle seems best one, use of DSL based on Groovy. huge codebase.
%    comparison: https://technologyconversations.com/2014/06/18/build-tools/
%    but all 3 are bad in my opinion.
% - javascript: gulp, grunt
% - ruby: rake, can write rules in Ruby. EDSL again. more verbose than make,
%   need of do, end, need '' around filenames, need of many builtins.
%   maybe inspiration for brew
% - haskell: shake, EDSL in haskell, seems too verbose, 
%   cabal
% - scala: sbt (scala build tool), even a book about this
% - rust: cargo, integrate in build system a dependency manager
% - ocaml: omake, opam (just dependency manager though),
%   jbuilder, jenga (advanced jbuilder) from jane street, 
% - lisp: asdf
% - clojure: leiningen
% - go: realize (with watchers?)


% see https://en.wikipedia.org/wiki/List_of_build_automation_software

%http://www.catb.org/esr/writings/taoup/html/ch15s04.html
% discussions of a few meta build systems, and nice history of make
%http://www.lihaoyi.com/post/WhatsinaBuildTool.html
% nice comparisons of a few tools
%http://hadihariri.com/2014/04/21/build-make-no-more/
% nice bashing of all make competitors, a la "evolution of a programmer"
%the essence of make, not too bad, nice relation between ninja and make
% http://bentnib.org/posts/2015-04-17-propositions-as-filenames-essence-of-make.html
%make advocacy :)
% http://bost.ocks.org/mike/make/
%http://aosabook.org/en/500L/contingent-a-fully-dynamic-build-system.html
% meh, EDSL
%make theory and practice
% http://www.ploxiln.net/make.html  meh
%how we use make:
% https://segment.com/blog/how-we-use-make/ list of targets

% complimentary tool: 
%  - https://github.com/cespare/reflex to rebuild automatically after a
%    file changed
%  - the one from facebook? 

\t used not only under plan9. Was ported to Unix (not just plan9port)
\t A few projects use it. Not as popular as Make though.

\section{Getting started}
\label{sec:getting-started}

%dup: (and adapted) Assembler.mw
To play with [[mk]], you will first need to install
the \plan fork used in \principia. See \urlinstall.
Once installed you can test [[mk]] under \plan with:

\begin{verbatim}
1   $ cd /tests/mk
2   $ mk -f hello.mk
3   5c -c hello.c
4   5c -c world.c
5   5l -o hello hello.5 world.5
6   $ mk -f hello.mk
7   mk: 'hello' is up to date
8   $ touch world.c
9   $ mk -f hello.mk
10  5c -c world.c
11  5l -o hello hello.5 world.5
12  $
\end{verbatim}
\n could get one from kencc or plan9port. issues though with rc vs sh ...

Line~2 runs [[mk]] with the [[-f hello.mk]] argument
to tell [[mk]] to use the rules in the [[hello.mk]] file
instead of the default [[mkfile]] 
([[mk]]'s equivalent of a [[Makefile]]).
%
Line~3 through 5 then output the shell commands run by
[[mk]] given the rules contained in [[hello.mk]]. 
%
Those commands compile and link a simple program called [[hello]]
using the ARM C Compiler [[5c]] (see the \book{Compiler})
and ARM linker [[5l]] (see the \book{Linker}).
%
Remember that [[.5]] is the filename extension of ARM object files
under \plan, hence the use of [[hello.5]] and [[world.5]] in the
linking command above.


Line~6 re-runs [[mk]], which should not recompile 
or relink anything because nothing has changed. Instead,
[[mk]] should indicate that [[hello]] is already up to date.
\n I added the already in mk-ocaml
%
Line~8 modifies the date of [[world.c]]. 
\l need to explain touch? modification time?
This time, re-running [[mk]] at Line~9 should trigger the recompilation
of [[world.c]] and relinking of [[hello]]. Note that
[[mk]] should not recompile [[hello.c]] because
[[hello.5]] depends only on [[hello.c]], not [[world.c]].
\l incremental



\section{Requirements}

%dup: from Assembler.nw
Because most of this book is made of C source code, 
you will need a good knowledge of 
the C programming language~\cite{k-r} to understand it.
%
I also assume you are already familiar with at least 
one build system, for instance, a variant of Make,
\l also a shell
%
and so are familiar with concepts such as
a [[Makefile]],
a {rule},
a {target}, 
a {prerequisite}, or
a {shell command}.
\n called recipe in mk, by shell command easier (said later)
%
If not, I suggest you to read one of the books about
GNU Make~\cite{managing-make, programming-with-gnu-software, gnu-make-manual}.

%dup: (and adapted) from Windows.nw
If, while reading this book,
you have specific questions on the interface of [[mk]],
I suggest you to consult the man page of [[mk]]
at [[docs/man/1/mk]] in my \plan repository.

Finally, the [[builders/docs/]] directory in my \plan repository
contains documents describing either [[mk]]~\cite{mk-successor},
or the [[mkfile]]s used in \plan~\cite{mk-plan9}.
%
Those documents are useful to understand some of the 
design decisions presented in this book, especially how and why
[[mk]] differs from Make.




\section{About this document}
#include "../docs/latex/About.nw"

\section{Copyright}
\input{../docs/latex/CopyrightPlan9Text}

\section{Acknowledgments}

I would like to acknowledge of course the author of [[mk]],
Andrew Hume,
who wrote in some sense most of this book.
\n and Bob Flandera? more for plan9 mkfiles than mk itself, 
\n apparently Hume did stuff also on optimizing Boyer Moore,
\n  cited by author of GNU grep https://lists.freebsd.org/pipermail/freebsd-current/2010-August/019310.html




\chapter{Overview}
\label{chap:overview}

%trans: %dup: Assembler.nw
Before showing the source code of [[mk]] in the following chapters, 
%toc:
I first give an overview in this chapter
of the general principles of a build system.
%
I also quickly describe the command-line interface of [[mk]]
and show a simple [[mkfile]] for a toy application.
\n actually hello.mk
%dup: from Assembler.nw
Finally, I define terms, explain how the code is organized, 
and more generally give the background necessary
to understand the code I will show later.

\section{Build system principles}
\n I try to not use too much [[mk]] here, and use "build system", to generalize

%trans:
To understand the goal of a build system, it is useful to remember
how programmers were compiling projects before Make was invented.
\l remember -> imagine? compiling -> maintaining? building?
%without: %history:
Before Make, programmers were using {shell scripts} to {record} the
compiling and linking commands for a project. For instance, here
is one such script called [[make.rc]]\footnote{This script
is using the \plan shell, which is called [[rc]] (see the \book{Shell}), 
hence the use of the [[.rc]] filename extension.
\l not sh or bash
}
to build the toy program mentioned in Section~\ref{sec:getting-started}:
\l can be used to build {from scratch}, or after modified

<<tests/mk/make.rc>>=
#!/bin/rc

5c -c hello.c
5c -c world.c
5l -o hello hello.5 world.5
@

However, as programs grow larger, and the number of files grows,
the shell-script approach becomes too {inefficient}\footnote{
%bootstrap:
This approach can still be useful to bootstrap the build system itself.
}.
\l or use https://notabug.org/akkartik/basic-build (minimal build system in sh)
\l inefficient, and also a bit long, but could factorize with shell variables
Indeed, in the previous example, if only
[[hello.c]] is modified, there is no need to recompile also [[world.c]],
but that is what the script will do when it will be run again to rebuild
the program.
\l not incremental. rebuild from scratch.
%without:
An alternative for the programmer is to keep track in his head of
all the modifications, and to recompile manually only what is necessary.
%
However, again, as programs grow larger, and dependencies between files
become more complex, 
\l make depend, gcc -MM (said later?)
it becomes difficult to remember
which files need to be recompiled and what are the precise flags 
used to recompile or link those files. 
\n actually as programs become really really big, ninja is better


%dup: intro/motiv (but longer version)
Thus, the goal of a build system is to 
describe {\em concisely} and 
maintain {\em efficiently}
{dependencies} between files.
\l shell is concise but inefficient
\l could write special programs to check date (including in a shell lang)
\l  which would be efficient, but not concise so tedious
%
For a programmer, those files are C or Assembly programs, object files, 
and executables.
For a writer, those files are Troff or TeX documents, pictures, and
PDFs. 
A build system can be used in many contexts, not just for programming.

%toc:
Note that every word in the first sentence of the previous paragraph 
is important:
``concisely'' led in [[mk]] to the creation of a domain-specific language;
``dependencies'' led to the use of a {graph} to represent the relationships
between files; and
``efficiently'' led to the use of a job scheduler to run in
parallel multiple tasks, as 
explained in the following sections.
\n there are different kinds of deps actually many-to-many

In the next sections, I will use examples using the syntax of [[mk]],
which is very close to the syntax of Make,
but the principles apply to most build systems.

\subsection{A domain-specific language}
\label{sec:dsl}

A {\em domain-specific language} (DSL), as its name suggests, is a language
specialized for one particular task. 
%dup: intro/motivations overview/principle
In the case of a build system, the task is 
(1) to describe file dependencies, and 
(2) to describe the commands to maintain those dependencies.
%
A DSL uses a special syntax to describe more concisely than
with a {general-purpose language} the solution to a particular problem.
%dup: intro/motivations
\l Usually not as expressive as full PL, but trade expressivity for compactness
\l and also special checks (will see later)
%trans:
The shell script [[make.rc]] above is already a good solution for (2).
Indeed, a shell can almost be considered a DSL for running commands;
\l before grew as full language
it uses a special syntax for launching programs (by just typing
the name of a command, without the need to call [[fork()]] or [[exec()]]),
for creating pipes (with [['|']]), and for file redirection 
(with [['>']] or [['<']]).
%
Thus, the idea behind the DSL of [[mk]] (and Make before) 
\l give a name to this DSL? Mk-lang?
was to extend slightly the shell syntax to accommodate also (1), 
with special constructs to express dependencies between files.
\l small superset actually. very small language.
%toc:
Those constructs are 
the rule, 
the pattern,
the variable,
and the file inclusion,
as explained in the next sections.

\subsubsection{The rule}
\label{sec:rule-simple-mkfile}

%trans:
The most important construct in 
\n [[mk]]'s DSL 
the DSL of a build system
is the rule.
%
A {\em rule} describes a {relation} between two or more files,
for instance, the relation between an object file [[hello.5]]
and its source [[hello.c]].
\l essence is state dependencies/graph between files and how to achieve it.
%
In [[mk]]'s terminology, the object file is called the {\em target},
and the source the {\em prerequisite}. Moreover, the
command to maintain the dependency between those two files,
that is the shell command to generate the target from the prerequisite,
is called the {\em recipe}.
\l recipe also called body and start of rule called head (said later?)
%\paragraph{Syntax}
Here is the syntax of a rule in [[mk]]'s DSL:

\begin{verbatim}
target : prerequisites
    recipe
\end{verbatim}
\l syntax of mk and Make, except TAB vs space
\l in fact target_s_: prerequisites
% {target}: {prerequisites}
%     {recipe}


Here are the rules corresponding to the script [[make.rc]] shown above
for the toy program mentioned in Section~\ref{sec:getting-started}:
\l ported to mk, ``program''

<<tests/mk/mkfile version 1>>=
# rule 1
hello.5: hello.c
    5c -c hello.c
# rule 2
world.5: world.c
    5c -c world.c
# rule 3
hello: hello.5 world.5
    5l -o hello hello.5 world.5
@
\n would be better to have hello first rule so that default rule but ok.
\l use of comment here

The rules are usually stored in a special {\em configuration file}:
an [[mkfile]] for [[mk]] (and a [[Makefile]] for Make).
\l ``program''
%
Note that a rule can have multiple prerequisites, like in the third
rule above with the multiple object files.
%
In fact, a rule can also have multiple targets, as you will see 
in Section~\ref{sec:graph-many-to-one}.
%
Moreover, the same file can be a target in one rule
and a prerequisite in another rule,
for instance, [[hello.5]] in respectively the first and third rule.


The syntax of an [[mkfile]] is very similar to the syntax of a shell script.
[[mk]] even uses the same syntax for comments, 
which are prefixed with a [['#']].
\l could be extracted automatically ... except hello.5 not visible from 
\l  5c command, would need -o hello.5 then.
The only syntactical addition is the use of [[":"]] to separate the target from
the prerequisites, and the newline and indentation
\l any indentation is fine, tab, space
to separate the prerequisites from the recipe.
\l x : y \n z1 \n z2. could also x : y ; z1; z2
%
This syntax is {\em minimalist}. Indeed, there is no
quote around filenames or around commands. Moreover, the different
elements in the list of prerequisites are simply separated by space.
\l no keyword, no braces, nothing!
\l same spirit than shell actually. no need () for calling
\l no need remember order of options (but then lead to -xxx named arguments)
\l actually sometimes need quote, for escaping (said later)

%\paragraph{Semantic}
%trans: syntax, now semantic. just like syntax rule is simple, semantic trivial
The semantic of a rule is also very simple. [[mk]] will check
\t recursive: mk will check if target up-to-date with prereqs
\t  by first checking recursively if prereqs up-to-date with their own prereqs
\t  and then check the modif time of target is more recent than ...
if the {\em modification time} of a target is more recent than {all} its
prerequisites.
\n actually more recent or equal in mk-in-ocaml, cos second not precise enough
\n actually DFS first, so semantic more subtle than what it looks (said later)
\n  check if prerequires are up to date, recursively, and then check if tgt ...
If not, it will run the recipe, which hopefully will update the 
modification time of the target.
\l can do the check? mk does? mk-in-ocaml does!

%\paragraph{Recipe}
As I said before, the recipe is simply a shell command.
Even though the commands in the [[mkfile]] above are simple, 
[[mk]] allows to use the full language of the \plan shell [[rc]]
(see the \book{Shell}), with loops, conditionals, functions, etc.
%
Indeed, one of the design principles of [[mk]] was to leverage existing tools.
\n and syntax (said later)


The shell language is {\em embedded} inside [[mk]]'s DSL.
\l Not even interpreted. As is. simpler than in make (said later?)
%
This is similar to other \unix DSLs such as Lex~\cite{lex} or Yacc~\cite{yacc}
where the programmer can use special syntax to define respectively
{regexps} and {grammar rules}, but where he can also use the full C language 
for the {actions} triggered when a regexp or grammar rule is recognized
(See the \book{CompilerGenerator}).
\n actually in Make weird mix for vars, but not unlike Yacc with dollar1
\n Similar to PHP too, with HTML and where embed dynamic ocde
%
\label{sec:mk-is-a-graph}
In [[mk]], the actions are not written in C but in the shell language 
of [[rc]], and those actions are embedded not inside the definition of 
a regexp or of a grammar, but inside the definition of a {\em graph}.
%
Indeed, the targets and prerequisites are similar to the sources and
destinations of {\em arcs} in a graph. The recipe corresponds to the 
{\em label} on an arc.
%
In fact, as you will see in Section~\ref{sec:graph-principles}, 
[[mk]] internally uses a graph to represent the dependencies between files.



%\paragraph{Dependency arities}
%What types of dependencies we need? can have multiple targets:
%    x.ml x.mly: x.mly
%  (but then can be confusing when making graph of deps. dont want
%  run recipe twice, so tricky, so need mark multiple nodes from one job done)
\n  also can be used to factorize simple rules like a.o b.o: x.h

% can have same target and multiple rules (but only if one recipe)
%    x.o: x.c  recipe     x.o: y.h
% (also with metarules below, when instantiate can add more rules??)
%  but maybe this one is really sugar, could put all prerequisites at once!

% lead to optional recipe. space/tab first char cos support multiline 
% for shell command (otherwise would need escaped newline there too, ugly).



\subsubsection{The pattern}
\label{sec:pattern}

%trans:
The [[mkfile]] in the previous section allows to maintain
efficiently dependencies between files:
if only [[hello.c]] is modified, then [[world.c]] will not be 
recompiled by [[mk]].
However, the [[mkfile]] is not very concise.
%
Note that the first two rules are very similar;
they differ only by the name of the file.
\t in make.rc, if long list, then can define in a var and loop at least!
\t but how in mk DSL? how loop? how generalize?
%
This is why to factorize rules, 
most build systems provide a way 
\n [[mk]] allows 
to use {patterns} inside a rule. 

In [[mk]],
a {\em pattern} is a sequence of characters where
the special character [['%']]
matches any sequence of characters.
\l separated by space? but then can also have quote.
\l longest match?
Here is an example of a pattern matching any C source files:

\begin{verbatim}
%.c
\end{verbatim}

A rule using a pattern for his target
is called a {\em meta rule} in [[mk]]'s terminology.
\l one of the target
\n can also be in prerequisite
\l rule generating rules
\l describe a set of arcs in a graph in a generic way.
%
Here is a better version of the [[mkfile]] for the same
toy program:


<<tests/mk/mkfile version 2>>=
# simple rule
hello: hello.5 world.5
    5l -o hello hello.5 world.5

# meta rule
%.5: %.c
    5c -c $stem.c
@
%$
\l order does not matter, declaratif

During the processing of the first rule above, [[mk]] will
recognize that [[hello.5]] and [[world.5]],
which are mentioned in the prerequisites,
{\em match} the target in the {second rule} if the percent is 
set to [[hello]] or [[world]].
\l first rule called simple rule, second meta rule.
%
[[mk]] will then {\em instantiate} the meta rule to generate
a specific rule for [[hello.5]], and another one for [[world.5]].
\l actually done on the fly while building the graph
\n but sometimes instantiate too many, possibly ambiguous, (see vacuous remove)
%
The percent in the prerequisite is then {\em substituted} by
the matched string ([[hello]] or [[world]]), and the 
{special variable} [[$stem]] %$
can be used from the shell command to access the matched string.
\t footnote could use percent in recipe, but then not shell recipe
\t  goal is be as less intrusive as possible (as opposed to Make,
\t  which does processing on recipe such as variable expansion)
\t maybe just footonote that sas explain later why different mechanism
\t  (if choose percent, then need process string, need escape mechanism
\t  complicated).
\l special variable good name?

The use of [['%']] to match any sequence of characters
is similar to the use of [[.*]] in a {\em regular expression}.
\l cite book on regexps?
%
In fact, [[mk]] supports also meta rules using 
regular expressions, as explained in Section~\ref{sec:regexp}.
%
However, in most cases, patterns using [['%']] are expressive enough
and simpler to write.
\l regexp notoriously difficult, and . is very common in filename but
\l  needs to be escaped in regexp

%real-world: 
Make supports meta rules with percents, but not with regular expressions.
It also supports {\em suffix rules} (e.g., [[.5.o: ...]]).
However, suffix rules are less intuitive to write and less expressive 
than meta rules. This is why they are not supported by [[mk]].
\l .SUFFIXES
\l generalize, simplify
\n can also use pattern in variable (said later)

\subsubsection{The variable}

%trans:
In the previous section, I have shown the use in a recipe 
of a variable ([[$stem]]) %$
set by [[mk]]. 
\l special var, internal var
%
Most build systems offer a way 
to define and use your own variables to factorize tings.
\n  in an [[mkfile]]
\l more uses than factorizing? readability?
\l actually also on the command line, and even override
%
In [[mk]],
those variables can contain a list of {\em words}, which
can correspond to anything: filenames, compiler flags, etc.
Here is an example of a variable containing two filenames:

\begin{verbatim}
OBJS=hello.5 world.5
\end{verbatim}

Here is a slightly different version 
% shorter (in numbers of characters) % actually not shorter
of the previous [[mkfile]] using a variable:

<<tests/mk/mkfile version 3>>=
OBJS=hello.5 world.5

hello: $OBJS
    5l -o hello $OBJS

%.5: %.c
    5c -c $stem.c
@
%$
\l CFLAGS=-g -x -...and then can use in many rules CFLAGS. easier to modify too.
\l but if use metarule then less useful

One of the design principles of [[mk]] was to leverage existing tools,
but also existing syntax.
Thus, the syntax to define and use variables in [[mk]] is
exactly the same than in the shell.
\n define and use! Make sucks here.
\l but not same exactly! handled by mk itself this time!
\l reuse syntax and some machinery (but not all machinery)
This syntax is again minimalist.
To {define} a variable, type a variable name, followed by an equal sign, 
followed by a list of words simply separated by space.
There is no braces, brackets, quotes, commas, or semicolons
\l or a type
like in other languages 
(e.g., [[ char* OBJS[] = {"hello.5", "world.5"}; ]] in C).
%
The next {newline} marks the end of the variable definition\footnote{
You can also escape newlines, to spread variable definitions over multiple
lines, as explained in Section~\ref{sec:escaped-newline}.
}.
\l  (unless it is {escaped} to scale to larger list, as explained in Section X)
To {use} a variable, prefix the variable name with the dollar sign.
\n also {} for special cases (said later)
\l history of this dollar sign?


Note that [[mk]], like the shell [[rc]], treats the content
of a variable as a list.
\l even if contains only one element
[[mk]] offers also a special syntax to concatenate lists
by simply juxtaposing a variable with other elements or other
variables, as in the following example:

\begin{verbatim}
X= b c d
# Y will contain a b c d e f
Y=a $X d e f
\end{verbatim}
%ocaml-found: scalar vs list context! can be stricter actually

[[mk]] offers also a special syntax to transform lists,
as explained in Section~\ref{sec:transform-list}
\l use pattern! orthogonal power


Once a variable is defined, you can use it in other variable
definitions, or in a rule (in the target, in the prerequisites, 
or even in the recipe). 
You can also use variables in patterns in meta rules.
\t can use before defined? empty then? and when define the can see new value?
\n also in file inclusion (said later)
%
Moreover, [[mk]] imports the variables from the environment,
so you can also use variables such as [[$HOME]] in your [[mkfile]]. %$
\l like the shell, also import and prefix with dollar
\n or objtype (said later)


Note that the term ``variable'' in the context of [[mk]] is slightly 
misleading.
Indeed, in [[mk]], variables are constants.
\n actually mk-in-ocaml check if override
\n even though can be overriden on commandline
Variable definitions are more {binding definitions}.
\t expanded in target and prerequisite and def as parse the file
\t  (but not in recipe, cos use different mechanism, export env, 
\t   again to not be intrusive on recipe, embeded DSL, so no extra escaping)
Indeed, [[mk]] needs to know statically the value of a variable to be
able to compute the graph of dependencies.
\t why? need explain more, at least footnote and ref to later
%real-world:
\t actually in Make vars are different and binding and scope is complex
\l Also clearer then diff between variables like OBJS=..
% and variables generated by mk such as target, stem etc.
\l also leaky abstraction, use of var in target/prereqs is diff than in recipe

%trans:
% more generic stuff when use modularity/inclusion

\subsubsection{File inclusion}

%trans:
The last construct 
\n of [[mk]]'s DSL 
found in most build systems
is the file inclusion. 
\l better term? the include?
%
In [[mk]],
a {\em file inclusion} is an {instruction} in the [[mkfile]],
starting with [['<']], used to load the rules and variables defined 
in another file.
This file can itself includes other files, recursively.
Here is an example of a file inclusion:

\begin{verbatim}
</$objtype/mkfile
\end{verbatim}

The effect is similar to a [[#include]] in C.
%
Note that the filename can contain variables defined
previously in the [[mkfile]] (or in the environment).
\l use variable! orthogonal power, can use variables everywhere
%
Here, [[$objtype]] %$
is a special \plan environment variable containing the
type of architecture of the current machine (e.g., [[arm]], [[386]]).
\t cite? plan9 article? one of my books?
%
You can then define for each architecture a specific [[mkfile]] with
variables such as [[$CC]] or [[$LD]]
containing the name of the native compiler and linker.
%
Section~\ref{sec:mkfile-objtype} presents such an [[mkfile]]
for the ARM architecture.
%
It is good practice to include [[/$objtype/mkfile]] %$
at the beginning of an [[mkfile]] for better portability.
%
Note that again [[mk]] reuses the syntax of other tools by
using the [['<']] symbol used for input redirection in the shell. 
\n not so convincing this one


Just like the rule, the pattern, and the variable,
a file inclusion allows to factorize things.
%
Indeed, you can store a library of meta rules
in a separate file (e.g., [[/shared/mkgeneric]]) and reuse this
file in multiple projects.
\l [['<']] so modularity and can factorize in different files
In fact, by combining variables and file inclusion, you can
also have a library of simple rules shared by multiple projects,
as shown by the example below:

<<tests/mk/mkfile version 4>>=
OBJS=hello.5 world.5
PROG=hello

</shared/mkone
@
%$
\n actually /sys/src/cmd/mkone, and it is OFILES and TARG not OBJS and PROG

<</shared/mkone>>=
$PROG: $OBJS
    5l -o $PROG $OBJS
%.5: %.c
    5c -c $stem.c
@
%$
%real-world: include (or -include)

% with variables above, offers already some kind of genericity!
% in fact many mkfiles are just  < /sys/src/cmd/mkone !
% (but still need metarules for that)

%\subsection{Generating some dependencies automatically}

% another use for file inclusion is include dependencies information generated
% automatically by other tools.
% Can do manually, but can be tedious.
% Many dependencies are implicit in files. if foo.c include
% foo.h, should recompile. Can declare deps in mkfile, but
% redundant, hard to maintain. 
% So can be good to auto generate some dependencies. But 
% different for each programming language.

% enter .depend 
% (and so also need for multiple rules for same targets (but with empty recipe))

% related is library of variables and rules for different languages.

% object level deps. libs.
% But also semantic level.

% question is can we generate every dependencies automatically?
% IDE tries to do that. But requires helps, AddFile, AddDir.
% what about when have multiple binaries in one distrib?
% hh_server nice, but works because generate a giant PHP library.

% in the end, could be job of compiler? like javac does? but then
% impose constrain on how to organize classes in files and path
% (or use hh_server, but again, impose constraint of one giant
% single binary/library)

%\subsubsection{Generality}

% works for all PLs. not adhoc to one PL like IDEs

%related: generate from mkfile .project, like Buck does.

%\subsection{Different types of dependencies}
% before? after?

\bigskip
[[mk]] offers a few more constructs
beyond 
the rule, 
the pattern,
the variable, and 
the file inclusion, 
but those are the main constructs of a build system.
%
See Chapter~\ref{chap:advanced} for the list of advanced
constructs supported by [[mk]].


\subsection{A graph of dependencies}
\label{sec:graph-principles}
\label{sec:rule-essence}


%trans: 
The pattern, the variable, and the file inclusion are nice features,
but they are not the essence of a build system;
%
rules are the essence of a build system.
\l rest is sugar
%
Indeed, once the build system has 
loaded included files,
expanded variables, and 
substituted patterns,
what remains is a set of rules with {concrete} filenames 
as targets and prerequisites.
%
Moreover, as I mentioned briefly in Section~\ref{sec:mk-is-a-graph},
the rules in a build system define essentially the 
{nodes} and {arcs} of a {graph}.
Thus, the essence of a build system is also this {\em graph of dependencies}.
\l or use dependency graph? then need to rename everywhere


%toc:
In the next sections, I will present different examples of graph
of dependencies, with increasing complexity.
\l Each example, each graph, different shapes, see subtelities.
\l Also easier explain algo on this graph than on rules. core DS.

\subsubsection{A simple tree}
% | and /\  one-to-one and one-to-many

\begin{figure}[!]\centering
\begin{verbatim}
           +-------+
           | hello |
           +-------+
               X
              / \
       /-----/   \-----\
      /                 \
     v                   v
+--------+          +--------+
|hello.5 |          |world.5 |
+--------+          +--------+
     |                   |
     |                   |
     v                   v
+--------+          +--------+
|hello.c |          |world.c |
+--------+          +--------+
\end{verbatim}
\caption{Graph of dependencies for [[hello]].}\label{fig:graph-hello}
\end{figure}
\n maybe good step to explain later // scheduler

Figure~\ref{fig:graph-hello} presents the graph of dependencies
for the [[hello]] program of Section~\ref{sec:getting-started}.
\n when target is hello for mk
\l when x : y then arc. when x : y z then two arcs. too simple? (said later)
%
In this example, the graph is simply a {tree}.
%
The {\em nodes} in the graph of dependencies correspond to concrete filenames.
\l pattern and vars has been substituted (said before a bit)
%
The {\em arcs} represent dependencies between files.
For instance, [[world.5]] depends on [[world.c]], hence the arc between
the two nodes in Figure~\ref{fig:graph-hello}.
\l arc and direction
%
Note that a rule with two prerequisites leads to the creation
of two arcs in the graph of dependencies.
\l same for two targets (said later?)
\l one-to-one, one-to-many

\l target vs prerequisites, parent vs child, src vs dest.

Given the [[mkfile]] in Section~\ref{sec:dsl}, [[mk]] will
internally build the graph of dependencies of Figure~\ref{fig:graph-hello}.
%
The use of either simple rules or meta rules to describe the dependencies
(or both) will result in the same graph.
%
Once [[mk]] matched and substituted patterns, what remains will
be a set of nodes with concrete filenanes.
\l if mkfile use ver1 or ver2 of tests/mkfile
\l (see Section for algo to build graph)


%trans: nice graph, but not enough to explain algo.

\subsubsection{A labeled tree}
\label{sec:labeled-tree}

\begin{figure}[!]\centering
\begin{verbatim}
                     +-------+
                     | hello | 08:00:13
                     +-------+
         5l -o hello     X     5l -o hello
       hello.5 world.5  / \  hello.5 world.5
                 /-----/   \-----\
                /                 \
               v                   v
          +--------+          +--------+
 08:00:10 |hello.5 |          |world.5 | 08:00:11
          +--------+          +--------+
               |                   |
          5c -c hello.c        5c -c world.c
               v                   v
          +--------+          +--------+
 08:00:00 |hello.c |          |world.c | 11:30:00
          +--------+          +--------+
\end{verbatim}
\caption{Graph of dependencies for [[hello]], with labels.}
\label{fig:graph-hello-labels}
\end{figure}
\n better with labels on arcs, see duplicated recipe on multiple arcs.
\n better with labels for time. see core algorithm.
\l could put same time for the objects cos in //
%alt: graph, but with set of files in nodes? so no pb ambiguous arcs?
\t why not put recipe on the node itself? anyway there will be
\t  a master rule no? could, but could also be more flexible and
\t  have different recipe on different arcs. But then if
\t  two dependent are modified, which command to run? ...


Figure~\ref{fig:graph-hello-labels} presents also the graph of
dependencies for the [[hello]] program, but where nodes
ands arcs are annotated with {\em labels}.
%
Arcs are {labeled} with a {recipe} whereas
nodes are labeled with the {modification time} of the file they represent.
%
If the file does not exist, the modification time is set to zero.
\t footnote 1970 unix time
\l zero, so idea is similar to very very old, so will need to be updated.
%
In Figure~\ref{fig:graph-hello-labels}, the day, month, and year
of the modification time of the files are omitted for simplification purpose;
only the hours, minutes, and seconds are shown.
I assume all the files were modified in the same day.


The scenario that led to the modification times
in Figure~\ref{fig:graph-hello-labels} is as follows: 
The programmer of the [[hello]] program finished modifying 
[[hello.c]] and [[world.c]] at 8am;
he then ran [[mk]] to build the program; [[mk]] finished compiling
[[hello.5]] at 8am and 10 seconds, and 
[[world.5]] at 8am and 11 seconds;
[[mk]] finished linking 
[[hello]] at 8am and 13 seconds;
\n actually seconds granularity not good enough (said later?), need nanosecond
finally, the programmer modified [[world.c]] at 11:30am
and stopped.
%
At this point, running [[mk]] should recompile [[world.c]]
(and relink [[hello]]), but should not recompile [[hello.c]]. 

%trans: Thx to labels, ready to talk about algo.

\subsubsection{Depth-first search}
\label{sec:algo-dfs}

\begin{figure}[!]\centering
\begin{verbatim}
               +-------+
           (1) | hello | (8)
               +--X--^-+
                   X  \
                / / \  \
          / - -  /   \  \----\
           /----/     \----\  \
        / /                 \  \
       v v                   v  \
    +--------+          +--------+
(2) |hello.5 + - - - - >|world.5 | (7)
    +--+-----+(4)    (5)+--+-----+
         | ^                 | ^
       | | |               | | |
       v v                 v v
    +------+-+          +------+-+
(3) |hello.c |      (6) |world.c |
    +--------+          +--------+
\end{verbatim}
\caption{Depth-first search traversal of the graph dependencies for [[hello]].}
\label{fig:graph-hello-dfs}
\end{figure}


Once the graph of dependencies is built,
\l and not so trivial
the main algorithm behind [[mk]] is to perform a 
\l and other build systems?
{\em depth-first search} (DFS) traversal on this graph.
\l cite cormen?
%
Figure~\ref{fig:graph-hello-dfs} presents the order in which 
the DFS visits the nodes on the previous graph.


The DFS starts from the {\em root} 
(step 1 in Figure~\ref{fig:graph-hello-dfs})
and goes as deep as possible along a {\em branch} (steps 2 and 3).
%
When it reaches a {\em leaf}, for instance [[hello.c]] (step 3),
the algorithm just checks whether the file exists.
If the file does not exist, then [[mk]] should report an error
because there is no instruction on how to generate this file
(there is no prerequisite connected to the node and so no recipe).
%
If the file exists, then the algorithm can continue and the
DFS can {backtrack} by going back up in the tree (step 4).
%
At this point, the algorithm checks whether the modification time 
of the node is more recent than all its prerequisites,
which have all been already visited by the DFS by now. 
\t this is important. need recurse first on children. Indeed
\t  in example hello is more recent than both objet files, but
\t  still need to relink it at some point cos very deep there is a 
\t  a modified file
%
If the node is more recent, for instance, [[hello.5]]
is more recent than [[hello.c]] in Figure~\ref{fig:graph-hello-labels},
then there is nothing to do but to continue the DFS 
(steps 5 and 6).
%
If the node is older than one or more of its prerequisites,
for instance, [[world.5]] is older than [[world.c]] 
in Figure~\ref{fig:graph-hello-labels},
then the algorithm should run the associated recipe
\l which recipe? recipe of arc?
in a separate shell process.
\l more on this later
Hopefully, running this process will modify the time of the node.
\l need to wait finish
%
This will in turn trigger the recompilation of all the
ancestors of the node while going back up to the root of
the graph (step 8 in Figure~\ref{fig:graph-hello-dfs}), because
the ancestors should now be older than this newly-generated file.


% But subtle, if modify hello.c and world.c, then 
% recompile both, but then dont want to run 2 times the 
% linking program. 2 arcs, but one command!
% ambiguous? no, same command here (see Section X for ambiguous check).
% (but maybe should really put recipe in node)

\subsubsection{A direct acyclic graph}
\label{sec:dag}
% /\  many-to-many
% \/

%trans:
In the previous examples, the graph was simply a tree. However,
%
most build systems support a more general form of graphs: 
{\em direct acyclic graphs} (DAGs), where
the same node can be referenced multiple times from different branches.
%
Figure~\ref{fig:graph-hello-dag} presents such as graph
for the same [[hello]] program, but where an additional
header file, [[common.h]], is shared and included by the 
two source files.


\begin{figure}[!]\centering
\begin{verbatim}
           +-------+
           | hello |
           +-------+
               X
              / \
       /-----/   \-------\
      /                   \
     v                     v
+--------+            +--------+
|hello.5 |            |world.5 |
+--------+            +--------+
     |                     |
     | \                  /|
     |  \------\ /-------/ |
     |          v          |
     v     +--------+      v
+--------+ |common.h| +--------+
|hello.c | +--------+ |world.c |
+--------+            +--------+
\end{verbatim}
\caption{Graph of dependencies for [[hello]] with [[common.h]].}
\label{fig:graph-hello-dag}
\end{figure}

Note that even though [[common.h]] is included by
[[hello.c]] and [[world.c]], there is no arc between those files
in the graph of dependencies.
%
Indeed, modifying [[common.h]] should not
cause the regeneration of [[hello.c]] or [[world.c]].
%
However, the modification of [[common.h]] should cause
the regeneration of the object files. Indeed, the header
file may define data structures that have an impact
on the object code generation, hence the two arcs
from the object files to the header file.
\t will talk later on how to automatically generate those dependencies

There are multiple situations where the same file can be
referenced multiple times in the graph of dependencies:
multiple executables may depend on and reuse the same library, 
multiple libraries may use the same object file,
multiple object files may depend on the same header file, etc.
\l Figure present a few of those examples.
Those shared files add arcs in the graph. However,
the graph must remain acyclic.
Indeed, a file can not depend on itself, directly or indirectly
\l would make no sense.
(Section~\ref{sec:cycle-check} presents the code to check for
{cycles} in the graph of dependencies).
\t Moreover, because DAG, DFS needs record if visited already a node
\t  to not update multiple times the same file
\l even though here cannot see that cos common.h is a leaf


%\subsubsection{Multiple rules with the same target}
\label{sec:master-rule} 

There are multiple ways to add the dependency
to [[common.h]] from [[hello.5]] (and [[world.5]]) in the
[[mkfile]]s of Section~\ref{sec:dsl}:

\begin{enumerate}
\item You can add [[common.h]] in the list of prerequisites
in the rule for [[hello.5]]:

\begin{verbatim}
hello.5: hello.c   common.h
    5c -c hello.c
\end{verbatim}

However, this approach does not work well when the rule
to compile [[hello.c]] is a meta-rule such as [[%.5: %.c]].
%
Indeed, each source file has its own header file dependencies,
which are impossible to factorize in a single meta-rule.


\item You can add a separate rule using the same target
and the same recipe:

\begin{verbatim}
hello.5: common.h
    5c -c hello.c
\end{verbatim}

It is important to impose to have the same recipe. 
If the recipe was different, [[mk]] would be confronted with an 
{\em ambiguity}
when both the source file [[hello.c]] and the header [[common.h]] are modified:
which recipe to choose to update the target [[hello.5]]?\footnote{
Section~\ref{sec:ambiguous-checks} presents the code to check
for the presence of ambiguities.}
%
However, it is difficult for [[mk]] to check whether
the recipe of a meta-rule such as [[5c -c $stem.c]] is %$
equivalent to the recipe of a simple rule such as [[5c -c hello.c]].
\t really? difficult? for stem it is known at compile time

\item You can add a separate rule using the same target but
without any recipe:

\begin{verbatim}
hello.5: common.h
\end{verbatim}

This would work if the build system
imposes that there must be another {single} rule,
called the {\em master rule}, with the same target but including
a recipe. In that case, there would be no ambiguity and no need
to check if two recipes are equivalent.

\end{enumerate}

[[mk]] supports (1) and (3),
\l I dont think mk supports 2. it is using pointer equality, not string equal
but (3) is more convenient for the programmer as it works well with meta-rules.
%\subsubsection{Automatic dependencies}
Moreover, when combined with file inclusion, (3) allows
to leverage programs that automatically extract header
dependencies from source files 
(e.g., [[gcc -MM]] for C files, [[ocamldep]] for OCaml files).
\l other? makedepend? furl?
%
Indeed, the output of such programs can simply be redirected
in a [[.depend]] file that can be included from the [[mkfile]].
\l would be hard to give recipe, cos flags, but gcc -MM
\l   could generate a template using metavariable though
\l So then need to find master recipe again.
\l example of .depend? .depend for mk ?
\n in \plan they dont use tools like gcc -MM because they hate header files
\n  they use very few header files

%\subsubsection{Depth-first search on a DAG}

% So have to take care when go through graph.
\l Because DAG, in code we will see a few times to set a flag on a node marked 
\l as done, to avoid repeat multiple times the same thing.
\n actually not that much
% but here DAG too simple, nothing after common.h, so a leaf is simple
% implications for graph update?

\subsubsection{Many-to-one dependencies}
\label{sec:graph-many-to-one}
%\/ many-to-one

\begin{figure}[!]\centering
\begin{verbatim}
                   +-------+
                   | prog  |
                   +-------+
      5l -o prog       X      5l -o prog
   lexer.5 parser.5   / \  lexer.5 parser.5
               /-----/   \-----\
              /                 \
             v                   v
        +--------+          +--------+
        |lexer.5 |          |parser.5|
        +--------+          +--------+
   5c -c   | \                   /  |   5c -c
  lexer.c  |  \--------\ /------/   | parser.c
           v            v           v
        +--------+ +--------+  +--------+
        |lexer.c | |parser.h|  |parser.c|
        +--------+ +--------+  +--------+
      lex    |    yacc  \           /   yacc
    lexer.l  |  parser.y \---\ /---/  parser.y
             v                v
        +--------+       +--------+
        |lexer.l |       |parser.y|
        +--------+       +--------+
\end{verbatim}
\caption{Graph with many-to-one dependencies.}\label{fig:graph-many-to-one}
\end{figure}

\l have shown one-to-one, one-to-many, and some kind of many-to-one.
%trans:
In Figure~\ref{fig:graph-many-to-one}, two object files,
[[lexer.5]] and [[parser.5]],
depend on the same file, [[parser.h]], but 
they also depend on other files ([[lexer.c]] and [[parser.c]])
and have different recipes ([[5c -c lexer.c]] and [[5c -c parser.c]]).
This is similar to the situation depicted by Figure~\ref{fig:graph-hello-dag}
with the shared file [[common.h]].
%
However, in Figure~\ref{fig:graph-many-to-one}, 
two files, [[parser.h]] and [[parser.c]], depend also {exclusively}
on the same file, [[parser.y]], with the same recipe ([[yacc parser.y]]).
%
This last file is a Yacc~\cite{yacc} grammar file.
The [[yacc]] program generates
both a header file ([[.h]]) and a source file ([[.c]]) from a
single grammar file ([[.y]]).
%Those are real many-to-one dependencies.
%
There are multiple ways to express this {\em many-to-one} dependency:

\begin{enumerate}
\item You could create two separate rules for each target:

\begin{verbatim}
parser.h: parser.y
    yacc parser.y
parser.c: parser.y
    yacc parser.y
\end{verbatim}

\item You could create a single rule with {\em multiple targets}:

\begin{verbatim}
parser.h parser.c: parser.y
    yacc parser.y
\end{verbatim}
\end{enumerate}

However, the semantics for (1) and (2) are different. Indeed,
with (1), if you modify [[parser.y]], then [[mk]] will
create two shell processes and execute two times the
[[yacc]] command, which is useless (and could even be incorrect
if the two commands are run in parallel and the writes on
the same file are intertwined).
%
With (2), it will create a single process 
and execute only one time the [[yacc]] command.


The use of multiple targets in one rule has implications on the
DFS traversal of the graph of dependencies.
%
Indeed, in Figure~\ref{fig:graph-many-to-one}, once the DFS has processed
[[parser.y]], backtracked on [[parser.h]], and ran the recipe
to update [[parser.h]], 
\n actually done later and indirectly through job queue
it is important that the algorithm adjusts the 
modification time of both the [[parser.h]] and [[parser.c]] nodes.
%
If only the [[parser.h]] node is updated, [[mk]]
would run another time the [[yacc]] command when the DFS
reaches the [[parser.c]] node with an obsolete modification time.
\l could also each time reread mtime during DFS
This is why, as you will see later in Section~\ref{sec:multi-targets},
the arc from [[parser.h]] to [[parser.y]] contains also
a reference to the [[parser.c]] node.
\t so mk put as BEINGMADE alltargets ?

%trans: this conclude examples of graphs? 


\subsection{A job scheduler}
\label{sec:job-scheduler-principles}

%trans: 
In the previous sections,
I have described the main features of the DSL of a build system,
the underlying representation of dependencies in a build system,
as well as the basic algorithm behind a build system (the DFS).
I will now focus on the efficiency of a build system.

%trans:
A build system maintains dependencies between files efficiently firstly
by being an {\em incremental} program. Indeed, if you modify
only one source file, the build system will recompile and 
relink only what is necessary.
%
This is made possible by comparing the modification times of nodes 
in the graph of dependencies.
%
In fact, this graph enables also the build system 
to be more efficient by running recipes in {\em parallel}.
%
Indeed, with a graph, it is easy to detect whether two commands
can be run in parallel when they belong to two independent branches 
in the graph.
\l independent is too informal?
\l In fact if 2 separate successors, can, even if at different level.
\l but algo does not do those branch-check? it uses the BEINGMADE thing
%
For instance, in Figure~\ref{fig:graph-many-to-one}, 
the recipes with the [[lex]] and [[yacc]] commands can be run in parallel.
%
However, if only the [[lex]] recipe finished, it is not possible
to run [[5c -c lexer.c]] in parallel with [[yacc]] because
there is an arc between [[lexer.5]] and [[parser.h]] in the graph;
you must also wait for the [[yacc]] recipe to finish.


To run recipes in parallel, a build system should not 
wait during the DFS that a shell process finishes executing a recipe.
%
This is why, during the DFS, [[mk]] adds instead the recipe in a {\em queue}
and continues the DFS.
%
Each element of this queue contains, in addition to the recipe,
a pointer to the target node (or target nodes) associated with
the recipe.
%
[[mk]] can add multiple recipes in the queue during the DFS, 
and can then execute in parallel those recipes once the DFS finished.
\n actually done during DFS via sched() (see later)
%alt: do DFS in parallel? but then concurrency issue? if DAG?
The recipe and associated target node(s) stored in the queue is called a
{\em job} in [[mk]]'s terminology. The queue is also called
the {\em job queue}. Thus, [[mk]] is also a {\em job scheduler}.


The use of a job queue has implications on the graph and the DFS.
%
Indeed, in Figure~\ref{fig:graph-hello-dfs}, as well as in
Figure~\ref{fig:graph-being-made}, if the job to regenerate
[[world.5]] from [[world.c]] is enqueued at step~7 (instead of being
executed {synchronously}), the modification time of 
the [[world.5]] node will not be updated directly. 
%
Then, when the DFS backtracks on the root node at step~8, 
the modification time of the root node may still be more
recent than all its prerequisites 
(as shown in Figure~\ref{fig:graph-hello-labels}),
so [[mk]] will not detect that it needs to re-link too [[hello]].
%
However, [[mk]] should not stop there and declare
that [[hello]] is up-to-date. Once the job to
generate [[world.5]] has finished, [[hello]] will not be up-to-date.
\l but can detect that situation so, as job queue not empty
This is why the modification time of nodes associated with a job
should be marked specially in the graph.

%coupling: same figure in chapter on findint outdated files
\begin{figure}[!]\centering
\begin{verbatim}
                +-------+ NotMade
            (1) | hello | (8)
                +--X--^-+
                    X  \
                 / / \  \
           / - -  /   \  \----\
            /----/     \----\  \
         / /                 \  \
        v v                   v  \
     +--------+          +--------+ BeingMade
 (2) |hello.5 + - - - - >|world.5 | (7)
     +--+-----+(4)    (5)+--+-----+
          | ^ Made            | ^
        | | |               | | |
        v v                 v v
Made +------+-+     Made +------+-+
 (3) |hello.c |      (6) |world.c |
     +--------+          +--------+
\end{verbatim}
\caption{Graph of dependencies with building-status labels.}
\label{fig:graph-being-made}
\end{figure}


\label{sec:build-status}
To keep track of the nodes involved in a job, [[mk]] uses
an extra label on each node to indicate the building status
of the node: [[NotMade]], [[Made]], and [[BeingMade]],
as shown in Figure~\ref{fig:graph-being-made}.
%
The algorithm behind [[mk]], exposed previously in Section~\ref{sec:algo-dfs},
is modified as follows. After the graph is built, every status labels in 
every nodes is set to [[NotMade]].
%
During the DFS, if the algorithm reaches a leaf containing an existing
file, the node is marked as [[Made]] (e.g., [[hello.c]] at step 3, and
[[world.c]] at step 6 in Figure~\ref{fig:graph-being-made}).
%
During backtracking, the algorithm will use different marks
depending on the situation:

\begin{itemize}
\item If the node is more recent than all its prerequisite nodes,
and all those nodes are marked as [[Made]], then this node is also
marked as [[Made]] 
(e.g., [[hello.5]] at step 4 in Figure~\ref{fig:graph-being-made}).

\item If the node is older than one or more of its prerequisites,
and all the prerequisites are marked as [[Made]], then a new
job will be enqueued and this node is marked as [[BeingMade]]
(e.g., [[world.5]] at step 7 in Figure~\ref{fig:graph-being-made}).

\item If the node is older or more recent, but one of its prerequisites
is marked as [[BeingMade]], then the status should be kept as [[NotMade]].
\l or BeingMade? then status should be unchanged (NotMade or BeingMade)

\end{itemize}

[[mk]] will run in a loop multiple times the DFS until
the root node is marked as [[Made]]. 
During each of those loops, the DFS will find new jobs 
to run in parallel.
\l mk needs also to wait for process to finish in this loop.
\l can have multiple waves before first wait if lots of processors!
\l FIGURE with waves?

% need just made vs notmade? why need also beingmade if have 
% also generic visited? and check on empty queue? because of the DAG?
%real-world: Make has the need for BeingMade? it kinda wait
% that all child are done still? 
\t then less opportunity for //ization? Example?

% mk is actually kind of a scheduler because run independent jobs in //, 
% and need to manage dependencies between those jobs
% then need coordinate. wait for finish, etc.
% Master/Workers?

% Note that in Figure with yacc, if BeingMade on parser.h,
% should also mark BeingMade for parser.c!

% Note that in second wave, can have more efficient DFS as no need
% go down in branch marked as Made
\l mk does that? does not because bottleneck is not there?
\t does DFS leverage the Made? Should have no need to go down on a node
\t  with Made!

%trans: ?

\section{[[mk]] command-line interface}
\label{sec:mk-interface}

The command-line interface of [[mk]] is very simple: just go
in a directory and type [[mk]]\footnote{
%real-world:
This interface is similar to the one in Make, 
except [[mk]] is even shorter to type than [[make]],
which is useful as [[mk]] is a command you will type a lot
(the two letters are even next to each other on a QWERTY keyboard).
}.
%dup: intro/getting-started
However, this assumes the directory contains a file named [[mkfile]]:

<<constant MKFILE>>=
#define	MKFILE		"mkfile"
@
%ocaml: used only once, so could be expanded where it's used
%real-world: make look for Makefile (said before)

Moreover, it assumes the first target in this [[mkfile]] is the file 
you want to build.


To change the default behavior, you can use the [[-f]] flag, as shown in
Section~\ref{sec:getting-started}, to specify another configuration file.
%
Moreover, you can change the default target by specifying a target
from the command line (e.g., [[mk hello.5]]). 
In fact, you can even give a list of targets on the command line.
\l great way to use mk is have list of one-liner (said later?)

[[mk]] supports also a few extra options to help debug [[mk]] itself
or to provide advanced features.
%chunks:
I will present gradually those options in this book.
%
Here is the full command-line interface of [[mk]]:

<<function badusage>>=
void
badusage(void)
{

    fprint(STDERR, 
           "Usage: mk [-f file] [-(n|a|e|t|k|i)] [-d[egp]] [targets ...]\n");
    Exit();
}
@
%old: fprint(STDERR, "Usage: mk [-f file] [-n] [-a] [-e] [-t] [-k] [-i] [-d[egp]] [targets ...]\n");
\l also have NPROC, NREP global variables
\t actually -I now, not -i
%ocaml: use Arg so cleaner --help (but true that redundant with man page)
\l also xxx=yyy

\ifallcode
%dead? used by some tools?
<<global version>>=
static char *version = "@(#)mk general release 4 (plan 9)";
@
\fi


\section{[[hello.mk]]}
\n [[hello.mk]] so consistent with my other books.
\l A simple [[mkfile]]

%trans:
Here is finally the content of the [[hello.mk]] file
mentioned in Section~\ref{sec:getting-started}:

<<tests/mk/hello.mk>>=
OBJS=hello.5 world.5
CFLAGS=
LDFLAGS=

hello: $OBJS
 5l $LDFLAGS -o $target $prereq

%.5: %.c
 5c $CFLAGS -c $stem.c
@
%$
\n minimum spirit, no <objfile, no factorize with mkone
\n  no objtype/mkfile, cos then cant use hello.5, should use hello.dollarO
\n simple var, simple rule, metarule, special vars; the essence of mk is there.

This file is named [[hello.mk]] to illustrate the [[-f]]
command-line flag of [[mk]], but a common practice is to name
[[mk]]'s configuration file [[mkfile]] instead.


I have described most of the features used in [[hello.mk]]
in Section~\ref{sec:dsl}, so I will not repeat the explanations here.
%
The only new feature is the use of the {\em special variables}
[[$target]] and [[$prereq]]. Those variables are set by
[[mk]] in the environment of the shell process executing the recipe.
\l that way recipe can access them
%
As their names suggest, they contain respectively the name of 
the target and the list of prerequisites of the rule in which
they occur.
\l note that prereq not always work, for instance for meta rule better use stem
\l  cos prereq will also match the .h in the .depend

\n show also simple output of running mk on a toy project (said before)
% VERBATIM 5c ...
% if modify hello.c
% VERBATIM 5c hello.c; 5l ...
% if nothing
% VERBATIM already done
\l maybe even include mk -e output?

The compilation and linking flags ([[$CFLAGS]] and [[$LDFLAGS]])
are set to an empty list in the example above, 
but those flags can be {\em overriden} from the command-line. 
%
Indeed, [[mk]] can take a list of variable definitions
as arguments (e.g., [[mk CFLAGS=-g]]). 
Those definitions override any definition contained in the [[mkfile]]
(or in files included from the [[mkfile]]).
\l not needed in plan9? by default compile info? or -a for acid
This is convenient because you can simply cross-compile
a project by overriding the definition of [[$objtype]]
from the command line (e.g., [[mk objtype=arm]] on a machine
where [[$objtype]] is by default set to [[386]]).

For more examples of [[mkfile]]s, 
%bootstrap:
notably the [[mkfile]] of the [[mk]] project itself,
see Appendix~\ref{chap:examples}.
The examples in this chapter were used just to illustrate
the main features of [[mk]]. 



%\section{Input [[mkfile]] language}

% precise syntax? can have OBJ = xxx or need OBJ=xxx ?
% need space before recipe? can have comments?


\section{Code organization}

%dup: (and adapted) from Assembler.nw
Table~\ref{tab:code-orga} presents short descriptions
of the source files of [[mk]], together with
the main entities (e.g., structures, functions, globals) the file defines,
and the corresponding chapter in this document in which the code
contained in the file is primarily discussed.
\n sorted by chapters, make more sense than sorted by dir
\t so? what this table is useful for/

\begin{table*}[tbh!]
\begin{center}
\begin{tabular}{lcllr}
\toprule
{\bf Function}  & {\bf Chapter} & {\bf File} & {\bf Entities} & {\bf LOC} \\
\otoprule
data structures and constants     & \ref{chap:core-ds}           & [[mk.h]]       & [[Symtab]] [[Word]] [[Rule]] [[Arc]] [[Node]] [[Job]] & 375 \\ % Bufblock
symbol table and cache            & \ref{chap:core-ds}           & [[symtab.c]]   & [[hash]] [[symlook()]] [[symtraverse()]]              & 89 \\ % symstat syminit?
variables                         & \ref{chap:core-ds}           & [[var.c]]      & [[setvar()]]                                          & 28 \\ % shname
list of strings (words)           & \ref{chap:core-ds}           & [[word.c]]     & [[newword()]]  [[wtos()]]                             & 90 \\ % wdup delword addw
globals                           & \ref{chap:core-ds}           & [[globals.c]]  & [[rules]] [[metarules]] [[jobs]]                      & 63 \\ %pad: was in main.c
function prototypes               & \ref{chap:core-ds}           & [[fns.h]]      &                                                       & 140 \\

\midrule
entry point                       & \ref{chap:main}              & [[main.c]]     & [[main()]]                                            & 331 \\ % badusage

\midrule
lexer                             & \ref{chap:parsing}           & [[lex.c]]      & [[assline()]] [[nextrune()]]                          & 160 \\ % bquote 
parser                            & \ref{chap:parsing}           & [[parse.c]]    & [[parse()]] [[rhead()]] [[rbody()]] [[addrules()]]    & 418 \\ % ipush ipop 
parsing and escaping methods for [[rc]]   & \ref{chap:parsing}           & [[rc.c]]       & [[charin()]] [[squote()]] [[escapetoken()]]   & 211 \\ 
parsing and expanding variables   & \ref{chap:parsing}           & [[varsub.c]]   & [[stow()]] [[expandvar()]] [[varname()]] [[varsub()]] & 439 \\ % subsub extractpat nextword
adding rules                      & \ref{chap:parsing}           & [[rule.c]]     & [[addrule()]]                                         & 163 \\ % rulecnt regerror

\midrule

building and checking the graph   & \ref{chap:graph}             & [[graph.c]]    & [[graph()]] [[applyrules()]] [[cyclechk()]]           & 455 \\ %newnode() newarc() nrep ambiguous vacuous
file and time management          & \ref{chap:graph}             & [[file.c]]     & [[timeof()]] [[touch()]] [[delete()]]                 & 104 \\
pattern matching and substituting & \ref{chap:graph}             & [[match.c]]    & [[match()]] [[subst()]]                               & 71 \\

\midrule

finding outdated files in the graph & \ref{chap:finding-outdated}  & [[mk.c]]       & [[mk()]] [[work()]] [[outofdate()]] [[update()]]    & 343 \\ % clrmade pretend update
constructing a job                    & \ref{chap:finding-outdated}  & [[recipe.c]]   & [[dorecipe()]] [[newjob()]]                       & 197 \\ % 

scheduling jobs                   & \ref{chap:scheduling}        & [[run.c]]      & [[run()]] [[sched()]] [[waitup()]] [[RunEvent]]       & 455 \\ % Process pnew pdelete nproc killchildren usage

shell environment                 & \ref{chap:shellenv}       & [[env.c]]      & [[buildenv()]] [[shellenv]] [[envinsert()]]              & 218 \\ %execinit initenv envupd
environment and process methods for \plan         & \ref{chap:shellenv}       & [[plan9.c]]    & [[readenv()]] [[exportenv()]] [[execsh()]]  & 562 \\ %waitfor pipecmd Exit catchnotes

\midrule
printing shell commands           & \ref{chap:debugging-support} & [[shprint.c]]  & [[shprint()]]                                         & 109 \\ %mygetenv vexpand front?

handling archives (libraries)     & \ref{chap:advanced}          & [[archive.c]]  & [[atimeof()]]                                         & 178 \\ % atouch() atimes()

\midrule
dumpers                           & \ref{chap:debugging-appendix} & [[dumpers.c]] & [[dumpv()]] [[dumpn()]] [[dumpr()]] [[dumpj()]]       & 105 \\ % was spread in other files before
error management                  & \ref{chap:error}             & [[utils.c]]    & [[Malloc()]] [[Realloc()]]                            & 34 \\
string buffer                     & \ref{chap:libc}              & [[bufblock.c]] & [[newbuf()]] [[insert()]]                             & 107 \\ % rinsert growbuf freebuf

\otoprule
Total                                    &                               &                 &                                              & 5445 \\
\bottomrule
\end{tabular}
\end{center}
\caption{Chapters and associated [[mk]] source files.}
\label{tab:code-orga}
\end{table*}

% still remains backward/mutual dependencies with codegraph on mk/ because:
%  - plan9.c and rc.c specific methods crosscut many files. They are called
%    from generic functions but also call themselves generic functions
%    (like an ugly framework). Examples:
%     * Lex.assline -> Rc.escapetoken -> Lex.nextrune
%     * Plan9.readenv -> Utils.Malloc -> Plan9.Exit
%     * Plan9.notifyf -> Run.killchildren -> Plan9.Exit
%  - parse.c and run.c are mixed (parsing call process for <|, for ``)
%    (should have separate eval.c)
%  - process are mixed (for ``, for varible expansion)
%  - mk.c, recipe.c, and run.c are mixed
%    (should merge mk.c and recipe.c, and put update() in graph.c)


\section{Software architecture}
\label{sec:soft-archi}

%dup: (and adapted) from Assembler.nw
Figure~\ref{fig:controlflow} describes 
the main control flow of [[mk]], whereas
Figure~\ref{fig:dataflow} describes 
the main data flow of [[mk]].
%
The main steps of the building pipeline of [[mk]] are as follows:

\begin{enumerate}
\item {\em Parse} the [[mkfile]] (via [[parse()]]) to extract
the rules and meta rules in the file
\n chapter 5

\item {\em Build} the graph of dependencies (via [[graph()]])
for a specific target given the rules extracted previously 
\n chapter 6

\item {\em Find} outdated files in the graph (via [[work()]])
\n chapter 7

\item {\em Schedule} jobs (via [[sched()]]) that will run the shell
recipes (via [[execsh()]]) to update the outdated files
\n chapter 8
\n chapter 9 = To {\em interact} environment

\end{enumerate}

\begin{figure}[!]\centering
\begin{verbatim}
              +------+
              | main |
              +------+
                  |
     +------------+------------+
     v                         v
 +-------+                 +-------+
 | parse |                 |  mk   |
 +-------+                 +-------+
     |                         |
     v             +-----------+
+--------+         v           +----------------+
|assline |    +---------+      |                |
+--------+    |  graph  |      v                v
              +---------+ +---------+      +---------+
                   |      |  work   |      | waitup  |
             +-----+      +---------+      +---------+
             v                 |                |
       +----------+     +------+----+      +----+-+
       |applyrules|     |           |      |      v
       +----------+     v           v      | +---------+
                   +---------+ +---------+ | | update  |
                   |outofdate| |dorecipe | | +---------+
                   +---------+ +----+----+ |
                                    |      |
                               +----v----+ |
                               |   run   | |
                               +------+--+ |
                                      |    |
                                   +--v----v-+
                                   |  sched  |
                                   +----+----+
                                        |
                                   +----v----+
                                   | execsh  |
                                   +---------+
\end{verbatim}
\caption{Control flow diagram of [[mk]].}\label{fig:controlflow}
\end{figure}
\l graphviz of files? or just of the types?

\begin{figure}[!]\centering
\begin{verbatim}
                             main target 
                           /              \
                          /- simple rules -\
mkfile -> lines -> words -                  > graph -> jobs
                          \- meta rules   -/

             (use global symbol table)
\end{verbatim}
\caption{Data flow diagram of [[mk]].}\label{fig:dataflow}
\end{figure}

%trans:?
\l from top to bottom, and left to right.
After some basic command-line processing and initializations,
the function [[main()]] calls [[parse()]] with the file to {parse}
as an argument (by default [[mkfile]], unless you specified
another filename with the [[-f]] command-line flag).
%
[[parse()]] first calls the lexer to assemble a {line} (via [[assline()]]),
which is then split in {words}, which are then analyzed
to populate important globals such as [[rules]] and [[metarules]],
which contain the lists of extracted {rules}.
%
[[parse()]] also sets the global [[target1]] with the name of the 
first {target} found in the [[mkfile]], unless you gave
a specific target on the command line (e.g., with [[mk hello.5]]).
%
Finally, [[parse()]] updates and uses a global {symbol table} containing
the values for the {variables} defined in the [[mkfile]]
and in the {environment}.
\n also process inclusion, so recursive function


After the rules have been extracted, [[main()]] calls [[mk()]] with
the name of a target as an argument (the name stored in [[target1]] by default).
\n actually target1 is a list (said lter)
%
[[mk()]] then calls [[graph()]] to build the {graph of dependencies}
for this target given the rules and metarules extracted during parsing.
%dup: overview/principles/graph
This graph contains nodes and arcs, as explained 
in Section~\ref{sec:graph-principles}.
%dup:
The {nodes} correspond to concrete files
(e.g., [[hello.5]], [[hello.c]]),
\n unless virtual targets (said later)
and the arcs connect two nodes when a node depends on another node 
(e.g., [[hello.5]] is connected to [[hello.c]]).
%
Those arcs are also labeled with the rule containing the recipe
to generate the target node.
\l also modification time, and build status.

[[graph()]] works by first creating a node for the target parameter,
called the {root} of the graph, and by then
calling [[applyrules()]] on this node.
\n actually applyrules build the node
%
[[applyrules()]] then finds a rule or meta rule with the node as a target,
\n can have ambiguity (said later)
and creates new nodes for all the prerequisites found in the rule. 
It then calls recursively [[applyrules()]] on those new nodes.
\l at some point leaf cos existing file with no matching rule for target
%
Note that as opposed to Make,
in [[mk]] the graph of dependencies is computed statically once and for all
at the very beginning.
\t so? better?


[[mk()]] then calls [[work()]] to find outdated files in the graph.
%
Starting from the root, [[work()]] performs a depth-first search
\l as explained in Section X
and goes down recursively in
the graph to find nodes corresponding to inexistent files, or
to files that are older than the files in the nodes they are connected to.
\l depth first search
%
Once it found such a node,
[[work()]] calls [[dorecipe()]]
with the outdated node as a parameter. 
\t note that DFS, go deep first! even if node more recent than all
\t  connected nodes, those nodes may not be up to date, so call
\t  work on them first! and then compare the date. (or said before)
[[dorecipe()]] then finds the arc containing the master rule 
\l see Section X for master rule
with the recipe to regenerate the file in the node.
\l can have ambiguity, but always a master arc, a master rule (said later)
%
[[dorecipe()]] then calls [[run()]] to add in a queue the job to 
run the recipe. 
[[run()]] possibly calls [[sched()]] to schedule the job if there was
a free processor to run the job in parallel.
%
[[sched()]] then calls [[execsh()]] to fork and execute in a shell the recipe.


[[mk()]] calls [[work()]] in a loop, to schedule jobs in
parallel until the root node is up-to-date ([[Made]]). 
However, during those loops,
[[work()]] may not be able to schedule any job. Indeed, all the processors
may already be in use, or certain jobs may not be able to start until
other jobs are finished.
This is why [[mk()]] also calls sometimes [[waitup()]] to wait for 
those jobs to finish.
Once a job is finished, [[waitup()]] calls [[update()]] to update 
the node in the graph associated with the job, and [[sched()]] 
to schedule another job.
\t as I said, ends when root node is up-to-date and no more process 
\t  to wait for.


%\section{Boostrapping}
% we use a mkfile and mk to compile mk :)
% can bootstrap simply using a shell script for the very first time,
% as mentioned before


\section{Book structure}

%trans: %dup: (and adapted) from Assembler.nw
You now have enough background to understand the source code of [[mk]].
%toc:
The rest of the book is organized as follows.
%
I will start by describing the core data structures of [[mk]]
in Chapter~\ref{chap:core-ds}. 
%
Then, I will use a top-down approach, starting with Chapter~\ref{chap:main}
with the description of [[main()]] and the initializations of [[mk]].
%
The following chapters will describe
the main components of the building pipeline:
Chapter~\ref{chap:parsing} will present the code to parse an [[mkfile]],
Chapter~\ref{chap:graph} the code to build the graph of dependencies,
Chapter~\ref{chap:finding-outdated} the code to find outdated files in the graph,
Chapter~\ref{chap:scheduling} the code to schedule jobs, and finally
Chapter~\ref{chap:shellenv} the code to communicate with the shell through
the environment.
%
In Chapter~\ref{chap:debugging-support}, I will present code to help you
debug and profile your [[mkfile]].
%
Chapter~\ref{chap:advanced} presents advanced
features of [[mk]] that I did not present before to simplify the explanations,
for instance, {rule attributes}.
%
Finally, Chapter~\ref{chap:conclusion} concludes
and gives pointers to other books in the \principia series.

%toc:
Some appendices present the code of non-functional properties:
code to help debug [[mk]] itself in Appendix~\ref{chap:debugging-appendix},
code which profiles [[mk]] itself in Appendix~\ref{chap:profiling-appendix},
and code to manage errors in Appendix~\ref{chap:error}.
%
Appendix~\ref{chap:libc} contains the code of generic
utility functions used by [[mk]] but which are not specific to [[mk]].
%
Finally, Appendix~\ref{chap:examples} presents examples of [[mkfile]]s.


%###############################################################################

\chapter{Core Data Structures}
\label{chap:core-ds}

\begin{verse}
\begin{flushright}
  {\it Show me your code and conceal your data structures, and I shall
    continue to be mystified. Show me your data structures, and I
    won't usually need your code; it'll be obvious.\\
    ~\\
    Fred Brooks}
\end{flushright}
\end{verse}


%toc:
In this chapter, I will present the core data structures of [[mk]]:
the {symbol table} (containing among other things the value of variables), 
the {list of rules} and {meta rules},
the {graph of dependencies},
and the description of a {job}.
\l appendix X for string buffer Bufblock? put also words there?
All those data structures are defined in the [[mk.h]] header file.

% core types: (see mk.h)
% Bufblock, Word
% Sym (a bit generic, could be in libc?)

% Rule (mk -dp?), recipe? metarule, target, prerequisite (see mk.ps)
% Node, Arc (mk -dg)
% Job, (mk -de)
% Envy (bad name, but presented later)

% core globals (see also mk.h externs):
% symbol hashtbl global (symbtab.c)
% rules, metarules, patrule,    jobs (globals.c)
% bout (globals.c)
% envy (env.c)


\section{Symbol table}

[[mk]] uses internally a {\em symbol table} to keep track
of different {things}:
the value of variables,
\l need manage variable of user.
the set of rules associated with a target,
\l need quick access to rules for a specific target.
the modification time of a file in the graph of dependencies,
etc.
\l node, dir bulk time?, more?
\l to find node corresponding to certain file (DAG) when building the graph

\l so need generic hashtbl.

\subsection{[[Symtab]]}

The structure below represents a symbol 
(e.g., variable, target, file)
and its property. 
It essentially associates a {\em key} to a {\em value}:

<<struct Symtab>>=
struct Symtab
{
    // the key: (name x space)

    // ref_own<string>
    char		*name;
    // enum<Namespace>, the ``namespace''
    short		space;

    // the value (generic)

    union{
        void*	ptr;
        uintptr	value;
    } u;

    // Extra
    <<[[Symtab]] extra fields>>
};
@
\l rename Sym? more consistent with 5a, 5l?
%ocaml: reuse hashtbl and use a record of hashtbl for namespaces
% so better types instead of overly generic union
\n is it the "world" data structure? hmm rules and metarules is more the world
\n here it's more an index over the rules, metarules, and some cache
\n  (e.g. time of a file)

Because the same string can be used
to represent a target, a file, or a variable, the key
in [[Symtab]] is a {pair} made of a string and an enumeration
constant called a {\em namespace} (stored in [[Symtab.space]]).
%
The first namespace is the one for variables:

<<enum Namespace>>=
enum Namespace {
    S_VAR,	/* variable -> value */ // value is a list of words
    <<[[Sxxx]] cases>>
};
@
\l actually should be S_CONST? well some variables can be overriden
\l  when passed on the command line, so technically it is kinda a variable
\l  but maybe better to differentate still
% use for fast access and also for memoization, operates as a cache
%dead:
%S_PID,		/* pid -> products */
%S_MAKEFILE,	/* target -> node */

%ex:
To look for the value of the variable [[$OBJS]], %$
you must use the key [[("OBJS", S_VAR)]].
\l I will speak later how, with [[symlook()]] function
\l to look for rules for target x, use [[(x,S_TARGET)]]
%chunks: 
I will gradually describe the other namespaces in the following chapters.

The value associated to a key can also be different things:
a list of words for a variable,
an integer representing a time for the modification time of a file, etc.
%
Thus, the value for a key in [[Symtab]] is a {union} containing
either an integer (in [[Symtab.u.value]]) or a generic
pointer (in [[Symtab.u.ptr]]).


\subsection{[[hash]]}

%dup: from Assembler
The symbol table itself is represented by a global 
{hash table} called [[hash]].
It makes sense to use a global because the symbol table will be accessed
by different components of the building pipeline.


One way to implement a hash table in C is to use a big array
of lists, also known as an array of {\em buckets}:

<<global hash>>=
// hash<(string * enum<Namespace>), 'a> (next = Symtab.next in bucket)
static Symtab *hash[NHASH];
@
<<constant NHASH>>=
#define	NHASH	4099
@
%ocaml: does not have to be a global, can be passed around
% if split in different parts for each namespace
% (var and internal vars, hnodes, time cache)

%dup: Assembler.nw
One way to implement a list of something in C is to embed in
this something a [[next]] field pointing to the next
element in the list:

<<[[Symtab]] extra fields>>=
// list<ref_own<Symtab>> (head = hash)
struct Symtab	*next;
@

%dup:
The end of the list is represented by the null pointer ([[nil]] in \plan).
% but no S in mk

% FIGURE? or too simple? does not hurt maybe, so can also see
% different namespaces?

%dup: (and adapted) from Linker.nw
The main interface to the symbol table is the
function [[symlook()]] below, which internally uses the global [[hash]].
%
[[symlook()]] takes a symbol name and a namespace, forming a full key, and 
returns the [[Symtab]] in the symbol table [[hash]] associated with this key.

\label{sec:symlook}
% many stuff -> <>
% will create new entry if not there!
<<function symlook>>=
Symtab*
symlook(char *sym, int space, void *install)
{
    Symtab *s;
    long h;
    <<[[symlook()]] other locals>>

    <<[[symlook()]] compute hash value [[h]] of [[sym]]>>

    // s = hash_lookup((sym, space), h, hash)
    for(s = hash[h]; s; s = s->next)
        if((s->space == space) && (strcmp(s->name, sym) == 0))
            return s;
    // else
    <<[[symlook()]] if symbol not found>>
}
@
%ocaml: just use Hashtbl.find
%compiler: called lookup in 5c (and 5a)
\t define symexists() that uses install == nil, cleaner?
\t  and return bool? #define symexists(sym, space) symlook(sym, space, nil) != nil



<<[[symlook()]] other locals>>=
char *p;
@
<<[[symlook()]] compute hash value [[h]] of [[sym]]>>=
//h = hash(sym, space)
for(p = sym, h = space; *p; h += *p++)
    h *= HASHMUL;
if(h < 0)
    h = ~h;
h %= NHASH;
@

<<constant HASHMUL>>=
#define	HASHMUL	79L	/* this is a good value */
@


If [[symlook()]] does not find the key and the install parameter is not
[[nil]], [[symlook()]] creates a new symbol:


<<[[symlook()]] if symbol not found>>=
if(install == nil)
    return nil;

s = (Symtab *)Malloc(sizeof(Symtab));
s->name = sym;
s->space = space;
s->u.ptr = install;

// add_list(s, hash)
s->next = hash[h];
hash[h] = s;

return s;
@

[[Malloc()]] called above is a small wrapper around
[[malloc()]] from the C library (see the \book{Libcore}).
%
[[Malloc()]] offers some [[mk]]-specific error management services,
as explained in Appendix~\ref{chap:error}.


In addition to [[symlook()]], [[mk]] relies also on the
generic function [[symtraverse()]] below to apply a function [[fn]]
to all elements in a specific namespace:

\label{sec:symtraverse}
% execinit -> <>
<<function symtraverse>>=
void
symtraverse(int space, void (*fn)(Symtab*))
{
    Symtab **s, *ss;

    for(s = hash; s < &hash[NHASH]; s++)
        for(ss = *s; ss; ss = ss->next)
            if(ss->space == space)
                (*fn)(ss);
}
@
%ocaml: used only for ecopy, so could use Hashtbl.copy?
\l used for instance for???



%dead:
%   main -> <>
%  <<function syminit>>=
%  void
%  syminit(void)
%  {
%      Symtab **s, *ss;
%  
%      for(s = hash; s < &hash[NHASH]; s++){
%          for(ss = *s; ss; ss = ss->next)
%              free((char *)ss);
%          *s = nil;
%      }
%  }
%  @
%   free necessary? should be nil no? syminit called at the start of main
%   but then no need for syminit at all!
%ocaml: use Hashtbl, which is initialized correctly by ocaml

%\subsection{[[setvar()]]}

\label{sec:setvar}
Finally, because setting the value of a variable is a common operation
in [[mk]], the function below provides a convenient
wrapper around [[symlook()]]:

% many stuff -> <>
<<function setvar>>=
void
setvar(char *name, void *value)
{
    symlook(name, S_VAR, value)->u.ptr = value;
}
@
%dead:    symlook(name, S_MAKEVAR, (void*)"");
%dead:
%<<[[Sxxx]] cases>>=
%S_MAKEVAR,	/* dumpable mk variable */
%@
%ocaml: since S_MAKEVAR is dead, can just use Hashtbl.set

\subsection{Namespaces}

%trans:
[[mk]] allows the user to define variables. [[mk]] also defines
{special variables} such as [[$stem]] or [[$target]].
%
To clearly separate those two kinds of variables, [[mk]] stores
them in different {namespaces}:
[[S_VAR]] for the user (and environment) variables, and
[[S_INTERNAL]] for the special (internal) variables.
\l can be in conflict with user variable  so multiple namespaces?
\l actually mk does not store value of special var in symbol table. just set.

<<[[Sxxx]] cases>>=
S_INTERNAL,	/* an internal mk variable (e.g., stem, target) */
@
%ocaml: use record
\l diff with S_VAR? not set by user! set by mk automatically
\l those one are really variables (maybe that's why they are in lowercase)

To know whether [[stem]] is the name of a special variable, call %$
the [[symlook()]] function with the pair [[("stem", S_INTERNAL)]].
\l can not used it to get the value of stem though! envy for that.

The {private global} [[specialvars]] below stores the list of special variables:

<<global myenv>>=
static char	*specialvars[] =
{
    "target",
    "prereq",
    "stem",

    <<[[myenv]] other array elements>>
    0,
};
@
\l $stem $prereq (better than $@, $^ )
\l but actually can be multiple targets, and prereqs!
%old: myenv -> specialvars, not an environment!

%chunks:
I will gradually describe the other internal variables used by [[mk]]
in the following chapters.
%chunks:
I will also gradually describe more namespaces.

[[specialvars]] is used to initialize entries in the symbol table
in [[inithash()]] called from [[main()]]:

\label{sec:inithash}
% main -> <>
<<function initenv>>=
void
inithash(void)
{
    char **p;

    for(p = specialvars; *p; p++)
        symlook(*p, S_INTERNAL, (void *)"");

    readenv();				/* o.s. dependent */
}
@
%old: initenv -> inithash, cos confusing with execinit (renamed initenv)
%bug? it is supposed to be words no? so at least newword("") ?

[[readenv()]] called above, and described in 
Section~\ref{sec:readenv}, initializes the symbol table
with the environment variables (e.g., [[$HOME]], [[$objtype]]).
%
[[mk]] will store those environment variables
in the [[S_VAR]] namespace.





\section{[[Word]]s}
\label{sec:word}
% and Bufblock

%trans:
There are many places in the code of [[mk]] where [[mk]]
manipulates a {list of words}:
when it manipulates a list of prerequisites, 
a list of targets,
or the content of a variable.
%ex: HFILES=fns.h mk.h => list of 2 words
C does not have any builtin support for lists, so [[mk]] uses the following
structure to represent a list of words:

<<struct Word>>=
struct Word
{
    // ref_own<string>
    char 		*s;

    // Extra
    <<[[Word]] extra fields>>
};
@
\n rename Words? but then less consistent with other use of C. 
\n You just need to better qualify the pointer each time in a comment.
\n But has to be Words; no much point in having Word just be a char* wrapper

<<[[Word]] extra fields>>=
// list<ref_own<Word>>
struct Word 	*next;
@
%ocaml: just use List, or even better W of word list


[[mk]] defines also a few convenient functions to manipulate those lists.
%
[[newword()]] below constructs a list with a single element from a string [[s]]:

\label{sec:newword}
<<constructor newword>>=
Word*
newword(char *s)
{
    Word *w;

    w = (Word *)Malloc(sizeof(Word));
    w->s = strdup(s);
    w->next = nil;
    return w;
}
@
\l rename newword_list? new_singleword?

[[freewords()]] frees a list of words:

<<destructor freewords>>=
void
freewords(Word *w)
{
    Word *v;

    while(v = w){
        w = w->next;
        if(v->s)
            free(v->s);
        free(v);
    }
}
@
%ocaml: just use the garbage collector
%old: was called [[delwords]]

[[addw()]] adds in a list of words [[w]] a word [[s]] (a string):

% treat list as a set, dont add if already there
<<function addw>>=
void
addw(Word *w, char *s)
{
    Word *lastw;

    for(lastw = w; w = w->next; lastw = w){
        if(strcmp(s, w->s) == 0)
            return;
    }
    lastw->next = newword(s);
}
@
%old: lw -> lastw, more consistent
%ocaml-found: but can not use Set, because then order
% may not be the order of insertion, and it is important
% since $prereq must be the same than the order in the prereq in the mkfile.

[[wdup()]] copies (duplicates) a list of words:

<<function wdup>>=
Word*
wdup(Word *w)
{
    Word *lastw, *new, *head;

    head = lastw = nil;
    while(w){
        new = newword(w->s);
        if(lastw)
            lastw->next = new;
        else
            head = new;
        lastw = new;
        w = w->next;
    }
    return head;
}
@
%old: v -> lastw, base -> head, more consistent
%ocaml: just use sharing done by ocaml. Pure DS.

Finally, [[wtos()]] (for ``words to string'') concatenates
together the words in a list of words with a special
character [[sep]] (for separator):

<<function wtos>>=
char *
wtos(Word *w, int sep)
{
    Bufblock *buf;
    char *cp;

    buf = newbuf();
    for(; w; w = w->next){
        for(cp = w->s; *cp; cp++)
            insert(buf, *cp);
        if(w->next)
            insert(buf, sep);
    }
    insert(buf, '\0');

    cp = strdup(buf->start);
    freebuf(buf);
    return cp;
}
@
%ocaml: Common.join (but using Buffer would be faster too)

[[wtos()]] relies on the [[Bufblock]] data structure
described in Appendix~\ref{sec:bufblock}.
%
[[Bufblock]] is an implementation of a {\em string buffer}.
It implements efficiently string concatenation to avoid quadratic
complexity when concatenating a set of strings together.
\l will use Bufblock many times. 
%ocaml: use Buffer.insert
The code for [[Bufblock]] is in [[mk/bufblock.c]], but 
this code could be put in a library and used by other projects
as it is a general-purpose data structure. This is why
its code is described in Appendix~\ref{chap:libc} and not here.
\l but then Word could be there too

\l stow() later, but complex because does variable expansion!



\section{Rules}

%trans:
As mentioned in Section~\ref{sec:rule-essence}, the {rule}
is one of the most important concepts in a build system, and
so one of the most important data structures in [[mk]].
%
It represents the content of an [[mkfile]] and it guides
the creation of the graph of dependencies.

\subsection{[[Rule]]}

The structure [[Rule]] below represents a {rule} in memory.
You can see that the first fields represent the major
elements of a rule I mentioned in Section~\ref{sec:rule-simple-mkfile}:
the target, the prerequisites, and the recipe.

<<struct Rule>>=
struct Rule
{
    // ref_own<string>
    char 		*target;	/* one target */
    // list<ref_own<Word>>
    Word 		*prereqs;		/* constituents of targets */
    // ref_own<string>, never nil, but can be the empty string (just '\0')
    char 		*recipe;	/* do it ! */

    <<[[Rule]] other fields>>
    <<[[Rule]] debug fields>>

    // Extra
    <<[[Rule]] extra fields>>
};
@
%old: tail -> prereqs, because tail abused (and prereqs more precise)
\l seems like recipe can not be nil actually, cos do *a->r->recipe
\t recipe can also just be '\0' ? ugly.
\t  can be nil? sometimes seems assume cannot, and sometimes can. fix it!
\t  according to rbody(), it cant be nil, at least it is empty string
\t see empty_recipe() macro

[[Rule.prereqs]] contains a list of words, hence the use of a pointer
to a [[Word]] (I described [[Word]] in Section~\ref{sec:word}).
%
[[Rule.target]] is a single string, not a list of words, 
even though some rules have multiple targets. 
I will explain later how rules with multiple targets are represented.
%
\l note that can have multiple rules with the same head, 
\l in which case you add prerequisites?
[[Rule.recipe]] is a string. This string can contain variables 
using the dollar sign.
However, the strings in [[Rule.target]] and [[Rule.prereqs]]
do not contain any variable. Indeed, as I will show in 
Section~\ref{sec:expand-vars}, [[mk]] expands 
variables used outside a recipe at parsing time.
% from man page:
%  Variable substitution in a rule is done when
%  the rule is read; variable substitution in the recipe is done
%  when the recipe is executed.
\l why? probably because recipe/rc will do the subst itself, but for
\l the graph we need concrete values so faster to do it eagerly when parsing.
% Also because for recipe, some values are known later, when instantiate 
% the rule (e.g., for $stem)
% can also be metarule, %.$O: %.c (but $O would have been expanded)
\l can contain percent

In addition to the major fields mentioned above, [[Rule]] contains
also information about where a rule comes from:

<<[[Rule]] debug fields>>=
char* 		file;		/* source file */
short 		line;		/* source line */
@
\l put in Debugging support section?

Those fields will be useful when 
reporting errors in the [[mkfile]] to the user.



\subsection{Simple [[rules]]}

The list of all {\em simple rules}, that is all non-meta rules, is stored
in the global [[rules]]:

<<global rules>>=
// list<ref_own<Rule>> (next = Rule.next, end = lr)
Rule *rules;
@
%ocaml: return them instead of using globals

When [[mk]] parses an [[mkfile]] (and possibly some included files),
it populates this global (using the [[addrule()]] function
described below in Section~\ref{sec:addrule}).

Again, in C, you can embed a [[next]] field in a structure to make it a
list:

<<[[Rule]] extra fields>>=
// list<ref_own<Rule>> (head = rules | metarules)
struct Rule	*next;
@

To quickly add a rule to the end of the list [[rules]], [[mk]]
maintains another global [[lr]] pointing to the last rule:

<<global lr>>=
// option<ref<Rule>> (head = rules)
static Rule *lr;
@

\subsection{[[metarules]]}

The list of all {\em meta rules} is stored instead in an another global:

<<global metarules>>=
// list<ref_own<Rule>> (next = Rule.next, end = lmr)
Rule *metarules;
@
%ocaml: return them instead of using globals

[[mk]] relies also on a global pointing to the last meta rule:

<<global lmr>>=
// option<ref<Rule>> (head = metarules)
static Rule *lmr;
@

Remember that a meta rule is a rule using the {special character} [['%']]
to specify a {\em pattern} in the target or prerequisites of a rule
(see Section~\ref{sec:pattern}).
In fact, [[mk]] supports another special character to represent
a pattern: [['&']], hence the code in the macro below:

\label{sec:PERCENT}
<<function PERCENT>>=
#define	PERCENT(ch)	(((ch) == '%') || ((ch) == '&'))
@

The difference between those two special characters
is explained in the manual page of [[mk]]:
\begin{itemize}

\item [['%']] matches a maximal length string of any characters

\item [['&']] matches a maximal length string of any characters 
except period or slash
\end{itemize}

Section~\ref{sec:ampersand-example} gives an example where the difference
between the two characters matter.
 
In addition to being stored in [[metarules]] instead of [[rules]],
a meta rule contains also the [[META]] {\em rule attribute} in 
[[Rule.attr]]:

<<[[Rule]] other fields>>=
// bitset<Rule_attr>
short 		attr;		/* attributes */
@
\l contain visible rule attribute between :: and some implicit like META

<<enum Rule_attr>>=
enum Rule_attr {
    META   = 0x0001,
    <<[[Rule_attr]] cases>>
};
@
\l could do RA_META? more consistent with S_VAR ?
%dead: indeed, unused :)
% UNUSED = 0x0002,
%dead:
% UPD    = 0x0004,
% <<[[rhead()]] when parsing rule attributes, switch rune cases>>=
% case 'U':
%     *attr |= UPD;
%     break;
% @
% "the targets are considered to have been updated even if recipe did not do so"

\l META is for metarule (and also regexp rules :R:)

%chunks: 
%I will describe gradually the other rule attributes in the following chapters.
\l users can add explicit attributes also to a rule
Most of the other rule attributes correspond to advanced features
of [[mk]] I will describe in Section~\ref{sec:rule-attributes-advanced}.


\subsection{Adding rules}
\label{sec:addrule}
% and indexing

Now that I described the data structures and globals related to
the rules and meta rules, I can explain the code to add rules.

\subsubsection{One rule with one target, [[addrule()]]}
\label{sec:addrule}

[[addrule()]] below adds a rule with a single target
(I will explain later the code to support rules with multiple targets):


% kind of ctor
% (main -> parse | main) -> <>
<<function addrule>>=
void
addrule(char *target, Word *prereqs, char *recipe, 
        Word *alltargets, int attr, int hline, char *prog)
{
    Rule *r = nil;
    <<[[addrule()]] other locals>>

    <<[[addrule()]] find if rule already exists, set reuse>>

    if(r == nil)
        r = (Rule *)Malloc(sizeof(Rule));

    r->target = target;
    r->prereqs = prereqs;
    r->recipe = recipe;

    r->attr = attr;
    r->line = hline;
    <<[[addrule()]] set more fields>>

    <<[[addrule()]] indexing [[r]] by target in [[S_TARGET]]>>

    <<[[addrule()]] if meta rule>>
    else {
        <<[[addrule()]] if simple rule>>
    }
}
@
%old: ahead -> alltargets, head -> target, tail -> prereqs, body -> recipe
%ocaml: use record, and no need wrapper constructor, construct directly with
% the fields

If the rule is a simple rule, [[mk]] populates [[rules]]:

<<[[addrule()]] if simple rule>>=
<<[[addrule()]] return if reuse, to not add the rule in a list>>
// else

// add_list(r, rules, lr)
if(rules == nil)
    rules = lr = r;
else {
    lr->next = r;
    lr = r;
}
@
%dead? needed? r->pat = nil;
\l r->pat ?? Malloc does not memset zero anyway?

If the rule is a meta rule, [[mk]] populates [[metarules]].
[[mk]] detects if the rule is a meta rule simply by looking
whether the target contains one of the special pattern character:

<<[[addrule()]] if meta rule>>=
if(charin(target, "%&") || (attr&REGEXP)){
    r->attr |= META;
    <<[[addrule()]] return if reuse, to not add the rule in a list>>
    // else
    <<[[addrule()]] if REGEXP attribute>>

    // add_list(r, metarules, lmr)
    if(metarules == nil)
        metarules = lmr = r;
    else {
        lmr->next = r;
        lmr = r;
    }
}
@

I will describe the function [[charin()]] later in Section~\ref{sec:charin}.
%
Note that [[charin()]] does not just search for a set of character 
in a string.
% (here [[head]], the target). 
Indeed, [[charin()]] must
also handle {\em escaped characters}, for instance, when the target name
is put inside a quote as in [['myfile%has%weird%characters.doc']],
in which case the target should not be considered a pattern.

For the rule attribute [[REGEXP]] used above, see Section~\ref{sec:regexp}.


\subsubsection{One target with multiple rules, [[S_TARGET]]}
\label{sec:indexing-target}

It is useful when building the graph of dependencies to quickly
know the rule associated to a specific target.
%
Thus, [[mk]] uses another namespace, [[S_TARGET]], to store
such information in the symbol table.

<<[[Sxxx]] cases>>=
S_TARGET,		/* target -> rules */ 
@
%old: was 'target -> rule' but really it is -> rules
%ocaml: build hash in separate phase

In fact, as I mentioned in Section~\ref{sec:master-rule}, [[mk]]
allows the user to write multiple rules using the same target.
%
For instance, an [[mkfile]] can contain a {master rule} such as
[[foo.5: foo.c ...]], and a [[.depend]] file included by this [[mkfile]]
can contain another rule without any recipe but extra dependencies such as
[[foo.5: foo.h bar.h]].
\l need :: ? will get ambiguous rule?
Thus, the symbol table and the namespace [[S_TARGET]] map a target
to a set of rules chained together by an extra field in [[Rule]]:

<<[[Rule]] extra fields>>=
// list<ref<Rule>> (head = symlook(x, S_TARGET))
struct Rule	*chain;		/* hashed per target */
@
% FIGURE note that Rule.chain is different than Rule.next! And also Sym.next!
% Here in same list because same target.
% For Sym.next in same list because same hashed (name x namespace).
% Can even have symbol in different namespace in the same bucket in Sym.next.
% "Rules are linked together with Rule.next so you can iterate
%  over all the rules in a mkfile.
%  Some rules share the same target and are chainted together with
%  the Rule.chain, as shown in Figure X"


Here is the code to update the symbol table when adding a rule:

<<[[addrule()]] other locals>>=
Symtab *sym;
Rule *rr;
@
<<[[addrule()]] indexing [[r]] by target in [[S_TARGET]]>>=
if(!reuse){
    sym = symlook(target, S_TARGET, r);
    rr = sym->u.ptr;
    if(rr != r){ // target had already a rule
        r->chain = rr->chain;
        rr->chain = r;
    } else 
        r->chain = nil;
}
@
%old: rr = symlook(head, S_TARGET, r)->u.ptr;
\t why add in S_TARGET if meta rule? anyway skipped after in applyrules
\t  and this forces some extra check in applyrules actually

Remember from Section~\ref{sec:symlook} that the last parameter
of [[symlook()]], called [[install]] (and here set to the argument [[r]]),
is used to initialize a new symbol if the symbol was not already
in the symbol table.
%
Thus, if the test [[if (rr != r)]] above succeeds, this means
a symbol was already there, in which case [[mk]] needs to add
the rule [[r]] to the chain.

I will explain the guard using the variable [[reuse]] above in the next section.


\subsubsection{Overwriting a previous rule}
\label{sec:overwrite-rule}

[[mk]] allows to overwrite the recipe of
a rule when another rule uses exactly the same target and prerequisites.
%
This can be useful when a generic [[mkfile.generic]] file
defines some default targets and recipes, but the user wants to overwrite
those defaults in his own [[mkfile]] (which can include [[mkfile.generic]]).
\l could be done with variables, but true that sometimes difficult

The code below detects whether a previous rule was using the
same target and prerequisites, in which case [[mk]] needs to
reuse and overwrite this previously allocated rule:

<<[[addrule()]] other locals>>=
bool reuse;
@
<<[[addrule()]] find if rule already exists, set reuse>>=
reuse = false;
if(sym = symlook(target, S_TARGET, nil)){
    for(r = sym->u.ptr; r; r = r->chain)
        if(rcmp(r, target, prereqs) == 0){
            reuse = true;
            break;
        }
}
@
%ocaml: return a warning at least, but overwrite otherwise it will
% be detected by ambiguous later.
%ocaml: can have a :O: attribute clearly saying you overwrite! @override spirit.

Note that the code above relies on the indexing of rules in [[S_TARGET]]
from the previous section.


<<function rcmp>>=
static int
rcmp(Rule *r, char *target, Word *prereqs)
{
    Word *w;

    if(strcmp(r->target, target))
        return 1;
    for(w = r->prereqs; w && prereqs; w = w->next, prereqs = prereqs->next)
        if(strcmp(w->s, prereqs->s))
            return 1;
    return (w || prereqs);
}
@
%old: tail -> prereqs
\t strcmp sucks, use string_equal, and return bool that makes sense
\t  so less rcmp(...) == 0, instead rule_equal(...)
%bug? what about when different orders of prereqs? should compare as a set.
% it will be detected as ambiguous then

%redundant:
If [[addrule()]] overwrites (reuses) a previous rule,
the [[Rule.next]] field of this rule should not be modified.
Otherwise, [[Rule.next]] should be set to [[nil]]:

<<[[addrule()]] set more fields>>=
if(!reuse){
    r->next = nil;
}
@
%old: was after the indexing of S_TARGET, but seems unrelated,
% so I put it earlier, with the code to set misc fields.

<<[[addrule()]] return if reuse, to not add the rule in a list>>=
if(reuse)
    return;
@




\subsubsection{One rule with multiple targets, [[addrules()]]}
\label{sec:multi-targets}
\n was in advanced topics, but  maybe fundamental features then.
\n Also too much code doing stuff to support multiple targets.

%trans:
I can now show the code to handle rules with multiple targets.
%
[[mk]] uses the function [[addrules()]] below to
add separate rules for each target in the original rule:

<<function addrules>>=
void
addrules(Word *targets, Word *prereqs, char *recipe, 
         int attr, int hline, char *prog)
{
    Word *w;

    assert(/*addrules args*/ targets && recipe);

    <<[[addrules()]] set [[target1]]>>
    for(w = targets; w; w = w->next)
        addrule(w->s, prereqs, recipe, targets, attr, hline, prog);
}
@
%old: head -> targets, tail -> prereqs, body -> recipe

%trans: dup: overview/graph/
As I mentioned in Section~\ref{sec:graph-many-to-one}, 
the use of multiple targets in a rule has implications on the
DFS traversal of the graph of dependencies:
[[mk]] needs to remember the other targets associated with a rule.
%
This is why in addition to passing [[w->s]] above, [[addrules()]]
passes also the set of targets in the fourth argument to [[addrule()]].
%
This argument is then stored in a special field in the rule:

<<[[Rule]] other fields>>=
// ref<list<ref_own<string>>
Word 		*alltargets;	/* all the targets */
@
% as in  gram.c gram.h: gram.y ...
% (hmm but could be rewritten with 2 rules, it's just
% that with mk the file would be possibly rebuilt two times!
% because it could run the command in parallel!!!!
%ocaml: merge with Rule.target, and make it a list instead of char*

<<[[addrule()]] set more fields>>=
r->alltargets = alltargets;
@

% thus even if add a rule with one target (in Rule.target),
% mk remembers other targets in the rule:

% Is there a few places where use Word* targets
% where in most cases it should really be simply char* target; ??

\l why target and alltargets? why not just have alltargets?
\l because we index per single target so in applyrules we can be
\l faster (but still slow on metarules)


%\subsubsection{Extracting the first target}
%target1

\section{Graph}

%trans:
% Rule and metarules are general static descriptions, patterns.
% Node and Arc are concrete instantiation with existing files.

The graph of dependencies is represented in [[mk]] essentially
as a set of {nodes} linked together through pointers.
%
[[mk]] does not use a matrix or an array of adjacent lists
to represent a graph; it just uses pointers, as you will see in
the following sections.

%toc: 
\l nodes and arcs, toc

\subsection{[[Node]]}

A node represents a file in the graph of dependencies.
%
As I mentioned in Section~\ref{sec:labeled-tree},
a node is also {labeled} with the modification time of the file.
That way, the DFS can find out-of-date files by comparing
the [[Node.time]] field in different nodes.

<<struct Node>>=
struct Node
{
    // ref_own<string>, usually a filename, or a virtual target like 'clean'
    char*		name; 
    // option<Time> (None = 0, for nonexistent files and virtual targets)
    ulong		time; // last mtime of file 

    <<[[Node]] arcs field>>
    <<[[Node]] other fields>>

    // Extra
    <<[[Node]] extra fields>>
};
@
\l name can also maybe be an archive member? like lib.a(foo.o) ?

The function below constructs a new node:

\label{sec:newnode}
%ctor. dtor = ?
% main -> mk -> graph -> applyrules -> <>
<<constructor newnode>>=
static Node*
newnode(char *name)
{
    Node *node;

    node = (Node *)Malloc(sizeof(Node));
    <<[[newnode()]] update node cache>>

    node->name = name;
    // call to timeof()! 
    node->time = timeof(name, false);
    node->flags = 0;
    <<[[newnode()]] adjust flags of node>>

    node->arcs = nil;
    node->next = nil;
    <<[[newnode()]] debug>>
    return node;
}
@
\n owning name after call! => call strdup in caller

% pass false, cos ok to use cache. nothing has been generated so nothing
%  can have invalidate the cache.
\t rename false to USE_CACHE_IS_OK, vs FORCE_READ_NO_CACHE
\l node cache? put now?

A node is also labeled with a set of {\em node attributes}:
\n seen rule attributes before

\label{sec:node-flags}
<<[[Node]] other fields>>=
// bitset<enum<Node_flag>>
ushort		flags;
@
<<enum Node_flag>>=
enum Node_flag {
    <<[[Node_flag]] cases>>
};
@
%old: used to be a set of #define
\l add N_ as a prefix? consistent with S_ for namespace

The building status of a node ([[Made]], [[NotMade]], and [[BeingMade]]),
which I introduced in Section~\ref{sec:build-status}, 
is stored in [[Node.flags]] 
(as well as other information used for advanced features of [[mk]]):

<<[[Node_flag]] cases>>=
NOTMADE    = 0x0020,
BEINGMADE  = 0x0040,
MADE       = 0x0080,
@
%ocaml: should be different field than other node flags
\l could use instead (1 < < 2) (1 < < 3) etc


%chunks: 
I will gradually describe the other {node attributes} in the following chapters.
\l mostly used during analysis of graph.
\l often also an inline of the union of the flags of the rule 
\l  linking to the prerequisites nodes

\subsection{[[Arc]]}

A [[Node]] contains also a set of arcs where each arc
contains a pointer to another node (a prerequisite):

<<[[Node]] arcs field>>=
// list<ref_own<Arc>> (next = Arc.next)
Arc		*arcs;
@
%old: prereqs -> arcs; better call it arcs because
% not always a prereq as for instance virtual rule have an arc
% from the target without any destination node

<<struct Arc>>=
struct Arc
{
    // option<ref<Node>>, the other node in the arc (the dependency)
    struct Node *n;
    // ref<Rule>, to generate the target node from the dependent node
    Rule *r;

    <<[[Arc]] other fields>>
    
    //Extra
    <<[[Arc]] extra fields>>
};
@
\n option Rule? I dont think so, every call to newarc have a defined rule arg
%ocaml: I think you just need the recipe and some flags of the rule,
% so separate rule vs rule_exec

As I mentioned in Section~\ref{sec:labeled-tree}, an arc is labeled
with a rule, hence the field [[Arc.r]] above.
%
Note that [[Arc.n]] can sometimes be [[nil]] when a rule does not have
any prerequisite (for instance, because it is a virtual target,
as explained in Section~\ref{sec:virtual}). In that case,
we still want the node corresponding to the target of the rule
to be connected to a rule, especially its recipe.
%alt: could have rule in Node, actually master rule in Node.
\l seen Rule before.

The head of the list of arcs of a node is stored in [[Node.arcs]],
but the arcs are chained together with the following field:

<<[[Arc]] extra fields>>=
// list<ref_own<arc> (head = Node.arcs)
struct Arc	*next;
@


Some nodes and arcs are derived from meta rules. 
For instance, in Figure~\ref{fig:graph-hello},
the nodes [[hello.5]] and [[hello.c]] could come from
a meta rule such as [[%.5: %.c ...]].
%
In that case, [[mk]] needs to remember in the arc 
connecting [[hello.5]] to [[hello.c]] the {\em stem}
that was used to instantiate the meta rule (here [[hello]]):

% so can set $stem for the actual value for the recipe to run
<<[[Arc]] other fields>>=
// option<ref_own<string>>, what '%' matched?
char		*stem;
@
\l why need that? Node.name is not expanded already?
%  yes, in graph, but in recipe still need to set dollar stem in env
\l here not in Rule because Rule is generic template; Here it is instantiated.
% Also used because in dorecipe we also use alltargets, which is not
%  expanded.
%ocaml: do a rule_exec containing all necessary information

The function below constructs a new arc
that can be added later to the list of arcs of a source node.
%
This arc will connect the source node to a destination node [[n]], 
with the rule [[r]], possibly instantiated
with the stem [[stem]] if the rule was a meta rule
(the last parameter [[match]] is used for regexp rules,
as explained in Section~\ref{sec:regexp}):

% ctor.  dtor = ?
% main -> mk -> graph -> applyrules -> <>
<<constructor newarc>>=
Arc*
newarc(Node *n, Rule *r, char *stem, Resub *match)
{
    Arc *a;

    a = (Arc *)Malloc(sizeof(Arc));
    a->n = n;
    a->r = r;
    a->stem = strdup(stem);

    a->next = nil;
    <<[[newarc()]] set other fields>>
    return a;
}
@
\l assert r ? (but not assert n)




\section{[[Job]] and [[jobs]]}
\label{sec:job}

%trans:
Finally, the last core data structure of [[mk]] is the
%
description of a job.
%
As I mentioned in Section~\ref{sec:job-scheduler-principles}, a job must contain
all the information needed to run a recipe and to update
the graph of dependencies: a rule (and its recipe), 
a list of nodes to update, and the value of special
variables such as [[$target]], [[$prereq]], or
[[$stem]]:


<<struct Job>>=
struct Job
{
    // ref<Rule>
    Rule		*r;	/* master rule for job */

    // list<ref<Node>> (next = Node.next)
    Node		*n;	/* list of node targets */

    // $target and $prereq
    // list<ref<Word>>
    Word		*t;	/* targets */
    // list<ref<Word>>
    Word		*p;	/* prerequisites */
    // ref<string> ($stem)
    char		*stem;

    <<[[Job]] other fields>>

    // Extra
    <<[[Job]] extra fields>>
};
@
%ocaml: I think you just need the recipe and some flags of the rule.
\l why need targets? info is already in node. To ease envupd?
\l why so short field names suddenly? avoid confusion with the one in Rule? meh
\l not owned, ref to words in rules or the graph
%$

% reference rule, node (so Arc) (and Word but less important)

% have list of nodes because finishing a job will modify
% the graph! put some MADE

% targets! prerequisites! plural!

% stem again? propagated from Arc? to pass to shell for dollar stem setting
%ocaml: factorize in rule_exec

The list of target nodes of a job are chained together through
an extra field in [[Node]]:
\t list because rules may have multiple targers, as in Figure X
\t  and important modify time of all nodes!

\label{sec:nodes}
<<[[Node]] extra fields>>=
// list<ref<Node>> (head = Job.n)
struct Node	*next;		/* list for a rule */
@
% list of all targets for a job (with a master rule), as when done
%  will need to mark as done multiple files (and check they are there)
% usually have set of nodes via Arc, but here, no need target (parent) node.

The function below constructs a new job:

% ctor
<<constructor newjob>>=
Job*
newjob(Rule *r, Node *nlist, char *stem, char **match, 
       Word *allprereqs, Word *newprereqs, 
       Word *alltargets, Word *oldtargets)
{
    Job *j;

    j = (Job *)Malloc(sizeof(Job));
    j->r = r;
    j->n = nlist;
    j->p = allprereqs;
    j->t = oldtargets;

    j->stem = stem;
    j->match = match;

    j->np = newprereqs;
    j->at = alltargets;

    j->next = nil;
    return j;
}
@
%old: 
% - tar -> oldtargets, atar -> alltargets, pre -> allprereqs, npre -> newprereqs
% - change order of parameters, put alltargets first
\t rename fields, aspectize setting certain fields


This job can then be added in the job queue, which is stored in the
global [[jobs]]:

<<global jobs>>=
// list<ref_own<jobs>> (next = Job.next)
Job *jobs;
@
\n here cos used by mk(). Cant delayed.
\t actually aspectized now, so maybe could delay? or maybe useful
\t  to know there is a list of jobs
%ocaml: not sure you need a global, can be a local in run.ml?
% also use a Queue

<<[[Job]] extra fields>>=
// list<ref_own<Job>> (head = jobs)
struct Job	*next;
@

%trans:?
% draw diagram showing relations to each other, what includes what?









\chapter{[[main()]]}
\label{chap:main}

%trans: %dup: (and adapted) from Assembler.nw
I now switch from the bottom-up approach of Chapter~\ref{chap:core-ds}
to a top-down approach;
%toc:
I will describe in the following chapters the main functions of [[mk]], 
starting in this chapter with [[main()]], the entry point of [[mk]].

\section{[[main()]] skeleton}

The main components of the building pipeline are illustrated in the
[[main()]] skeleton below:

<<function main>>=
void
main(int argc, char **argv)
{
    <<[[main()]] locals>>

    // Initializing

    <<[[main()]] initializations>>

    // Parsing the mkfile

    <<[[main()]] parsing mkfile, call [[parse()]]>>

    // Building the graph, finding out-of-date files

    <<[[main()]] initializations before building>>
    <<[[main()]] setting the targets, call [[mk()]]>>

    // Reporting (optional)

    <<[[main()]] print profiling stats if uflag>>

    // Exiting

    exits(nil);
}
@

%trans:
The next chapters will detail those different components.
%toc:
In this chapter, I will focus mostly on the initializations
and the processing of command-line arguments.

An important global set by [[main()]] is [[bout]]:

<<global bout>>=
Biobuf bout;
@
%ocaml: stdlib already buffer output

[[mk]] uses this global to print messages to the user 
(e.g., errors, job progress, profiling information). 
[[bout]] is a buffer connected to the standard output:

<<[[main()]] initializations>>=
Binit(&bout, STDOUT, OWRITE);
@

[[Biobuf]], the type of  [[bout]], is a data structure defined
in the [[libbio]] (for ``buffered IO'') library, which
extends the C library (see the \book{Libcore}).
\l  compared to scanf/printf? No big win vs printf I think.
\l  actually use fprintf(STDERR,) for other stuff
\l appendix X for quick ref?


[[mk]] processes the command-line arguments in three steps,
as hinted in the code below, and as explained in the following sections.

<<[[main()]] initializations>>=
<<[[main()]] argv processing part 1, -xxx>>
<<[[main()]] setup profiling>>
inithash();
<<[[main()]] argv processing part 2, xxx=yyy>>
<<[[main()]] set variables for recursive mk>>
<<[[main()]] argv processing part 3, skip xxx=yyy>>
<<[[main()]] profile initializations>>
@
%old: 
% - the processing part 3 was before catchnotes() before but I think better here
% - calls to second usage was earlier
%dead: syminit();
\l also profiling stuff and recursive mk, explained later.
\t can put inithash before?
\t can put profiling closer together?

%/*
% *  start with a copy of the current environment variables
% *  instead of sharing them
% */
\t for execinit?? for initenv/

As I explained in Section~\ref{sec:inithash}, [[inithash()]] called above
initializes the special variables in the symbol table 
(e.g., [[$target]], [[$prereqs]]),
and {imports} variables from the environment in the symbol table 
(e.g., [[$objtype]], [[$HOME]]).
%$

\section{[[mk -]]{\em flag} arguments processing}

%trans:
The first step in the processing of command-line arguments
is an iteration over [[argv]]:

<<[[main()]] argv processing part 1, -xxx>>=
USED(argc);
for(argv++; *argv && (**argv == '-'); argv++)
{
    <<[[main()]] add [[argv[0]]] in [[buf]]>>

    switch(argv[0][1]) {
    <<[[main()]] -xxx switch cases>>
    default:
        badusage();
    }
}
@
%ocaml: use Arg module

This iteration looks for command-line arguments prefixed by [['-']]
(e.g., [[-f]]).
\l or debug flags.
%chunks:
I will gradually described the cases of the [[switch]] above 
in the following chapters.
\l note that modify argv!
\l also special code for buf? explained later

\section{[[mk ]]{\em var}[[=]]{\em values} arguments processing}
\label{sec:vardef-command-line}

%trans:
The second step in the processing of command-line arguments
is also an iteration over [[argv]], but this time looking
for arguments containing an equal sign.
%
Indeed, [[mk]] allows the user to overwrite variables defined
in the [[mkfile]] by adding a command-line argument
in the form [[x=y]] before the target, as in [[mk objtype=arm all]].


Because the parser of [[mkfile]] I will describe in Chapter~\ref{chap:parsing}
contains already code to process variable definitions, [[mk]]
reuses this code to deal with command-line definitions.
%
Indeed, after storing those definitions in a temporary file, [[mk]] can
then simply call [[parse()]] on this temporary file to load those definitions.


The temporary file is first 
a filename ([[temp]]), then 
a file descriptor once opened ([[tfd]]), and finally 
an output buffer once initialized ([[tb]]):

<<[[main()]] locals>>=
char *temp = nil;
fdt tfd = -1;
Biobuf tb;
int i;
@

<<[[main()]] argv processing part 2, xxx=yyy>>=
for(i = 0; argv[i]; i++) 
  if(utfrune(argv[i], '=')){
    <<[[main()]] add [[argv[i]]] in [[buf]]>>

    <<[[main()]] create temporary file if not exist yet and set [[tb]]>>
    Bprint(&tb, "%s\n", argv[i]);
    <<[[main()]] mark [[argv[i]]] for skipping>>
  }

if(tfd >= 0){
    Bflush(&tb);
    seek(tfd, 0L, SEEK__START);
    parse("<command line args>", tfd, true);
    remove(temp);
}
@
% abuse parse(), pass true so will get varoverride to true
%  so those variables will be marked specially S_OVERRIDE
%old: seek() used to be LSEEK() a macro expanding ultimately to seek

[[utfrune()]] called above is a function looking for a certain character
in a string. However, the character and the string use a particular format.
Indeed, [[utfrune()]] looks for a {rune} in a sequence of UTF-8 
encoded characters.
%
In \plan, a {\em rune} is the term used to represent a {Unicode} character.
\l cite unicode?
%
There are multiple ways to encode a Unicode character (a rune) in
a sequence of bytes.
%
\plan uses the UTF-8\footnote{
The popular UTF-8 encoding was designed by Rob Pike 
and Ken Thompson, two of the designers of \plan.
See \url{http://doc.cat-v.org/bell_labs/utf-8_history} for the history
of UTF-8.
} encoding, hence the use of the [[utfrune()]] function above.
%
Indeed, \plan supports filenames using Unicode characters, as well
as command-line arguments using Unicode characters, which are encoded
with the UTF-8 format
(see the \book{Libcore} for more information on Unicode and [[utfrune()]]).


The last argument to [[parse()]] above is a boolean indicating
whether [[parse()]] should accept definitions overwriting previous definitions.
\l called before mkfile actually, so more marked as non-overwritable
%
Obviously, here [[main()]] passes [[true]] to [[parse()]].


<<[[main()]] create temporary file if not exist yet and set [[tb]]>>=
if(tfd < 0){
    temp = maketmp();
    <<[[main()]] when creating temporary file, sanity check temp>>
    tfd = create(temp, ORDWR, 0600);
    <<[[main()]] when creating temporary file, sanity check tfd>>
    Binit(&tb, tfd, OWRITE);
}
@

The code above omits the error management code shown below:

<<[[main()]] when creating temporary file, sanity check temp>>=
if(temp == nil) {
    perror("temp file");
    Exit();
}
@
\l perror?  Exit? actually could be exits() here I think

<<[[main()]] when creating temporary file, sanity check tfd>>=
if(tfd < 0){
    perror(temp);
    Exit();
}
@

%dup: Windows.nw
In the rest of this book, I will usually not comment the error-management code. 
Such code is often trivial (but necessary).



The last step in the processing of command-line arguments
is to skip variable definitions:

<<[[main()]] mark [[argv[i]]] for skipping>>=
/*
 *   assignment args become null strings
 */
*argv[i] = '\0';
@

<<[[main()]] argv processing part 3, skip xxx=yyy>>=
/* skip assignment args */
while(*argv && (**argv == '\0'))
    argv++;
@
% but what if mix target and assignments? then will get an error?
\t test it  mk XX=1 default YY=2,  get good error message?

Once mk has cleaned up [[argv]], the strings remaining
in [[argv]] are the targets the user wants to build.


\section{Parsing the [[mkfile]] or [[mk -f]]{\em file}}

%trans:
I described before in Section~\ref{sec:getting-started}
the use of [[-f]] to change the default file used by [[mk]]:

<<[[main()]] locals>>=
char *f = nil;
@
% can have more than one file? can use multiple -f, but meh.
%old: char *files[256];
%old: char **f = files;
%ocaml: one -f is enough

<<[[main()]] -xxx switch cases>>=
case 'f':
    if(*++argv == nil)
        badusage();
    f = *argv;
    <<[[main()]] add [[argv[0]]] in [[buf]]>>
    break;
@
% saw -xxx processing before.
%old:    *f++ = *argv;
%old: <<[[main()]] locals>>=
%old: char **ff;
%old: @


<<[[main()]] parsing mkfile, call [[parse()]]>>=
if(f == nil){
    if(access(MKFILE, AREAD) == OK_0)
        parse(MKFILE, open(MKFILE, OREAD), false);
} else
    parse(f, open(f, OREAD), false);
<<[[main()]] if DEBUG(D_PARSE)>>
@
%old: use AREAD and OREAD instead of 4 and 0 argument
%old:
%  if(f == files){
%      if(access(MKFILE, AREAD) == OK_0)
%          parse(MKFILE, open(MKFILE, OREAD), false);
%  } else
%      for(ff = files; ff < f; ff++)
%          parse(*ff, open(*ff, OREAD), false);

%bug? if mkfile not there, no error? it displays "mk: nothing to mk"
% because target will be empty

The [[parse()]] function called above will process the [[mkfile]]
(or another file if [[-f]] was used) and modify
[[rules]], [[metarules]], as well as a few other globals.
%
Note that this time [[main()]] passes [[false]] to [[parse()]], so overwriting
variable definitions (e.g., the ones given on the command-line) is disabled.
\l is it actually? or it is just not processed and silently passed over?
\l and why want that? give example?

\section{Building the target(s)}

%trans:
Once [[parse()]] processed the [[mkfile]] and modified some globals,
[[mk]] is ready to build a target.
%toc:
There are multiple ways to specify the target to build and how
to build it, as explained in the following sections,
and as hinted by the following code:

<<[[main()]] setting the targets, call [[mk()]]>>=
if(*argv == nil){
    <<[[main()]] when no target arguments>>
} else {
    <<[[main()]] if sequential mode and target arguments given>>
    else {
       <<[[main()]] parallel mode and target arguments given>>
    }
}
@

\subsection{Default target, [[target1]]}
\label{sec:default-target}

%dup: overview/interface overview/soft-archi
As I mentioned in Section~\ref{sec:mk-interface}, 
if the user did not specify any target on the command-line,
[[mk]] uses the target of the first simple rule found 
in the [[mkfile]] as the default target.
%
This default target is stored in the following global:

<<global target1>>=
Word *target1;
@
\l e.g. 'all, 

Section~\ref{sec:set-target1} contains the code in [[parse()]] 
modifying [[target1]].
\n actually it is in addrules() called from parse()
%
If the user did not provide a target on the command-line
and [[target1]] was set, then [[mk]] builds this target by calling [[mk()]]:

<<[[main()]] locals>>=
Word *w;
@
<<[[main()]] when no target arguments>>=
if(target1)
    for(w = target1; w; w = w->next)
        // The call!
        mk(w->s);
else {
    fprint(STDERR, "mk: nothing to mk\n");
    Exit();
}
@
\l could be exits here again

Note that the first simple rule can contain multiple targets, which
is why the code above iterates over the list of words in [[target1]].
\l The function [[Exit]] called above is
% will see later that Exit is special, but here could be just exits I think.
\l actually you could call exit here I think

\subsection{Sequential mode}

%trans:
The second way to build one or more targets is 
to specify a set of targets on the command-line.
%
Moreover, [[mk]] supports a special flag, [[-s]] (for ``sequential''),
to build in sequence those targets:

% sequential not parallel command line arguments
<<[[main()]] locals>>=
bool sflag = false;
@

<<[[main()]] -xxx switch cases>>=
case 's':
    sflag = true;
    break;
@
\t why do you want that? to debug? 
\t at least know does not run things in parallel and so will not
\t  get error message in parallel. Just one file at a time!

<<[[main()]] if sequential mode and target arguments given>>=
if(sflag){
    for(; *argv; argv++)
        if(**argv)
            mk(*argv);
}
@
\l but actually for that target it will do things in parallel.
% (unless you put nproclimit to 1), so maybe not that useful.
\l put in advanced feature?

\subsection{Parallel mode}
\label{sec:parallel-mode}

%trans:
The last way to build one or more targets is to specify
them on the command-line without the  [[-s]] flag. 
%
In that case, [[mk]] builds the targets in parallel.
%
To do so, [[mk]] creates a new rule
with the command-line targets as the prerequisites of the new rule,
and an arbitrary string for its target.
%
[[mk]] then calls [[mk()]] with this arbitrary string, which will
trigger the DFS to build its prerequisites in parallel.

<<[[main()]] parallel mode and target arguments given>>=
Word *head, *tail;
Word *t;

/* fake a new rule with all the args as prereqs */
tail = nil;
t = nil;
for(; *argv; argv++)
    if(**argv){
        // add_list(newword(*argv), t)
        if(tail == nil)
            tail = t = newword(*argv);
        else {
            t->next = newword(*argv);
            t = t->next;
        }
    }
if(tail->next == nil)
    // a single target argument
    mk(tail->s);
else {
    head = newword("<command line arguments>");
    addrules(head, tail, strdup(""), VIR, mkinline, nil);
    mk(head->s);
}
@
\l why not call addrule() directly? why addrules? because addrules set target1
\l confusing that tail is actually the head of the prereqs
\t tail -> prereqs, t -> lastprereqs, head -> targets

You can see in the code above a few calls to functions 
I described in Chapter~\ref{chap:core-ds},
for instance, [[newword()]] and [[addrules()]]. I will describe [[mk()]],
the most important function of [[mk]], in Chapter~\ref{chap:finding-outdated}.

The [[VIR]] argument above indicates that the target is a 
{\em virtual target}. [[VIR]] is a rule attribute I will
explain fully in Section~\ref{sec:virtual}.
%
The arbitrary string used in the first argument to [[newword()]] above
is a virtual target because it does not correspond to a file. 
%
In that case, it is not an error if the target does not exist 
after [[mk]] ran the recipe.
\l Virtual targets are often used for targets such as [[mk clean]] (said later)
\n but actually mk does not really check that, the virtual tag is more
\n  to skip files in the current dir that may have the same filename


The last two arguments to [[addrules()]] above
are the line and file location of the rule, which are used
for error reporting. Because here the rule was created artificially
by [[mk]], the file location is set to [[nil]].
\l mkinline, meh. could pass -1 or 0 instead




\chapter{Parsing the [[mkfile]]}
\label{chap:parsing}

%trans:
Now that you have seen [[main()]], I can explain
the different components in the building pipeline,
starting in this chapter with the parsing functions.

%dup?
I mentioned before [[parse()]], which 
takes a path to an [[mkfile]] as a parameter, 
parses this [[mkfile]] to identify rules, meta rules, and definitions, and 
stores those entities in different globals
([[rules]], [[metarules]], and the symbol table [[hash]]).
\n also patrules and target1 (said later)
%ocaml: return those instead of modifying globals
%toc:
[[parse()]] is a complex function that relies on many other
functions to 
scan a file,
identify rules,
expand variables,
process included files,
or define variables,
as explained in the following sections.

\l mk has a very particular way to parse. Partly because mimimal-syntax.
% But maybe grew out with more escaping rules and in the end it's
% a bit complicated.
%real-world: Unix Make was actually using yacc.
%ocaml: cleaner to use Yacc. But, in recipe the tokens
% have a different meaning so you need a lexer with different states/modes 
% (a la Perl/PHP). Also escaping is tricky. Unicode also is tricky.

\section{[[parse()]]}
\label{sec:parse}

%trans:
Before showing the code of [[parse()]],
%
I describe here a few globals used by [[parse()]] (and a few other
functions) to report errors to the user.

[[infile]] below contains the name of the file currently 
processed by [[parse()]] (an [[mkfile]] or one of its included files):

<<global infile>>=
char *infile;
@

This global is used in [[addrule()]]:

<<[[addrule()]] set more fields>>=
r->file = infile;
@

[[mkinline]] below contains the line number of the line currently processed
by [[parse()]]:

<<global mkinline>>=
int mkinline;
@
\t rename just inline? conflicting keyword? rename lineno.
%ocaml: so many code in this chapter that updates mkinline, ugly; a bit better 
% in mk-in-ocaml where it is more centralized

Both globals are used in the following macro to report syntax 
errors to the user:

<<function SYNERR>>=
#define	SYNERR(l)	(fprint(STDERR, "mk: %s:%d: syntax error; ", \
                            infile, ((l)>=0)? (l) : mkinline))
@
\l when call SYNERR with explicit line? when dont want default of using
%  mkinline because error was actually before.
\l note that useful to put mk: before, to indicate errors comes from mk,
%  not one of its subprocess (and there may be many)

%trans:
Here is finally the code of [[parse()]]:

<<function parse>>=
void
parse(char *f, fdt fd, bool varoverride)
{
    Biobuf in;
    Bufblock *buf;
    char c; // one of : = <
    Word *head, *tail;
    int hline; // head line number
    <<[[parse()]] other locals>>

    <<[[parse()]] sanity check fd>>
    <<[[parse()]] start, push>>

    // Initialization
    infile = strdup(f);
    mkinline = 1;
    Binit(&in, fd, OREAD);
    buf = newbuf();

    // Lexing
    while(assline(&in, buf)){
        hline = mkinline;

        // Parsing
        c = rhead(buf->start, &head, &tail,     &attr, &prog);

        // Semantic actions (they may read more lines)
        switch(c)
        {
        <<[[parse()]] switch rhead cases>>
        }
    }
    close(fd);
    freebuf(buf);
    <<[[parse()]] end, pop>>
}
@
\l leak strdup for infile? when free?

The code of [[parse()]] operates in 4 steps:

\begin{enumerate}

\item Initialization of the globals [[infile]] and [[mkinline]]
mentioned above, as well as two local buffers:
[[in]] is an input buffer (using the [[libbio]] library see the \book{Libcore})
that is connected to the file descriptor of the opened [[mkfile]];
\n use Biobuf because will sometimes unget a character (said later)
[[buf]] is a string buffer (see Appendix~\ref{sec:bufblock}) that will
be used to store one line of the [[mkfile]].

%compiler:
\item Reading and assembling of a line from the [[mkfile]] in [[buf]]
(via the function [[assline()]]).
This is similar to the lexing phase in a compiler (see the \book{Compiler}).

\item Parsing of a line to extract its elements:
the target and prerequisites around the special character [[':']] in a rule,
or the variable name and values around the special character [['=']] in
a variable definition.
%
[[rhead()]] uses the buffer containing a line from the [[mkfile]]
as an argument and returns the special character [[c]] used in the line
([[':']] for a rule, [['=']] for a definition, and [['<']] for an inclusion).
%
It also modifies the [[head]] and [[tail]] arguments passed by address
to contain respectively the left and right parts around
the special character (for [['<']], the left part [[head]] is empty).
\l head because head vs body. for body just for rule. def have no body.
\l also hline for head line, so report error first line

\item Acting based on the special character read in the previous step. 
This is where [[mk]] populates [[rules]] and [[metarules]],
as well as the symbol table.
%chunk:
I will describe in the next sections the cases of the [[switch]] above.

\end{enumerate}

\t FIGURE? where start from file and then group in line and
\t  then expand var? summary of the process.

%dup: Windows.nw
The skeleton of [[parse()]] above omits the error management
code shown below:

<<[[parse()]] sanity check fd>>=
if(fd < 0){
    perror(f);
    Exit();
}
@
\l Exit cos parse can create children

<<[[parse()]] switch rhead cases>>=
default:
    SYNERR(hline);
    fprint(STDERR, "expected one of :<=\n");
    Exit();
    break;
@
\l explain Exit?

There are two other locals in [[parse()]] that are passed by address
to [[rhead()]]: 
[[attr]], which will contain possibly a rule attribute, 
\l or variable attribute
and [[prog]].
Both locals are used for advanced features of [[mk]] I will describe later.


\subsection{Assembling a line, [[assline()]]}

The [[assline()]] function below reads characters from the 
input buffer [[bp]], and fills the string buffer [[buf]] with
those characters until it finds a newline.
It also returns [[true]] when there are more lines to assemble
and [[false]] when [[assline()]] reaches the end of the file.


% nice function name
<<function assline>>=
/*
 *	Assemble a line skipping blank lines, comments, and eliding
 *	escaped newlines
 */
bool
assline(Biobuf *bp, Bufblock *buf)
{
    int c;
    <<[[assline()]] other locals>>

    resetbuf(buf);
    while ((c = nextrune(bp, true)) >= 0){
        switch(c)
        {
        case '\n':
            if (!isempty(buf)) {
                insert(buf, '\0');
                return true;
            }
            break;		/* skip empty lines */
        <<[[assline()]] switch character cases>>
        default:
            rinsert(buf, c);
            break;
        }
    }
eof:
    insert(buf, '\0');
    return *(bufcontent(buf)) != '\0';
}
@
\l int -> Rune?
%pad: I introduced and use resetbuf(), isempty(), and bufcontent()
%ocaml: use Buffer too
\l superior or equal zero here? why diff with escapetoken later?

[[insert()]], [[rinsert()]] (for ``Rune insert''), 
[[resetbuf()]], [[bufcontent()]], and [[isempty()]], 
called above, are all functions (or macros) 
operating on a string buffer and are described in Appendix~\ref{sec:bufblock}.
\l what is a rune? (said later, with nextrune)


The code of [[assline()]] may look trivial, but as its 
(inappropriate) name suggests, [[assline()]] does not just
{read} a line: it {assembles} a line.
%
Indeed, if the current line is a {comment},
[[assline()]] will
skip the line to return the next meaningful line to [[parse()]].
%
Indeed, as I said in Section~\ref{sec:rule-simple-mkfile},
[[mk]] allows the user to add comments in his [[mkfile]] by prefixing
a line with the special character [['#']]. 
\l like shell, cos reuse syntax
%
Moreover, [[assline()]] handles also blank lines, escaped newlines,
and certain quoted characters, as explained in the following sections.

%compiler: %dup: parsing/parse
%[[assline()]] is similar to the lexing function in a
%compiler (see the \book{Compiler} or \book{Assembler}).
\l return false when EOF after line parsed
\l pass true to nextrune so elide

%ocaml-found:
% skip only empty lines
% if have line with only spaces, it generate an error because not one of :=<

\subsubsection{Escaped newline, [[nextrune()]]}
\label{sec:escaped-newline}

[[assline()]] relies on the function [[nextrune()]] below to read
the next character from the input buffer [[bp]].
%
%In \plan, a {\em rune} is the term used to denote a {Unicode}
%character (see the \book{Libcore}).
%
[[nextrune()]] is essentially a wrapper over [[Bgetrune()]] from
the [[libbio]] library.
\l which reads sequence of bytes using UTF-8 encoding and returns
\l 24 bits rune.


<<function nextrune>>=
/*
 *	get next character stripping escaped newlines
 *	the flag specifies whether escaped newlines are to be elided or
 *	replaced with a blank.
 */
int
nextrune(Biobuf *bp, bool elide)
{
    int c;

    for (;;) {
        c = Bgetrune(bp);
        <<[[nextrune()]] if escape character>>
        <<[[nextrune()]] handle mkinline>>
        return c;
    }
}
@
\l int -> Rune?? also in bio.h?
\t when do you not elide? for escapetoken() and bquote()
%ocaml: ugly to have to handle mkinline here again.

<<[[nextrune()]] handle mkinline>>=
if (c == '\n')
    mkinline++;
@



%Again, this may sound trivial, but [[nextrune()]] is not. 
%Indeed, 
[[nextrune()]] must also handle {escaped newlines}.
%
An {\em escaped newline} is a newline character prefixed by
the special {\em escape character} [['\']].

%ex: with escaped newline? after all, new feature needs more introduction.

<<[[nextrune()]] if escape character>>=
if (c == '\\') {
    if (Bgetrune(bp) == '\n') {
        // an escaped newline!
        mkinline++;
        if (elide)
            continue;
        // else
        return ' ';
    }
    // else, it was just \
    Bungetrune(bp);
}
@

%dup: overview/principles/dsl/rule
As I mentioned in Section~\ref{sec:rule-simple-mkfile}, the syntax
of [[mk]] (and Make) is minimalist. For example, a variable definition
consists simply of a name followed by an equal sign and a set
of values separated by space and terminated by a newline.
%compiler:
Thus, spaces and newlines have a meaning in [[mk]]  (as opposed
to most programming languages).
\l TAB used too,  but ugly cos dont see diff in editor and can do without
%
However, if the list of values is very long, it would be convenient
to split the list over multiple lines.
%
This is why [[mk]] allows to split such definitions on multiple lines if each
newline is preceded by the special character [['\']];
the newline is then said to be {\em escaped}.
%alt: have syntax for lists, with terminator different than newline,
%  for instance ']' in ocaml
%compiler:
This is similar to what the C preprocessor [[cpp]] provides
for defining long macros over multiple lines (see the \book{Compiler}).


When [[nextrune()]] reads an escaped newline, it does not
return the newline character to the caller [[assline()]].
Instead, it {consumes} this escaped newline and returns the character
after (unless this character is again an escaped newline or if
the second argument to [[nextrune()]] is [[false]]).
%
By consuming the escaped newline, [[nextrune()]]
will cause [[assline()]] to read more characters 
until the next true (non-escaped) newline.


Note that if [['\']] is not followed by a newline, [[nextrune()]]
must just return the [['\']] character. However, [[nextrune()]]
already went too far in the input buffer by reading an extra
character (to check whether this character was a newline).
%
This is why [[libbio]] provides the function [[Bungetrune()]] called above
to go back in the input buffer.
%
This is one of the reasons [[mk]] uses the [[libbio]] library instead
of the reading and seeking functions of the C library.

% FIGURE? with going to far and so going back?


\subsubsection{Comments}

As mentioned before, [[assline()]] recognizes and skips
comments:

%ex: with comment? after all just showed briefly before.

<<[[assline()]] other locals>>=
int prevc;
@
%old: lastc -> prevc


<<[[assline()]] switch character cases>>=
case '#':
    prevc = '#';
    // skip all characters in comment until newline
    while ((c = Bgetc(bp)) != '\n') {
        if (c < 0)
            goto eof;
        prevc = c;
    }
    mkinline++;
   <<[[assline()]] when processing comments, if escaped newline>>
   <<[[assline()]] when processing comments, if not only comment on the line>>
    // else, skip lines with only a comment
    break;
@
%old:        if(c != '\r')
%            lastc = c;
% seems windows-only stuff
\l handle unicode here? can have end of unicode char finishing with [[\n]]?

A comment can be alone on its line, or it can be used at the end
of a variable definition or rule, as in [[FOO=1 # true]].
%
When a comment is not alone on its line, the characters
before the comments are not skipped but returned instead by [[assline()]]:

<<[[assline()]] when processing comments, if not only comment on the line>>=
if (!isempty(buf)) {
     insert(buf, '\0');
     return true;
 }
@
%pad: I now use empty()
\n like '\n' case 


The [[prevc]] local variable above is used to handle escaped newlines 
in a comment as in

<<example of escaped newline>>=
A=foo # this is a long definition mixed with a comment\
  bar 
@

In that case, the definition will be parsed as the single line
[[A=foo bar]].
\l convenient for
% X=foo\
%  bar\
%  #notthisone\
%  foobar
% ...

<<[[assline()]] when processing comments, if escaped newline>>=
if (prevc == '\\')
    break;		/* propagate escaped newlines??*/
@

\subsubsection{Quoted characters}
% part 1

%trans: %dup: parsing/parse/assline
Assembling a line sounds like an easy ask, but as you have just
seen [[assline()]] is not trivial: it must handle escaped newlines
(through [[nextrune()]]), blank lines, and comments.


The use of the special character [['#']] to denote comments introduces
in turn another complication for [[assline()]].
%
Indeed, what if the target or prerequisite in a rule contains
a [['#']] in its filename?
\l not that it can not contain a newline
We do not want [[assline()]] to skip all the characters following 
that [['#']] in the rule.
%
In fact, certain filenames in a project may contain
other special characters used by [[mk]] such as [[':']], [['=']], [['<']],
or even space.
\l but not newline


To reference filenames using special characters in an [[mkfile]], you must
{\em quote} them in order for [[mk]] to not {interpret} them.
%compiler: 
This is a feature found in most programming languages.
\l Lisp. Statchey introduced it (Bourne said it). used in shell also a lot.
%
For [[mk]], the character used to quote a filename
is the single quote character [[']].
\l hence the name of the technique? in Lisp it is also (quote)
\l similar to escape, but for set of chars instead of just one
%dup: (but rearranged)
When [[assline()]] reads a line thats contains a quote,
the [['#']] inside the quote has a different meaning; it is not
a comment anymore.

%ex: with quoted chars? after all, new feature needs more introduction.


<<[[assline()]] switch character cases>>=
case '\'':
case '"':
case '\\':
    rinsert(buf, c);
    if (escapetoken(bp, buf, true, c) == ERROR_0)
        Exit();
    break;
@
\n meta! here use of double \, and single \ to escape quote itself (said later)

You can configure [[mk]] to use either the \plan shell [[rc]]
(see the \book{Shell}) or a Bourne-alike shell.
%
However, each shell has its own escaping rules: sometimes a single quote,
sometimes double quotes, or sometimes the antislash character.
Moreover, as I mentioned before, [[mk]] tries to reuse as 
much as possible the syntax of the shell.
%
This is why the cases of [[assline()]] above are as generic
as possible and delegate instead the shell escaping policy to
the shell-specific [[escapetoken()]] function.
\l method.

The function below is the [[escapetoken()]] for
[[rc]] in [[mk/rc.c]].
%
Like [[assline()]], it reads characters from the 
input buffer [[bp]], and fills the string buffer [[buf]] with
those characters, but this time not until the next newline
but until the next quote.
\n indeed, assline and escapetoken are very similar


% rc.c
<<function escapetoken>>=
/*
 *	Input an escaped token.  Possible escape chars are single-quote,
 *	double-quote and backslash.  Only the first is a valid escape for
 *	rc; the others are just inserted into the receiving buffer.
 */
error0
escapetoken(Biobuf *bp, Bufblock *buf, bool preserve, int esc)
{
    int c;
    int line = mkinline;

    if(esc != '\'')
        return OK_1;

    while((c = nextrune(bp, false)) > 0){
        if(c == '\''){
            if(preserve)
                rinsert(buf, c);
            <<[[escapetoken()]] return, unless double quote>>
        }
        // else
        rinsert(buf, c);
    }
    // must have reached EOF
    SYNERR(line); 
    fprint(STDERR, "missing closing %c\n", esc);
    return ERROR_0;
}
@
%old: initialization of line was done after first if (useless opti).
\l this time it is strictly superior to 0 test for nextrune? why the diff?
\l rename? input_quote?
\l could lpize the first if

Of course, since [[']] is now a special character {reserved} to quote
special characters, how do you quote [[']] itself?
%compiler:
A common technique found in most programming languages is to {double}
the escaping or quoting character
(as shown for C in the code of [[assline()]] above, where the antislash
character is doubled).
%
Thus, in [[rc]] and [[mk]], two [[']] inside a quoted
string are interpreted as a single [[']].
%
For example, [[mk]] interprets [['foo''bar']] as a filename
containing a single quote between the strings [[foo]] and [[bar]]
([[foo'bar]]).
%
Here is the code to handle double quotes:

<<[[escapetoken()]] return, unless double quote>>=
c = Bgetrune(bp);
if (c < 0)
    break; // eof
if(c != '\''){
    Bungetrune(bp);
    return OK_1;
}
// else, '', so continue the while loop
@

%real-world: antislash quote, and double antislash


\l use nextrune so handle escaped newline also in quoted string
% pass false this time to nextrune, so transform escaped newline in space
% escaped newline also has a different meaning, it is then a space.

\l escapetoken keep the quote when preserve is true

%\subsubsection{Quote and escaping}
% similar thing? why 2 mechanisms?

\subsection{Parsing the head of a line, [[rhead()]]}
\label{sec:rhead}

%trans:
Once [[parse()]] assembled a line,
%
it can analyze the line with [[rhead()]] to return the 
special character {separator} involved in the line.
\l rhead for rule head, or maybe read head because used not only for rule.
%dup: parsing/parse
[[rhead()]] also sets the second and third arguments passed by address
to contain the list of words on the left (the {head} [[h]]) 
and right (the {tail} [[t]]) of the separator:

% read head
<<function rhead>>=
static int
rhead(char *line, Word **h, Word **t,    int *attr, char **prog)
{
    char *p;
    int sep; // one of : = < 
    <<[[rhead()]] other locals>>

    p = charin(line, ":=<");
    if(p == nil)
        return '?';

    sep = *p;
    *p++ = '\0';
    <<[[rhead()]] adjust sep if dynamic mkfile [[<|]]>>
    <<[[rhead()]] adjust [[attr]] and [[prog]]>>

    // potentially expand variables in head
    *h = stow(line);
    <<[[rhead()]] sanity check h>>
    // potentially expand variables in tail
    *t = stow(p);

    return sep;
}
@
%old:     *h = w = stow(line);
% but why needed intermediate w? so sanity check test code was simpler, 
% with less indirection? *(*h)->s instead of just *w->s
\l int -> Rune for return? or even char!

[[rhead()]] relies on the function [[charin()]] to find one of
the characters mentioned in the second argument of [[charin()]] in the string
passed in the first argument; I will describe [[charin()]] soon.
\l complex actually, handle head which can contain a :=< (said later)

The address of the special character separator is stored first in the local
variable [[p]]. The content of [[p]] is saved in the other local
variable [[sep]], before being overwritten by the end-of-string
null character as illustrated in the middle of
Figure~\ref{fig:mk-rhead-overwrite}.
%
At this point, [[line]] and [[p]] point to the start of two
independent strings. Both strings are then processed by [[stow()]]
(for ``string to words''), which I will explain in Section~\ref{sec:stow}.
%
[[stow()]] splits the content of a string in a list of words, as shown
at the bottom of Figure~\ref{fig:mk-rhead-overwrite}.
\l again, more complex, need expand variables (said later)


\begin{figure}[!]\centering
\begin{verbatim}
 line
   |
   v-------------------+          start of
   |foo.5 : foo.c foo.h|          rhead()
   +-------------------+

 line     p
   |      |                         after
   v------v------------+         *p++ = '\0'
   |foo.5 | foo.c foo.h|
   +------+------------+
 h          t
 |          |
 v--+       v--+
 |. |       |. |
 ++-+       ++-+
  |          |
  v-----+    v-----+   +-----+     end of
  |foo.5|    |foo.c|-->|foo.h|     rhead()
  +-----+    +-----+   +-----+
\end{verbatim}
\caption{Evolution of local variables in [[rhead()]].}\label{fig:mk-rhead-overwrite}
\end{figure}


%compiler:
\l equivalent to  parsing in compiler

% FIGURE where before long string,
% then string split in 2 because of \0 put at place of special char


Note that if the separator in the line is [['<']] 
(for an inclusion instruction), it is normal for [[h]] to be empty;
\l actually it will contain one element, the empty string
%
otherwise, [[mk]] should report an error to the user:

<<[[rhead()]] sanity check h>>=
if(empty_words(*h) && sep != '<' && sep != '|') {
    SYNERR(mkinline-1);
    fprint(STDERR, 
           "no var (or target) on left side of assignment (or rule)\n");
    Exit();
}
@
%old: w -> *h (less clear, but avoid introduce an extra local var),
%pad: 
% - I added the (or target) in error message 
% - I added use of empty_words

<<macro empty_words>>=
#define empty_words(w) ((w) == nil || (w)->s == nil || (w)->s[0] == '\0')
@
%pad: I introduced that
\t should never be nil! should always contain at list an empty first word
\n actually might be equivalent to empty_prereqs


\l explain pipe, advanced feature
\l could have an if sep == '=' and give precise error message for the 2 cases





\subsubsection{Finding special characters, [[charin()]]}
\label{sec:charin}

%trans:
I mentioned the functions [[charin()]] a few times before.
%
In Section~\ref{sec:addrule}, [[addrule()]] calls [[charin()]] to check
whether the target contains a special pattern character.
%
In Section~\ref{sec:rhead}, [[rhead()]] calls [[charin()]] to get
the position of the [[':']], [['=']], or [['<']] character
in a line.
In both cases, [[charin()]] does not just look for a character
in a string; it must also handle quoted characters.


[[charin()]] below essentially iterates over the characters
\l actually rune
in [[cp]] by incrementing [[cp]] until the start of [[cp]] points
to one of the characters in [[pat]]:

% addrule | rhead -> <>
<<function charin>>=
/*
 *	Search a string for characters in a pattern set.
 *	Characters in quotes and variable generators are escaped.
 */
char*
charin(char *cp, char *pat)
{
    Rune r;
    int n;
    bool vargen = false;

    while(*cp){
        n = chartorune(&r, cp);
        switch(r){
        <<[[charin()]] switch rune cases>>
        default:
            if(utfrune(pat, r) && !vargen)
                return cp;
            break;
        }
        cp += n;
    }
    <<[[charin()]] sanity check vargen>>
    return nil;
}
@
%$
\t rename vargen to  invargenerator (in code and below also)
\n unicode stuff makes the code hard to read. Regexps or lex are much cleaner.
%ocaml: redo a bit some of the lexing work done in assline, sad.
% I use lex so work less repeated
%old: I reversed utfrune() with !vargen, but then less efficient

[[chartorune()]] called above is a function from the C library 
(see the \book{Libcore}).
It is similar to [[Bgetrune()]] used in [[nextrune()]] before, but
operates on a plain string instead of a string buffer.
\l return chars read to form a rune in n, correspond to advanced in buffer
In both cases, the string is a sequence of bytes using the UTF-8 encoding
and terminated by the null character.

[[utfrune()]] is another function from the C library. It checks
whether a rune is part of one of the characters in a set of characters.
\l (called the ``pattern'' above). but confusing with mk pattern.

The local variable [[vargen]] is used to handle variable generator,
an advanced feature of [[mk]] I will explain later in 
Section~\ref{sec:var-generator}.

\subsubsection{Skipping quoted characters}
% part 2

Just like [[assline()]] needs special code to handle quoted strings 
(because they may contain a [['#']] that needs to be treated differently), 
[[charin()]] needs also special code to handle quoted strings,
because they may contain one of the special characters
[[rhead()]] is looking for ([[':']], [['=']], or [['<']]).


<<[[charin()]] switch rune cases>>=
case '\'':			/* skip quoted string */
    cp = squote(cp+1);	/* n must = 1 */
    if(!cp)
        return nil;
    break;
@

% charin -> <>
<<function squote>>=
/*
 *	skip a token in single quotes.
 */
static char *
squote(char *cp)
{
    Rune r;
    int n;

    while(*cp){
        n = chartorune(&r, cp);
        if(r == '\'') {
            <<[[squote()]] return, unless double quote>>
        }
        cp += n;
    }
    SYNERR(-1);		/* should never occur */
    fprint(STDERR, "missing closing '\n");
    return nil;
}
@

<<[[squote()]] return, unless double quote>>=
n += chartorune(&r, cp+n);
if(r != '\'')
    return cp;
// else, double '', continue while loop
@
\n simpler that escapetoken because no need to update mkinline, or
\n to modify a buffer. We just need to skip it.
\l newline should never occur because escapetoken already handles the case?

%\subsubsection{Skipping variables generators}
% now in advanced topics


\subsection{Splitting a string in words, [[stow()]]}
\label{sec:stow}

%trans:
After [[rhead()]] found the position of the special character in 
the line assembled by [[assline()]],
%dup: parsing/parse parsing/parse/rhead
[[rhead()]] calls [[stow()]] to split in multiple words the strings
on the left and right parts of the special character.
%
The space character marks the boundaries between words.
\l so used for target, prereqs, but also varname, var defs, and also file in <
%
The code of [[stow()]] below is very simple because it 
delegates most of the complexity to [[nextword()]], which I will
explain after.

% string to word
<<function stow>>=
Word *
stow(char *s)
{
    // list<ref_own<Word>>
    Word *head, *new;
    // option<ref<Word>>
    Word *lastw;

    head = lastw = nil;
    while(*s){
        new = nextword(&s);
        if(new == nil)
            break;

        // head = concat_list(head, new)
        if (lastw)
            lastw->next = new;
        else
            head = lastw = new;

        while(lastw->next)
            lastw = lastw->next;
        
    }
    <<[[stow()]] if head still nil>>
    return head;
}
@
%old: w -> lastw
\l return always a list with 1 element at least (which can be the empty string)
\t rename newword() -> new_singleword()
\t rename nextword() to nextwords()?
%ocaml: expanding is done later in eval.ml instead, but 
% the splitting is done at lexing/parsing time.

Note that [[nextword()]] does not return a string but a list
of words, for reasons I will explain soon.
%
This is why [[stow()]] must concatenate the lists in
[[head]] and [[new]], not just adding one word to a list of words.

If [[head]] remains still [[nil]] after [[stow()]] processed [[s]],
then [[stow()]] does not return [[nil]] but instead returns
the {empty word}: a list containing one word where the string
is just the null character (see the code of [[newword()]]
in Section~\ref{sec:newword}).

<<[[stow()]] if head still nil>>=
if (!head)
    head = newword("");
@
\l hence invariant later that Rule.prereqs can not be nil, and other stuff

\subsubsection{Assembling the next words, [[nextword()]]}
\l rename nextwords?

The code for [[nextword()]] sounds trivial again: look for
the next space character 
\l or tab. or newline?
in the string as the separation marker between two words.
%
However, again, [[nextword()]] must also handle quoted strings, because
they may contain a space that must not be treated as a {word separator}.
%
In fact, [[nextword()]] must also handle variables and other features
of [[mk]], as explained in the following sections.

% stow -> <>
<<function nextword>>=
/*
 *	break out a word from a string handling quotes, executions,
 *	and variable expansions.
 */
static Word*
nextword(char **s)
{
    char *cp = *s;
    Bufblock *buf;
    Rune r;
    // list<ref_own<Word>>
    Word *head;
    // option<ref<Word>>
    Word  *lastw;
    bool empty;
    <<[[nextword()]] other locals>>

    buf = newbuf();

restart:
    head = lastw = nil;
    empty = true;
    <<[[nextword()]] skipping leading white space>>

    while(*cp){
        cp += chartorune(&r, cp);
        switch(r)
        {
        case ' ':
        case '\t':
        case '\n':
            goto out;
        <<[[nextword()]] switch rune cases>>
        default:
            empty = false;
            rinsert(buf, r);
            break;
        }
    }
out:
    *s = cp;
    if(!isempty(buf)){
        <<[[nextword()]] when buffer not empty, if there was already an head>>
         else {
            insert(buf, '\0');
            head = newword(buf->start);
        }
    }
    freebuf(buf);
    return head;
}
@
%$
%old: tail -> lastw, b -> buf
%pad: use empty()


The presence of the [[restart]] label above, as well as the local
variables [[empty]] and [[lastw]] will become clear later.
%
They are needed because certain strings can expand in other strings
that need to be reprocessed by [[nextword()]].
\l those cases are contain in hidden cases in switch above
%chunks: will see those cases gradually.
% advanced variables and advanced words gradually

[[nextword()]] first skips the leading white spaces in the string:

<<[[nextword()]] skipping leading white space>>=
while(*cp == ' ' || *cp == '\t')		/* leading white space */
    cp++;
@

Thus, there is no difference between writing a rule like
[[foo.5:foo.c]] and [[foo.5: foo.c]], or between a definition like
[[FOO=a b]] and [[FOO = a b]].

\subsubsection{Expanding quoted characters}
% part 3

As I mentioned before, [[nextword()]] must handle quoted strings.
%
As opposed to [[assline()]], which just {inputs} a quoted string,
or [[charin()]], which {skips} over a quoted string, [[nextword()]]
{expands} a quoted string and stores the expansion in the buffer [[b]].
%
Indeed, [[stow()]] and [[nextword()]] are the last steps in the
parsing phase; there is no need to keep the quotes (or double
quotes inside those quotes) anymore. 

<<[[nextword()]] switch rune cases>>=
case '\'':
case '"':
case '\\':
    empty = false;
    cp = expandquote(cp, r, buf);
    if(cp == nil){
        fprint(STDERR, "missing closing quote: %s\n", *s);
        Exit();
    }
    break;
@
\l but missing closing quote should never happen
\l what if empty quoted string? empty is not false then

<<function expandquote>>=
/*
 *	extract an escaped token.  Possible escape chars are single-quote,
 *	double-quote, and backslash.  Only the first is valid for rc. The
 *	others are just inserted into the receiving buffer.
 */
char*
expandquote(char *s, Rune r, Bufblock *buf)
{
    if (r != '\'') {
        rinsert(buf, r);
        return s;
    }

    while(*s){
        s += chartorune(&r, s);
        if(r == '\'') {
            <<[[expandquote()]] return, unless double quote>>
        }
        rinsert(buf, r);
    }
    return nil;
}
@
\l could lpize again the first if
%old: b -> buf

<<[[expandquote()]] return, unless double quote>>=
if(*s == '\'')
    s++; // skip one of the double quotes
else
    return s;
@

\l '' to handle escaping of ' itself? but then handled by

\subsubsection{Expanding variables}
\label{sec:expand-vars}

The expansion of variables in rules, variable definitions, and inclusions
\l but not recipe (said later)
is done at parsing-time by [[nextword()]], just like the expansion
of quotes (and backquotes, as explained in Section~\ref{sec:backquotes}).
\l why parsing-time? why doing it at all?
A single variable can expand to multiple words because a variable
holds a list of words in [[mk]].

<<[[nextword()]] other locals>>=
// list<ref_own<Word>
Word *w;
@

This is why [[nextword()]] returns a list of words, 
and why [[stow()]] concatenate a list of words;
what may seem like a single word (a variable) can expand to multiple words.


The code of [[nextword()]] below relies on [[varsub()]] to return 
the value of a variable. 
%
[[varsub()]] can also substitute certain variables, as explained 
in Section~\ref{sec:subst-var}, hence its name.

<<[[nextword()]] switch rune cases>>=
case '$':
    w = varsub(&cp);
    <<[[nextword()]] when in variable case, if w is nil>>
    empty = false;

    <<[[nextword()]] when in variable case, if non-space chars before var>>
    <<[[nextword()]] when in variable case, if head is not empty>>
    else
        head = lastw = w;

    while(lastw->next)
        lastw = lastw->next;
    break;
@
%$

\l expand vars in rules target, prerequisites, in variable uses.
% but not recipe!
% this is done% by shell, and so it allows late binding of variables, use
% before define, etc.

\l will expand not just variable set in mkfile but also
% plan9 variables, because of [[readenv()]] at the beginning
% of mk.


[[varsub()]] in turn relies on 
[[varname()]] to extract the name of the variable from the string 
pointed by [[s]] ([[varname()]] also increments [[s]] by side effect), and
[[varmatch()]] to grab the value of the variable from the symbol table:

% nextword -> <>
<<function varsub>>=
Word*
varsub(char **s)
{
    Bufblock *buf;
    Word *w;

    <<[[varsub()]] if variable starts with open brace>>
    // else

    buf = varname(s);
    <<[[varsub()]] sanity check buf>>
    w = varmatch(buf->start);

    freebuf(buf);
    return w;
}
@
%old: b -> buf
\n not just [[$V]], can have complex pattern there too.
\n sometimes ${xx:%.y=%.z} ! but later in adv topics
%$

Note that if the user mentions a variable not defined anywhere
(neither in the [[mkfile]] nor in the environment), [[nextword()]]
will skip over this variable:

<<[[varsub()]] sanity check buf>>=
if(buf == nil)
    return nil;
@
\l no error? could

<<[[nextword()]] when in variable case, if w is nil>>=
if(w == nil){
   <<[[nextword()]] when in variable case, if w is nil and no char before>>
    break;
}
@
\t what is that?

In fact, if the variable is not defined and there was no character
before the variable, as in [[FOO= $bar abc]], %$
then [[nextword()]] must skip over the undefined variable and also
skip again the possible whitespaces after the variable, hence the jump to
[[restart]] below:

<<[[nextword()]] when in variable case, if w is nil and no char before>>=
if(empty)
    goto restart;
@
\n but no reprocessing of words already parsed here, cp is still empty
\n reprocessing is done for backquote only I think


%\subsection{Simple variables, }
%$

[[varname()]] below
returns a buffer containing the name of a variable in [[s]], and 
increments [[s]] to point after the variable name:

<<function varname>>=
/*
 *	extract a variable name
 */
static Bufblock*
varname(char **s)
{
    Bufblock *buf;
    char *cp = *s;
    Rune r;
    int n;

    buf = newbuf();
    for(;;){
        n = chartorune(&r, cp);
        if (!WORDCHR(r))
            break;
        rinsert(buf, r);
        cp += n;
    }
    <<[[varname()]] sanity check buf>>
    *s = cp;
    insert(buf, '\0');
    return buf;
}
@

What constitutes a variable name is mostly anything except
the set of special characters in the macro below:

<<function WORDCHR>>=
#define WORDCHR(r)	((r) > ' ' && !utfrune("!\"#$%&'()*+,-./:;<=>?@[\\]^`{|}~", (r)))
@
%$
\l  maybe specific to shell too?


<<[[varname()]] sanity check buf>>=
if (isempty(buf)){
    SYNERR(-1);
    fprint(STDERR, "missing variable name <%s>\n", *s);
    freebuf(buf);
    return nil;
}
@


%trans:
Here is finally [[varmatch()]] called from [[varsub()]]:

<<function varmatch>>=
static Word*
varmatch(char *name)
{
    Word *w;
    Symtab *sym;
    
    sym = symlook(name, S_VAR, nil);
    if(sym){
        /* check for at least one non-NULL value */
        for (w = sym->u.ptr; w; w = w->next)
            if(w->s && *w->s)
                return wdup(w);
    }
    return nil;
}
@
\l rename? bad name


Note that if a variable is preceded directly by non-space characters,
as at the middle of Figure~\ref{fig:mk-nextword1}, the first word in the list of
words in the value of the variable must be {adjusted}:

<<[[nextword()]] when in variable case, if non-space chars before var>>=
if(!isempty(buf)){
    bufcpy(buf, w->s, strlen(w->s));
    insert(buf, '\0');
    free(w->s);
    // adjust the first word
    w->s = strdup(buf->start);
    
    resetbuf(buf);
}
@
%pad: use isempty() and resetuf()
%ocaml: I actually forbid that if variable is not a scalar

\begin{figure}[!]\centering
\begin{verbatim}
  cp           where A=foo bar
   |                 B=bar foo
  +v------+
s:|@$A$B: |                                   start of
  +-------+                                  nextword()


             start
   cp          | current
    |          | |
  +-v-----+    v-v---------+                 processing
s:|@$A$B: |  b:|@          |                    '@'
  +-------+    +-----------+

            start           start
               |current     current
     cp        | |             |
      |        v-v---------+   v----+------+
  +---v---+  b:|@          | b:|@foo|      |
s:|@$A$B: |    +-----------+   +----+------+ processing
  +-------+    +---+  +---+    +----+  +---+    '$A'
             w:|foo|->|bar|  w:|@foo|->|bar|
               +---+  +---+    +----+  +---+

               (before)          (after)

                  adjusting the first
                         word
\end{verbatim}
\caption{[[nextword()]] edge cases (part 1).}\label{fig:mk-nextword1}
\end{figure}

Moreover, if a variable is followed by another variable, 
as at the top of Figure~\ref{fig:mk-nextword2}, the last
word of the first variable must be {merged} with the first word
of the second variable:

<<[[nextword()]] when in variable case, if head is not empty>>=
if(head){
    // merge the last and first words
    bufcpy(buf, lastw->s, strlen(lastw->s));
    bufcpy(buf, w->s, strlen(w->s));
    insert(buf, '\0');
    free(lastw->s);
    lastw->s = strdup(buf->start);

    lastw->next = w->next;
    free(w->s);
    free(w);
    resetbuf(buf);
}
@
%ocaml: I actually forbid that if variable is not a scalar
%pad: use resetbuf()


Finally, if a variable is followed directly by non-space
characters, as at the bottom of Figure~\ref{fig:mk-nextword2}, the last word
must be adjusted:

<<[[nextword()]] when buffer not empty, if there was already an head>>=
if(head){
    cp = buf->current;
    bufcpy(buf, lastw->s, strlen(lastw->s));
    bufcpy(buf, buf->start, cp - buf->start);
    insert(buf, '\0');
    free(lastw->s);
    // adjust the last word
    lastw->s = strdup(cp);
}
@
%ocaml: I actually forbid that if variable is not a scalar
\l ugly abuse cp here

\begin{figure}[!]\centering
\begin{verbatim}
                   where A=foo bar
                         B=bar foo

            start               start
            current             current
               |                   |
       cp      v-----------+       v------+----+
        |    b:|           |     b:|barbar|    |
  +-----v-+    +-----------+       +------+----+ processing
s:|@$A$B: |    +---+  +---+        +---+  +---+     '$B'
  +-------+  w:|bar|->|foo|      w:|bar|->|foo|
               +---+  +---+        +---+  +---+
               +----+  +---+       +----+  +------+  +---+
          head:|@foo|->|bar|  head:|@foo|->|barbar|->|foo|
               +----+  ^---+       +----+  ^------+  +---+
                       |                   |
                      lastw               lastw

                   (before)          (after)

                     merging the last and
                          first word

            start                    start
               |current                |cp   current
        cp     | |                     | |    |
         |     v-v---------+           v-v---+v----+ processing
  +------v+  b:|:          |         b:|:foo:|     |    ':'
s:|@$A$B: |    +-----------+           +-----+-----+
  +-------+
            +----+  +------+  +---+  +----+  +------+  +----+
      head: |@foo|->|barbar|->|foo|  |@foo|->|barbar|->|foo:|
            +----+  +------+  ^---+  +----+  +------+  ^----+
                              |                        |
                             lastw                    lastw
                (before)                   (after)

                       adjusting the last
                              word
\end{verbatim}
\caption{[[nextword()]] edge cases (part 2).}\label{fig:mk-nextword2}
\end{figure}
\n actually the buffer contain [[@foo]] at the beginning here

\t maybe say ok if variables are scalar, but could issue a warning otherwise

\n could put the code for dollar{name} here, important escaping mechanism
\n will see backquote later too


\section{Rules, {\em target}[[:]]{\em prereqs}}
\label{sec:parse-rules-add-rules}
\n actually targets [[:]] prereqs, not just one target

%trans:
Now that you have seen the generic parts of the code to
parse an [[mkfile]],
%
I can describe the specific parts with the {actions} in [[parse()]]
to process the rules, definitions, and inclusions.
%
Those actions are based on
the information returned by [[rhead()]]:
%dup: parsing/parse parsing/parse/rhead parsing/parse/stow
the special character in the line, as well as the list of words on the 
left and right parts of this special character.
%
I will start in this section with the action to manage rules,
when the special character returned by [[rhead()]] is [[':']].


<<[[parse()]] other locals>>=
char *body;
@
<<[[parse()]] switch rhead cases>>=
case ':':
    body = rbody(&in);
    addrules(head, tail, body,   attr, hline, prog);
    break;
@
\t mv hline as the last parameter, or fourth

The action above relies on [[rbody()]] to read the body
of a rule, that is its recipe.
%
I will explain [[rbody()]] in the next section.

%dup: core-ds/rule/addrules
I have described [[addrules()]] called above in Section~\ref{sec:multi-targets}.
%
[[addrules()]] can handle rules with multiple targets by
calling [[addrule()]] for each target.


I mentioned also in Section~\ref{sec:default-target} 
that [[mk]], during parsing, sets the global [[target1]] to contain
the first target found in the mkfile.
%
Here is finally the code that sets [[target1]]:

\label{sec:set-target1}
<<[[addrules()]] set [[target1]]>>=
/* tuck away first non-meta rule as default target*/
if(target1 == nil && !(attr&REGEXP)){
    for(w = targets; w; w = w->next)
        if(charin(w->s, "%&"))
            break;
    if(w == nil) // head does not contain any pattern
        target1 = wdup(targets);
}
@
\n I could mv this before, but addrules is in Core DS and target1
\n mentioned in main. Also use charin below described in this chapter.

Note the use of [[charin()]] above to make sure the first target
is not contain a {pattern} and so does not contain a special
pattern character.
%
Indeed, [[mk]] would not
know how to instantiate this pattern to build a concrete target.
\l pattern can be [['%']] or [['&']] as described in X

For the rule attribute [[REGEXP]] used above, see Section~\ref{sec:regexp}.
\l why not also attr&META? because we didnt call yet addrule, and
\l addrule does not return the rule


\subsection{Parsing the recipe, [[rbody()]]}

[[rbody()]], like [[assline()]], takes as a parameter an input buffer [[in]].
%
The {cursor} in this buffer should now point to the character following
the newline of the line containing the target and prerequisites of the rule.
% FIGURE?
[[rbody()]] then fills its local string buffer [[buf]] until
a non-spacing character is found in the first column.
%
This character marks the end of the recipe
and the start of a new rule, definition, or inclusion.

% read body
<<function rbody>>=
static char *
rbody(Biobuf *in)
{
    Bufblock *buf;
    int r, lastr;
    char *p;

    lastr = '\n';
    buf = newbuf();

    for(;;){
        r = Bgetrune(in);
        if (r < 0)
            break; // eof
        // in first column?
        if (lastr == '\n') {
            <<[[rbody()]] if comment in first column>>
            else 
              if (r != ' ' && r != '\t') {
                Bungetrune(in);
                break;
            }
        } else
            // not in first column
            rinsert(buf, r);

        lastr = r;
        <<[[rbody()]] handle mkinline>>
    }

    insert(buf, '\0');
    p = strdup(buf->start);
    freebuf(buf);

    return p;
}
@
\l int -> Rune

<<[[rbody()]] handle mkinline>>=
if (r == '\n')
    mkinline++;
@

If [[rbody()]] find a comment in the first column, this comment can not be
the start of a rule, definition, or inclusion, so it is
added in the buffer for the recipe:

<<[[rbody()]] if comment in first column>>=
if (r == '#')
    rinsert(buf, r); // the shell recognize comments too
@

If a rule has no recipe, [[rbody()]] returns
the empty string to its caller [[parse()]]. 
%
Note that the empty string is not the same thing than [[nil]]. Indeed,
an empty string contains one byte: the end-of-string null character [['\0']].
\l why insist? because invariant used later

Note also that [[mk]] does not impose to use a [[TAB]] 
in the first column like Make. A space character is also valid.
Moreover, you can use more than one space.
\l but then weird shprint?
You can also write multiple shell commands on multiple lines as long
as they all have a leading spacing character in the first column.
%
You do not need to escape newlines in a recipe (as you have to do in
a variable definition).
\l Make force to escape newline?

\t note that no call to  stow() here! no variable expansion


\subsection{Parsing rule attributes}
\label{sec:syntax-rule-attributes}

[[mk]] allows to customize certain rules by using
{\em rule attributes}.
%dup: main/build/parallel-mode (for VIR)
An attribute often used is the attribute to indicate that
the target in a rule does not correspond to a filename;
then [[mk]] should not expect from the recipe to generate such a target.
\l but in fact mk does not check that, so it is more to skip
\l the reading of time if by bad luck there is a file named clean in the dir
For instance, many [[mkfile]]s use the target [[clean]] to
cleanup a directory.
%dup: main/build/parallel-mode (for VIR)
In [[mk]]'s terminology, such a target is called a {\em virtual target}
(see Section~\ref{sec:virtual} for a full explanation).
%real-world:
In GNU Make, the use of [[.PHONY:]] followed by a string in a
[[Makefile]] indicates that the string is a virtual target.

In [[mk]], the syntax to add attributes to a rule is to add
non-spacing characters after the first [[':']] of a rule, and
to add another [[':']] after the non-spacing characters, 
as in the following rule:

<<tests/mkfile/mkclean>>=
clean:V:
    rm -f *.5 $PROG $LIB
@

Each character between the two [[':']] can correspond to a 
different attribute.

Rule attributes are stored in [[Rule.attr]], but before they
are stored in a local variable in [[parse()]]:

<<[[parse()]] other locals>>=
// bitset<Rule_attr>
int attr;
@
\l or var_attr?

This variable is passed by address to [[rhead()]]
(see the call to [[rhead()]] in [[parse()]] in Section~\ref{sec:parse}).
[[rhead()]] then initializes this variable and adjusts it
depending on the separator:

<<[[rhead()]] adjust [[attr]] and [[prog]]>>=
*attr = 0; // Nothing
*prog = nil;
// variable attributes
<<[[rhead()]] if sep is [[=]]>>
// rule attributes
<<[[rhead()]] if sep is [[:]]>>
@

Finally, [[rhead()]] modifies [[attr]] in the cases of
the [[switch]] below:

<<[[rhead()]] other locals>>=
Rune r;
int n;
@

<<[[rhead()]] if sep is [[:]]>>=
if((sep == ':') && *p && (*p != ' ') && (*p != '\t')){
    while (*p) {
        n = chartorune(&r, p);
        if (r == ':')
            break;
        p += n;
        switch(r)
        {
        <<[[rhead()]] when parsing rule attributes, switch rune cases>>
        default:
            SYNERR(-1);
            fprint(STDERR, "unknown attribute '%c'\n", p[-1]);
            Exit();
        }
    }
    if (*p++ != ':') {
eos:
        SYNERR(-1);
        fprint(STDERR, "missing trailing :\n");
        Exit();
    }
}
@
%old: switch cases originally sorted alphabetically, but after
% LP distribution, they might not be anymore

%chunks: 
%I will describe gradually the cases above and the different rule attributes.
\l actually only in advanced chapter
%dup: (and adapted) core-ds/rules/metarules
Most of the rule attributes correspond to advanced features
of [[mk]] I will describe in Section~\ref{sec:rule-attributes-advanced}.



\section{Included files, [[<]]{\em file}}

%trans:
I will now describe the action to manage inclusions,
when the special character returned by [[rhead()]] is [['<']].

A file inclusion in an [[mkfile]] will result in the opening of
a new file, hence the following additional variables in [[parse()]]:

<<[[parse()]] other locals>>=
char *p;
fdt newfd;
@

Here is the code using [[newfd]]:

<<[[parse()]] switch rhead cases>>=
case '<':
    p = wtos(tail, ' ');
    <<[[parse()]] when parsing included file, sanity check p>>
    newfd = open(p, OREAD);
    <<[[parse()]] when parsing included file, sanity check newfd>>
    else
        // recurse
        parse(p, newfd, false);
    break;
@

<<[[parse()]] when parsing included file, sanity check p>>=
if(*p == '\0'){
    SYNERR(-1);
    fprint(STDERR, "missing include file name\n");
    Exit();
}
@
<<[[parse()]] when parsing included file, sanity check newfd>>=
if(newfd < 0){
    fprint(STDERR, "warning: skipping missing include file: ");
    perror(p);
}
@


Note that [[tail]] above contains the list of words on the right
of [['<']] ([[head]], which contains the list of words on the left,
should be empty).
%
Just like for the rules, this list of words is set in [[rhead()]]
and is the result of a call to [[stow()]], which performs
quote and variable expansions.
%
Thus, you can also use variables in the filename to include, 
as in

<<tests/mkfile/mkincludearc>>=
</$objtype/mkfile
@
%$

I described [[wtos()]] called above in Section~\ref{sec:word}.
It converts a list of words back to a string. In practice, this
list should contain only one element for file inclusions.
\l handle filename with spaces now ... no need escaping
%ocaml: be more consistent and stricter



To include a file, [[mk]] simply calls recursively [[parse()]].
However, the globals [[infile]] and [[mkinline]] must
be saved before the call and restored after, hence the following
calls in [[parse()]]:

<<[[parse()]] start, push>>=
ipush();
@

<<[[parse()]] end, pop>>=
ipop();
@
%ocaml: do that in eval, and use call stack to handle recursivity

Both functions use the following structure to remember the list
of ``parent'' files in the stack of opened files.

<<struct input>>=
struct Input
{
    char *file;
    int line;

    // Extra
    <<[[Input]] extra fields>>
};
@


<<global inputs>>=
// list<ref_own<Input>> (next = Input.next)
static struct Input *inputs = nil;
@

<<[[Input]] extra fields>>=
// list<ref_own<Input>> (head = inputs)
struct Input *next;
@

<<function ipush>>=
void
ipush(void)
{
    struct Input *in, *me;

    me = (struct Input *)Malloc(sizeof(*me));
    me->file = infile;
    me->line = mkinline;
    me->next = nil;

    // add_list(me, inputs)
    if(inputs == nil)
        inputs = me;
    else {
        for(in = inputs; in->next; )
            in = in->next;
        in->next = me;
    }
}
@
%ocaml: do not use globals for infile and inline? actually I do,
% so my code is very similar here.

<<function ipop>>=
void
ipop(void)
{
    struct Input *in, *me;

    assert(/*pop input list*/ inputs != 0);
    // me = pop_list(inputs)
    if(inputs->next == nil){
        me = inputs;
        inputs = nil;
    } else {
        for(in = inputs; in->next->next; )
            in = in->next;
        me = in->next;
        in->next = nil;
    }
    infile = me->file;
    mkinline = me->line;
    free((char *)me);
}
@




\section{Variable definitions, {\em var}[[=]]{\em values}}
\n more Constant definitions. the ninja build system calls them {bindings}

%trans:
The final action of [[parse()]] manages variable definitions,
when the special character returned by [[rhead()]] is [['=']].

<<[[parse()]] other locals>>=
bool set = true;
@
%pad: not set initially, but clearer that way

<<[[parse()]] switch rhead cases>>=
case '=':
    <<[[parse()]] when parsing variable definitions, sanity check head>>
    <<[[parse()]] when parsing variable definitions, override handling>>
    if(set){
        setvar(head->s, (void *) tail);
        <<[[parse()]] when parsing variable definitions, extra setting>>
    }
    <<[[parse()]] when parsing variable definitions, if variable with attr>>
    break;
@
%dead: was already commented in the original code
%    /*
%    char *cp;
%    dumpw("tail", tail);
%    cp = wtos(tail, ' '); print("assign %s to %s\n", head->s, cp); free(cp);
%    */

The code above relies mainly on the function [[setvar()]], which I
described in Section~\ref{sec:setvar}, to add an entry in the
[[S_VAR]] namespace in the symbol table.

Note that for variable definitions, the list of words
on the left of [['=']] should contain only one element:

<<[[parse()]] when parsing variable definitions, sanity check head>>=
if(head->next){
    SYNERR(-1);
    fprint(STDERR, "multiple vars on left side of assignment\n");
    Exit();
}
@
\n the check whether head->s == '\0' is done before in rhead

Note also that both [[head]] and [[tail]] used above
are the results of calls to [[stow()]] in [[rhead()]], which
expands variables.
%
Thus, you can use variables on the right side of [['=']],
%ocaml-found:
but also more surprisingly on the left as in the following example:

<<indirection in mkfile example>>=
A=B
D=d e f
$A = a b c $D
# => B contains a b c d e f
@
\l say bad practice?

\subsection{Overriding variable definitions}

As I mentioned in Section~\ref{sec:vardef-command-line}, you can
{override} variable definitions in an [[mkfile]] (or in files included
from this [[mkfile]]) by passing definitions through the command-line,
as in [[mk objtype=arm CFLAGS=-g]].
%
In that case, [[mk]] creates a temporary file containing those
command-line definitions and calls [[parse()]] with [[true]]
for its [[varoverride]] parameter (see Section~\ref{sec:vardef-command-line}).
%
Here is the code using this [[varoverride]] parameter:

<<[[parse()]] when parsing variable definitions, override handling>>=
if(symlook(head->s, S_OVERRIDE, nil)){
    set = varoverride;
} else {
    set = true;
    if(varoverride)
        symlook(head->s, S_OVERRIDE, (void *)"");
}
@
\t could simplify maybe, if there, then set set=false; clearer.
\t also generate warning when set again same var? 
\t mk allows it? value is used until next def since evaluate vars as we parse.

The code above relies on the new namespace [[S_OVERRIDE]], which
contains the set of variables defined through the command-line
(and whose definitions can not be overriden by definitions in the [[mkfile]]).

<<[[Sxxx]] cases>>=
S_OVERRIDE,	/* can't override */
@

\l explain more? scenario? first call varovveride is true,
\l add in namespace, and do setvar above. Then when parse mkfile,
\l varovveride is false, and if same var, then look, set become false
\l and no setvar so keep old definition 
\t confusion is that old def is called the overriding definition, hmmm

\subsection{Parsing variable attributes}

%trans:
Variables, likes rules, can have attributes.
%
The syntax for variable attributes uses a scheme similar to the
rule attributes (see Section~\ref{sec:syntax-rule-attributes} 
where the special character (here [['=']]) is
doubled and attributes are between the two special characters,
as in the following example:

<<mkfile using a private variable>>=
MYVAR=U= foo.5 bar.5 # a private variable
@

Here is the code in [[rhead()]] to extract variable attributes:

<<[[rhead()]] if sep is [[=]]>>=
if(sep == '='){
    pp = charin(p, termchars);	/* termchars is shell-dependent */
    if (pp && *pp == '=') {
        while (p != pp) {
            n = chartorune(&r, p);
            switch(r)
            {
            <<[[rhead()]] when parsing variable attributes, switch rune cases>>
            default:
                SYNERR(-1);
                fprint(STDERR, "unknown attribute '%c'\n",*p);
                Exit();
            }
            p += n;
        }
        p++;		/* skip trailing '=' */
    }
}
@
% can chain? X=V=foo ? or it is more C=U=bla

%chunks: 
%I will describe gradually the cases above and the different variable attributes.
Variable attributes correspond to advanced features of [[mk]]
rarely used. I will describe those attributes and the
cases of the [[switch]] above in Section~\ref{sec:variable-attributes}.


The code above relies on the following constant to check whether
a line contains a variable attribute:

% rc.c
<<global termchars>>=
char	*termchars = "=' \t";	/*used in parse.c to isolate assignment attribute*/
@
% what we really want is to find first =, but if there is
% a quote before, then the = should not be interpreted as the = before
% attribute.
%old: "'= \t" but I prefer my order


Note that [[termchars]] does not contain just [['=']].
%
Indeed, variable definitions can contain quoted string containing
the equal character, as in [[A='U=1']], in which case the equal sign
inside the quote should not be interpreted as the end mark of variable
attributes.
%
This is why the code above is looking for the first character that is
either an equal or quote and stops there.
%
In fact, [[termchars]] contains also spacing characters to remove
some possible ambiguities as shown in the following example:

<<mkfile using space to disambiguate variable attributes>>=
MYVAR=U=a b c d  # private var MYVAR containing the list: a b c d
MYVAR= U=a b c d # MYVAR contains the list: U=a b c d
@
\n mentioned in man page of mk as a limitation

%\section{Variable uses (expanding variables part 2), [[$]]{\em C} }
%$




\chapter{Building the Graph of Dependencies}
\label{chap:graph}

%trans:
The next component in the building pipeline is the construction of
the {graph of dependencies}. Section~\ref{sec:graph-principles} showed
a few examples of graph of dependencies for small projects.
You will see in this chapter how [[mk]] builds those graphs.

%toc:
The most important function in this chapter is [[graph()]],
\n well really it is [[applyrules()]] (said later)
which takes a target name as a parameter and returns the {root}
of the graph of dependencies for this target.
%trans:
[[graph()]] will use the globals [[rules]] and [[metarules]]
populated by [[parse()]] in Section~\ref{sec:parse-rules-add-rules}.
\n through addrules()
\l and this graph will be used by work in next chapter for DFS to find jobs
%toc:
I will also describe in this chapter the code to check for mistakes
in the rules. Those mistakes translate in issues while building the graph,
for instance, the presence of cycles in the graph.
%(checked with the function [[cyclechk()]]).
\l Will see [[mk()]] later. But will call [[graph(target)]].

\section{[[graph()]] and [[applyrules()]]}
\label{sec:graph-applyrules}

[[graph()]] is mostly a wrapper around [[applyrules()]], which is
the function containing the logic to build the graph of dependencies.

% main -> mk -> <>
<<function graph>>=
Node*
graph(char *target)
{
    Node *root;
    <<[[graph()]] other locals>>

    <<[[graph()]] set cnt for infinite rule detection>>
    root = applyrules(target, cnt);
    <<[[graph()]] free cnt>>

    <<[[graph()]] checking the graph>>
    <<[[graph()]] propagate attributes>>

    return root;
}
@
%old: node -> root, clearer I think

I will present the code to check the graph in Section~\ref{sec:check-graph}.

I mentioned briefly in Section~\ref{sec:soft-archi} the algorithm
behind [[applyrules()]].
%dup: overview/soft-archi
The algorithm first
builds a node corresponding to the target (via [[newnode()]]),
then finds the rules or meta rules with {matching} targets, and finally
applies recursively [[applyrules()]] on the prerequisites of
those matching rules and meta rules.
%
The algorithm stops when a node is a {leaf}, which happens
when a node has no matching rules or rules with no prerequisites.
%
Here is the skeleton of [[applyrules()]]:

<<function applyrules>>=
static Node*
applyrules(char *target, char *cnt)
{
    Node *node;
    // list<ref<Arc> (next = Arc.next, last = lasta)
    Arc head;
    // ref<Arc>
    Arc *lasta = &head;
    <<[[applyrules]] other locals>>

    <<[[applyrules]] debug>>
    <<[[applyrules]] check node cache if target is already there>>
    // else

    target = strdup(target);
    node = newnode(target); // calls timeof() internally
    head.next = nil;
    <<[[applyrules]] other initializations>>

    // apply regular rules with target as a head (modifies lasta)
    <<[[applyrules()]] apply regular rules>>

    // apply meta rules (modifies lasta)
    <<[[applyrules()]] apply meta rules>>

    node->arcs = head.next;

    return node;
}
@
%old: a -> lasta, far clearer!
\l confusing again to use head, this time for head of arcs. head -> arcs?
%dead? bug? node->arcs should be nil no at this point? assert?
%    a->next = node->arcs;
%dead? bug? useless, it is never used anyway
%    head.n = nil;
%old: in comment in the original code!
%/*		if(r->attr&VIR)
% *			node->flags |= VIRTUAL;
% *		if(r->attr&NOREC)
% *			node->flags |= NORECIPE;
% *		if(r->attr&DEL)
% *			node->flags |= DELETE;
% */
% now done in attribute() anyway.

\label{sec:idiom-easy-add-list}
The code above relies on a local variable [[head]] containing
an [[Arc]] allocated in the stack, and another local variable
[[lasta]] pointing originally to this arc.
\l and evolving as you will see in next sections
This is a standard idiom in C allowing later to write code adding
an element in a list without having to worry whether this list
is originally empty or not (as the code of [[mk]] does for example in
Section~\ref{sec:addrule} with the list of rules, or in
Section~\ref{sec:parallel-mode} with a list of words).


%toc:
I will explain the code to apply rules and meta rules
in the next two sections.

\section{Finding the simple rules for a target}
\label{sec:finding-simple-rules}

%trans:
The first step to find the dependencies of a node is to look
for all the rules mentioning the node as a target.
%
Fortunately, [[applyrules()]] can rely on the symbol table and the [[S_TARGET]]
namespace to quickly access all the rules mentioning a specific
target (as explained in Section~\ref{sec:indexing-target}):
\l built index while parsing in hash

<<[[applyrules]] other locals>>=
Symtab *sym;
Rule *r;
Word *pre;
Arc *arc;
@
%old: w -> prereq? then it's a long var name for an iterator. w -> pre
%pad: I introduced arc local, clearer.

<<[[applyrules()]] apply regular rules>>=
sym = symlook(target, S_TARGET, nil);
r = sym? sym->u.ptr : nil;
for(; r; r = r->chain){
    <<[[applyrules()]] skip this rule and continue if some conditions>>
    <<[[applyrules()]] infinite rule detection part1>>
    <<[[applyrules()]] when found a regular rule for target [[node]], set flags>>

    <<[[applyrules()]] if no prerequisites in rule r>>
    else
        for(pre = r->prereqs; pre; pre = pre->next){
            // recursive call!
            arc = newarc(applyrules(pre->s, cnt), r, "", rmatch);
            // add_list(head, arc)
            lasta->next = arc;
            lasta = lasta->next;
    }
    <<[[applyrules()]] infinite rule detection part2>>
}
@
%old: I put the initialization of r outside the for
%dead? bug? what is the point of setting head.n? it is not used anyway
%    head.n = node;

As I mentioned before, [[applyrules()]] simply calls itself
recursively for each prerequisite of a matching rule and adds
a new arc between the current node and the root of the subgraph
returned by [[applyrules()]].
%
\l stem is [[""]] here
\l so for each prereq there is an arc! (but all of those arcs
\l will share the same rule/recipe)



Some tools such as [[gcc -MM]] can generate in a [[.depend]] file included
from your [[mkfile]] rules without neither a recipe nor prerequisites
(e.g., [[foo.5: #no deps]]).
\n ocamldep
%
[[applyrules()]] can simply skip those rules:
\l we could warn.

<<[[applyrules()]] skip this rule and continue if some conditions>>=
if(empty_recipe(r) && empty_prereqs(r))
      continue;	/* no effect; ignore */
@
%TODO safe to continue when no recipe? should still have an arc
% so than can check if outofdate no?
%dead? how can be META and in S_TARGET? it could! but we should
% not add in S_TARGET in addrule when META, useless.
%  if(r->attr&META) continue;
%dead? bug? how can be in S_TARGET and not strcmp?
% if(strcmp(target, r->target)) continue; 

The code above relies on two macros that factorize the checks
for empty recipe and empty prerequisites:

<<macro empty_recipe>>=
#define empty_recipe(r) (!r->recipe || !*r->recipe)
@
%pad: I introduced that, was repeating ugly expressions everywhere
\t but no need !r->recipe, every rule has recipe (which can be empty)
\t  so do assert in empty_recipe()

<<macro empty_prereqs>>=
#define empty_prereqs(r) (!r->prereqs || !r->prereqs->s || !*r->prereqs->s)
@
%pad: I introduced that, ugly those chain of test on prereqs. 
\t Should assert always has a prereqs?
\t  or should forbid singleton empty word.


%dup: ds/graph/arc main/build/parallel parse/rules/attrs
Some rules may have a recipe but no prerequisites, for instance, when
the target is a virtual target (see Section~\ref{sec:virtual}).
%
In those cases, [[mk]] still adds an arc to the node, but without
a destination node.
%
You will see later code using those ``fake'' arcs.
\l example: clean:, there will be no prereqs

<<[[applyrules()]] if no prerequisites in rule r>>=
// no prerequisites, a leaf, still create fake arc
if(empty_prereqs(r)) {
    arc = newarc((Node *)nil, r, "", rmatch);
    // add_list(head, arc)
    lasta->next = arc;
    lasta = lasta->next;
} 
@
%pad: use my empty_prereqs macro, clearer
\l stem is "" here
\t why need that? to at least have something propagate in attribute()?
\l  so will pass the VIRT to the node?
% because there is a rule attached here!! a recipe for virtual targets.


\l Note that applyrules goes until the end, even if leaf is up to date
\l already. it will compute also the dependencies for this up-to-date
\l leaf. Because applyrules just does one thing! Build static graph,
\l and then can do analysis on this graph. Better separate concerns
\l (so can do tricky analysis like implicit dependencies, etc)


\section{Finding matching metarules}

%trans:
The other step to find the dependencies of a node is to look for metarules
containing a target that {matches} the node.
%
This time, [[applyrules()]] needs to go through
all the meta rules stored in the global [[metarules]]:

<<[[applyrules()]] apply meta rules>>=
for(r = metarules; r; r = r->next){
    <<[[applyrules()]] skip this meta rule and continue if some conditions>>
    <<[[applyrules()]] if regexp rule then continue if some conditions>>
    else {
        if(match(node->name, r->target, stem)) {
            <<[[applyrules()]] infinite rule detection part1>>

            <<[[applyrules()]] if no prerequisites in meta rule r>>
            else
                for(pre = r->prereqs; pre; pre = pre->next) {
                    <<[[applyrules()]] if regexp rule, adjust buf and rmatch>>
                    else
                        subst(stem, pre->s, buf, sizeof(buf));
                    // recursive call!
                    arc = newarc(applyrules(buf, cnt), r, stem, rmatch);
                    // add_list(head, arc)
                    lasta->next = arc;
                    lasta = lasta->next;
                }
             <<[[applyrules()]] infinite rule detection part2>>
       }
    }
}
@
%old: I removed a continue and reorged the code, was ugly

The code above relies on the functions [[match()]] and [[subst()]],
which I will describe fully in the next two sections.
%
[[match()]] checks whether a string can match another string
called the {\em template}. This template can contain 
a pattern character (e.g., [[foo%.5]]),
as explained in Section~\ref{sec:pattern}.
%
If the template matches the string, [[match()]] then stores the
string matched by the pattern character, called the {\em stem},
in a buffer passed in its last parameter.
%
Here is the stem buffer [[applyrules()]] passes to [[match()]]:

<<[[applyrules]] other locals>>=
char stem[NAMEBLOCK];
@
<<constant NAMEBLOCK>>=
#define	NAMEBLOCK	1000
@

[[subst()]] then substitutes the stem in another template 
(e.g., [[foo%.c]])
and stores the result in another buffer passed as a parameter.
%
Here is the output buffer [[applyrules()]] passes to [[subst()]]:

<<[[applyrules]] other locals>>=
char buf[NAMEBLOCK];
@


Just like for the simple rules, [[mk]] can ignore certain meta rules:

<<[[applyrules()]] skip this meta rule and continue if some conditions>>=
if(empty_recipe(r) && empty_prereqs(r)) 
    continue;	/* no effect; ignore */
@
\l could issue a warning, can tools generate such meta rules? 
\l could factorize with previous one now that cleaned up dead code

Some meta rules can also contain virtual targets:

<<[[applyrules()]] if no prerequisites in meta rule r>>=
if(empty_prereqs(r)) {
    arc = newarc((Node *)nil, r, stem, rmatch);
    // add_list(head, arc)
    lasta->next = arc;
    lasta = lasta->next;
} 
@

The code above is almost identical to code 
in Section~\ref{sec:finding-simple-rules},
except [[applyrules()]] passes here the [[stem]] buffer 
to [[newarc()]] instead of the empty string.
\l why? need store stem in arc! otherwise lose information for jobs
%pad: use empty_prereqs() again

\subsection{Matching a pattern, [[match()]]}

[[match()]] below iterates over two string parameters
([[name]] and [[template]])
at the same time by incrementing both pointers until it finds the special
pattern character ([['%']] or [['&']]) in the [[template]] parameter.
%
Once it found the pattern character, it computes the length of
the string matched by the pattern character and it makes sure the rest
of the template also matches the end of the name.
%
Finally it modifies the [[stem]] buffer passed
from [[applyrules()]].
\l stem is OUT. pointer to array to modify.
Figure~\ref{fig:mk-match} illustrates how [[match()]] works
on a simple example.

<<function match>>=
bool
match(char *name, char *template,    char *stem)
{
    Rune r;
    int n;

    // Before the pattern character
    while(*name && *template){
        n = chartorune(&r, template);
        if (PERCENT(r))
            break;
        while (n--)
            if(*name++ != *template++)
                return false;
    }

    // On pattern character
    if(!PERCENT(*template))
        return false;
    // how many characters % is matching
    n = strlen(name) - strlen(template+1);
    if (n < 0)
        return false;

    // After the pattern character
    if (strcmp(template+1, name+n))
        return false;

    strncpy(stem, name, n);
    stem[n] = '\0';

    <<[[match()]] if ampersand template>>

    return true;
}
@
\l return false many times
\l unicode and variable-length characters makes the code harder to read

\begin{figure}[!]\centering
\begin{verbatim}
                              name
      name                    |  name+n
      |                       |  |
      v----------+        +---v--v---+
      |foobar.5  |        |foobar.5  |
      +----------+        +----------+
                              <->
                               n

     template                template
       |                       |
       v--------+          +---v----+
       |foo%.5  |          |foo%.5  |
       +--------+          +--------+

       +-----+             +-------+
  stem |     |        stem |bar    |
       +-----+             +-------+

----------------------------------------
    start of match()     end of match()

\end{verbatim}
\caption{[[match("foobar.5", "foo\%.5", ...)]].}\label{fig:mk-match}
\end{figure}


%dup: ds/rules/metarules
As I mentioned in Section~\ref{sec:PERCENT}, 
[[mk]] supports two kinds of pattern characters: [['%']] and [['&']].
%
The [['&']] pattern can not match filenames containing a dot or
a slash:

<<[[match()]] if ampersand template>>=
if(*template == '&')
    return !charin(stem, "./");
@
\l adv topics?

\subsection{Substituting the stem, [[subst()]]}

Figure~\ref{fig:mk-subst} illustrates how [[subst()]] works
on a simple example.

<<function subst>>=
void
subst(char *stem, char *template,   char *dest, int dlen)
{
    Rune r;
    char *s, *e;
    int n;

    e = dest + dlen - 1;
    while(*template){
        n = chartorune(&r, template);
        if (PERCENT(r)) {
            template += n;
            for (s = stem; *s; s++)
                if(dest < e)
                    *dest++ = *s;
        } else
            while (n--){
                if(dest < e)
                    *dest++ = *template;
                template++;
            }
    }
    *dest = '\0';
}
@

\begin{figure}[!]\centering
\begin{verbatim}
       +-------+           +-------+
  stem |bar    |      stem |bar    |
       +-------+           +-------+

     template                template
       |                          |
       v----------+        +------v---+
       |foo%.c    |        |foo%.c    |
       +----------+        +----------+

                                      e
                                      |
       +-----------+       +----------v+
  dest |           |  dest | foobar.c  |
       +-----------+       +-----------+

----------------------------------------
    start of subst()     end of subst()
\end{verbatim}
\caption{[[subst("bar", "foo\%.c", ...)]].}\label{fig:mk-subst}
\end{figure}


\section{Node cache}
\label{sec:node-cache}
\l generalize to Cache? with node cache, time cache, etc?
% not really a cache here

As I mentioned in Section~\ref{sec:dag}, the graph of dependencies
can be more than a simple tree; it can also be a 
direct acyclic graph (DAG).
%
Thus, before creating a new node for a target, [[applyrules()]] 
makes sure the target node does not exist already.
%
To do so, it relies on the new namespace below:
\l realpath issues? should always use same path to reference same file
\l  in the mkfile (if multiple mkfiles, then anyway recursive mk)

<<[[Sxxx]] cases>>=
S_NODE,		/* target name -> node */
@
\n S_NODE also used at a few places so cant delay that to opti section

Each time [[newnode()]] creates a new node for a given file,
\l or virtual target
it adds this node in the symbol table before returning it
(e.g., to [[applyrules()]]).

<<[[newnode()]] update node cache>>=
symlook(name, S_NODE, (void *)node);
@

Then, [[applyrules()]] can consult the symbol table and 
possibly returns a pointer to a previously created node instead
of recomputing the subtree for this node:

<<[[applyrules]] check node cache if target is already there>>=
sym = symlook(target, S_NODE, nil);
if(sym)
    return sym->u.ptr;
@

Creating a single node per file is important not only to avoid unnecessary
calls to [[applyrules()]]. Indeed, once [[mk]] finished
executing a recipe, [[mk]] can update the modification time of the target
created by the recipe by modifying a single node.
\l also otherwise you will run multiple times the same recipe, you will
\l  not realize it is the same node


\section{[[timeof()]]}
% for the leaves!

%trans: 
The last important function used to build the graph of dependencies
is [[timeof()]] called from [[newnode()]] (see Section~\ref{sec:newnode})
to {label} a node.
% with its modification time.
%dup: overview/principles/graph/labeled-tree
Indeed, as I explained in Section~\ref{sec:labeled-tree}, each node
is labeled with the modification time of the file it represents.

% newnode() -> <> -> mkmktime -> bulktime
%                             -> dirstat (libc)
<<function timeof>>=
ulong
timeof(char *name, bool force)
{
    <<[[timeof()]] locals>>

    <<[[timeof()]] if name archive member>>
    if(force)
        return mkmtime(name, true);
    // else
    <<[[timeof()]] if not force, use time cache>>
}
@
%old: ulong mtime(char *name) {  return mkmtime(name, true);}
%but simpler to remove, refactor caller sites

I will explain in Section~\ref{sec:time-cache} 
the [[force]] argument.
\l actually newnode calls timeof with false! so leverage cache



%\subsection{[[mkmtime()]]}

<<function mkmtime>>=
ulong
mkmtime(char *name, bool force)
{
    Dir *d;
    ulong t;
    <<[[mkmtime]] locals>>

    <<[[mkmtime()]] bulk dir optimisation>>
    d = dirstat(name);
    <<[[mkmtime()]] check if inexistent file>>
    t = d->mtime;
    free(d);

    return t;
}
@
%old: I moved assign [[d]] outside if.


Note that if the file does not exist, [[mkmktime()]] 
and [[timeof()]] return [[0]]:

<<[[mkmtime()]] check if inexistent file>>=
if(d == nil)
    return 0;
@




\section{Checking the graph and the rules}
\label{sec:check-graph}
\n was 'checking the rules', but really it is more checking the graph

%trans:
Once [[applyrules()]] returned the graph of dependencies,
%
[[graph()]] can check for special properties of the graph
that are signs of mistakes in the [[mkfile]]:

<<[[graph()]] checking the graph>>=
cyclechk(root);

<<[[graph()]] before [[ambiguous()]]>>
ambiguous(root);
@
\l can vacuous after ambiguous? NO! cos goal of vacuous is
\l to delete some nodes and arcs that would generate ambiguity (said later?)

%toc:
There a few mistakes [[mk]] can detect, as explained
in the following sections.
\l Also simplify the graph by removing some arcs (see togo()).
% Give priority for instance to specialized over meta rules. So
% important calls below.
\l detect statically vs dynamically? important?



\subsection{Cycle detection}
\label{sec:cycle-check}

%trans:
The first error [[mk]] can detect is the presence of a {\em cycle}
in the graph, as shown in Figure~\ref{fig:graph-cycle}.
\l need explain why bad? because then would loop forever?
%
Here is an example of an [[mkfile]] that leads to the graph 
in Figure~\ref{fig:graph-cycle}:

<<tests/mk/mkfile-with-cycle>>=
foo: foo.5 bar.5
    5l -o foo foo.5 bar.5
%.5: %.c
    5c -c $stem
foo.5: foo.h
bar.5: foo.h

VERSION=2
foo.h: foo    # typo, should be foo.x
    cat foo.x | sed -e s/VERSION/$VERSION/ > foo.h
@
%$

In the [[mkfile]] above, the user made a typo and added a dependency
from [[foo.h]] to [[foo]] instead of [[foo.x]].
%
Here is the error message displayed by [[mk]] for this mkfile:

\begin{verbatim}
$ mk -f mkfile-with-cycle
mk: cycle in graph detected at target foo
\end{verbatim}
%ocaml: better error message with full trace

\begin{figure}[!]\centering
\begin{verbatim}
           +-------+
           |  foo  |<---------------+
           +-------+                |
               X                    |
              / \                   |
       /-----/   \-------\          |
      /                   \         |
     v                     v        |
+--------+            +--------+    |
| foo.5  |            | bar.5  |    |
+--------+            +--------+    |
     |                     |        |
     | \                  /|        |
     |  \------\ /-------/ |        |
     |          v          |        |
     v     +--------+      v        |
+--------+ | foo.h  | +--------+    |
| foo.c  | +--------+ | bar.c  |    |
+--------+      |     +--------+    |
                |                   |
                |                   |
                +-------------------+
\end{verbatim}
\caption{A cycle in a graph of dependencies.}
\label{fig:graph-cycle}
\end{figure}

To detect the mistake above, [[mk]] performs a DFS on the graph
\l actually not really DFS, performs multiple times the same thing
\l Could optimize and have both a visited field and cycle field
\l  so dont explore twice the same already checked subtree?
\l  NO, cos context might be different and we might have a loop.
\l tarjan?
and marks nodes with the special flag below when it visits a node:

<<[[Node_flag]] cases>>=
CYCLE      = 0x0002,
@

This flag is stored in a unique bit in the [[Node.flag]] field;
it will not conflict with the other node flags that I introduced
in Section~\ref{sec:node-flags} (e.g., [[NOTMADE]] is [[0x0020]]).

The function below assumes it is called with
the [[CYCLE]] flag unset for every nodes, 
which is true because [[newnode()]] set
[[Node.flag]] to [[0]] (see Section~\ref{sec:newnode}).
\n actually to PROBABLY sometimes (said later)

<<function cyclechk>>=
static void
cyclechk(Node *n)
{
    Arc *a;

    if((n->flags&CYCLE)){
        fprint(STDERR, "mk: cycle in graph detected at target %s\n", n->name);
        Exit();
    }
    n->flags |= CYCLE;
    for(a = n->arcs; a; a = a->next)
        if(a->n)
            cyclechk(a->n);
    n->flags &= ~CYCLE;
}
@
%ocaml: sucks that does not show trace, so hard to debug => I give a trace!
%dead? bug?
%  if((n->flags&CYCLE && n->arcs)) ????? why n->arcs check?

Note that it is important for [[cyclechk()]] to remove
the [[CYCLE]] flag at the very end before returning.
%
Indeed, the graph of dependencies can be a DAG, in which case the same node
may be referenced from multiple parents 
(e.g., [[foo.h]] in Figure~\ref{fig:graph-cycle}).
%
Without the last instruction in [[cyclechk()]], 
[[mk]] would report another (this time wrong) cycle when visiting
[[foo.h]] for the second time from [[bar.5]] instead of [[foo.5]].

\l static detection of cycle. possible only because restriction on
\l  variables and time of evaluation of those variables.
% Need to be known at parsing-time. Static cycle detection check.
% (but dynamic could be ok too).

%real-world:
% Make drops the circular dependency 
%   make: Circular foo.h <- foo dependency dropped.

\subsection{Infinite rule detection}%$

%trans:
The second mistake [[mk]] can detect is the presence of
meta rules that [[mk]] could instantiate infinitely.
%
Here is an example of an [[mkfile]] illustrating the issue:

<<tests/mk/mkfile-infinite>>=
all: foo bar

%: %.5
   5l $stem.5 -o $stem
%.5: %.c
	5c -c $stem.c
@
%$

When building the graph for [[foo]], [[applyrules()]]
can instantiate first the meta rule [[%: %.5]] with [[% = foo]].
%
However, [[applyrules()]] when called recursively
with [[foo.5]] as a target could again
instantiate the meta rule [[%: %.5]], this time with [[% = foo.5]]
\label{sec:ampersand-example}
\footnote{This is one of the motivations for the  
special pattern character [['&']].}.
This process could go forever, which would stuck [[mk]].



To avoid this infinite process, [[mk]] still allows the use of
the meta rule above, but severely restricts its application.
%
Indeed, [[mk]] allows to apply a specific rule only once during
the building of a branch in the graph of dependencies.
%
To do so, each rule gets first a unique {\em rule identifier}
stored in [[Rule.rule]]:
\l useful to do check rulecnt for simple rules? mk does but why?

<<[[Rule]] other fields>>=
int 		rule;		/* rule number */
@
\l rule -> ruleid

<<global nrules>>=
static int nrules = 0;
@
<<[[addrule()]] set more fields>>=
r->rule = nrules++;
@

Then, before calling [[applyrules()]], [[graph()]] initializes
a map that will count how many times [[applyrules()]] used a rule:

<<[[graph()]] other locals>>=
// map<ruleid, int>
char *cnt;
@
<<[[graph()]] set cnt for infinite rule detection>>=
cnt = rulecnt();
@

<<function rulecnt>>=
char*
rulecnt(void)
{
    char *s;

    s = Malloc(nrules);
    memset(s, 0, nrules);
    return s;
}
@
\n this is not for Profiling.

<<[[graph()]] free cnt>>=
free(cnt);
@

[[graph()]] then passes this map as the second argument
to [[applyrules()]] (see Section~\ref{sec:graph-applyrules}).
%
Finally, [[applyrules()]] modifies [[cnt]]
when it uses a rule {before} possibly calling itself recursively.
%
[[applyrules()]] can then skip the rule if it already used the rule
in a parent call to [[applyrules()]]:

<<global nreps>>=
int nreps = 1;
@
% n repetition?


<<[[applyrules()]] infinite rule detection part1>>=
if(cnt[r->rule] >= nreps) 
    continue;

cnt[r->rule]++;
@

Note that [[applyrules()]] must decrement the rule counter
{after} the recursive call to itself. 
%
Indeed, it is ok to instantiate multiple times the same meta rule
in independent branches of the graph of dependencies, for instance,
the meta rule [[%.5: %.c]]
with the [[foo.5]] and [[bar.5]] nodes in Figure~\ref{fig:graph-cycle}.

<<[[applyrules()]] infinite rule detection part2>>=
cnt[r->rule]--;
@
\l We just dont want to forbid it when call
\l applyrules recursively with the prereqs as the new targets.

\l can change nrep dynamically, see adv topics



\subsection{Ambiguous rules detection}
\label{sec:ambiguous-checks}
\l also modify graph to fix things.

%trans:
The third error [[mk]] can detect is the use of {\em ambiguous rules}.
%
For example, in the [[mkfile]] below, if both [[foo.c]] and [[foo.h]]
are more recent than [[foo.5]], [[mk]] can not decide
which recipe to run to update [[foo.5]].

<<tests/mk/mkfile-ambiguous>>=
all: foo

foo: foo.5
    5l -o foo foo.5

foo.5: foo.c
    5c -c foo.c
foo.5: foo.h
    5c -g -c foo.c
@

Here is the error message displayed by [[mk]] for this mkfile:

\begin{verbatim}
$ mk -f mkfile-ambiguous
mk: ambiguous recipes for foo.5:
	foo.5 <-(mk-ambiguous-simple:7)- foo.c
	foo.5 <-(mk-ambiguous-simple:9)- foo.h
\end{verbatim}
%other real example:
%mk: ambiguous recipes for install:
%	install <-(/sys/src/cmd/mklib:28)- libthreads.a <-(/sys/src/cmd/mklib:12)- libthreads.a(scheduler.8)
%	install <-(mkfile:19)- 

To check for ambiguities, [[mk]] goes through every nodes and checks
whether a node has two arcs with different recipes.
%
As I said in Section~\ref{sec:master-rule}, [[mk]] allows to write
multiple rules involving the same target as long as only one rule,
the {master rule}, has a recipe.


<<function ambiguous>>=
static void
ambiguous(Node *n)
{
    Arc *a;
    Rule *master_rule = nil;
    Arc *master_arc = nil;
    bool error_reported = false;

    for(a = n->arcs; a; a = a->next){
        // recurse
        if(a->n)
            ambiguous(a->n);

        // arcs without any recipe do not generate ambiguity
        if(empty_recipe(a->r)) 
            continue;
        // else

        // first arc with a recipe (so no ambiguity)
        if(master_rule == nil) {
            master_rule = a->r;
            master_arc = a;
        }
        else{
            <<[[ambiguous()]] give priority to simple rules over meta rules>>
            if(master_rule->recipe != a->r->recipe){
                if(!error_reported){
                    fprint(STDERR, "mk: ambiguous recipes for %s:\n", n->name);
                    error_reported = true;
                    trace(n->name, master_arc);
                }
                trace(n->name, a);
            }
        }
    }
    if(error_reported)
        Exit();
    <<[[ambiguous()]] get rid of all skipped arcs>>
}
@
%old: r -> master_rule, la -> master_arc, bad -> error_reported
%old: *a->r->recipe == '\0' -> empty_recipe() (and assert != nil)
\l could optimize again and avoid visiting node already visited

%ocaml-found:
Note that before reporting an ambiguity, the code above checks
whether the recipes in the two different arcs are different
(using a simple pointer comparison, not [[strcmp()]]).
\l why not? could.
Having a node with multiple arcs does not necessarily mean ambiguity.
For example, [[foo]] in Figure~\ref{fig:graph-cycle} has arcs to
[[foo.5]] and [[bar.5]]. 
However those two arcs share the same recipe (the linking command),
so there is no ambiguity in building [[foo]].
%
When [[mk]] builds the graph, it splits simple rules
such as [[foo: foo.5 bar.5 ...]] in multiple arcs, but it
remembers also that all those arcs come from the same rule.


[[ambiguous()]] calls [[trace()]] below for each ambiguous arc:

<<function trace>>=
static void
trace(char *s, Arc *a)
{
    fprint(STDERR, "\t%s", s);
    while(a){
        fprint(STDERR, " <-(%s:%d)- %s", a->r->file, a->r->line,
            a->n? a->n->name:"");
        <<[[trace()]] possibly continue if prereq is also a target>>
        else
            a = nil;
    }
    fprint(STDERR, "\n");
}
@


<<[[trace()]] possibly continue if prereq is also a target>>=
if(a->n){
    for(a = a->n->arcs; a; a = a->next)
        if(*a->r->recipe) 
            break;
}
@
\l put full error so see full trace?

\subsubsection{Specialized versus generic rules}

It is common to want to write a generic meta rule that can
handle most files and some {specialized rules} for a few files. 
%
For example, only a few files may require to be compiled with special flags, 
as in the following example:

<<tests/mk/mk-generic-specialized>>=
foo: foo.5 bar.5 foobar.5

%.5: %.c
    5c -c $stem.c

foobar.5: foobar.c
    5c -c -D VERSION=1 foobar.c
@
%$

Normally, [[mk]] should report an ambiguity with the previous [[mkfile]].
%
Indeed, [[foobar.5]] matches the meta rule,
which would lead to a second arc from [[foobar.5]] to [[foobar.c]]
in the graph of dependencies, with the recipe of the meta rule.
%
However, [[mk]] allows ambiguity between two rules 
when one of the two rules is a simple rule.
%
{\em Simples rules have priority over meta rules}, thanks to the code
below:

<<[[ambiguous()]] give priority to simple rules over meta rules>>=
if(master_rule->recipe != a->r->recipe){
    if((master_rule->attr&META) && !(a->r->attr&META)){
        master_rule = a->r;
        master_arc->remove = true;
        master_arc = a;
    } else if(!(master_rule->attr&META) && (a->r->attr&META)){
        a->remove = true;
        continue;
    }
}
@
%old: was using TOGO
\l mk allows also to overwrite a previous simple rule, but this is done
\l  at parsing time

The code above not only adjusts the master rule to be the
specialized rule, it also marks the arc containing
the meta rule as ``to-be-removed'', thanks to the following field:

<<[[Arc]] other fields>>=
short remove;
@
%old: was flag, like Node.flag, but used only for removing
% <<constant TOGO>>=
% /* Arc.flag */
% #define		TOGO		true
% @

<<[[newarc()]] set other fields>>=
a->remove = false;
@

\subsubsection{Adjusting the graph, [[togo()]]}

[[ambiguous()]],
%calls itself recursively, but 
before returning, removes all the arcs marked as to-be-removed 
on the current node:

<<[[ambiguous()]] get rid of all skipped arcs>>=
togo(n);
@

<<function togo>>=
static void
togo(Node *node)
{
    Arc *a; 
    Arc *preva = nil;

    /* delete them now */
    for(a = node->arcs; a; preva = a, a = a->next)
        if(a->remove){
            //remove_list(a, node->arcs)
            if(a == node->arcs)
                node->arcs = a->next;
            else {
                preva->next = a->next;
                a = preva;
            }
        }
}
@
%old: la -> preva
%pad: I orginally added a warning, but actually we should not!
%            fprint(STDERR, "mk: vacuous arc found %s->%s\n", 
%                     node->name, a->n->name);
\l could do it for explain mode
%ocaml: useful only for ambiguous for specialized vs generic so do
% adjustments there by resetting the list of prereqs

\l is there a leak! should remove all children of this arc? 
% No because DAG so potentially referenced elsewhere

\l why not mark TOGO for rule without recipe? because we still want this
% arc for outdated check! this arc does not contain a recipe, but it
% is still important, e.g. dependencies like foo.o: foo.h


\subsubsection{Vacuous nodes removal}
\n def of vacuous =~ stupid

%trans: 
As we have just seen before,
%dup: graph/check/ambiguous/specialised-vs-genric
the use of a specialized simple rule and a generic meta rule can lead to 
unfortunate ambiguities. 
%
[[mk]] handles some ambiguities by giving priorities to specialized rules,
which is convenient.
% 
The use of multiple meta rules can also lead to unfortunate ambiguities.
%ocaml-found:
For example, in the [[mkfile]] below, [[foo.5]] could be generated
either from a [[foo.c]] C file or from a [[foo.s]] assembly file.

<<tests/mk/mkfile-vacuous>>=
all: foo

foo: foo.5 bar.5
   5l foo.5 bar.5 -o foo
%.5: %.c
   5c -c $stem.c
%.5: %.s
   5a $stem.s
@
\l (as in plan9/sys/src/cmd/mkone)

Again, [[mk]] should normally report an ambiguity with this
[[mkfile]] because there will be two arcs from [[foo.5]]
with two different recipes in the graph of dependencies
(one arc to [[foo.c]] and another one to [[foo.s]]).
%
However, it is convenient to factorize rules for different
programming languages with different meta rules.
%
In practice, only one of the two meta rules above should apply
for each source file in a project. The directory of the project above
should contain either [[foo.c]] or [[foo.s]], but not both.


This is why [[mk]] allows the use of multiple meta rules
that can conflict with each other if it can detect that certain nodes
instantiated from certain meta rules (e.g., [[foo.c]], [[foo.s]]) are
{\em vacuous} because they do not correspond to existing files.
%
To detect and remove vacuous nodes, [[mk]] relies first on
the following node flag:

<<[[Node_flag]] cases>>=
PROBABLE   = 0x0100,
@
\l not very clear name, but means file exist so probably a good node

[[mk]] marks nodes as [[PROBABLE]] when they contain
an existing file, which can be checked by looking at the
modification time of the file in [[newnode()]]:


<<[[newnode()]] adjust flags of node>>=
if(node->time)
    node->flags = PROBABLE;
@
%old: node->flags = (node->time? PROBABLE : 0);
% but then cant say newnode initialize flags to 0 without speaking
% about PROBABLE

In the example above, [[mk]] would mark [[foo.c]] as [[PROBABLE]]
but not [[foo.s]] if the directory contains only [[foo.c]].


Moreover, [[mk]] marks also nodes mentioned as targets
of simple rules as [[PROBABLE]]:

<<[[applyrules()]] when found a regular rule for target [[node]], set flags>>=
node->flags |= PROBABLE;
@
\l make sense, even if file does not exist, probably node we are interested in?

Finally, the root node is also marked as [[PROBABLE]]
because it is not a node we want [[mk]] to remove,
even if it does not correspond yet to an existing file:

<<[[graph()]] before [[ambiguous()]]>>=
root->flags |= PROBABLE;	/* make sure it doesn't get deleted */
@


Once [[mk]] initialized the nodes in the graph of dependencies,
[[graph()]] calls [[vacuous()]] to explore the graph to
detect and remove vacuous nodes.

<<[[graph()]] before [[ambiguous()]]>>=
vacuous(root);
@

Note that [[graph()]] must call [[vacuous()]] before [[ambiguous()]],
to remove the vacuous nodes and arcs,
otherwise [[ambiguous()]] would report ambiguities.

[[vacuous()]] relies on two extra node flags to operate:

<<[[Node_flag]] cases>>=
VACUOUS    = 0x0200,
@
\l goal is to mark certain nodes as VACUOUS.

<<[[Node_flag]] cases>>=
READY      = 0x0004,
@
\l for what? not very clear name. rename DONE? VISITED?
\l also marked node as vacuous_checked; used later in mk()

[[VACUOUS]] marks vacuous nodes and [[READY]] marks nodes
[[vacuous()]] already visited (the graph can be a DAG).

%trans:
Here is finally the code of [[vacuous()]]:

<<function vacuous>>=
static bool
vacuous(Node *node)
{
    Arc *a;
    bool vac = !(node->flags&PROBABLE);
    <<[[vacuous()]] other locals>>

    if(node->flags&READY)
        return node->flags&VACUOUS;
    node->flags |= READY;

    for(a = node->arcs; a; a = a->next)
        if(a->n && vacuous(a->n) && (a->r->attr&META))
            a->remove = true;
        else
            vac = false;
    <<[[vacuous]] possibly undelete some arcs>>

    togo(node);
    if(vac) {
        node->flags |= VACUOUS;
    }
    return vac;
}
@
%pad: I've originally added the warning
%        fprint(STDERR, "mk: vacuous node found %s\n", node->name);
%   but this is wrong. a vacuous node is ok.

Note that [[vacuous()]] removes only arcs derived from meta rules.
%
In the example I mentioned before, [[vacuous()]] would mark
[[foo.c]] as [[PROBABLE]] but not [[foo.s]].
%
Then, [[vacuous()]] would return true when exploring the
[[foo.s]] node, because the node did not have the [[PROBABLE]] mark
and the node does not have any children (so [[vac]] will remain true).
%
Then, when [[vacuous()]] backtracks on the [[foo.5]] node, 
during the arc itration, then [[vacuous()]] will mark
the arc to [[foo.s]] as to-be-removed.

<<[[vacuous()]] other locals>>=
Arc *a2;
@
%old: la -> a2

<<[[vacuous]] possibly undelete some arcs>>=
/* if a rule generated arcs that DON'T go; no others from that rule go */
for(a = node->arcs; a; a = a->next)
    if(!(a->remove))
        for(a2 = node->arcs; a2; a2 = a2->next)
            if((a2->remove) && (a2->r == a->r)){
                a2->remove = false;
            }
@
\t when this matter? example?





\chapter{Finding Outdated Files}
\label{chap:finding-outdated}

%trans:
The next component in the building pipeline is responsible for 
%
finding outdated files in the graph of dependencies built 
in Chapter~\ref{chap:graph}.
%toc:
In this chapter, you will see the function containing the core algorithm
behind [[mk]].
%
This function, appropriately named [[mk()]], 
takes a target as a parameter, 
builds the graph of dependencies for this target (with [[graph()]]), 
explores the graph to find outdated files, 
with a function called [[work()]],
and schedules recipes to be executed to update those outdated files,
with a function called [[dorecipe()]].
%toc:
The following sections will explain the code of [[mk()]], [[work()]], and
[[dorecipe()]].

\section{[[mk()]]}
\label{sec:mk}

%dup: main/parallel
[[mk()]] is arguably the most important function in [[mk]].
%
I outlined its core algorithm 
in Section~\ref{sec:job-scheduler-principles} when I introduced 
the principles of a job scheduler, 
and in Section~\ref{sec:soft-archi} when I presented 
the software architecture of [[mk]].
%dup: overview/principles/scheduler
Figure~\ref{fig:graph-being-made2} is a copy of 
Figure~\ref{fig:graph-being-made}.
\n actually I added the time
%
The goal is just to remind you of the order in which [[mk]] visits
the nodes of a graph during the DFS, and of the changes to the 
building status of nodes ([[NOTMADE]], [[BEINGMADE]], and [[MADE]])
during the DFS.
%
In the scenario of Figure~\ref{fig:graph-being-made2}, the user
modified [[world.c]], which should trigger a compilation to regenerate
[[world.5]] and a linking command to regenerate [[hello]].


%dup: overview/principles/scheduler
\begin{figure}[!]\centering
\begin{verbatim}
                     +-------+
                     | hello |08:00:13
                 (1) +--X--^-+ (8)
                         X  \  NotMade
                      / / \  \
                / - -  /   \  \----\
                 /----/     \----\  \
              / /                 \  \
             v v                   v  \
          +--------+          +--------+
 08:00:10 |hello.5 + - - - - >|world.5 |08:00:11
      (2) +--+-----+(4)    (5)+--+-----+ (7)
               | ^ Made            | ^  BeingMade
             | | |               | | |
             v v                 v v
          +------+-+          +------+-+
  08:00:00|hello.c |      (6) |world.c |11:30:00
      (3) +--------+     Made +--------+
     Made
\end{verbatim}
\caption{Graph of dependencies with building-status labels.}
\label{fig:graph-being-made2}
\end{figure}

% FIGURE displaying a tree of files and see how mk is running some waves 
% of jobs, and how at each node there is a ready or outofdate boolean value.

%trans:
Here is the code of [[mk()]]:

% main -> <> -> graph() ; clrmade(); work()
<<function mk>>=
void
mk(char *target)
{
    Node *root;
    bool everdid = false;
    bool did = false;
    // enum<WaitupResult>
    int res;

    <<[[mk()]] initializations>>

    root = graph(target);
    <<[[mk()]] if DEBUG(D_GRAPH)>>
    clrmade(root);

    while(root->flags&NOTMADE){
        did = false;
        work(root, &did,   (Node *)nil, (Arc *)nil);
        if(did)
            everdid = true;	/* found something to do */
        else {
            res = waitup(EMPTY_CHILDREN_IS_OK, (int *)nil);
            <<[[mk()]] if no child to waitup and root not MADE, possibly break>>
        }
    }
    if(root->flags&BEINGMADE)
        waitup(EMPTY_CHILDREN_IS_ERROR, (int *)nil);

    <<[[mk()]] before returning, more [[waitup()]] if there was an error>>
    if(!everdid)
        Bprint(&bout, "mk: '%s' is up to date\n", root->name);
    return;
}
@
%old: 
% - node -> root
% - was if(waitup(1, (int *)nil) > 0) but hard to read, I introduced res
% - the code was threading a 'did' local variable in work and dorecipe and doing
%   code like did = foo() || did; which was ugly. I thought of introducing
%   a global 'did' instead. Then in mk-ocaml I pass a 'did' local
%   by reference. cleaner. This changes the interface of work, dorecipe;
%   they dont return a bool anymore.
% - '1' '-1' -> EMPTY_CHILDREN_IS_XXX
%pad:
% - I added an everdid
% - I added the return
%ocaml: When can have did==false so print 'xx is up to date'?
% when did nothing, because prog was _already_ up to date.
% in mk-ocaml I changed the message.

[[mk()]] operates in 6 steps:

\begin{enumerate}

\item [[mk()]] builds the graph of dependencies with [[graph()]]
(see Chapter~\ref{chap:graph}).

\item [[mk()]] sets the {building status} of all nodes in the graph
to [[NOTMADE]] 
%(see Section~\ref{sec:build-status} and Section~\ref{sec:node-flags}),
with [[clrmade()]] 
(see Section~\ref{sec:clrmade}).

%<<[[Node_flag]] cases>>=
%NOTMADE    = 0x0020,
%BEINGMADE  = 0x0040,
%MADE       = 0x0080,
%@
%ocaml: should be different field than other node flags

% NOTMADE initially (see clrmade())
% BEINGMADE? when start recipe, but mk can run in // so it returns!
%   can go back to node with BEINGMADE? when
%   2 files with deps on same Node, because of the DAG
% MADE finally when recipe done! (or time was already good, so no need update)

\item [[mk()]] explores the graph of dependencies repeatedly
with [[work()]] (see Section~\ref{sec:work}), in successive {waves}, 
to find outdated files.
%
When [[work()]] finds up-to-date files, it marks them as [[MADE]],
(e.g., [[hello.c]] in Figure~\ref{fig:graph-being-made2}).
%
When it finds outdated files, [[work()]] schedules a recipe to be executed
with [[dorecipe()]] (see Section~\ref{sec:dorecipe}), 
and marks the node as [[BEINGMADE]].
%
For example, in Figure~\ref{fig:graph-being-made2}, [[work()]]
discovered that [[world.5]] was older than [[world.c]]
and so scheduled a recipe to update [[world.5]]; [[work()]] also marked
[[world.5]] as [[BEINGMADE]].
%
[[work()]] also sets to true the [[did]] boolean passed by reference
when it schedules a job. 
%That way, [[mk()]] can know if ??

\item Sometimes, [[work()]] does not schedule any new job and [[did]] 
stays false.
%
One reason might be that there is nothing to build because everything
is already up-to-date.
%
Another reason is that [[work()]] may need to wait for some processes
to finish. For example, in Figure~\ref{fig:graph-being-made2}, 
if a previous wave of [[work()]] scheduled [[world.5]] to be built
(hence the [[BEINGMADE]] flag), another wave of [[work()]]
might not be able to schedule anything because [[hello]] can be built only if
all the object files are [[MADE]].
\t then why not always call waitup after did is true? why run another wave?
\t why not waitup after work anyway even if did = true?
\t  when can you schedule jobs, and then do another wave that can find more?
\t  anyway you need to waitup for update() to be called

%
In that case, [[mk()]] calls [[waitup()]] (see Section~\ref{sec:waitup})
to wait for some user processes executing recipes to finish.
\n waitup, wait user process?
%
[[waitup()]] will wait for a job to finish and will set as [[MADE]]
the nodes that were the targets of the job.
%
Thus, the next wave of [[work()]] will be able to progress.
\l note that waitup waits for only 1 child, and schedules possibly
\l  only one child, so really you call work() a lot! after each process
\l  finishes you try to find more opportunities.


\item At some point, [[work()]] will schedule a job for the root target
in which case its building status will switch from [[NOTMADE]] to [[BEINGMADE]].
\l can be to MADE directly? yes if virtual target (hmm or BEINGMADE cos fake?)
\l (or if nothing to build from the start)
%ocaml-found:
[[mk()]] will then wait one more time for
the children process responsible for building the root target to finish.

\item [[mk()]] returns.

\end{enumerate}

The code of [[mk()]] is small, but subtle.
%Here are a few additional notes on [[mk()]]:
For example, the first call to [[waitup()]] in [[work()]] 
uses the [[EMPTY_CHILDREN_IS_OK]] flag (see Section~\ref{sec:waitup})
to indicate that it is ok if [[waitup()]] does not find any children
process to wait for.
%
Indeed, everything was maybe already up-to-date, in which case
[[work()]] will not schedule any job ([[work()]] will have marked the 
root node as [[MADE]]) and [[did]] will stay false.
%
However, the second call to [[waitup()]] in [[work()]] uses
the [[EMPTY_CHILDREN_IS_ERROR]] flag because [[mk]] knows
at this point that there is still the process responsible 
to update the root target to wait for.
%ocaml-found:
% Also when top is virtual target with no recipe e.g., all: prog,
% and previous round built prog and updated to MADE the prog node.
% Still the root node is not made, and work will not trigger
% any new process; it will update though all to MADE but we should
% not waitup() and expect children.

The last two parameters of [[work()]] contains the parent
node and parent arc of the current node. Here in [[mk()]] we
start from the root node, so both arguments are [[nil]].
%
You will see in Section~\ref{sec:pretending} why [[work()]]
needs such information for advanced features of [[mk]].

\l can have runerrs only when use mk -k ? so could put that in assert too?
% NO, can have it also when C-c.



\section{Initializing nodes, [[clrmade()]]}
\label{sec:clrmade}

[[clrmade()]] below simply marks all nodes as [[NOTMADE]]:

<<function clrmade>>=
void
clrmade(Node *n)
{
    Arc *a;

    <<[[clrmade()]] [[n->flags]] pretend adjustments>>
    MADESET(n, NOTMADE);
    for(a = n->arcs; a; a = a->next)
        if(a->n)
            // recurse
            clrmade(a->n);
}
@
%bug? DAG? should avoid repetitive work too there?
%ocaml: no need, do that in newnode() thx to another field.

Note that [[Node.flag]] does not contain only the building status of a node.
%(see Section~\ref{sec:node-flags}).
\l ex? virtual attribute?
This is why the macro below makes sure it resets only the bits
in [[Node.flag]] that contain the building status, and leaves unchanged
the other bits.

<<function MADESET>>=
#define	MADESET(n,m)	n->flags = (n->flags&~(NOTMADE|BEINGMADE|MADE))|(m)
@
%ocaml: use a separate field than the other node attributes



\section{Exploring the graph, [[work()]]}
\label{sec:work}

[[work()]] below performs a DFS on the graph while looking for
outdated [[NOTMADE]] nodes:

% main -> mk -> <> -> outofdate(); dorecipe()
<<function work>>=
void
work(Node *node, bool *did,   Node *parent_node, Arc *parent_arc)
{
    <<[[work()]] locals>>

    <<[[work()]] debug>>
    if(node->flags&BEINGMADE)
        return;
    <<[[work()]] possibly unpretending node>>

    if(node->arcs == nil){
        <<[[work()]] no arcs, a leaf>>
    } else {
        <<[[work()]] some arcs, a node>>
    }
}
@
%old: 
% - <<[[work()]] locals>>=
%   bool did = false;
%   @
%   I now pass it as a reference! so no need return bool in work() also.
% - I remove some return in leaf case so and rewrite with cleaner if then else
% - p -> parent_node, parc -> parent_arc
% extra params for advanced functionality not needed
\l BEINGMADE test because DAG? No, because previous round may have not
\l   waitup the process for this node.
\t should also return if MADE no? faster then!

%toc:
The following two sections explain the code of [[work()]] when
the node is a leaf and when it has children.

\subsection{The leaf case}

%trans:
The leaf case is easy.
%
If a node has no prerequisites and contains an existing file,
for instance, [[hello.c]] in Figure~\ref{fig:graph-being-made2},
then this file is already [[MADE]].

<<[[work()]] no arcs, a leaf>>=
/* consider no prerequisite case */
if(node->time == 0){
    <<[[work()]] print error when inexistent file without prerequisites>>
} else
    MADESET(node, MADE);
@
%old: was return did; 
% - but actually there 'did' was always true
% - clearer to remove return so can rewrite work skeleton
\l why not use update(node, false) here? leaf so for sure not virtual?
\l remember that virtual rules have an arc

Otherwise, [[mk]] should report an error:

<<[[work()]] locals>>=
char cwd[256];
@
<<[[work()]] print error when inexistent file without prerequisites>>=
if(getwd(cwd, sizeof cwd))
    fprint(STDERR, "mk: don't know how to make '%s' in directory %s\n", node->name, cwd);
else
    fprint(STDERR, "mk: don't know how to make '%s'\n", node->name);
<<[[work()]] when inexistent target without prerequisites, if kflag>>
else
    Exit();
@
\l the directory part is useful in recursive mk context
\l aspectize that part and put in recursive mk section?
\l when getwd can fail? should never fail ...

\subsection{The parent case}
\l confusing children vs prerequites vs arcs?

%trans:
The parent case is more complex.
%
For the parent case, [[work()]] relies on the two important booleans below:

<<[[work()]] locals>>=
bool weoutofdate = false;
bool ready = true;
@
%old: initialization used to be done later, and weoutofdate used to be aflag
\n called weoutofdate to avoid conflict with outofdate function

[[weoutofdate]] checks whether the current node is out-of-date
with one of its children.
%
[[ready]] checks whether the node is ready to be built because all
its children are [[MADE]].

%trans:
Here is finally the code of [[work()]] handling nodes with children
(and using the locals above):

<<[[work()]] locals>>=
Arc *a;
@
<<[[work()]] some arcs, a node>>=
<<[[work()]] adjust weoutofdate if aflag>>
/*
 *   now see if we are out of date or what
 */
for(a = node->arcs; a; a = a->next) {
    if(a->n){
        // recursive call! go in depth
        work(a->n, did,  node, a);

        if(a->n->flags&(NOTMADE|BEINGMADE))
            ready = false;
        if(outofdate(node, a, false)){
            weoutofdate = true;
            <<[[work()]] update [[ra]] when outofdate [[node]] with arc [[a]]>>
        }
    } else {
        if(node->time == 0){
            weoutofdate = true;
            <<[[work()]] update [[ra]] when no dest in arc and no src>>
        }
    }
}

if(!ready)	/* can't do anything now */
    return;
if(!weoutofdate){
    MADESET(node, MADE);
    return;
}
<<[[work()]] possibly pretending node>>
// else, out of date

dorecipe(node, did);
return;
@
%old: changed lots of code because I now thread did
% could refactor in did ||= foo(); ? No! we actually want the side effect!
\t impossible to have NOTMADE in if above? 

There are a few important things to note about the code above.
%
First, it is important for [[work()]] to call itself recursively
{before} checking whether the current node is out-of-date with one
of its children.
%
Indeed, in Figure~\ref{fig:graph-being-made2}, the root node may appear
to be up-to-date because all the object files are older than the executable.
However, the root may still be out-of-date because deep in the graph
a source file may be more recent than its object file. In that case,
the object file is out-of-date and should be recompiled, which in turn
makes the root node out-of-date.
%
[[work()]] must first go in depth by calling itself recursively to
perform a DFS.


Secondly, when [[work()]] finds out that the current node is not ready,
[[work()]] should not return yet.
%
Indeed, [[work()]] must continue to loop over the remaining arcs
and continue to call itself recursively. That way, [[work()]] may
find jobs to schedule in other branches.
\l when can happen have no node attached to an arc? virtual targets
%  or when file does not exist, so for sure we are out of date.


%\section{[[outofdate()]]}

The code to check whether a node is out-of-date simply compares
the modification time of two nodes:

<<function outofdate>>=
bool
outofdate(Node *node, Arc *arc, bool eval)
{
    <<[[outofdate()]] locals>>

    <<[[outofdate()]] if arc->prog>>
    else 
     <<[[outofdate()]] if arc node is an archive member>>
     else
        /*
         * Treat equal times as out-of-date.
         * It's a race, and the safer option is to do
         * extra building rather than not enough.
         */
        return node->time < arc->n->time;
}
@
%old: was <=, but annoying, so I ignored the comment
\l inexistent file has zero time (like very very old time)
\l eval argument? seems used only for :P: prog stuff

Note that on recent machines, where compilation and linking can
be extremely fast, it is not uncommon to generate object files and
executables in the same second.
%
Thus, it is important for the modification time of a file to
be at a granularity finer than the second.
%note: on very fast machine the <= above test can be actually annoying.
% For instance in ocaml both the .cmo and .cmi may have
% been created in the same second by ocamlc, which means
% then that the .cmo file will always be recompiled because
% its .cmi has the same date (and recompiling the .cmo
% will generate a new .cmi, which then will trigger at the next
% round another compilation of the .cmo and so on).




\section{Scheduling recipes, [[dorecipe()]]}
\label{sec:dorecipe}

The job of [[dorecipe()]], called from [[work()]], is to construct a [[Job]] 
(see Section~\ref{sec:job}) to run a recipe that will update
a target node.
\l pun, job Job (maybe confusing?)
%
To do so, [[dorecipe()]] must first find the {master rule}
of a node. Indeed, as I mentioned in Section~\ref{sec:master-rule},
multiple rules can mention the same target, but only one of
those rules can have a recipe, the master rule:

<<[[dorecipe()]] other locals>>=
Rule *master_rule = nil;
Arc *master_arc = nil;
@
%old: aa -> master_ arc, r -> master_rule

Moreover, as I mentioned in Section~\ref{sec:graph-many-to-one}, 
a rule may contain multiple targets. Thus, [[dorecipe()]]
must also compute the set of all targets mentioned in the master rule:

% set list of targets (most of the time single one)
<<[[dorecipe()]] other locals>>=
// list<string> (last = last_alltargets)
Word alltargets;
@
%old: ahead -> alltargets (and head -> oldtargets)

It is also important for [[dorecipe()]] to compute not only
the set of target names, but also to the set of target nodes.
%
Indeed, once a job finished, [[mk]] must update the modification
times of all those nodes in the graph of dependencies:

<<[[dorecipe()]] other locals>>=
// list<ref<Node>> (next = Node.next)
Node *nlist = node;
@

In practice, most rules have a single target so [[alltargets]]
and [[nlist]] should contain only one element.


Finally, [[dorecipe()]] computes also the set of prerequisites:

<<[[dorecipe()]] other locals>>=
// list<string> (last = last_allprereqs)
Word allprereqs;
@
%old: l (for list) -> allprereqs

This set will be used when exporting the special variable
[[$prereqs]] in Section~\ref{sec:buildenv}.%$

\l other locals. will iterate over arcs, node in args, words, and symbols.

%trans:
Here is finally the code of [[dorecipe()]] leveraging the locals above:

% main -> mk -> work -> ... -> run(newjob())
<<function dorecipe>>=
void
dorecipe(Node *node, bool *did)
{
    // iterators
    Arc *a;
    Node *n;
    Word *w;
    Symtab *s; 
    <<[[dorecipe()]] other locals>>

    /*
     *   pick up the master rule
     */
    for(a = node->arcs; a; a = a->next)
        if(!empty_recipe(a->r)) {
            master_rule = a->r;
            master_arc = a;
        }

    <<[[dorecipe()]] if no recipe found>>
    // else

    /*
     *   build the node list
     */
    <<[[dorecipe()]] build lists of targets and node list>>

    /*
     *   gather the params for the job
     */
    allprereqs.next = newprereqs.next = nil;
    for(n = nlist; n; n = n->next){
        <<[[dorecipe()]] build lists of prerequisites>>
        MADESET(n, BEINGMADE);
    }

    // run the job
    run(newjob(master_rule, nlist, 
               master_arc->stem, master_arc->match, 
               allprereqs.next, newprereqs.next, 
               alltargets.next, oldtargets.next));
    *did = true; // finally
    return;
}
@
%/*print("lt=%s ln=%s lp=%s\n",wtos(head.next, ' '),wtos(ln.next, ' '),wtos(lp.next, ' '));
%old: 
%  - <<[[dorecipe()]] other locals>>=
%    bool did = false;
%    @
%  - I use empty_recipe, was just doing *a->r->recipe
%  - I change order of call to newjob to put the allxxx first
%dead? bug?
%  <<[[dorecipe()]] return if one target not READY>>=
%  for(n = node; n; n = n->next)
%      if(!(n->flags&READY))
%          return did;
%  @
%  did was always false I think
%  and look vacuous(), READY is always set ... 
%pad: 
% - I added 'master' in comment about picking up the rule
% - I added nlist alias, was ugly to modify node

I described [[newjob()]] called above in Section~\ref{sec:job}.
%
I will explain [[run()]], which adds the job in a job queue
and possible schedules the job, in Chapter~\ref{chap:scheduling}.
%
\t set as BEINGMADE all nodes! all target nodes! important in FigureX
% otherwise would launch another yacc command.


If [[work()]] found an outdated file but [[dorecipe()]] can not
find any recipe to update the file, [[mk]] should report an error:

<<[[dorecipe()]] other locals>>=
char cwd[256];
@

<<[[dorecipe()]] if no recipe found>>=
/*
 *   no recipe? go to buggery!
 */
if(master_rule == nil){
    <<[[dorecipe()]] when no recipe found, if virtual or norecipe node>>
    else {
        if(getwd(cwd, sizeof cwd))
            fprint(STDERR, "mk: no recipe to make '%s' in directory %s\n", 
                   node->name, cwd);
        else
            fprint(STDERR, "mk: no recipe to make '%s'\n", node->name);
        Exit();
    }
}
@


\subsection{Building the list of target nodes}

To build the list of all targets, [[dorecipe()]] can rely
on the [[Rule.alltargets]] field setup in [[addrule()]]
(see Section~\ref{sec:multi-targets}):

<<[[dorecipe()]] other locals>>=
Word *last_alltargets = &alltargets;
char buf[BIGBLOCK];
@
%old: aw -> last_alltargets;

<<[[dorecipe()]] build lists of targets and node list>>=
nlist->next = nil;
alltargets.next = oldtargets.next = nil;
<<[[dorecipe()]] if regexp rule>>
else {
    for(w = master_rule->alltargets; w; w = w->next){
        if(master_rule->attr&META)
            subst(master_arc->stem, w->s, buf, sizeof(buf));
        else
            strecpy(buf, buf + sizeof buf - 1, w->s);

        //add_list(newword(buf), alltargets)
        last_alltargets->next = newword(buf);
        last_alltargets = last_alltargets->next;

        s = symlook(buf, S_NODE, nil);
        <<[[dorecipe()]] sanity check s>>
        n = s->u.ptr;

        <<[[dorecipe()]] update list of outdated targets>>

        // add_set(n, nlist)
        if(n == node) 
            continue;
        n->next = nlist->next;
        nlist->next = n;
    }
}
@
%ocaml: can do the subst once and for all and have a better rule_exec!
%old: was initializing w, aw here, but I prefer earlier, easier to aspectize

Again, [[mk]] uses a pointer ([[last_alltargets]]) to point to
the head of a list allocated in the stack ([[alltargets]]).
%dup: ??
This is the same C idiom I introduced in Section~\ref{sec:idiom-easy-add-list}
that allows to add an element in a list without having to worry whether
the list is empty.

[[dorecipe()]] stores the set of target names in the list [[alltargets]].
It also uses the node cache and the [[S_NODE]] namespace 
(see Section~\ref{sec:node-cache}) to access the node of a target,
and then chains together all the nodes with the [[Node.next]] field
(see Section~\ref{sec:nodes}).
%
[[alltargets]] will be used when exporting the special variable
[[$target]] %$
\l actually target and alltarget
to the process executing the recipe. 
%dup: (but changed) outdated/?
The node list will be used by [[waitup()]] 
\l and update
to update the modification time and building status of those nodes in the graph.


<<[[dorecipe()]] sanity check s>>=
if(s == nil)
    continue;	/* not a node we are interested in */
@
\t when can have the case of node not found in S_NODE? because
%  when we build the graph we start from one target which may
%  be part of a multi-target rule, e.g. parser.ml parser.mli: parser.mly
%  and if we did mk parser.mli, then parser.ml will not be in
%  the graph even though we want to add it for the alltargets variable



\subsection{Building the list of prerequisites}

To build the list of prerequisites, [[dorecipe()]] does not use 
the master rule but instead go through all the arcs.
%
Indeed, many arcs do not contain a recipe but still contributes
a dependency.
\t ex foo.5 foo.h, but then why want add this prerequisites?
This is also why the code below uses [[addw()]], which
treats the list of words as a set and avoids adding duplicates:

<<[[dorecipe()]] build lists of prerequisites>>=
for(a = n->arcs; a; a = a->next){
    if(a->n){
        addw(&allprereqs, a->n->name);

        if(outofdate(n, a, false)){
            <<[[dorecipe()]] when outofdate node, update list of newprereqs>>
            <<[[dorecipe()]] explain when found arc [[a]] making target [[n]] out of date>>
        }
    } else {
        <<[[dorecipe()]] explain when found target [[n]] with no prerequisite>>
    }
}
@
\l false param for eval?







\chapter{Scheduling Jobs}
\label{chap:scheduling}
\l Fixing outdated files (after finding outdated files)

%trans:
In this chapter, you will see the remaining important functions
of [[mk]] that I introduced in Figure~\ref{fig:controlflow}.
%trans: 
I mentioned 
[[run()]] in Section~\ref{sec:dorecipe}, called from [[dorecipe()]],
which {enqueues} a job, and 
[[waitup()]] in Section~\ref{sec:mk}, called from [[mk()]],
which {waits} for a job to finish.
\l (and which calls update() to change the building status of a node to MADE)
%
Both functions eventually call [[sched()]], which takes a job from
the job queue and {schedules} it for execution by a shell with [[execsh()]].
%toc:
The following sections will explain the code of
[[run()]],
[[waitup()]],
\l update
[[sched()]], and 
[[execsh()]].

\section{Enqueuing jobs, [[run()]]}

%\subsection{[[run()]]}

[[run()]] simply adds a job in the global job queue [[jobs]]
(see Section~\ref{sec:job}) and runs the scheduler:

% main -> mk -> work -> dorecipe -> <> (-> sched())
<<function run>>=
void
run(Job *j)
{
    Job *jj;

    // enqueue(j, jobs)
    if(jobs){
        for(jj = jobs; jj->next; jj = jj->next)
            ;
        jj->next = j;
    } else 
        jobs = j;
    j->next = nil;

    /* this code also in waitup after parse redirect */
    if(nrunning < nproclimit)
        sched();
}
@
%ocaml: use Queue.add, faster, at least he should have a ljob ...
\l j->next = nil  redundant since does it already in newjob

%\subsection{[[nrunning]] and [[nproclimit]]}

[[run()]] relies on the two globals below:

<<global nrunning>>=
static int nrunning;
@

<<global nproclimit>>=
static int nproclimit;
@

[[nrunning]] counts the number of processes currently running a recipe, while
[[nproclimit]] sets a limit on the number of processes to run at the same time.
\l that mk should not go over
%
This last global usually corresponds to the number of processors on the machine.
%
If there are no more free processors, then [[run()]] just enqueues the job.
%
Hopefully at some point [[mk]] will call [[waitup()]], which will
wait for some process to finish, and which will call [[sched()]] to
schedule one of the enqueued jobs.


You can modify [[nproclimit]] by modifying the [[$NPROC]] environment variable.
%$
Indeed, [[mk]] at startup time reads the environment 
(with [[readenv()]]) and looks for this variable to setup [[nproclimit]]
with the code below:

<<[[mk()]] initializations>>=
nproc();	/* it can be updated dynamically */
@
\n mk not main, so run for each mkfile, but not a big difference, could be
\n  in main honestly, but must be after parsing the mkfile

<<function nproc>>=
void
nproc(void)
{
    Symtab *sym;
    Word *w;

    if(sym = symlook("NPROC", S_VAR, nil)) {
        w = sym->u.ptr;
        if (!empty_words(w))
            nproclimit = atoi(w->s);
    }
    if(nproclimit < 1)
        nproclimit = 1;
    <<[[nproc()]] if DEBUG(D_EXEC)>>

    <<[[nproc()]] grow nevents if necessary>>
}
@
%pad: use empty_words
\n like [[nrep()]]
\l can also be modified in mkfile, by doing NPROC=xxx but stupid.
\l is it setup by \plan by default at init time? after kernel boots?



\section{Scheduling jobs}

%trans:
Before showing the code of [[sched()]], 
I need to present an important data structure used by [[sched()]].

\subsection{[[RunEvent]] and [[events]]}

%dup: (but extended) scheduling
[[sched()]] takes a job from the job queue and creates a process
to execute a shell that will interpret shell commands from a recipe.
%
This same process will be waited for later by [[waitup()]].
%
However, [[waitup()]] needs to know which job corresponds to which process,
so it can know which nodes in the graph of dependencies to update
once a process finished.
%
This is why [[sched()]], in addition to executing new processes, 
needs also to remember the mapping between a process and a job.
%
[[RunEvent]] below records a single mapping between a process id and a job:
\l see \book{Kernel}, process identifier

<<struct RunEvent>>=
struct RunEvent {
    // option<Pid> (None = 0)
    int pid;

    // ref_own<Job>
    Job *job;
};
@
%bug? seems that job never freed

The list of mappings is stored in the following global:

<<global events>>=
// growing_array<Runevent> (size = nevents (== nproclimit))
static RunEvent *events;
@
%ocaml: use instead map (e.g., hash) from pid to job.
<<global nevents>>=
static int nevents;
@
\t should be always == nproclimit, so maybe can remove it
\l  or sometimes nproclimit can go down, in which case we dont really
\l  but useless opti
\t  just remember old_nproclimit in nproc() so know if need realloc
\t  but anyway should not call nproc, and realloc does not matter.

[[events]] is a {growing array}. The index of a mapping in this
array is called a {\em slot}.
%
The growing array is initialized in [[nproc()]]:

<<[[nproc()]] grow nevents if necessary>>=
if(nproclimit > nevents){
    if(nevents)
        events = (RunEvent *)Realloc((char *)events, 
                                     nproclimit*sizeof(RunEvent));
    else
        events = (RunEvent *)Malloc(nproclimit*sizeof(RunEvent));

    while(nevents < nproclimit)
        events[nevents++].pid = 0;
}
@
%ocaml: no need, Hashtbl.t grows as needed
\t replace nproclimit by != nevents
\t nproc called from mk, so code above should never call the Realloc part.
\l so nevents should be = to nproclimit at the end


[[sched()]] relies on a few helper functions to operate on [[events]].
%
[[nextslot()]] below finds an available slot in [[events]]:

% sched -> <>
<<function nextslot>>=
int
nextslot(void)
{
    int i;

    for(i = 0; i < nproclimit; i++)
        if(events[i].pid <= 0) 
            return i;
    assert(/*out of slots!!*/ false);
    return 0;	/* cyntax */
}
@
%ocaml: no need growing_array, just use hash

[[pidslot()]] below finds the slot of the mapping for
a certain process id:

% waitup -> <>
<<function pidslot>>=
int
pidslot(int pid)
{
    int i;

    for(i = 0; i < nevents; i++)
        if(events[i].pid == pid) 
            return i;
    // else
    <<[[pidslot()]] if DEBUG(D_EXEC)>>
    return -1;
}
@
\t this one iterate on all nevents, not just nproclimit
% but should be the same between two runs of mk
% so maybe could simplify.
\t use nproclimit, not nevents above, more consistent
%ocaml: use Hashtbl.find



\subsection{[[sched()]]}

%trans:
Here is finally the code of [[sched()]]:

% main -> mk -> work -> dorecipe -> run -> sched()
<<function sched>>=
static void
sched(void)
{
    Job *j;
    int slot;
    char *flags;
    ShellEnvVar *e;
    <<[[sched()]] other locals>>

    <<[[sched()]] return if no jobs>>

    // j = pop(jobs)
    j = jobs;
    jobs = j->next;
    <<[[sched()]] if DEBUG(D_EXEC)>>

    slot = nextslot();
    events[slot].job = j;

    e = buildenv(j, slot);
    <<[[sched()]] print recipe command on stdout>>

    <<[[sched()]] if dry mode or touch mode, alternate to execsh>>
    else {
       <<[[sched()]] if DEBUG(D_EXEC) print recipe>>
        flags = "-e";
       <<[[sched()]] reset flags if NOMINUSE rule>>

        // launching the job!
        events[slot].pid = execsh(flags, j->r->recipe, nil, e);

        usage();
        nrunning++;
       <<[[sched()]] if DEBUG(D_EXEC) print pid>>
    }
}
@

The most important call in the code above is the call to [[execsh()]].
%dup: (but extended) scheduling scheduling/scheduling/runevent
[[execsh()]] will create a new shell process that will interpret the 
commands from the recipe of a job (stored in [[j->r>recipe]]).
%
This is why [[sched()]] above increments [[nrunning]] after the
call to [[execsh()]].
%
[[execsh()]] will return the process id of this newly created process
and [[sched()]] associates this id with the dequeued job in [[events]].


[[execsh()]] takes also as a first parameter a string containing a set of flags
to pass to the shell as shell arguments.
%
[[mk]] passes the [[-e]] flag so that every commands from the recipe returning
an error will abort the whole execution of the recipe (see the \book{Shell}).
\l why this is good? safe
\l what is third parameter of execsh? buf, for backquote exec to capture output


The last argument to [[execsh()]] ([[e]]) contains the {environment}
in which to execute the shell.
%
This environment will contain the values for the special [[mk]] variables
(e.g., [[$target]], [[$prereq]], [[$stem]]) and user variables. %$
%
This environment is built by [[buildenv()]] called above, which I will present
in Chapter~\ref{chap:shellenv}.
\l it is the function using most of the fields in j;
Thanks to this environment, the shell process executing the recipe
will be able to get the values for the [[mk]] variables referenced 
in this recipe.


As you will see soon, [[sched()]] is also called from [[waitup()]], in which
case the job queue may be empty. In that case, [[sched()]] simply returns.

<<[[sched()]] return if no jobs>>=
if(jobs == nil){
    usage();
    return;
}
@
\n usage() is for profiling
\l explain usage?
\t usage -> processor_usage_profile()


\section{Executing Jobs, [[execsh()]]}

[[execsh()]] relies on the global below to know which shell to execute.
%
Under \plan, [[mk]] uses the shell [[rc]] (see the \book{Shell}):

%\subsection{Shell configuration}

<<global shell>>=
char 	*shell =	"/bin/rc";
@
<<global shellname>>=
char 	*shellname =	"rc";
@

We do not want the shell to print a {prompt} before each command
from the recipe, so [[execsh()]] adds the [[-I]] flag as a shell argument
(in addition to [[-e]] added by [[sched()]]):
\l unless you added :E: for NOMINUSE

<<global shflags>>=
char	*shflags = "-I";	/* rc flag to force non-interactive mode */
@

%\subsection{[[execsh()]]}

[[execsh()]] below essentially forks a shell interpreter, creates a pipe, 
and feeds this shell with commands from the recipe through this pipe
(via another forked process):

%plan9.c 
<<function execsh>>=
int
execsh(char *shargs, char *shinput, Bufblock *buf, ShellEnvVar *e)
{
    int pid1, pid2;
    fdt in[2]; // pipe descriptors
    int err;
    <<[[execsh()]] other locals>>

    <<[[execsh()]] if buf then create pipe to save output>>

    pid1 = rfork(RFPROC|RFFDG|RFENVG);
    <<[[execsh()]] sanity check pid rfork>>
    // child
    if(pid1 == 0){
        <<[[execsh()]] in child, if buf, close one side of pipe>>
        err = pipe(in);
        <<[[execsh()]] sanity check err pipe>>
        pid2 = fork();
        <<[[execsh()]] sanity check pid fork>>
        // parent of grandchild, the shell interpreter
        if(pid2 != 0){
            // input must come from the pipe
            dup(in[0], STDIN);
            <<[[execsh()]] in child, if buf, dup and close>>
            close(in[0]);
            close(in[1]);
            <<[[execsh()]] in child, export environment before exec>>
            if(shflags)
                execl(shell, shellname, shflags, shargs, nil);
            else
                execl(shell, shellname, shargs, nil);
            // should not be reached
            perror(shell);
            _exits("exec");
        }
        // else, grandchild, feeding the shell with recipe, through a pipe
        <<[[execsh()]] in grandchild, if buf, close other side of pipe>>
        close(in[0]);
        // feed the shell
        <<[[execsh()]] in grandchild, write cmd in pipe>>
        close(in[1]); // will flush
        _exits(nil);
    }
    // parent
    <<[[execsh()]] in parent, if buf, close other side of pipe and read output>>
    return pid1;
}
@
%pad: 
% - I introduced int err; so can better aspectize sanity checks
% - I added pid2 (and the pid -> pid1), so better see subtelity of
%   shell must be in child1
%old: 
% - args -> shargs, cmd -> shinput
\n put flags after cmd in argument of execsh, more logical? no! cmd is input!


[[execsh()]] relies on the 
[[rfork()]], 
[[pipe()]], 
[[dup()]],
[[close()]], and 
[[_exits()]] 
functions, which are syscalls to the kernel (see the \book{Kernel}),
as well as on the 
[[fork()]],
[[execl()]], and 
[[perror()]] 
functions from the C library (see the \book{Libcore}), which are
thin wrappers around syscalls.
\t diff rfork and fork, RFENVG vs RFREND ??? RFENVG so not share env var
\t  and good because will modify those vars in the child! based on
\t  modif in mkfile (hmm but ENV anyway done by /env/ no? NO! shared namespace)
\t  but anyway call rfork(RFENVG) in readenv, so does not matter no?
\t and why call _exits instead of exits?


%dup: (a bit different) scheduling/execsh
[[execsh()]] creates two processes: one for the shell, and one
whose only job is to feed the shell through a pipe.
%alt: sh -e 'cmd' ? but then need escape mess?
\t why this approach? for escaping mess? simpler design?
\l good, but then pb with :I:
\l  because feed stdin and then close, that means programs called from
%   shell can not read from stdin :( for instance make sync works but
%   mk sync does not. so added :I: in mk-ocaml
\l FIGURE? cos 2 forks below.
%
By relying on this last process, [[mk]] can continue to schedule
jobs without having to wait for the shell to finish reading
the commands from the recipe.
\l can do stuff in //. Make originally didn't (apparently).
\l and sometimes multiple lines and stop before each line so bad
\l can also pass a buf for bquote (backquote execution), to store
%  stdout instead of just displaying it gradually


%ocaml-found:
Note that it is important for [[execsh()]] to execute the
shell interpreter in the direct child, not the grandchild.
%
Indeed, the parent process of the child, the [[mk]] process,
knows only about the process id of the child. It is this process id
that is stored later in [[events]] and looked for by [[waitup()]].
\t also not child of mk, grandchild, so does not interfere with waitup!
%
The parent process does not know about the process id of the grandchild.
Moreover, this grandchild will terminate quickly, before the shell finishes
executing the recipe.
\l exportenv! so set in child the variables in environment

Here is the code to feed the shell in the grandchild:

<<[[execsh()]] other locals>>=
char *endshinput;
@
<<[[execsh()]] in grandchild, write cmd in pipe>>=
endshinput = shinput + strlen(shinput);
while(shinput < endshinput){
    n = write(in[1], shinput, endshinput - shinput);
    if(n < 0)
        break;
    shinput += n;
}
@
%old: p -> endshinput (and before cmd -> shinput)





<<[[execsh()]] sanity check pid rfork>>=
if(pid1 < 0){
    perror("mk rfork");
    Exit();
}
@
<<[[execsh()]] sanity check pid fork>>=
if(pid2 < 0){
    perror("mk fork");
    Exit();
}
@
<<[[execsh()]] sanity check err pipe>>=
if(err < 0){
    perror("pipe");
    Exit();
}
@





\section{Waiting for jobs to finish}

%trans:
The interface of [[waitup()]] is complex.
%toc:
I will focus first on the easy case where I do not care
about the arguments passed to [[waitup()]],
and describe later the use of those arguments and the 
different edge cases of [[waitup()]].

\subsection{[[waitup()]]}
\label{sec:waitup}
\l wait user process?

%trans: %dup: scheduling/scheduling/sched
Once [[sched()]] scheduled a job and modified [[events]],
[[mk()]] can call [[waitup()]] to wait for a job to finish.
%
Indeed, [[waitup()]] can now rely on [[events]] to know which job is associated
with the waited process. Then, [[waitup()]] can call [[update()]] to update
the graph of dependencies and [[sched()]] to schedule more jobs:

% mk -> work; <>
<<function waitup>>=
int
waitup(int echildok, int *retstatus)
{
    // child process
    int pid;
    // return string of child process
    char buf[ERRMAX];
    // index in events[]
    int slot;
    Job *j;
    Symtab *sym;
    Node *node;
    Word *w;
    bool fake = false;
    <<[[waitup()]] other locals>>

    <<[[waitup()]] if retstatus, check process list>>
again:		/* rogue processes */

    pid = waitfor(buf);
    <<[[waitup()]] if no more children>>
    <<[[waitup()]] if DEBUG(D_EXEC) print pid>>
    <<[[waitup()]] if retstatus, check if matching pid>>

    slot = pidslot(pid);
    <<[[waitup()]] if slot not found, not a job pid, update process list>>

    j = events[slot].job;
    usage();
    nrunning--;
    // free events[slot]
    events[slot].pid = -1;

    <<[[waitup()]] if error in child process, possibly set fake or exit>>
    // else

    for(w = j->t; w; w = w->next){
        sym = symlook(w->s, S_NODE, nil);
        node = (Node*) sym->u.ptr;
        <<[[waitup()]] skip if node not found>>
        update(node, fake);
    }

    if(nrunning < nproclimit)
        sched();
    return JOB_ENDED;
}
@
%bug? could free job in events[i] too
%old: 
% - fake was called [[uarg]] (for update arg)
%pad: I added node local.

[[waitup()]] above relies on the helper function [[waitfor()]] below,
which calls itself [[wait()]] from the C library (see the \book{Libcore}),
which itself relies on the [[await()]] system call (see the \book{Kernel}).

%plan9.c
<<function waitfor>>=
int
waitfor(char *msg)
{
    Waitmsg *w;
    int pid;

    // blocking call, wait for any children
    w = wait();
    // no more children
    if(w == nil)
        return -1;
    strecpy(msg, msg+ERRMAX, w->msg);
    pid = w->pid;
    free(w);
    return pid;
}
@
% strecpy set msg[0] to \0 if nothing?

[[wait()]] provides an easier interface than [[await()]] to
wait for a child. Indeed, [[wait()]] returns a [[Waitmsg]] data structure
(see the \book{Libcore}), which allows easy access to the
pid and returned string of the child process.
\l await just store information in buffer string, so need parse it


\subsection{[[update()]]}
\l Update the graph of dependencies

[[update()]], called in [[waitup()]], 
marks the target nodes of a job as [[MADE]]
and updates the modification time of those nodes.

% mk -> waitup -> <>
<<function update>>=
void
update(Node *node, bool fake)
{
    Arc *a;

    <<[[update()]] if fake>>
    else
       MADESET(node, MADE);
    <<[[update()]] debug>>

    <<[[update()]] if virtual node or inexistent file>>
    else {
        node->time = timeof(node->name, true);
        <<[[update()]] unpretend node>>
        <<[[update()]] set outofdate prereqs if arc prog>>
    }
}
@
%old: 
% - I changed the order of the parameter, better put node before
% - I reorg code so easier to aspectize fake and virtual
\t pass true to timeof, to force, to not use the cache because indeed
%  time probably changed

I will present the use of the [[fake]] parameter later in
Section~\ref{sec:fake-update}.


\subsection{[[waitup()]] edge cases}

%dup: scheduling-jobs/waitup
As I mentioned before, the interface of [[waitup()]] is complex:

<<signature waitup>>=
int waitup(int echildok, int *retstatus);
@
%syncweb: not included anywhere, it is normal

The first parameter of [[waitup()]], of the type below, 
encodes whether it is ok or not for [[mk]] to have children to wait for.

<<type WaitupParam>>=
enum WaitupParam { 
  EMPTY_CHILDREN_IS_OK = 1, 
  EMPTY_CHILDREN_IS_ERROR = -1, 
  <<[[WaitupParam]] other cases>>
};
@
%pad: I added the enum above; use of 1 and -1, -2, -3 was cryptic
\l could transform in a bool, and get rid of the ERROR2 ERROR3, unify

The return value of [[waitup()]], of the type below, describes
a few possible scenarios:

<<type WaitupResult>>=
enum WaitupResult { 
  JOB_ENDED = 0, 
  EMPTY_CHILDREN = 1, 
  NOT_A_JOB_PROCESS = -1 
};
@
%pad: I added that, those 1, -1 was cryptic

To understand [[NOT_A_JOB_PROCESS]] above, it is important
to understand that under \plan, with the system call [[await()]]
(called from [[wait()]]), 
you can not specify which child you are interested in to wait for. 
%
You can just wait for any children to finish.
%because you do not have control usually on which children will finish first.
%
In fact, [[await()]] will also report {children that already finished}.
%
This is partly the reason for the complexity of [[waitup()]]. Indeed,
certain advanced features of [[mk]] (see Section~\ref{sec:dynamic-mkfile},
Section~\ref{sec:backquotes}, and Section~\ref{sec:custom-comparison})
will create processes that are not related to a job.
%
Those processes will interfere with the call to [[wait()]].
The end of those processes must also be processed by [[waitup()]].
\l but they will popup in waitup (unless you called waitpid yourself before)
%alt: could have another waitxx function, to not interfere with job processes
%
In fact, the second parameter of [[waitup()]] ([[retstatus]])
is used only by those advanced features and processes.
\l used for <| and :P:
% INOUT. for IN it is a pid and OUT it is the return status (ugly)
% put here because can have only one call to wait. wait is waiting
%  indiscrimently, so have to centralize in one place.
% For more info, see Section X.


%trans:
I can now describe the code to handle the edge cases of [[waitup()]].
%
As I showed in Section~\ref{sec:mk}, [[mk()]] calls [[waitup()]]
in different contexts and the presence or not 
of a child can trigger an error or be perfectly ok depending on the context:


<<[[waitup()]] if no more children>>=
if(pid == -1){
    if(echildok == EMPTY_CHILDREN_IS_OK)
        return EMPTY_CHILDREN;
    else {
        fprint(STDERR, "mk: (waitup %d) ", echildok);
        perror("mk wait");
        Exit();
    }
}
@
%old: echildok > 0

If a child returns an error string, for example by doing
[[exits("error")]] instead of [[exits(nil)]], then this error
string will be captured by [[wait()]] and stored in the
local buffer [[buf]] of [[waitup()]], hence the condition below:

<<[[waitup()]] other locals>>=
Bufblock *bp;
@

<<[[waitup()]] if error in child process, possibly set fake or exit>>=
if(buf[0]){
    bp = newbuf();
    <<[[waitup()]] if error in child process, print recipe in [[bp]]>>
    fprint(STDERR, "mk: %s: exit status=%s", bp->start, buf);
    freebuf(bp);
    <<[[waitup()]] when error in child process, delete if DELETE node>>
    fprint(STDERR, "\n");

    <<[[waitup()]] when error in child process, if kflag>>
    else {
        jobs = nil;
        Exit();
    }
}
@

<<[[waitup()]] skip if node not found>>=
if(sym == nil)
    continue;	/* not interested in this node */
@
\t should never happen I think, cos node list is made with
\t  node we are interested in

% because up to date target anyway?

\section{Process management, [[Exit()]]}
\l quitting gracefully

When one of the children of [[mk]] returns an error,
[[mk]] can not just exit.
%
For example, if a C file in a project contains a syntax error,
then [[5c]] run from [[mk]]
\l actually from forked rc shell
will exit with an error.
%
We do not want however [[mk]] to exit immediately.
Indeed, there may still be other jobs running in parallel, 
for example, compiling other C files in the same project.
\t sure that children are killed under plan9? maybe just unix?
If [[mk]] was exiting, those other jobs may get a signal
that abruptly interrupts them. In those cases, the files generated
by those jobs (e.g., object files for a compiler or assembler) 
may become corrupted.
\l  or source file if interrupt yacc or lex (or ocamllex!)
\l or when [[mk]] detects an error
\n Saw pb with mk-rc.byte when didnt implement that; I got many corrupted cmo.
%
Thus, is is important before exiting to let those other
jobs finish quietly.
%
This is why [[mk]] calls [[Exit()]] below instead of
calling directly [[exits()]]:

%\subsection{[[Exit()]]}

<<function Exit>>=
void
Exit(void)
{
    while(waitpid() >= 0)
        ;
    exits("error");
}
@
\l Malloc, Realloc, they call Exit in case of error.
% there should be no calls directly to exits (except in main
% when everything went fine)


\section{Notes (signals) management}

\t [[C-c]] => C-c all? who gets C-c? Which process
% connected to stdin? mk? shell? which shell?
% (not shell! shell stdin connected to pipe!)

%trans:
% good to wait in Exit, but what if too long recipe? % enter C-c!
\t But then need to do the right thing! What could go wrong?

<<[[main()]] initializations before building>>=
catchnotes();
@
%old: argv processing part 3 was there before, in the middle


% main -> <>
<<function catchnotes>>=
void
catchnotes()
{
    atnotify(notifyf, 1);
}
@

<<function notifyf>>=
int
notifyf(void *a, char *msg)
{
    <<[[notifyf()]] sanity check not too many notes>>
    if(strcmp(msg, "interrupt")!=0 && strcmp(msg, "hangup")!=0)
        return 0;
    killchildren(msg);
    return -1;
}
@
% 0 -> IGNORE_NOTE?


<<[[notifyf()]] sanity check not too many notes>>=
static int nnote;

USED(a);
if(++nnote > 100){	/* until andrew fixes his program */
    fprint(STDERR, "mk: too many notes\n");
    notify(0);
    abort();
}
@

<<function killchildren>>=
void
killchildren(char *msg)
{
    <<[[killchildren()]] locals>>

    jobs = nil;		/* make sure no more get scheduled */
    kflag = true;	/* to make sure waitup doesn't exit */

    <<[[killchildren()]] expunge not-job processes>>

    while(waitup(EMPTY_CHILDREN_IS_OK, (int *)nil) == JOB_ENDED)
        ;
    Bprint(&bout, "mk: %s\n", msg);
    Exit();
}
@

% so at some point should get WEmptyChild
% (or WNotJobProcessDone but would be weird)

\l why not send also postnote to job child process?

\chapter{The Shell Environment}
\label{chap:shellenv}

%toc:
In this chapter, you will see the functions to
initialize,
import,
adjust, and
export the {environment} to the shell processes launched from [[mk]].
%
Indeed, it is through the environment that [[mk]] communicates
to the shell interpreter
the values of [[mk]]'s special variables (e.g., [[$target]], [[$stem]]) or
user variables (e.g., [[$CFLAGS]]) %$
used in the recipes.


%\section{Interacting with the shell}

% main strategy/approach of mk for interacting with shell is to
% pass variable value through environment! (simple)
% and to pass recipe through stdin (simpler than through -c, no
% need quote escaping)

%real-world:
% mk simpler than make on this. no Make variables vs shell variables.
% (but then stronger coupling with the shell language? portable?)
% Just reuse mechanism of variables from the shell, and syntax!

% as opposed to make, no $(XX), no $$i, the string is passed
% as is to shell! less escaping need!

% The recipe is a shell command (or multiple commands).
% Seen execsh() above with global shell, shellname stuff. 
%  (already plan9 specific and some stuff rc specific)
% was taking an Envy because recipe can contain variables.
% buildenv() in sched() before execsh, passing an Envy.

%\section{Variables expansion (part 3)}
% not done actually! shell does it!  simpler! no $$i mess. simplify some things.

% variables, different occurences: in rule target, prerequisites,
% or in recipe. Different handling depending on context.
% See quote from mk manual.

\section{[[Shellenv]] and [[shellenv]]}
\label{sec:shellenv}
\n could be in core DS, but used just here, and outside, S_VAR is good enough.

The symbol table of [[mk]] ([[hash]]) contains already 
in the [[S_VAR]] namespace the values of user variables,
as well as the values of the variables in the environment of [[mk]] itself.
\l good for recursive mk, readenv
[[mk]] also stores the set of special variables in the symbol table
in the [[S_INTERNAL]] namespace (but without any value).
%
However, [[mk]] uses another data structure to communicate
the value of all those variables to the shell.
\t why? faster? because anyway exec excepts an array? not really.
%
The structure below maps a variable name to a list of words.

<<struct Envy>>=
struct ShellEnvVar
{
    // ref<string>, the key
    char 		*name;

    // list<ref_own<string>>, the value
    Word 		*values;
};
@
%old: was Envy, but why y?
%ocaml: just use env, no need for extra data structure.
% update: I use env env and shellenv type, but really shellenv is just
% because exec requires an array.

All the variables and their values are stored in the following global:

<<global envy>>=
// growing_array<ShellEnvVar> (endmarker = (nil,nil), size = envinsert.envsize)
ShellEnvVar	*shellenv;
@
%old: envy -> shellenv
%ocaml: shellenv_of_env

The size of this array is stored in a static local variable 
in [[envinsert()]] below. However, you can iterate over [[shellenv]]
without having to know the value of this static variable. 
%
Indeed, [[mk]] uses a special marker, [[(nil, nil)]], for 
the last [[ShellEnvVar]] entry in [[shellenv]].
\l why this design choice? uselessly complicated?


Here is the function to add an entry in [[shellenv]]:

<<global nextv>>=
// idx for next free entry in shellenv array
static int nextv;
@
\l why not put envsize here too?

% envupd | ecopy | execinit -> <>
<<function envinsert>>=
static void
envinsert(char *name, Word *value)
{
    <<[[envinsert()]] locals>>

    <<[[envinsert()]] grow array if necessary>>
    shellenv[nextv].name = name;
    shellenv[nextv].values = value;
    nextv++;
}
@
%old: I put the increment of nextv after, more symetric
%ocaml: just Hashtbl.add


<<[[envinsert()]] locals>>=
static int envsize = 0;
@
%pad: I added the =0
<<[[envinsert()]] grow array if necessary>>=
if (nextv >= envsize) {
    envsize += ENVQUANTA;
    shellenv = (ShellEnvVar *) Realloc((char *) shellenv, 
                                       envsize*sizeof(ShellEnvVar));
}
@
<<constant ENVQUANTA>>=
#define ENVQUANTA 10
@

The execution of each recipe requires a specific shell environment.
%
Indeed, the values for [[$target]], [[$stem]], and other special variables
are different for each rule. However, the values of the user variables
are always the same.
%
To avoid allocating a new shell environment for each execution,
\t but could just modify the symbol table, hmmm
[[mk]] reuses [[shellenv]] for all executions,
but relies on the function below to adjust the values of a few variables:

<<function envupd>>=
static void
envupd(char *name, Word *value)
{
    ShellEnvVar *e;

    for(e = shellenv; e->name; e++)
        if(strcmp(name, e->name) == 0){
            freewords(e->values);
            e->values = value;
            return;
        }
    <<[[envupd()]] if variable not found>>
}
@
%ocaml: just Hashtbl.replace


<<[[envupd()]] if variable not found>>=
// else
e->name = name;
e->values = value;
envinsert(nil,nil);
@
\t else if not found, should generate error! 


\section{Initializing the shell environment, [[initenv()]]}
\n would be better initshellenv, but too many other xxxenv so more consistent

To initialize [[shellenv]], [[main()]] calls [[initenv()]]
before calling [[mk()]]:

<<[[main()]] initializations before building>>=
initenv();
@
%ocaml: just reuse env, no need for another environment-like data structure.

% main -> parse; mk; <>
<<function execinit>>=
void
initenv(void)
{
    char **p;

    nextv = 0; // reset envy

    // internal mk variables
    for(p = specialvars; *p; p++)
        envinsert(*p, stow(""));

    // user variables in mkfiles, or mk process environment
    symtraverse(S_VAR, ecopy);

    // end marker
    envinsert(nil, nil);
}
@
\l potential leak when reset on Envy.values?
%old: execinit -> initshellenv -> initenv

I described in Section~\ref{sec:symtraverse} [[symtraverse()]].
It allows to iterate over a namespace while applying
a function, here [[ecopy]]:

% execinit -> symtraverse -> <> (as x <- symtraverse(..., <>) <- execinit)
<<function ecopy>>=
static void
ecopy(Symtab *s)
{
    char **p;

    <<[[ecopy()]] return and do not copy if S_NOEXPORT symbol>>
    <<[[ecopy()]] return and do not copy if conflict with mk internal variable>>
     // else
     envinsert(s->name, s->u.ptr);
}
@
%ocaml: just Hashtbl.copy, but not even sure we need that

Note that [[initenv()]] calls [[envinsert()]] to
create first the entries for the special [[mk]] variables.
%
It is those variables that [[mk]] needs to adjust for each shell execution. 
%
By adding those entries first
in [[shellenv]], [[envupd()]] will be slightly faster because
[[envupd()]] tries to find the variable to update by starting from the
start of the [[shellenv]] array.
\l use myenv also in initenv(inithash), but it is for the S_INTERNAL stuff and
%  used later for readenv


<<[[ecopy()]] return and do not copy if conflict with mk internal variable>>=
for(p = specialvars; *p; p++)
    if(strcmp(*p, s->name) == 0)
        return;
@
\t why need that?
\l could also symlook in S_INTERNAL instead of iterating on specialvars
\l maybe should issue a warning? anyway it is only for user variables
%  because for readenv(), those variables are skipped.


%ocaml-found:
% when use mk recursively, you will get a $stem in the environment! and
% you need to filter it otherwise in exportenv you would need to take care
% of duplicate and give priority to the fresh $stem


\section{Importing the environment, [[readenv()]]}
\label{sec:readenv}

%trans:
[[initenv()]] iterates over the [[S_VAR]] namespace with
[[symtraverse()]] to add in [[shellenv]] the user variables,
for instance, a variable [[$CFLAGS]] defined in the [[mkfile]]. %$
%
In fact, the [[S_VAR]] namespace contains also the variables
from the environment of [[mk]] itself.
%
Indeed, as I mentioned in Section~\ref{sec:inithash}, [[main()]]
calls [[inithash()]] to initialize the symbol table, and
[[inithash()]] calls [[readenv()]] below to populate the symbol table
with variables from the environment (e.g., [[$HOME]], [[$objtype]], [[$CC]]). %$

Under \plan, the environment variables of a process are accessible
through the filesystem under [[/env/]] (see the \book{Kernel}).
%
[[readenv()]] below simply iterates over all the files under [[/env/]].

% plan9 specific, in plan9.c
% inithash -> <>
<<function readenv>>=
void
readenv(void)
{
    fdt envdir;
    fdt envfile;
    Dir *e;
    int i, n, len, len2;
    char *p;
    char name[1024];
    Word *w;

    rfork(RFENVG);	/*  use copy of the current environment variables */

    envdir = open("/env", OREAD);
    <<[[readenv()]] sanity check envdir>>
    while((n = dirread(envdir, &e)) > 0){
        for(i = 0; i < n; i++){
            len = e[i].length;
            <<[[readenv()]] skip some names>>

            snprint(name, sizeof name, "/env/%s", e[i].name);
            envfile = open(name, OREAD);
            <<[[readenv()]] sanity check envfile>>
            p = Malloc(len+1);
            len2 = read(envfile, p, len);
            <<[[readenv()]] sanity check len2>>
            close(envfile);
            <<[[readenv()]] add null terminator character at end of [[p]]>>
            w = encodenulls(p, len);
            free(p);
            p = strdup(e[i].name);

            // populating symbol table
            setvar(p, (void *) w);
        }
        free(e);
    }
    close(envdir);
}
@
%old: envf -> envdir, f -> envfile, nam -> name
%dead: 
% <<[[Sxxx]] cases>>=
% S_EXPORTED,	/* var -> current exported value */
% @
%  symlook(p, S_EXPORTED, (void*)"")->u.ptr = "";
\t rfork RFENVG? copy namespace, so independent of parent (probably shell).
\t but anyway mk does not modify anywhere /env/ ? when it does it is in
\t  one of its child that already rfork(RFENVG) so useless I think.

[[readenv()]] skips entries under [[/env/]] that would
lead to empty variables or variables that would conflict
with one of [[mk]]'s special variables:

<<[[readenv()]] skip some names>>=
/* don't import funny names, NULL values,
 * or internal mk variables
 */
if(len <= 0 
   || *shname(e[i].name) != '\0' 
   || symlook(e[i].name, S_INTERNAL, nil))
    continue;
@
%alt: could import variables with same name, just give priority to
% S_INTERNAL when symlook



<<function shname>>=
char *
shname(char *a)
{
    Rune r;
    int n;

    while (*a) {
        n = chartorune(&r, a);
        if (!WORDCHR(r))
            break;
        a += n;
    }
    return a;
}
@



<<[[readenv()]] sanity check envdir>>=
if(envdir < 0)
    return;
@
<<[[readenv()]] sanity check envfile>>=
if(envfile < 0)
    continue;
@

<<[[readenv()]] sanity check len2>>=
if(len2 != len){
    perror(name);
    close(envfile);
    continue;
}
@


Under \plan, environment variables can contain a list of words,
just like [[mk]]'s variables (and [[rc]]'s variables).
%
The format of this list uses the null character not to
mark the end of a string but as a {word separator}.
%alt:
An alternative would be to use the space character to separate words.
However, because under \plan certain filenames can contain spaces (but not
null characters), and because certain environment variables reference
a list of filenames (e.g., [[$PATH]]), %$
it is more convenient to use the null character as a separator.
%real-world: xargs and find -print0 and other ugly stuff
This avoids the need to escape space characters 
(and later to parse escaped characters).


The function below, called from [[readenv()]], recognizes the
null character as a word separator and splits the string [[s]]
in a list of words.
%
Note that you must also pass the length of the string as an
argument to [[encodenulls()]]
because you can not rely anymore on the null character 
to mark the end of the string.

\label{sec:encodenulls}
<<function encodenulls>>=
/* break string of values into words at 01's or nulls*/
static Word *
encodenulls(char *s, int n)
{
    Word *head, *lastw;
    char *cp;

    head = lastw = nil;
    while (n-- > 0) {
        for (cp = s; *cp && *cp != '\0'; cp++)
                n--;
        *cp = '\0';

        // add_list(newword(s), head)
        if (lastw) {
            lastw->next = newword(s);
            lastw = lastw->next;
        } else
            head = lastw = newword(s);

        s = cp+1;
    }
    if (!head)
        head = newword("");
    return head;
}
@
%old: w -> lastw
\t        *cp = '\0'; // redundant? also *cp above redundant?

<<[[readenv()]] add null terminator character at end of [[p]]>>=
if (p[len-1] == '\0')
    len--;
else
    p[len] = '\0';
@
% why? some variable have and some have not? not specified in plan9?
\t look content in /env/ under QEMU ? test? see need for that?


\section{Adjusting the shell environment, [[buildenv()]]}
\label{sec:buildenv}
\l with special variables

%trans:
Once [[shellenv]] has been initialized, [[sched()]] can call
[[buildenv()]] to adjust the environment with the specific
values of [[mk]]'s special variable for a specific job:

% ->  run -> sched -> <>
<<function buildenv>>=
ShellEnvVar*
buildenv(Job *j, int slot)
{
    <<[[buildenv()]] locals>>

    // main variables 
    envupd("target", wdup(j->t));
    <<[[buildenv()]] if regexp rule>>
    else
        envupd("stem", newword(j->stem));
    envupd("prereq", wdup(j->p));

    // advanced variables 
    <<[[buildenv()]] envupd some variables>>

    return shellenv;
}
@
\l rename adjustenv? or instantiate_env? no because would leak that
\l  reuse shellenv. it is an implementation opti, not part of the interface.

Note that as I mentioned in Section~\ref{sec:shellenv}, [[buildenv()]]
above reuses [[shellenv]];
[[buildenv()]] does not allocate each time a new environment.
%
Note also that some rules do not have prerequisites, or a stem, 
in which case [[buildenv() ]] will store the empty list for those entries
in [[shellenv]].



\section{Exporting the shell environment, [[exportenv()]]}

%trans:
Once [[sched()]] updated [[shellenv]] with [[buildenv()]]
and called [[execsh()]] with this environment, 
%
[[execsh()]] forks a shell interpreter and calls [[exportenv()]]
in the child process to modify its own environment:

<<[[execsh()]] in child, export environment before exec>>=
if (e)
    exportenv(e);
@

Under \plan, a process can modify its environment
\l and this environment is preserves through exec and inhited through fork
by writing in files under [[/env/]].
%
[[exportenv()]] below iterates over the entries in [[shellenv]]
(bound to the [[e]] parameter), and writes into files under [[/env/]]:

% execsh -> <>
<<function exportenv>>=
/* as well as 01's, change blanks to nulls, so that rc will
 * treat the words as separate arguments
 */
void
exportenv(ShellEnvVar *e)
{
    int n;
    fdt f;
    bool first;
    Word *w;
    char name[256];
    <<[[exportenv()]] other locals>>

    for(;e->name; e++){
        <<[[exportenv()]] skip entry if not a user variable and no value>>
        // else
        snprint(name, sizeof name, "/env/%s", e->name);
        <<[[exportenv()]] if existing symbol but no value, remove from env>>
        // else
        f = create(name, OWRITE, 0666L);
        <<[[exportenv()]] sanity check f>>
        first = true;
        for (w = e->values; w; w = w->next) {
            n = strlen(w->s);
            if (n) {
                <<[[exportenv()]] write null separator>>
                if (write(f, w->s, n) != n)
                    perror(name);
            }
        }
        close(f);
    }
}
@
%old: sy -> sym, nam -> name
\t if file already exist? OWRITE ok?

As I mentioned in Section~\ref{sec:encodenulls}, the files
under [[/env/]] use the null character as a word separator:

<<[[exportenv()]] write null separator>>=
if(first)
    first = false;
else{
    if (write (f, "\000", 1) != 1)
        perror(name);
}
@
\l just [[\0]] does not work?

There are a few situations where it is useless to write in [[/env/]].
Indeed, certain entries in [[shellenv]] might not contain any value
because the variable is undefined for a job, for instance, [[$stem]] %$ 
in a non-meta rule has no value (it is just the empty word).
[[exportenv()]] can skip those entries:

<<[[exportenv()]] other locals>>=
Symtab *sym;
bool hasvalue;
@

<<[[exportenv()]] skip entry if not a user variable and no value>>=
hasvalue = !empty_words(e->values);
sym = symlook(e->name, S_VAR, nil);
if(sym == nil && !hasvalue)	/* non-existant null symbol */
    continue;
@
%pad: use empty_words


However, if you defined a variable but assigned it the empty list,
[[exportenv()]] will delete this environment variable:

<<[[exportenv()]] if existing symbol but no value, remove from env>>=
if (sym != nil && !hasvalue) {	/* Remove from environment */
    /* we could remove it from the symbol table
     * too, but we're in the child copy, and it
     * would still remain in the parent's table.
     */
    remove(name);
    freewords(e->values);
    e->values = nil;		/* memory leak */
    continue;
}
@
\t useful for ?
\l memory leak comment?

<<[[exportenv()]] sanity check f>>=
if(f < 0) {
    fprint(STDERR, "can't create %s, f=%d\n", name, f);
    perror(name);
    continue;
}
@





\chapter{Debugging and Profiling Support TODO}
\label{chap:debugging-support}


% seen file/line fields in Rule, so can report error accurately.

\section{Printing jobs, [[shprint()]]}
\l not sure it can be considered a debugging facility, more mandatory

% With use of variables, can be hard to know what was actually executed
% so when there is an error, useful to expand those vars.

% There are some bugs though, so do not trust completely.

<<[[sched()]] other locals>>=
Bufblock *buf;
@

<<[[sched()]] print recipe command on stdout>>=
buf = newbuf();
shprint(j->r->recipe, e, buf);
if(!tflag && (nflag || !(j->r->attr&QUIET)))
    Bwrite(&bout, buf->start, (long)strlen(buf->start));
freebuf(buf);
@
\t should do shprint inside the if, useless otherwise



<<[[waitup()]] other locals>>=
ShellEnvVar *e;
@
<<[[waitup()]] if error in child process, print recipe in [[bp]]>>=
e = buildenv(j, slot);
shprint(j->r->recipe, e, bp);
front(bp->start);
@
% nice shprint! see final command with expanded variables!


<<function front>>=
void
front(char *s)
{
    char *t, *q;
    int i, j;
    char *flds[512];

    q = strdup(s);
    i = getfields(q, flds, nelem(flds), 0, " \t\n");
    if(i > 5){
        flds[4] = flds[i-1];
        flds[3] = "...";
        i = 5;
    }
    t = s;
    for(j = 0; j < i; j++){
        for(s = flds[j]; *s; *t++ = *s++);
        *t++ = ' ';
    }
    *t = 0;
    free(q);
}
@
\t ??? getfields? Libcore


%\subsection{[[shprint()]]}

% called to print what mk is executing, and also when error in a child.

% sched | update -> <> 
<<function shprint>>=
void
shprint(char *s, ShellEnvVar *env, Bufblock *buf)
{
    Rune r;
    int n;

    while(*s) {
        n = chartorune(&r, s);
        if (r == '$')
            s = vexpand(s, env, buf);
        else {
            rinsert(buf, r);
            s += n;
            s = copyq(s, r, buf);	/*handle quoted strings*/
        }
    }
    insert(buf, 0);
}
@
%$
\l abuse s as argument and iterator. Maybe could do char *p = s; 

\subsection{Expanding and printing variables}
% part 4?

<<function vexpand>>=
static char*
vexpand(char *w, ShellEnvVar *env, Bufblock *buf)
{
    char *s;
    char *p, *q;
    char carry;

    assert(/*vexpand no $*/ *w == '$');
    p = w+1;	/* skip dollar sign */
    if(*p == '{') {
        p++;
        q = utfrune(p, '}');
        if (!q)
            q = strchr(p, 0);
    } else
        q = shname(p);

    carry = *q;
    *q = '\0';
    s = mygetenv(p, env);
    *q = carry;

    if (carry == '}')
        q++;

    if (s) {
        bufcpy(buf, s, strlen(s));
        free(s);
    } else 		/* copy name intact*/
        bufcpy(buf, w, q-w);

    return q;
}
@
\l rename carry to savechar?

% either $xxx where xxx must be part of certain chars
% or ${...} and can be anything inside



% expand only "recognizable" ("interesting") variables

<<function mygetenv>>=
static char*
mygetenv(char *name, ShellEnvVar *env)
{
    if (!env)
        return nil;
    if (!symlook(name, S_WESET, nil) && 
        !symlook(name, S_INTERNAL, nil))
        return nil;
    // else

    /* only resolve internal variables and variables we've set */
    for(; env->name; env++){
        if (strcmp(env->name, name) == 0)
            return wtos(env->values, ' ');
    }
    return nil;
}
@


<<[[Sxxx]] cases>>=
S_WESET,	/* variable; we set in the mkfile */
@
% set in mkfile or via mk xxx=yyy? when mk xxx=yyy, varoverride is true
%  so also S_WESET i guess.
% useful for shprint, to expand only weset variables.

<<[[parse()]] when parsing variable definitions, extra setting>>=
symlook(head->s, S_WESET, (void *)"");
@


\subsection{Printing quoted strings}
% part 4

% -> <>
<<function copyq>>=
/*
 *	check for quoted strings.  backquotes are handled here; single quotes above.
 *	s points to char after opening quote, q.
 */
char *
copyq(char *s, Rune q, Bufblock *buf)
{
    if(q == '\'')				/* copy quoted string */
        return copysingle(s, buf);

    if(q != '`')				/* not quoted */
        return s;
    // else

    while(*s){				/* copy backquoted string */
        s += chartorune(&q, s);
        rinsert(buf, q);
        if(q == '}')
            break;
        if(q == '\'')
            s = copysingle(s, buf);	/* copy quoted string */
    }
    return s;
}
@

<<function copysingle>>=
/*
 *	copy a single-quoted string; s points to char after opening quote
 */
static char *
copysingle(char *s, Bufblock *buf)
{
    Rune r;

    while(*s){
        s += chartorune(&r, s);
        rinsert(buf, r);
        if(r == '\'')
            break;
    }
    return s;
}
@



\section{Explain mode, [[mk -e]]}

<<global explain>>=
bool explain = false;
@
% used to a char*
<<[[main()]] -xxx switch cases>>=
case 'e':
    explain = true;
    break;
@


<<[[dorecipe()]] explain when found arc [[a]] making target [[n]] out of date>>=
if(explain)
    fprint(STDOUT, "%s(%ld) < %s(%ld)\n",
        n->name, n->time, a->n->name, a->n->time);
@

<<[[dorecipe()]] explain when found target [[n]] with no prerequisite>>=
if(explain)
    fprint(STDOUT, "%s has no prerequisites\n", n->name);
@

\section{Dry mode, [[mk -n]]}

% just print recipe, do not execute them
<<global nflag>>=
bool nflag = false;
@
<<[[main()]] -xxx switch cases>>=
case 'n':
    nflag = true;
    break;
@

<<[[sched()]] other locals>>=
Node *n;
@
<<[[sched()]] if dry mode or touch mode, alternate to execsh>>=
if(nflag || tflag){
    for(n = j->n; n; n = n->next){
        <<[[sched()]] if touch mode>>
        n->time = time((long *)nil);
        MADESET(n, MADE);
    }
}
@
% marked as MADE (and still print recipe)

\section{What-if mode, [[mk -w]]{\em file}}

% what if I change a file, what needs to be done?
% usually works with -n, mk -n -wprog.h

<<[[main()]] locals>>=
Bufblock *whatif = nil;
@

<<[[main()]] -xxx switch cases>>=
case 'w':
    if(whatif == nil)
        whatif = newbuf();
    else
        insert(whatif, ' ');
    if(argv[0][2])
        bufcpy(whatif, &argv[0][2], strlen(&argv[0][2]));
    else {
        if(*++argv == '\0')
            badusage();
        bufcpy(whatif, &argv[0][0], strlen(&argv[0][0]));
    }
    break;
@
\t why not use Words instead? since you join to later split

<<[[main()]] initializations before building>>=
if(whatif){
    insert(whatif, '\0');
    timeinit(whatif->start);
    freebuf(whatif);
}
@
%old: was before execinit, catchnotes before, but I dont think it matters

% fake time of file to be just modified now by modifying the time cache
<<function timeinit>>=
void
timeinit(char *s)
{
    ulong t;
    char *cp;
    Rune r;
    int c, n;

    t = time(nil);
    while (*s) {
        cp = s;
        do{
            n = chartorune(&r, s);
            if (r == ' ' || r == ',' || r == '\n')
                break;
            s += n;
        } while(*s);
        c = *s;
        *s = '\0';

        symlook(strdup(cp), S_TIME, (void *)t)->u.value = t;

        if (c)
            *s++ = c;
        while(*s){
            n = chartorune(&r, s);
            if(r != ' ' && r != ',' && r != '\n')
                break;
            s += n;
        }
    }
}
@

%related: buck query, but operates on hundreds of .buck files so can
% answer complex queries?

%\chapter{Profiling Support}

\section{Processor utilization, [[mk -u]]}

% could provide UI like in buck where see all processors
% and which one are busy and which one are IDLE
% (buck does it in the console but via screen-like extensions, so on plan9
% I may have to do a proper UI instead)

% display time spent in different nrunnning configuration
% also display time to parse and build graph since at that time
%  nrunning is zero.

<<global uflag>>=
bool uflag = false;
@
<<[[main()]] -xxx switch cases>>=
case 'u':
    uflag = true;
    break;
@


<<global tslot>>=
// map<nrunning, int>
static ulong tslot[1000];
@
% must be more than nproclimit

% could be made local static!
<<global tick>>=
static ulong tick;
@

<<[[main()]] setup profiling>>=
usage();
@

<<[[main()]] profile initializations>>=
usage();
@
%old: used to be just after inithash()

% called from many places
<<function usage>>=
void
usage(void)
{
    ulong t;

    t = time(0);
    if(tick)
        tslot[nrunning] += t - tick;
    tick = t;
}
@
\t rename usage, to profile







<<[[main()]] print profiling stats if uflag>>=
if(uflag)
    prusage();
@

<<function prusage>>=
void
prusage(void)
{
    int i;

    usage();
    for(i = 0; i <= nevents; i++)
        fprint(STDOUT, "%d: %lud\n", i, tslot[i]);
}
@
% nevents = nproclimit



\chapter{Advanced Features TODO}
\label{chap:advanced}

\section{Regular-expression rules, [[:R:]]}
\label{sec:regexp}

% use libregexp/: regcomp(), regexec(), regsub()

%in sys/src/cmd/mkfile
%^([$OS])\.(.*):R:	\2.\1
%	$stem1^l -o $target $stem2.$stem1
%
%.*\.[$OS]:R:	$HFILES
%
%(.*)\.([$OS])'$':R:	\1.c
%	$stem2^c $CFLAGS $stem1.c


<<[[Rule_attr]] cases>>=
REGEXP = 0x0020,
@
<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'R':
    *attr |= REGEXP;
    break;
@

% need attribute, otherwise no way to infer that quoted regexp
% is a regexp and not a weird filename
% (for %& we look outside quoted strings with charin())

<<[[Rule]] other fields>>=
Reprog		*pat;		/* reg exp goo */
@


% this is for error management, regcomp can probably trigger
% the regerror callback! subtle!
<<global patrule>>=
Rule *patrule;
@

<<[[addrule()]] if REGEXP attribute>>=
if(attr&REGEXP){
    patrule = r;
    r->pat = regcomp(target);
}
@


<<[[dorecipe()]] if regexp rule>>=
if(master_rule->attr&REGEXP){
    last_oldtargets->next = newword(node->name);
    last_alltargets->next = newword(node->name);
}
@
% seems to do very little compared to the meta rule case
% where we update the node list. Because cant have multiple targets
% with regexp rule?

<<[[buildenv()]] if regexp rule>>=
if(j->r->attr&REGEXP)
    envupd("stem", newword(""));
@
% instead use stem1, stem2, etc

<<function regerror>>=
//@Scheck: not dead, called via regcomp() when have regexp syntax error
void regerror(char *s)
{
    if(patrule)
        fprint(STDERR, "mk: %s:%d: regular expression error; %s\n",
            patrule->file, patrule->line, s);
    else
        fprint(STDERR, "mk: %s:%d: regular expression error; %s\n",
            infile, mkinline, s);
    Exit();
}
@


<<constant NREGEXP>>=
#define		NREGEXP		10
@
% so stem1, stem2, stem9

% instead of stem
<<[[Arc]] other fields>>=
char		*match[NREGEXP];
@

<<[[newarc()]] set other fields>>=
rcopy(a->match, match, NREGEXP);
@

<<function rcopy>>=
void
rcopy(char **to, Resub *match, int n)
{
    int c;
    char *p;

    *to = match->sp;		/* stem0 matches complete target */
    for(to++, match++; --n > 0; to++, match++){
        if(match->sp && match->ep){
            p = match->ep;
            c = *p;
            *p = 0;
            *to = strdup(match->sp);
            *p = c;
        }
        else
            *to = 0;
    }
}
@


% instead of stem
<<[[Job]] other fields>>=
char		**match;
@

<<[[myenv]] other array elements>>=
"stem0",		/* must be in order from here */
"stem1",
"stem2",
"stem3",
"stem4",
"stem5",
"stem6",
"stem7",
"stem8",
"stem9",
@

<<[[buildenv()]] locals>>=
char **p;
int i;
@

<<[[buildenv()]] envupd some variables>>=
/* update stem0 -> stem9 */
for(p = specialvars; *p; p++)
    if(strcmp(*p, "stem0") == 0)
        break;
for(i = 0; *p; i++, p++){
    if((j->r->attr&REGEXP) && j->match[i])
        envupd(*p, newword(j->match[i]));
    else 
        envupd(*p, newword(""));
}
@

<<[[applyrules]] other locals>>=
Resub rmatch[NREGEXP];
@
<<[[applyrules]] other initializations>>=
memset((char*)rmatch, 0, sizeof(rmatch));
@
% rmatch??? for metarules? for newarc?

<<[[applyrules()]] if regexp rule then continue if some conditions>>=
if(r->attr&REGEXP){
    stem[0] = '\0';
    memset((char*)rmatch, 0, sizeof(rmatch));
    patrule = r;
    if(regexec(r->pat, node->name, rmatch, NREGEXP) == 0)
        continue;
}
@

<<[[applyrules()]] if regexp rule, adjust buf and rmatch>>=
if(r->attr&REGEXP)
    regsub(pre->s, buf, sizeof(buf), rmatch, NREGEXP);
@

\section{Dynamic [[mkfile]], [[<|]]{\em prog}}
\label{sec:dynamic-mkfile}

% How to do the ifdef I use in pfff's makefiles?

% Useful to provide an alternative to ifdef in Make.
% instead you put the configurable part in an external
% shell program that just does some echo FOO=X.

% used by plan9 kernel to accept DSL for kernel configuration.

<<[[rhead()]] adjust sep if dynamic mkfile [[<|]]>>=
if(sep == '<' && *p == '|'){
    sep = '|';
    p++;
}
@

% mkfile content through pipe
% <|cmd  instead of <file

<<[[parse()]] other locals>>=
int pid;
@
% abused to store pid and later return status

<<[[WaitupParam]] other cases>>=
EMPTY_CHILDREN_IS_ERROR2 = -2,
EMPTY_CHILDREN_IS_ERROR3 = -3,
@
%old:   
%  could use __LINE__ instead of those ERROR and ERROR2


<<[[parse()]] switch rhead cases>>=
case '|':
    p = wtos(tail, ' ');
    if(*p == '\0'){
        SYNERR(-1);
        fprint(STDERR, "missing include program name\n");
        Exit();
    }

    initenv();
    pid = pipecmd(p, shellenv, &newfd);
    if(newfd < 0){
        fprint(STDERR, "warning: skipping missing program file: ");
        perror(p);
    } else
        parse(p, newfd, 0);

    while(waitup(EMPTY_CHILDREN_IS_ERROR3, &pid) >= 0)
        ;
    if(pid != 0){
        fprint(STDERR, "bad include program status\n");
        Exit();
    }
    break;
@
% why use waitup? it's a parsing time, so there should be no jobs
% yet so stupid to introduce Process machinery for that.

% execinit cos command string can contain variables?

%plan9.c
<<function pipecmd>>=
int
pipecmd(char *cmd, ShellEnvVar *e, int *fd)
{
    int pid;
    fdt pfd[2];

    if(DEBUG(D_EXEC))
        fprint(STDOUT, "pipecmd='%s'\n", cmd);/**/

    if(fd && pipe(pfd) < 0){
        perror("pipe");
        Exit();
    }
    pid = rfork(RFPROC|RFFDG|RFENVG);
    if(pid < 0){
        perror("mk fork");
        Exit();
    }
    if(pid == 0){
        if(fd){
            close(pfd[0]);
            dup(pfd[1], 1);
            close(pfd[1]);
        }
        if(e)
            exportenv(e);
        if(shflags)
            execl(shell, shellname, shflags, "-c", cmd, nil);
        else
            execl(shell, shellname, "-c", cmd, nil);
        perror(shell);
        _exits("exec");
    }
    if(fd){
        close(pfd[1]);
        *fd = pfd[0];
    }
    return pid;
}
@



\section{Shell-command expansion, [[`]]{\em cmd}[[`]]}
\label{sec:backquotes}

% actually two styles, `{...} or `...`

% useful for `data`.
% also useful for globbing! SRC=`echo *.c`

<<[[assline()]] switch character cases>>=
case '`':
    if (bquote(bp, buf) == ERROR_0)
        Exit();
    break;
@

\subsection{[[bquote()]]}

<<function bquote>>=
/*
 *	assemble a back-quoted shell command into a buffer
 */
static error0
bquote(Biobuf *bp, Bufblock *buf)
{
    int c, line, term;
    int start;

    line = mkinline;
    while((c = Bgetrune(bp)) == ' ' || c == '\t')
            ;
    if(c == '{'){
        term = '}';		/* rc style */
        while((c = Bgetrune(bp)) == ' ' || c == '\t')
            ;
    } else
        term = '`';		/* sh style */

    start = buf->current - buf->start;
    for(;c > 0; c = nextrune(bp, false)){
        if(c == term){
            insert(buf, '\n');
            insert(buf, '\0');
            buf->current = buf->start + start;

            initenv();
            // running the command, passing a buf argument
            execsh(nil, buf->current, buf, shellenv);

            return OK_1;
        }
        if(c == '\n')
            break;
        if(c == '\'' || c == '"' || c == '\\'){
            insert(buf, c);
            if(!escapetoken(bp, buf, 1, c))
                return ERROR_0;
            continue;
        }
        rinsert(buf, c);
    }
    SYNERR(line);
    fprint(STDERR, "missing closing %c after `\n", term);
    return ERROR_0;
}
@
\l should waitup here? to consume the Wait records

% pass false to nextrune, so do not elide

\subsection{Adjusting [[execsh()]]}

% but so different that maybe should write another function
% instead of trying to factorize too much.


<<[[execsh()]] other locals>>=
fdt out[2];
@
<<[[execsh()]] if buf then create pipe to save output>>=
if(buf && pipe(out) < 0){
    perror("pipe");
    Exit();
}
@

<<[[execsh()]] in child, if buf, close one side of pipe>>=
if(buf)
    close(out[0]);
@

<<[[execsh()]] in child, if buf, dup and close>>=
if(buf){
    // output now goes in the pipe
    dup(out[1], STDOUT);
    close(out[1]);
}
@

<<[[execsh()]] in grandchild, if buf, close other side of pipe>>=
if(buf)
    close(out[1]);
@
%bug: %pad: was not guarded by if(buf), but was bug I think



<<[[execsh()]] other locals>>=
int tot, n;
@

<<[[execsh()]] in parent, if buf, close other side of pipe and read output>>=
if(buf){
    close(out[1]);
    tot = 0;
    for(;;){
        if (buf->current >= buf->end)
            growbuf(buf);
        n = read(out[0], buf->current, buf->end-buf->current);
        if(n <= 0)
            break;
        buf->current += n;
        tot += n;
    }
    if (tot && buf->current[-1] == '\n')
        buf->current--;
    close(out[0]);
}
@

% note that child process, but not a job, but still will
% be returned in waitup. No choice. But to avoid interference,
% we could call wait() in a loop to discards all those processes
% until no children process.

\section{Substitution variables, [[${]]{\em name}[[:]]{\em pattern}[[=]]{\em subst}[[}]]}
%$
\label{sec:transform-list}
\label{sec:var-generator}
\label{sec:subst-var}

% 2 uses for ${}. One for escaping, or for concatenation
% with things that would be interpreted as part of variable name.
% Other is variable "generator"

% note that can use ${x:%=%} only outside recipe :(
% they are not recognized by the shell and recipe are
% not processed by mk.
% So variables are a bit leaky abstraction. Vars in targets/prereqs
% are treated differently than in recipe.

%gmake: can not even do ${DEP_LIBS:%=../external/%/%.cma}
% does not allow multiple '%'
% See gmake manual: 
% "Only the first % in the pattern and replacement is treated this way; any subsequent % is unchanged."

%\subsubsection{Variables} 
% was in Parsing chapter before, after charin()
\subsection{Parsing adjustments}

% variables can contain ':' so need special handling here.
% Didnt do that in assline because variables dont influence the
% semantic of # or newline.

<<[[charin()]] switch rune cases>>=
case '$':
    if(*(cp+1) == '{')
        vargen = true;
    break;
case '}':
    if(vargen)
        vargen = false;
    else
       // same as default: case
       if(utfrune(pat, r))
          return cp;
    break;
@
%$
\l could goto default:

%ocaml-found:
% note that in ${name} name must also be simple, but need that
% so can contatenate a variable name with a letter, as in ${name}ici
% otherwise ambiguity with $nameici, and dont want put a space because
% space have a meaning.
%real-world: not a pb in make since impose $(xxx) format for every variables.

%rc deals with that with ^ so can do in a recipe $stem1^l to get
% 5l, 8l, etc.


<<[[charin()]] sanity check vargen>>=
if(vargen){
    SYNERR(-1);
    fprint(STDERR, "missing closing } in pattern generator\n");
}
@
% pattern, meh, actually not always a pattern




% charin -> <>
<<[[varsub()]] if variable starts with open brace>>=
if(**s == '{')		/* either ${name} or ${name: A%B==C%D}*/
    return expandvar(s);
@

<<function expandvar>>=
static Word*
expandvar(char **s)
{
    Word *w;
    Bufblock *buf;
    Symtab *sym;
    char *cp, *begin, *end;

    begin = *s;
    (*s)++;						/* skip the '{' */
    buf = varname(s);
    if (buf == nil)
        return nil;
    cp = *s;
    if (*cp == '}') {				/* ${name} variant*/ //$
        (*s)++;					/* skip the '}' */
        w = varmatch(buf->start);
        freebuf(buf);
        return w;
    }


    if (*cp != ':') {
        SYNERR(-1);
        fprint(STDERR, "bad variable name <%s>\n", buf->start);
        freebuf(buf);
        return nil;
    }
    cp++;
    end = charin(cp , "}");
    if(end == nil){
        SYNERR(-1);
        fprint(STDERR, "missing '}': %s\n", begin);
        Exit();
    }
    *end = '\0';
    *s = end+1;
    
    sym = symlook(buf->start, S_VAR, 0);
    if(sym == nil || sym->u.value == 0)
        w = newword(buf->start);
    else
        w = subsub(sym->u.ptr, cp, end);
    freebuf(buf);
    return w;
}
@
%$

\subsection{Substitutions, [[subsub()]]}

% note that does not support recursive vars! like ${name:${O}%.c=%.o}

<<function subsub>>=
static Word*
subsub(Word *v, char *s, char *end)
{
    int nmid;
    Word *head, *tail, *w, *h;
    Word *a, *b, *c, *d;
    Bufblock *buf;
    char *cp, *enda;

    a = extractpat(s, &cp, "=%&", end);
    b = c = d = nil;
    if(PERCENT(*cp))
        b = extractpat(cp+1, &cp, "=", end);
    if(*cp == '=')
        c = extractpat(cp+1, &cp, "&%", end);
    if(PERCENT(*cp))
        d = stow(cp+1);
    else if(*cp)
        d = stow(cp);

    head = tail = nil;
    buf = newbuf();
    for(; v; v = v->next){
        h = w = 0;
        if(submatch(v->s, a, b, &nmid, &enda)){
            /* enda points to end of A match in source;
             * nmid = number of chars between end of A and start of B
             */
            if(c){
                h = w = wdup(c);
                while(w->next)
                    w = w->next;
            }
            if(PERCENT(*cp) && nmid > 0){	
                if(w){
                    bufcpy(buf, w->s, strlen(w->s));
                    bufcpy(buf, enda, nmid);
                    insert(buf, '\0');
                    free(w->s);
                    w->s = strdup(buf->start);
                } else {
                    bufcpy(buf, enda, nmid);
                    insert(buf, '\0');
                    h = w = newword(buf->start);
                }
                buf->current = buf->start;
            }
            if(d && *d->s){
                if(w){

                    bufcpy(buf, w->s, strlen(w->s));
                    bufcpy(buf, d->s, strlen(d->s));
                    insert(buf, '\0');
                    free(w->s);
                    w->s = strdup(buf->start);
                    w->next = wdup(d->next);
                    while(w->next)
                        w = w->next;
                    buf->current = buf->start;
                } else
                    h = w = wdup(d);
            }
        }
        if(w == 0)
            h = w = newword(v->s);
    
        if(head == 0)
            head = h;
        else
            tail->next = h;
        tail = w;
    }
    freebuf(buf);
    freewords(a);
    freewords(b);
    freewords(c);
    freewords(d);
    return head;
}
@

% another parsing function ...
% subsub -> <>
<<function extractpat>>=
static Word*
extractpat(char *s, char **r, char *term, char *end)
{
    int save;
    char *cp;
    Word *w;

    cp = charin(s, term);
    if(cp){
        *r = cp;
        if(cp == s)
            return nil;
        save = *cp;
        *cp = '\0';
        w = stow(s);
        *cp = save;
    } else {
        *r = end;
        w = stow(s);
    }
    return w;
}
@

% subsub -> <>
<<function submatch>>=
static bool
submatch(char *s, Word *a, Word *b, int *nmid, char **enda)
{
    Word *w;
    int n;
    char *end;

    n = 0;
    for(w = a; w; w = w->next){
        n = strlen(w->s);
        if(strncmp(s, w->s, n) == 0)
            break;
    }
    if(a && w == nil)		/*  a == NULL matches everything*/
        return false;

    *enda = s+n;		/* pointer to end a A part match */
    *nmid = strlen(s)-n;	/* size of remainder of source */
    end = *enda+*nmid;
    for(w = b; w; w = w->next){
        n = strlen(w->s);
        if(strcmp(w->s, end-n) == 0){
            *nmid -= n;
            break;
        }
    }
    if(b && w == nil)		/* b == NULL matches everything */
        return false;
    return true;
}
@


\section{Rule attributes}
\label{sec:rule-attributes-advanced}
\n Advanced? well I know explain all rule attributes here except fake one
\n  like META or REGEXP.



%\section{Rule attributes propagation}
% mv with adv topics? anyway, not much used otherwise

<<[[graph()]] propagate attributes>>=
// propagate attributes in rules to their node
attribute(root);
@

% need attribute? anyway can access rule of node each time
% or probably because it is a union of attr for all matching rules.
%  not just the master rule.
% But in which situation it matters? for Virtual! because timeof
% may have been wrong and considered a file with the same name of
% a target. When we find out it is a virtual node, we need to reset
% the time of this node.

% propagate from rule to node
\t why? just for the union of virtual?

<<function attribute>>=
static void
attribute(Node *n)
{
    Arc *a;

    for(a = n->arcs; a; a = a->next){
        <<[[attribute()]] propagate rule attribute to node cases>>
        // recurse
        if(a->n)
            attribute(a->n);
    }
    <<[[attribute()]] if virtual node>>
}
@



\subsection{Virtual target, [[:V:]]}
\label{sec:virtual}
%real-world: .PHONY

% this is useful

% ex, clean:V: so even if there is a file called clean
% in the directory (and no prereqs),
%  mk clean   will still run the recipe
% or all:V: $PROG in mkone

<<[[Rule_attr]] cases>>=
VIR    = 0x0010,
@
<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'V':
    *attr |= VIR;
    break;
@

<<[[Node_flag]] cases>>=
VIRTUAL    = 0x0001,
@
<<[[attribute()]] propagate rule attribute to node cases>>=
if(a->r->attr&VIR)
    n->flags |= VIRTUAL;
@


<<[[attribute()]] if virtual node>>=
if(n->flags&VIRTUAL)
    n->time = 0;
@
%ocaml-found:
% need that because maybe there was a file with the same name
% than the virtual target in the directory
% with a date that timeof would have used! then maybe this
% date was very recent, more recent than the prereqs and
% so the recipe will not have been triggered.


<<[[dorecipe()]] when no recipe found, if virtual or norecipe node>>=
if((node->flags&VIRTUAL) || (node->flags&NORECIPE)){
    <<[[dorecipe()]] when no recipe found, if archive name>>
    else
        update(node, false);
    <<[[dorecipe()]] when no recipe found, if tflag>>
}
@
%old: change !X && !Y  to X || Y   so easier to aspectize



<<[[update()]] if virtual node or inexistent file>>=
if((node->flags&VIRTUAL) || (access(node->name, AEXIST) != OK_0)){
    node->time = 1;
    for(a = node->arcs; a; a = a->next)
        if(a->n && outofdate(node, a, true))
            node->time = a->n->time;
}
@
% pass true to outofdate here!!

% so if target not exist, still put MADE for node
% because anyway, otherwise would loop infinitely.
% and set date of newest prereq.

% what if it did not change? return an error? No! it can happen legitimely
%  in some cases, like when do x.tab.h: y.tab.h (see plan9/shell/rc/mkfile).
% why set time to 1 for virtual node? just to be different than None
%  and then anway later take latest time of one of the prereqs
\t but then what if no prereq? then time is 1. important?


\subsection{Deleting a target when the recipe returns an error, [[:D:]]}

% seems safe, but really the tool involved should not generate
% the target in the first place.
% Note that ocamllex has this problem. 

<<[[Rule_attr]] cases>>=
DEL    = 0x0080,
@
<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'D':
    *attr |= DEL;
    break;
@

<<[[Node_flag]] cases>>=
DELETE     = 0x0800,
@
<<[[attribute()]] propagate rule attribute to node cases>>=
if(a->r->attr&DEL)
    n->flags |= DELETE;
@
% why need to propagate this on the node? could not just
% look for info of the rule from the job? Maybe.




<<[[waitup()]] other locals>>=
Node *n;
bool done;
@

<<[[waitup()]] when error in child process, delete if DELETE node>>=
for(n = j->n, done = false; n; n = n->next)
    if(n->flags&DELETE){
        if(!done) {
            fprint(STDERR, ", deleting");
            done = true;
        }
        fprint(STDERR, " '%s'", n->name);
        delete(n->name);
    }
@
%old: if(done++ == 0)

% waitup -> <>
<<function delete>>=
void
delete(char *name)
{
    if(utfrune(name, '(') == 0) {		/* file */
        if(remove(name) < 0)
            perror(name);
    } else
        fprint(STDERR, "hoon off; mk can't delete archive members\n");
}
@
%ugly: bug if filename contain open parenthesis

\subsection{Not printing the recipe (quiet mode), [[:Q:]]}
% work with -n too? nflag || !QUIET?

<<[[Rule_attr]] cases>>=
QUIET  = 0x0008,
@

<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'Q':
    *attr |= QUIET;
    break;
@




\subsection{Running a shell script without [[-e]], [[:E:]]}
% "continue execution if the recipe draws errors"

% when want that?

<<[[Node_flag]] cases>>=
NOMINUSE   = 0x1000,
@

<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'E':
    *attr |= NOMINUSE;
    break;
@

<<[[sched()]] reset flags if NOMINUSE rule>>=
if (j->r->attr&NOMINUSE)
    flags = nil;
@
%old: flags = (j->r->attr&NOMINUSE)? nil : "-e" ; but was harder to aspectize


\subsection{Disabling the no-recipe warning, [[:N:]]}
%"if there is no recipe, the target has its time updated"

% when want that?

<<[[Rule_attr]] cases>>=
NOREC  = 0x0040,
@
<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'N':
    *attr |= NOREC;
    break;
@


<<[[Node_flag]] cases>>=
NORECIPE   = 0x0400,
@
<<[[attribute()]] propagate rule attribute to node cases>>=
if(a->r->attr&NOREC)
    n->flags |= NORECIPE;
@

% when no recipe found, print warning or not?


\subsection{Forbidding metarules to match virtual targets,  [[:n:]]}

%" the rule is a meta-rule that cannot be a target of a virtual rule,
% only files match the pattern in the target"

% when want that? git grep :n:

<<[[Rule_attr]] cases>>=
NOVIRT = 0x0100,
@
<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'n':
    *attr |= NOVIRT;
    break;
@

<<[[applyrules()]] skip this meta rule and continue if some conditions>>=
if ((r->attr&NOVIRT) && lasta != &head && (lasta->r->attr&VIR))
    continue;
@
% really useful?

\subsection{Custom-dependency comparison program, [[:P:]]}
\label{sec:custom-comparison}
% big one

% "the characters after the P until terminating : are taken as a program name.
% It will be invoked as rc -c prog 'arg1' 'arg2' and should return a null
% exit status if and only if arg1 is up to date with respect to arg2 ..."

% it's a way to have a hook in the out-of-date and time of file system.
% If it is not good enough for you, you can give adhoc stuff.

% see mk man page.  use for x.tab and y.tab.h, but
% can do without
% x.tab.h:Pcmp -s: y.tab.h
%    cp y.tab.h x.tab.h

% but can do with
% x.tab.h: y.tab.h
%    cmp y.tab.h x.tab.h || cp y.tab.h x.tab.h

<<[[Rule]] other fields>>=
char		*prog;		/* to use in out of date */
@
% prog??


<<[[parse()]] other locals>>=
char *prog;
@
% passed as rhread(..., &prog) so can then
% be passed to addrules as an argument

<<[[addrule()]] set more fields>>=
r->prog = prog;
@



<<[[Arc]] other fields>>=
char		*prog;
@

<<[[newarc()]] set other fields>>=
a->prog = r->prog;
@




<<[[rhead()]] other locals>>=
char *pp;
@

<<[[rhead()]] when parsing rule attributes, switch rune cases>>=
case 'P':
    pp = utfrune(p, ':');
    if (pp == 0 || *pp == 0)
        goto eos;
    *pp = 0;
    *prog = strdup(p);
    *pp = ':';
    p = pp;
    break;
@


<<[[Sxxx]] cases>>=
S_OUTOFDATE,	/* n1\377n2 -> 2(outofdate) or 1(not outofdate) */
@
% abuse string ...


<<[[update()]] set outofdate prereqs if arc prog>>=
for(a = node->arcs; a; a = a->next)
    if(a->prog)
        outofdate(node, a, true);
@


<<[[outofdate()]] locals>>=
char buf[3*NAMEBLOCK];
char *str = nil;
Symtab *sym;
int ret;
@

<<[[outofdate()]] if arc->prog>>=
if(arc->prog){
    snprint(buf, sizeof buf, "%s%c%s", node->name, 0377,
        arc->n->name);
    sym = symlook(buf, S_OUTOFDATE, 0);
    if(sym == nil || eval){
        if(sym == nil)
            str = strdup(buf);
        ret = pcmp(arc->prog, node->name, arc->n->name);
        if(sym)
            sym->u.value = ret;
        else
            symlook(str, S_OUTOFDATE, (void *)ret);
    } else
        ret = sym->u.value;
    return (ret-1);
}
@

<<function pcmp>>=
static int
pcmp(char *prog, char *p, char *q)
{
    char buf[3*NAMEBLOCK];
    int pid;

    Bflush(&bout);
    snprint(buf, sizeof buf, "%s '%s' '%s'\n", prog, p, q);
    pid = pipecmd(buf, 0, 0);
    while(waitup(EMPTY_CHILDREN_IS_ERROR3, &pid) >= 0)
        ;
    return (pid? 2:1);
}
@
\l could call a different waitup, so no interference with job





\subsubsection{[[Process]]}

% with all those extensions (backquote, :P:, <|)
% you can run not just recipe jobs. However we need to wait for
% those process to finish too. wait() does not discriminate
% so we need to put more logic in waitup() and remember
% which process are job process and which one are not.

<<struct Process>>=
struct Process {
    int pid;
    int status;

    // Extra
    // double_list<ref_own<Process> backward, forward
    Process *b, *f;
};
@
%ocaml: another ex of redundant code. should be factorized in container library.
\t why need that? [[events]] not enough? seems to store results of process
\t  so rename  ProcessStatus?
\t  I think needed only for process that are not jobs! not in events!
\t  such as backquote, or :P: stuff.
\t rename status to bool error.
\t rename NotJobProcesses

<<global phead>>=
// double_list<ref_own<Process> (next = Process.f)
static Process *phead;
@

% classic, dynamic allocator
<<global pfree>>=
// double_list<ref_own<Process> (next = Process.f)
static Process *pfree;
@

% ctor
% waitup -> <>
<<function pnew>>=
static void
pnew(int pid, int status)
{
    Process *p;

    // p = pop_list(pfree)
    if(pfree){
        p = pfree;
        pfree = p->f;
    } else
        p = (Process *)Malloc(sizeof(Process));

    p->pid = pid;
    p->status = status;

    // add_list(p, phead)
    p->f = phead;
    phead = p;
    if(p->f)
        p->f->b = p;
    p->b = nil;
}
@

% dtor
<<function pdelete>>=
static void
pdelete(Process *p)
{
    // remove_double_list(p, phead, pfree)
    if(p->f)
        p->f->b = p->b;
    if(p->b)
        p->b->f = p->f;
    else
        phead = p->f;
    p->f = pfree;
    pfree = p;
}
@
\l rename pfree instead?

\subsubsection{[[waitup()]] adjustments}

<<[[waitup()]] if slot not found, not a job pid, update process list>>=
if(slot < 0){
   <<[[waitup()]] if DEBUG(D_EXEC) and slot < 0>>
    pnew(pid, buf[0]? 1:0);
    goto again;
}
@
% when can this happen? can waitfor child that was not in 
%  events? maybe child 2 in execsh?



% used only for <|, maybe move to later

<<[[waitup()]] other locals>>=
Process *p;
@

<<[[waitup()]] if retstatus, check process list>>=
/* first check against the process list */
if(retstatus)
    for(p = phead; p; p = p->f)
        if(p->pid == *retstatus){
            *retstatus = p->status;
            pdelete(p);
            return -1;
        }
@

<<[[waitup()]] if retstatus, check if matching pid>>=
if(retstatus && pid == *retstatus){
    *retstatus = buf[0]? 1:0;
    return -1;
}
@

\subsubsection{[[killchildren()]] adjustments}

<<[[killchildren()]] locals>>=
Process *p;
@
<<[[killchildren()]] expunge not-job processes>>=
for(p = phead; p; p = p->f)
    expunge(p->pid, msg);
@


<<function expunge>>=
void
expunge(int pid, char *msg)
{
    postnote(PNPROC, pid, msg);
}
@
% PNPROC?



\section{Variable attributes}
\label{sec:variable-attributes}

% constant with attributes?? just private var
%  to not transmit to recipe process?

\subsection{Private variables, [[=U=]]}

% U for? unexport?

<<[[rhead()]] when parsing variable attributes, switch rune cases>>=
case 'U':
    *attr = 1;
    break;
@
<<[[parse()]] when parsing variable definitions, if variable with attr>>=
if(attr)
    symlook(head->s, S_NOEXPORT, (void *)"");
@

<<[[Sxxx]] cases>>=
S_NOEXPORT,	/* var -> noexport */ // set of noexport variables
@

% ecopy called from execinit
<<[[ecopy()]] return and do not copy if S_NOEXPORT symbol>>=
if(symlook(s->name, S_NOEXPORT, nil))
    return;
@


\section{Advanced [[mk]] variables}

\subsection{[[$target]] versus [[$alltargets]]}
% should be oldtargets vs targets, would be more consistent
% with prereqs vs newprereqs


<<[[dorecipe()]] other locals>>=
Word oldtargets;
Word *last_oldtargets = &oldtargets;
@
%old: head -> oldtargets, ww -> last_oldtargets

<<[[dorecipe()]] update list of outdated targets>>=
if(!aflag && n->time) {
    for(a = n->arcs; a; a = a->next)
        if(a->n && outofdate(n, a, false))
            break;
    // no out of date arc, node does not need to be regenerated
    if(a == nil)
        continue; 
    // else, find an outdated arc for node of target
}
last_oldtargets->next = newword(buf);
last_oldtargets = last_oldtargets->next;
@





<<[[myenv]] other array elements>>=
"alltarget",
@
% alltargets? 

<<[[Job]] other fields>>=
Word		*at;	/* all targets */
@

<<[[buildenv()]] envupd some variables>>=
envupd("alltarget", wdup(j->at));
@





\subsection{[[$prereq]] versus [[$newprereq]]}

<<[[dorecipe()]] other locals>>=
Word newprereqs; 
@
%old: ln -> newprereqs, 

% seems used only for aggregate rules to optimize library building.

<<[[dorecipe()]] when outofdate node, update list of newprereqs>>=
addw(&newprereqs, a->n->name);
@


<<[[myenv]] other array elements>>=
"newprereq",
@
% so does not put *.5 but only the .5 that were more recent than
% the archive.


<<[[Job]] other fields>>=
Word		*np;	/* new prerequisites */
@

<<[[buildenv()]] envupd some variables>>=
envupd("newprereq", wdup(j->np));
@

% related to aggregates?


\subsection{[[$NREP]]}
%$

<<[[mk()]] initializations>>=
nrep();		/* it can be updated dynamically */
@

% mk -> nrep
<<function nrep>>=
void
nrep(void)
{
    Symtab *sym;
    Word *w;

    sym = symlook("NREP", S_VAR, nil);
    if(sym){
        w = sym->u.ptr;
        if (w && w->s && *w->s)
            nreps = atoi(w->s);
    }
    if(nreps < 1)
        nreps = 1;
    <<[[nrep()]] if DEBUG(D_GRAPH)>>
}
@
\t mv to graph.c? or check.c?


\subsection{[[$pid]]}
%$


<<[[myenv]] other array elements>>=
"pid",
@

<<[[buildenv()]] locals>>=
char buf[256];
@

% note that buildenv is called in the master process, not the execsh one.

<<[[buildenv()]] envupd some variables>>=
snprint(buf, sizeof buf, "%d", getpid());
envupd("pid", newword(buf));
@
% use of pid? mk.ps say can be useful to communicate between rules
% meh

\subsection{[[$nproc]]}
%$

<<[[myenv]] other array elements>>=
"nproc",
@

<<[[buildenv()]] envupd some variables>>=
snprint(buf, sizeof buf, "%d", slot);
envupd("nproc", newword(buf));
@
% use of nproc??


%dead: (was used only in dumper but never set correctly)
%<<[[Job]] other fields>>
%// option<int> (None = -1)
%int		nproc;	/* slot number */
%@
% in newjob()
%    j->nproc = -1;


%\subsection{[[$NPROC]]}%$
% now before


\section{Dealing with archives (libraries)}
\l aggregate
% meh

<<[[myenv]] other array elements>>=
"newmember",
@

<<[[buildenv()]] locals>>=
Word *w, *v, **l;
char *cp, *qp;
@

<<[[buildenv()]] envupd some variables>>=
// newmember
l = &v;
v = w = wdup(j->np);
while(w){
    cp = strchr(w->s, '(');
    if(cp){
        qp = strchr(cp+1, ')');
        if(qp){
            *qp = 0;
            strcpy(w->s, cp+1);
            l = &w->next;
            w = w->next;
            continue;
        }
    }
    *l = w->next;
    free(w->s);
    free(w);
    w = *l;
}
envupd("newmember", v);
@


<<[[Sxxx]] cases>>=
S_AGG,		/* aggregate -> time */
@


<<[[dorecipe()]] when no recipe found, if archive name>>=
if(strchr(node->name, '(') && node->time == 0)
    MADESET(node, MADE);
@

<<[[timeof()]] if name archive member>>=
if(utfrune(name, '('))
    return atimeof(force, name);		/* archive */
@

<<function atimeof>>=
ulong
atimeof(int force, char *name)
{
    Symtab *sym;
    ulong t;
    char *archive, *member, buf[512];

    archive = split(name, &member);
    if(archive == 0)
        Exit();

    t = mkmtime(archive, true);
    sym = symlook(archive, S_AGG, 0);
    if(sym){
        if(force || t > sym->u.value){
            atimes(archive);
            sym->u.value = t;
        }
    }
    else{
        atimes(archive);
        /* mark the aggegate as having been done */
        symlook(strdup(archive), S_AGG, "")->u.value = t;
    }
        /* truncate long member name to sizeof of name field in archive header */
    snprint(buf, sizeof(buf), "%s(%.*s)", archive, utfnlen(member, SARNAME), member);
    sym = symlook(buf, S_TIME, 0);
    if (sym)
        return sym->u.value;
    return 0;
}
@

<<function atouch>>=
void
atouch(char *name)
{
    char *archive, *member;
    int fd, i;
    struct ar_hdr h;
    long t;

    archive = split(name, &member);
    if(archive == 0)
        Exit();

    fd = open(archive, ORDWR);
    if(fd < 0){
        fd = create(archive, OWRITE, 0666);
        if(fd < 0){
            perror(archive);
            Exit();
        }
        write(fd, ARMAG, SARMAG);
    }
    if(symlook(name, S_TIME, 0)){
        /* hoon off and change it in situ */
        seek(fd, SARMAG, 0);
        while(read(fd, (char *)&h, sizeof(h)) == sizeof(h)){
            for(i = SARNAME-1; i > 0 && h.name[i] == ' '; i--)
                    ;
            h.name[i+1]=0;
            if(strcmp(member, h.name) == 0){
                t = SARNAME-sizeof(h);	/* ughgghh */
                seek(fd, t, 1);
                fprint(fd, "%-12ld", time(0));
                break;
            }
            t = atol(h.size);
            if(t&01) t++;
            seek(fd, t, 1);
        }
    }
    close(fd);
}
@

<<function atimes>>=
static void
atimes(char *ar)
{
    struct ar_hdr h;
    ulong at, t;
    int fd, i;
    char buf[BIGBLOCK];
    Dir *d;
    
    fd = open(ar, OREAD);
    if(fd < 0)
        return;

    if(read(fd, buf, SARMAG) != SARMAG){
        close(fd);
        return;
    }
    if((d = dirfstat(fd)) == nil){
        close(fd);
        return;
    }
    at = d->mtime;
    free(d);
    while(read(fd, (char *)&h, SAR_HDR) == SAR_HDR){
        t = strtoul(h.date, nil, 0);
        if(t >= at)	/* new things in old archives confuses mk */
            t = at-1;
        if(t == 0)	/* as it sometimes happens; thanks ken */
            t = 1;
        for(i = sizeof(h.name)-1; i > 0 && h.name[i] == ' '; i--)
            ;
        if(h.name[i] == '/')		/* system V bug */
            i--;
        h.name[i+1]=0;		/* can stomp on date field */
        snprint(buf, sizeof buf, "%s(%s)", ar, h.name);
        symlook(strdup(buf), S_TIME, (void*)t)->u.value = t;
        t = atol(h.size);
        if(t&01) t++;
        seek(fd, t, 1);
    }
    close(fd);
}
@

<<[[Sxxx]] cases>>=
S_BITCH,	/* bitched about aggregate not there */
@

<<function type>>=
static int
type(char *file)
{
    int fd;
    char buf[SARMAG];

    fd = open(file, OREAD);
    if(fd < 0){
        if(symlook(file, S_BITCH, 0) == 0){
            Bprint(&bout, "%s doesn't exist: assuming it will be an archive\n", file);
            symlook(file, S_BITCH, (void *)file);
        }
        return 1;
    }
    if(read(fd, buf, SARMAG) != SARMAG){
        close(fd);
        return 0;
    }
    close(fd);
    return strncmp(ARMAG, buf, SARMAG) == 0;
}
@

<<function split>>=
static char*
split(char *name, char **member)
{
    char *p, *q;

    p = strdup(name);
    q = utfrune(p, '(');
    if(q){
        *q++ = 0;
        if(member)
            *member = q;
        q = utfrune(q, ')');
        if (q)
            *q = 0;
        if(type(p))
            return p;
        free(p);
        fprint(STDERR, "mk: '%s' is not an archive\n", name);
    }
    return 0;
}
@



<<[[outofdate()]] if arc node is an archive member>>=
if(strchr(arc->n->name, '(') && arc->n->time == 0)
   /* missing archive member */
   return true;
@

\section{Optimizations}

%\subsection{Node cache}
% not really opti, needed to build a graph

%\subsection{Time cache}

\subsection{Missing-intermediates optimization, [[mk -I]]}
\label{sec:pretending}

% mk -i suppress this behavior
% pad: mk -I! new mode :) because I don't like the default mode!

% it is there to optimize disk space, to allow to remove .o
% the intermediate betweem the .out and the .c by pretending
% they are there if we know no .c files has actually changed.
% useful at a time where disk space was expensive, and so having
% both the .o and the .a were seen as redundant waste.

% from man page:
%Nonexistent targets that have prerequisites
%and are themselves prerequisites are treated specially.

\t put to true by default? and have 'I' instead to do the opti?

<<global iflag>>=
bool iflag = false;
@
<<[[main()]] -xxx switch cases>>=
case 'i':
    iflag = true;
    break;
@


<<[[Node_flag]] cases>>=
CANPRETEND = 0x0008,
PRETENDING = 0x0010,
@
% again, should be a different field, not agglomerated with the other node flags


<<[[clrmade()]] [[n->flags]] pretend adjustments>>=
n->flags &= ~(CANPRETEND|PRETENDING);
if(strchr(n->name, '(') == 0 || n->time)
    n->flags |= CANPRETEND;
@


<<[[work()]] possibly pretending node>>=
/*
 *   can we pretend to be made?
 */
if((!iflag) && (node->time == 0) 
        && (node->flags&(PRETENDING|CANPRETEND))
        && parent_node && ra->n && !outofdate(parent_node, ra, false)){
    node->flags &= ~CANPRETEND;
    MADESET(node, MADE);
    if(explain && ((node->flags&PRETENDING) == 0))
        fprint(STDOUT, "pretending %s has time %lud\n", node->name, node->time);
    node->flags |= PRETENDING;
    return;
}
/*
 *   node is out of date and we REALLY do have to do something.
 *   quickly rescan for pretenders
 */
for(a = node->arcs; a; a = a->next)
    if(a->n && (a->n->flags&PRETENDING)){
        if(explain)
            Bprint(&bout, "unpretending %s because of %s because of %s\n",
            a->n->name, node->name, 
            ra->n? ra->n->name : "rule with no prerequisites");

        unpretend(a->n);
        work(a->n, did, node, a);
        ready = false;
    }
if(!ready) { /* try later unless nothing has happened for -k's sake */
    work(node, did, parent_node, parent_arc);
    return;
}
@



<<[[work()]] possibly unpretending node>>=
if((node->flags&MADE) && (node->flags&PRETENDING) && parent_node
    && outofdate(parent_node, parent_arc, false)){
    if(explain)
        fprint(STDOUT, "unpretending %s(%lud) because %s is out of date(%lud)\n",
            node->name, node->time, parent_node->name, parent_node->time);
    unpretend(node);
}
/*
 *   have a look if we are pretending in case
 *   someone has been unpretended out from underneath us
 */
if(node->flags&MADE){
    if(node->flags&PRETENDING){
        node->time = 0;
    }else
        return;
}
@

<<function unpretend>>=
static void
unpretend(Node *n)
{
    MADESET(n, NOTMADE);
    n->flags &= ~(CANPRETEND|PRETENDING);
    n->time = 0;
}
@

<<[[work()]] locals>>=
Arc *ra = nil;
@
% need to save in ra for pretending stuff

<<[[work()]] update [[ra]] when outofdate [[node]] with arc [[a]]>>=
if((ra == nil) || (ra->n == nil) || (ra->n->time < a->n->time))
    ra = a;
@

<<[[work()]] update [[ra]] when no dest in arc and no src>>=
if(ra == nil)
    ra = a;
@


<<[[update()]] unpretend node>>=
node->flags &= ~(CANPRETEND|PRETENDING);
@

\subsection{Touching-mode optimization, [[mk -t]]}

% when the user knows what he is doing

<<global tflag>>=
bool tflag = false;
@
<<[[main()]] -xxx switch cases>>=
case 't':
    tflag = true;
    break;
@

<<[[dorecipe()]] when no recipe found, if tflag>>=
if(tflag){
    if(!(node->flags&VIRTUAL))
        touch(node->name);
    else if(explain)
        Bprint(&bout, "no touch of virtual '%s'\n", node->name);
}
@

<<[[sched()]] if touch mode>>=
if(tflag){
    if(!(n->flags&VIRTUAL))
        touch(n->name);
    else if(explain)
        Bprint(&bout, "no touch of virtual '%s'\n", n->name);
}
@

<<function touch>>=
void
touch(char *name)
{
    Bprint(&bout, "touch(%s)\n", name);
    if(nflag)
        return;

    if(utfrune(name, '('))
        atouch(name);		/* archive */
    else
     if(chgtime(name) < 0) {
        perror(name);
        Exit();
    }
}
@

<<function chgtime>>=
int
chgtime(char *name)
{
    Dir sbuf;

    if(access(name, AEXIST) >= 0) {
        nulldir(&sbuf);
        sbuf.mtime = time((long *)nil);
        return dirwstat(name, &sbuf);
    }
    return close(create(name, OWRITE, 0666));
}
@


\subsection{Time cache}
\label{sec:time-cache}

% multiple levels of cache ... first in cache, 
% then bulk so will get more hit cache.


% force is here to say if want to bypass cache and redo
% the actual read! for instance after the recipe is done
% we want to update the time of the file and disable the cache for sure!

<<[[Sxxx]] cases>>=
S_TIME,		/* file -> time */
@

<<[[timeof()]] locals>>=
ulong t;
@

<<[[timeof()]] if not force, use time cache>>=
<<[[timeof()]] check time cache>>
t = mkmtime(name, false);
<<[[timeof()]] update time cache>>
return t;
@


<<[[timeof()]] locals>>=
Symtab *sym;
@

<<[[timeof()]] check time cache>>=
sym = symlook(name, S_TIME, nil);
if (sym)
    return sym->u.value;		/* uggh */
@
% comment?

<<[[timeof()]] update time cache>>=
if(t == 0)
    return 0;
symlook(name, S_TIME, (void*)t);		/* install time in cache */
@

% other opti about bulktime later.


\subsection{Bulk time optimisation}

% in addition to time cache, mk also batch the call to time
% by reading/caching the dir?

<<[[mkmtime]] locals>>=
//char *s, *ss;
//char carry;
//Symtab *sym;
@


<<[[mkmtime()]] bulk dir optimisation>>=
<<[[mkmtime()]] cleanup name>>
USED(force);
//TODO    s = utfrrune(name, '/');
//TODO    if(s == name)
//TODO        s++;
//TODO    if(s){
//TODO        ss = name;
//TODO        carry = *s;
//TODO        *s = '\0';
//TODO    }else{
//TODO        ss = nil;
//TODO        carry = '\0';
//TODO    }
//TODO    if(carry)
//TODO        *s = carry;
//TODO
//TODO bulkmtime(ss);
//TODO if(!force){
//TODO     sym = symlook(name, S_TIME, 0);
//TODO     if(sym)
//TODO         return sym->u.value;
//TODO     return 0;
//TODO }
@
% subtle: this opti has subtle implications with weird filesystem like VFAT!!
% With this optimisation mk will not be able to run correctly
% in simple projects like tests/8c/! It's because the directory
% contains HELLOC.C not helloc.c and so even if dirstat("helloc.c")
% will work, once you read the directory you will create an
% entry for HELLOC.C in S_TIME but not for helloc.c and then when asking
% for helloc.c you will look in the cache and see nothing and so
% return 0.


<<[[mkmtime]] locals>>=
char buf[4096];
@

<<[[mkmtime()]] cleanup name>>=
strecpy(buf, buf + sizeof buf - 1, name);
cleanname(buf);
name = buf;
@
% cleanname! so normalize! otherwise lost opportunity to realize 
%  same file in bulk cache


<<[[Sxxx]] cases>>=
S_BULKED,	/* we have bulked this dir */
@

<<function bulkmtime>>=
void
bulkmtime(char *dir)
{
    char buf[4096];
    char *ss, *s, *sym;

    if(dir){
        sym = dir;
        s = dir;
        if(strcmp(dir, "/") == 0)
            strecpy(buf, buf + sizeof buf - 1, dir);
        else
            snprint(buf, sizeof buf, "%s/", dir);
    }else{
        s = ".";
        sym = "";
        buf[0] = 0;
    }
    if(symlook(sym, S_BULKED, 0))
        return;
    // else
    ss = strdup(sym);
    symlook(ss, S_BULKED, (void*)ss);
    dirtime(s, buf);
}
@

<<function dirtime>>=
void
dirtime(char *dir, char *path)
{
    int i, fd, n;
    ulong mtime;
    Dir *d;
    char buf[4096];

    fd = open(dir, OREAD);
    if(fd >= 0){
        while((n = dirread(fd, &d)) > 0){
            for(i=0; i<n; i++){
                mtime = d[i].mtime;
                /* defensive driving: this does happen */
                if(mtime == 0)
                    mtime = 1;
                snprint(buf, sizeof buf, "%s%s", path,
                    d[i].name);
                if(symlook(buf, S_TIME, 0) == nil)
                    symlook(strdup(buf), S_TIME,
                        (void*)mtime)->u.value = mtime;
            }
            free(d);
        }
        close(fd);
    }
}
@


\section{Recompiling everything, [[mk -a]]}

<<global aflag>>=
bool aflag = false;
@
<<[[main()]] -xxx switch cases>>=
case 'a':
    aflag = true;
    iflag = true;
    break;
@
% iflag = true; used to be after the switch, but same
% when -a, means we want to build _also_ the intermediate files when
% they are not there.

<<[[work()]] adjust weoutofdate if aflag>>=
if(aflag)
    weoutofdate = true;
@
%old: weoutofdate = aflag;

\section{Recursive [[mk]]}

% if one of the mk return error codes, then should stop mk right?
% convenient then.
% TODO: this is not the case right now!!

%\section{Setting [[MKFLAGS]], [[MKARGS]]}
% used by? at least mkfile called recursively can expect those variables


<<[[main()]] locals>>=
Bufblock *buf = newbuf();
@

<<[[main()]] add [[argv[0]]] in [[buf]]>>=
bufcpy(buf, argv[0], strlen(argv[0]));
insert(buf, ' ');
@

<<[[main()]] add [[argv[i]]] in [[buf]]>>=
bufcpy(buf, argv[i], strlen(argv[i]));
insert(buf, ' ');
@



<<[[main()]] set variables for recursive mk>>=
<<[[main()]] set MKFLAGS variable>>
<<[[main()]] set MKARGS variable>>
@

<<[[main()]] set MKFLAGS variable>>=
if (buf->current != buf->start) {
    buf->current--;
    insert(buf, '\0');
}
symlook("MKFLAGS", S_VAR, (void*) stow(buf->start));
@

<<[[main()]] set MKARGS variable>>=
buf->current = buf->start;
for(i = 0; argv[i]; i++){
    if(*argv[i] == '\0') 
        continue;
    if(i)
        insert(buf, ' ');
    bufcpy(buf, argv[i], strlen(argv[i]));
}
insert(buf, '\0');
symlook("MKARGS", S_VAR, (void *) stow(buf->start));

freebuf(buf);
@




\section{[[mk -k]]}

\subsection{[[kflag]] and [[runerrs]]}

% kflag also used for mk -k, but does not make much sense I think.

<<global kflag>>=
bool kflag = false;
@
% continue even if errors.
% not so useful in general, but useful for killchildren when get a C-c ???
<<[[main()]] -xxx switch cases>>=
case 'k':
    kflag = true;
    break;
@


<<global runerrs>>=
int runerrs;
@
<<[[mk()]] initializations>>=
runerrs = 0;
@



\subsection{Adjusting [[mk()]], [[work()]], and [[waitup()]]}

% instead of Exit
<<[[work()]] when inexistent target without prerequisites, if kflag>>=
if(kflag){
    node->flags |= BEINGMADE;
    runerrs++;
}
@

% instead of Exit again
<<[[waitup()]] when error in child process, if kflag>>=
if(kflag){
    runerrs++;
    fake = true;
}
@

\label{sec:fake-update}
<<[[update()]] if fake>>=
if(fake)
    MADESET(node, BEINGMADE);
@
%old: I reorged th ecode so can aspectize it, was
% MADESET(node, fake? BEINGMADE : MADE);


<<[[mk()]] if no child to waitup and root not MADE, possibly break>>=
if(res > 0){
    if(root->flags&(NOTMADE|BEINGMADE)){
        assert(/*must be run errors*/ runerrs);
        break;	/* nothing more waiting */
    }
}
@
\t res == WEmptyChild ?

% subtle
<<[[mk()]] before returning, more [[waitup()]] if there was an error>>=
while(jobs)
    waitup(EMPTY_CHILDREN_IS_ERROR2, (int *)nil);
assert(/*target didnt get done*/ runerrs || (root->flags&MADE));
@
\t when do we have this? really need that?






\chapter{Conclusion}
\label{chap:conclusion}

%dup: (and adapted) from Assembler.nw
You now know how the \plan build system [[mk]] works, 
to the smallest details, and more generally how many
build systems work.

Because [[mk]] relies heavily on the shell [[rc]], 
the \book{Shell} is the next logical step after this book.
%
Many features of [[mk]] are inspired by features from the shell.
\l ability to do loops, calls programs easily, redirection, pipe, etc.
% Reuse everything.

% could be nice to have a mk -d session at the end
% where see how all make sense, the flag values of nodes with 0x4
% because all are READY, etc.

% Note that did not cover much automatic dependencies.
% Also tricky when change flags, flag should also be source
% of dependencies (redo does that?)

% how to allow :: of GNU make? bad to allow :: in the end? can generate
%  ambiguity? just agglomerate recipe?

% related futur work? Cargo, Yarn, and other package-manager/build-system?
% true that nice to put a few library names and cargo automatically
% fetch and compile them (and transitively, so also the deps of those libs)
% and add them to the linking command. It hides
% complexity. Still in the end, under the hood, cargo does
% recompile only what is necessary and so have some of the mechanic
% I described in this book.

\appendix

\chapter{Debugging}
\label{chap:debugging-appendix}

%todo? http://bashdb.sourceforge.net/remake/
% apparently make with more debugging capabilities

%\section{Dumping internals, [[mk -d]]}

%mk -d[epg] EXEC PARSE GRAPH

<<enum Dxxx>>=
enum Dxxx {
    // for rules
    D_PARSE =		0x01,
    // for node and arcs
    D_GRAPH =		0x02,
    // for jobs
    D_EXEC  =		0x04,

    // tracing some calls
    D_TRACE  =		0x08,
};
@
%old: used to be a set of #define
%pad: I added D_TRACE

% -d flag
<<global debug>>=
// bitset<enum<dxxx>>
int debug;
@

<<function DEBUG>>=
#define	DEBUG(x)	(debug&(x))
@

<<[[main()]] locals>>=
char *s;
@
<<[[main()]] -xxx switch cases>>=
case 'd':
    if(*(s = &argv[0][2]))
        while(*s) 
         switch(*s++) {
         case 'p':	debug |= D_PARSE; break;
         case 'g':	debug |= D_GRAPH; break;
         case 'e':	debug |= D_EXEC; break;
        }
    else
        debug = 0xFFFF; // D_PARSE | D_GRAPH | D_EXEC
    break;
@
\t D_TRACE?


\section{The rules, [[mk -dp]]}
% p for parsing

<<[[main()]] if DEBUG(D_PARSE)>>=
if(DEBUG(D_PARSE)){
    dumpw("default targets", target1);
    dumpr("rules", rules);
    dumpr("metarules", metarules);
    dumpv("variables");
}
@


<<dumper dumpw>>=
void
dumpw(char *s, Word *w)
{
    Bprint(&bout, "%s", s);
    for(; w; w = w->next)
        Bprint(&bout, " '%s'", w->s);
    Bputc(&bout, '\n');
}
@

% todo: remove the pointer thing %p, not that useful I think
% note that the variable expansion has been done for the target and prerequestes
<<dumper dumpr>>=
void
dumpr(char *s, Rule *r)
{
    Bprint(&bout, "%s: start=%p\n", s, r);
    for(; r; r = r->next){
        Bprint(&bout, "\tRule %p: %s:%d attr=%x next=%p chain=%p alltarget='%s'",
            r, r->file, r->line, r->attr, r->next, r->chain, wtos(r->alltargets, ' '));
        if(r->prog)
            Bprint(&bout, " prog='%s'", r->prog);
        Bprint(&bout, "\n\ttarget=%s: %s\n", r->target, wtos(r->prereqs,' '));
        Bprint(&bout, "\trecipe@%p='%s'\n", r->recipe, r->recipe);
    }
}
@

<<dumper dumpv>>=
void
dumpv(char *s)
{
    Bprint(&bout, "%s:\n", s);
    symtraverse(S_VAR, print1);
}
@

<<function print1>>=
static void
print1(Symtab *s)
{
    Word *w;

    Bprint(&bout, "\t%s=", s->name);
    for (w = s->u.ptr; w; w = w->next)
        Bprint(&bout, "'%s'", w->s);
    Bprint(&bout, "\n");
}
@





\section{The graph, [[mk -dg]]}

% node = graph(target) from mk(target)
<<[[mk()]] if DEBUG(D_GRAPH)>>=
if(DEBUG(D_GRAPH)){
    dumpn("new target\n", root);
    Bflush(&bout);
}
@
% below the buf is here to compute indentation, depth of the graph in spaces

<<dumper dumpn>>=
void
dumpn(char *s, Node *n)
{
    char buf[1024];
    Arc *a;

    Bprint(&bout, "%s%s@%p: time=%ld flags=0x%x next=%p\n",
        s, n->name, n, n->time, n->flags, n->next);
    for(a = n->arcs; a; a = a->next){
        snprint(buf, sizeof buf, "%s   ", (*s == ' ')? s:"");
        dumpa(buf, a);
    }
}
@

<<dumper dumpa>>=
void
dumpa(char *s, Arc *a)
{
    char buf[1024];

    Bprint(&bout, "%sArc@%p: n=%p r=%p flag=0x%x stem='%s'",
        s, a, a->n, a->r, a->remove, a->stem);
    if(a->prog)
        Bprint(&bout, " prog='%s'", a->prog);
    Bprint(&bout, "\n");

    if(a->n){
        snprint(buf, sizeof(buf), "%s    ", (*s == ' ')? s:"");
        dumpn(buf, a->n);
    }
}
@

<<[[nrep()]] if DEBUG(D_GRAPH)>>=
if(DEBUG(D_GRAPH))
    Bprint(&bout, "nreps = %d\n", nreps);
@

\section{The jobs, [[mk -de]]}
% e for execution


<<[[sched()]] if DEBUG(D_EXEC)>>=
if(DEBUG(D_EXEC))
    fprint(STDOUT, "firing up job for target %s\n", wtos(j->t, ' '));
@

<<[[sched()]] if DEBUG(D_EXEC) print recipe>>=
if(DEBUG(D_EXEC))
    fprint(STDOUT, "recipe='%s'\n", j->r->recipe);
Bflush(&bout);
@

<<[[sched()]] if DEBUG(D_EXEC) print pid>>=
if(DEBUG(D_EXEC))
    fprint(STDOUT, "pid for target %s = %d\n", wtos(j->t, ' '), events[slot].pid);
@


<<[[waitup()]] if DEBUG(D_EXEC) print pid>>=
if(DEBUG(D_EXEC))
    fprint(STDOUT, "waitup got pid=%d, status='%s'\n", pid, buf);
@

<<[[waitup()]] if DEBUG(D_EXEC) and slot < 0>>=
 if(DEBUG(D_EXEC))
     fprint(STDERR, "mk: wait returned unexpected process %d\n", pid);
@

<<[[pidslot()]] if DEBUG(D_EXEC)>>=
if(DEBUG(D_EXEC))
    fprint(STDERR, "mk: wait returned unexpected process %d\n", pid);
@

<<[[nproc()]] if DEBUG(D_EXEC)>>=
if(DEBUG(D_EXEC))
    fprint(STDERR, "nprocs = %d\n", nproclimit);
@



% actually never used
<<dumper dumpj>>=
void
dumpj(char *s, Job *j, int all)
{
    Bprint(&bout, "%s\n", s);
    while(j){
        Bprint(&bout, "job@%p: r=%p n=%p stem='%s'\n",
            j, j->r, j->n, j->stem);
        Bprint(&bout, "\ttarget='%s' alltarget='%s' prereq='%s' nprereq='%s'\n",
            wtos(j->t, ' '), wtos(j->at, ' '), wtos(j->p, ' '), wtos(j->np, ' '));
        j = all? j->next : nil;
    }
}
@
%old:  nproc=%d ... , j->nproc

\section{The function calls, [[mk -dt]]}

<<[[applyrules]] debug>>=
if(DEBUG(D_TRACE)) 
    print("applyrules(%lux='%s')\n", target, target);
@

<<[[newnode()]] debug>>=
if(DEBUG(D_TRACE)) 
    print("newnode(%s), time = %d\n", name, node->time);
@


<<[[work()]] debug>>=
if(DEBUG(D_TRACE))
    print("work(%s) flags=0x%x time=%lud\n", node->name, node->flags, node->time);
@

<<[[update()]] debug>>=
if(DEBUG(D_TRACE))
    print("update(): node %s time=%lud flags=0x%x\n", node->name, node->time, node->flags);
@




\chapter{Profiling}
\label{chap:profiling-appendix}

<<global buf>>=
short buf[10000];
@


<<[[main()]] setup profiling>>=
#ifdef	PROF
    {
        extern int etext();
        monitor(main, etext, buf, sizeof buf, 300);
    }
#endif
@


% dead function actually
<<function symstat>>=
void
symstat(void)
{
    Symtab **s, *ss;
    int n;
    int l[1000];

    memset((char *)l, 0, sizeof(l));
    for(s = hash; s < &hash[NHASH]; s++){
        for(ss = *s, n = 0; ss; ss = ss->next)
            n++;
        l[n]++;
    }
    for(n = 0; n < 1000; n++)
        if(l[n]) 
            Bprint(&bout, "%d of length %d\n", l[n], n);
}
@


\chapter{Error Management}
\label{chap:error}

% related is mk -k, but not really error management in the end.

% automatic error handling

% Also call Exit, not exit! so carefully wait for children.

<<function Malloc>>=
void*
Malloc(int n)
{
    void *s;

    s = malloc(n);
    if(!s) {
        fprint(STDERR, "mk: cannot alloc %d bytes\n", n);
        Exit();
    }
    return s;
}
@
%old: used to have 'register void *s', but 8c does not use it I think

<<function Realloc>>=
void *
Realloc(void *s, int n)
{
    if(s)
        s = realloc(s, n);
    else
        s = malloc(n);
    if(!s) {
        fprint(STDERR, "mk: cannot alloc %d bytes\n", n);
        Exit();
    }
    return s;
}
@




\chapter{Utilities}
\label{chap:libc}

%\section{Memory management}
% Malloc and Realloc now in error management section

\section{Buffer management}
\label{sec:bufblock}

<<struct Bufblock>>=
struct Bufblock
{
    char 		*start;
    char 		*end;

    // between start and end
    char 		*current;

    // Extra
    struct Bufblock *next;
};
@

% classic idiom
<<global freelist>>=
static Bufblock *freelist;
@


<<constant QUANTA>>=
#define	QUANTA	4096
@

% ctor
<<constructor newbuf>>=
Bufblock *
newbuf(void)
{
    Bufblock *p;

    if (freelist) {
        p = freelist;
        freelist = freelist->next;
    } else {
        p = (Bufblock *) Malloc(sizeof(Bufblock));
        p->start = Malloc(QUANTA*sizeof(char));
        p->end = p->start+QUANTA;
    }
    p->current = p->start;
    *p->start = '\0';
    p->next = nil;

    return p;
}
@
%old: sizeof(*p->start)

% dtor
<<destructor freebuf>>=
void
freebuf(Bufblock *p)
{
    p->next = freelist;
    freelist = p;
}
@





<<macro isempty>>=
#define isempty(buf) (buf->current == buf->start)
@

<<macro resetbuf>>=
#define resetbuf(buf) do { buf->current = buf->start; } while(0)
@

<<macro bufcontent>>=
#define bufcontent(buf) buf->start
@


<<function insert>>=
void
insert(Bufblock *buf, int c)
{

    if (buf->current >= buf->end)
        growbuf(buf);
    *buf->current++ = c;
}
@

<<function rinsert>>=
void
rinsert(Bufblock *buf, Rune r)
{
    int n;

    n = runelen(r);
    if (buf->current+n > buf->end)
        growbuf(buf);
    runetochar(buf->current, &r);
    buf->current += n;
}
@
% indeed code more complicated when use runes instead of char.
% variable-lenght characters are more complicated than just plusplus

<<function bufcpy>>=
void
bufcpy(Bufblock *buf, char *cp, int n)
{

    while (n--)
        insert(buf, *cp++);
}
@


<<function growbuf>>=
void
growbuf(Bufblock *p)
{
    int n;
    Bufblock *f;
    char *cp;

    n = p->end-p->start+QUANTA;
        /* search the free list for a big buffer */
    for (f = freelist; f; f = f->next) {
        if (f->end-f->start >= n) {
            memcpy(f->start, p->start, p->end-p->start);
            cp = f->start;
            f->start = p->start;
            p->start = cp;
            cp = f->end;
            f->end = p->end;
            p->end = cp;
            f->current = f->start;
            break;
        }
    }
    if (!f) {		/* not found - grow it */
        p->start = Realloc(p->start, n);
        p->end = p->start+n;
    }
    p->current = p->start+n-QUANTA;
}
@








\section{File management}

<<function maketmp>>=
char*
maketmp(void)
{
    static char temp[] = "/tmp/mkargXXXXXX";

    mktemp(temp);
    return temp;
}
@


\chapter{Examples of [[mkfile]]s TODO}
\label{chap:examples}


%How add to an existing variable? e.g. CFLAGS+= -p?
% easy, CFLAGS= -xxx $CFLAGS
%Can do the $(BLA:.cmo=.cmx)? this is convenient
% yes you can: OBJS=${SRC:%.ml=%.cmo}

% there is even a Makefile->mkfile converter in mk/mkconv

\section{The [[mkfile]] of [[mk]]}
% relies on next section

%bootstrap:
% can also bootstrap simply using a shell script for the very first time.
% show the script? (just mk clean; mk > bootstrap.sh), could
% even be a target of the mkfile :)


\section{The [[mkfile]]s of \plan}
% mentioned in mk.ms ?

\subsection{[[/$objtype/mkfile]] for the ARM} %$
\label{sec:mkfile-objtype}

\subsection{[[/sys/src/mkfile.proto]]}


\subsection{[[/sys/src/cmd/mkone]]}
% also mkmany

\subsection{[[/sys/src/cmd/mklib]]}
% also mksyslib


% and then the instances
%\subsection{[[/sys/src/mkfile]]}
%\subsection{[[/sys/src/cmd/mkfile]]}

\chapter{Extra Code}

\ifallcode
#include "Make_extra.nw"
\fi

%\chapter{Changelog}
% code via make loc = 4763 LOC, after full lpized 5285 LOC
% orig Make.nw = 5400 LOC, after full lpized and comments in sections 8540
% now: 13340 LOC so added 5000 LOE (Lines of explanations)
% mk in ocaml: 2300 LOC (but not all features: no archive, no :P, etc)


\chapter*{Glossary}
\addcontentsline{toc}{chapter}{Glossary}
\label{sec:glossary}

\begin{verbatim}
LOC = Lines Of Code
DSL = Domain Specific Language
IDE = Integrated Developement Environment
DAG = Directed Acyclic Graph
DFS = Depth-First Search
\end{verbatim}

\chapter*{Indexes}
\addcontentsline{toc}{chapter}{Index}

\nowebindex

%\chapter{References} 
\addcontentsline{toc}{chapter}{References}

\bibliography{../docs/latex/Principia}
\bibliographystyle{alpha}

%******************************************************************************
% Postlude
%******************************************************************************

\end{document}
