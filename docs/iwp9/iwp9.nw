\documentclass{article}
%CONFIG:
%\usepackage{fullpage}

\input{../latex/Packages}
\usepackage{caption}
\captionsetup{
  justification=centering,
  singlelinecheck=false
}

%alt: \input{../latex/Noweb}
% but pb with relative path so inlined here
\usepackage{../latex/noweb}
 \noweboptions{footnotesizecode,nomargintag}
 \def\nwendcode{\endtrivlist \endgroup}
 \let\nwdocspar=\par
\usepackage{../latex/syncweb}

%alt: \input{../latex/Config}
% but we actually dont want the width setting of Config so inlined
% here just is requires by Macros below
\newif\iffinal
\newif\ifverbose
\newif\ifallcode

\input{../latex/Macros}
%alt: \newcommand{\repourl}[1]{Also available at \url{#1}}
\newcommand{\repourl}[1]{}

\title{
Principia Softwarica: \plan Code Explained
}
\author{
Yoann Padioleau\\
yoann.padioleau@gmail.com
}

\begin{document}
\date{}
\maketitle

\begin{abstract}
%dup: Principia.nw (Introduction)
{\em \principia} is a series of books explaining how things work in a
computer by describing with full details all the source code of all
the essential \plan programs used by a programmer: 
the kernel, the shell, the assembler, the linker, the C compiler, the editor,
the windowing system, the build system, etc.
Each program is covered by a separate book and explained using
the {\em literate programming} technique.
%
\plan thus becomes a teaching operating system for students to learn deeply
about programming.

\principia is also another fork of \plan available at
\url{https://github.com/aryx/principia-softwarica}
%as well as a new distribution
that can be easily cloned, cross-compiled, modified, and tested
from Linux, macOS, as well as Windows, which is convenient for students.
%alt:removes friction for students.
%said-later:
% QEMU 386 and Raspberry Pi arm
% but can cross compile from amd64 or arm64 machines as hosts

\end{abstract}

\section{Introduction}

\plan didn't have the success it deserved. 
It was
innovative,
elegant,
powerful,
small,
and designed by great programmers.
%
Those great minds didn't just rethink the kernel, they rethinked
the whole operating system (OS) in a holistic manner with 
a kernel pushing the ``everything is a file'' \unix motto to
its limit~\cite{namespaces-plan9},
an integrated graphic and network stack,
a windowing system that could even
run under itself~\cite{concurrent-window-system},
a compact cross-compiling toolchain 
(with assemblers, linkers, C compilers, and debuggers
%and profilers
for most architectures),
minimalist C libraries,
a simpler build system and
shell,
and more.
% just missed browser :( and being OSS from day one before 1991, and VCS
%% sad it failed; Rob Pike then sad wrote "software research is irrelevant"
%could \cite mk, rc, toolchain paper, but too many entries then


What is even more impressive is that those programs
contain only in the order of thousands lines of code (LOC).
%
[[rio]] for example, the \plan windowing system~\cite{rio-slides},
contains just 8 800 LOC and is arguably more poweful than
Xorg (an implementation of X~Window~\cite{x-window})
which contains millions LOC.
%
The X~Window program [[xterm]], which is just a terminal, not the
windowing system, contains already 80 000 LOC while [[rio]] contains
a built-in terminal in less than 1000 LOC.
% that's a 80x reduction! possible thx to namespace, /dev/cons, etc.
%
%coupling: Table 1 Total entry
In fact, with only 183 000 LOC, the \plan authors implemented from scratch all
the major components of an OS. 


However, the Linux/GNU/Xorg OS won over \plan, partly because it
was the first open-source (OSS) system available.
% Arguably not as innovative, not as elegant, not as simple, but it won.
% Plan9 was Open source too, but a bit late.
%%History repeat itself ... 
%%Unix v6 was closed by AT&T Freebsd in prison for a long time.
%% Plan9 was closed, then inferno, then open source but late, and
%% really open sourced far later by plan9 foundation.
% Linux/GNU/Xfree86 nice to have robust open source Unix still
Unfortuntately, even if the source code is open, it is very hard to understand
most of this code because each component (e.g., the Linux kernel,
the [[bash]] shell, the GNU C library, [[gcc]])
%said-later: binutils, gdb
contains multiple orders of magnitude more code than \plan with millions LOC.
% Compare LOC rc vs bash. actually bash not millions but 180 000 LOC
%see-later: TABLE
This situation is sad, and potentially dangerous, because most
programmers rely on a giant software stack that very few people (if any)
fully understand.
The situation is even worse for students who don't have any path
to understand deeply the software stack.

Enter {\em \principia}, a new project to repurpose \plan
from an old research OS to a new teaching OS
(OS in a general sense: not just the kernel but the whole
software stack).
%Understand essence, useful for many things.
%
%dup: Principia.nw (intro)
Concretely, \principia is a series of books explaining how things work in a
{computer} by describing with full details all the source code of all
the essential \plan programs used by a programmer.
Each program will be covered by a separate book. 

%dup: Principia.nw (intro)
The books not only describe the implementations of those programs,
{\em they are} the implementations of those programs. Indeed,
each book in \principia comes from a 
{\em literate program}~\cite{lp-book}, 
which is a document containing both source code and documentation
and where the code is organized and presented in a way 
to facilitate its comprehension.
The actual code and the book are derived both automatically from this literate
program.
%said-later: said soon below "as explained later in Section Y"

The rest of the article is organized as follows.
First, I present the full list of \principia books in Section~\ref{sec:bookset}
and give a brief overview of literate programming in Section~\ref{sec:lp}.
Then, in Section~\ref{sec:new-fork} I explain the motivation for a new \plan fork
and describes its content and key features
while Section~\ref{sec:new-distribution}
introduces the associated new \plan distribution.
Finally, I discuss future work in Section~\ref{sec:future-work}
and related work in Section~\ref{sec:related-work}.

\section{The bookset}
\label{sec:bookset}

%dup: Principia.nw intro
Similar to {\em Principia Mathematica}~\cite{principia}, 
which is a series of books
covering the foundations of mathematics, the goal of \principia
is to cover the fundamental programs.
%
Those programs are mostly all {\em meta programs}, which are
programs in which the input and/or output are other programs.
For instance, the kernel is a program that {manages} other programs;
the compiler is a program that {generates} other programs.
%
Those programs are also sometimes referred as {\em system software},
in opposition to {\em application software} (e.g., spreadsheets,
word processors, email clients), which is not covered by \principia.

Table~\ref{tab:books} presents the full list of books in \principia
and the corresponding \plan programs or libraries they document.

\begin{table}[htbp]
\centering
\begin{tabular}{p{2.2cm} l p{2.5cm} r r r}
\hline
\textbf{Category} & \textbf{Book} & \textbf{Program(s)} & \textbf{LOC} & \textbf{LOE} & \textbf{Pages} \\
\hline

\multirow{3}{=}{\\Core system}
  & Kernel
  & \texttt{9pi}
  & 35000 & 3200 & 732 \\

  & Core libraries
  & \texttt{libc libregexp libthread}
  & 19000 & 1600 & 438 \\
%TODO libbio? (buffering is important concept)
%TODO? add libflate? or add in Utilities.nw next to gzip?
%less: libstring? (seems mostly unused)
%less: libmp?

  & Shell
  & \texttt{rc}
  & 6500 & 1700 & 166 \\
\hline

\multirow{3}{=}{Development\\ toolchain}
  & C compiler
  & \texttt{5c libcc}
% there is no libcc really, but just cc
  & 18500 & 1900 & 471 \\

  & Assembler
  & \texttt{5a}
  & 3600 & 4400 & 176 \\

  & Linker
  & \texttt{5l}
  & 7500 & 5400 & 296 \\
%skipped: 5i
%skipped: ocamlc, ocamlopt
%skipped: olex, oyacc
\hline

\multirow{4}{=}{\\Developer\\tools}
  & Editor
  & \texttt{ed}
  & 1600 & 200 & 45 \\
%skipped: editor efuns

  & Build system
  & \texttt{mk}
  & 4350 & 4050 & 197 \\

  & Debuggers
  & \texttt{db acid strace libmach}
  & 13100 & 1000 & 321 \\

  & Profilers
  & \texttt{prof time kprof stats iostats}
  & 3900 & 350 & 102 \\
%TODO? maybe skip profilers?
%skipped: ogit
%skipped: troff
\hline

\multirow{2}{=}{\\Graphics}
  & Graphics stack
  & \texttt{/dev/draw libdraw libmemdraw libmemlayer}
  & 18500 & 3400 & 507 \\
% 10k+ (kernel) + 16k
% also kernel/devices/screen/ and some windows/apps/
% add libimg?

  & Windowing system
  & \texttt{rio libframe libcomplete libplumb}
  & 8800 & 4000 & 289 \\
%skipped: libpanel GUI widget toolkit
\hline

\multirow{1}{=}{Networking}
  & Network stack
  & \texttt{/dev/net libip lib9p}
  & 18300 & 4800 & 457 \\
% 23k+ (kernel) + 51k
% skipped Network_apps.nw which is additional 60000 LOC, hmm
%skipped: mmm
\hline

\multirow{1}{=}{Misc}
  & CLI utilities
  & \texttt{cat ls grep sed diff tar gzip \ldots}
  & 23900 & 650 & 493 \\
% also bc, hoc, awk, etc

\hline
\textbf{Total: } & & & 182550 & 36650 & 4690 \\
%origin: the total was computed by chatGPT by copy pasting table
% and asking to compute total
\hline

\end{tabular}
\caption{%
\principia Books and Their Statistics.\\
{\footnotesize
(\textbf{LOC} = lines of code;
 \textbf{LOE} = lines of explanation;
 \textbf{Pages} = number of typeset pages)
}
}
\label{tab:books}
\end{table}

When a program involves multiple architectures, such as the C compiler
or assembler, I chose to present only the ARM~\cite{arm-refman} variant
of the program, here respectively [[5c]] and [[5a]].
%
The LOC column in Table~\ref{tab:books} accounts for the generic
part of the code that is architecture independent and the ARM-specific
code, but not the code for the other architectures (e.g., x86).


I chose the ARM architecture because it is one of the
simplest architecture while being also one of the most
used architecture in the world. Indeed, almost every phone
% mobile phone
contains an ARM processor.
% simpler than 86, and goal is understand essence, RISC vs MIPS
% not useless detail. But still real arch! not toy arch!
%
It is also the processor of the extremely cheap Raspberry 
Pi\furl{https://www.raspberrypi.org/}, a machine
used by many electronic hobbyists. Thus,
ARM is a great candidate for my teaching purpose.
%
In fact, the kernel book will describe [[9pi]]\furl{https://9p.io/sources/contrib/miller/9pi.img.gz},
the port by Richard Miller of the \plan kernel to the Raspberry Pi.
% not in original plan9. fantastic work.

%said-later: too in Future work
As you can see from the LOE column in Table~\ref{tab:books}, 
which accounts for the number of lines of explanations,
\l explain more LOE? but harder here that didn't see yet .nw content
\principia is still a work in progress and many books have a low
number of LOE.
% I started in 2014 but was on pause between 2019 and 2024,
% and resume almost full-time on it mid 2025.
A few books are almost complete such as the assembler, the linker, the
windowing system, and the build system, which all have a LOE/LOC ratio
close to 1 so this should probably be the target for the other books
as well.

%10 000 LOC is explainable in a book of a reasonable size!
%bash for example? vs rc ? gcc vs 5c? 9pi vs Linux?
%millions LOC would require 400 books of 300 pages, just for bash :)

%dup: Principia.nw
The \principia programs form together the minimal foundation
on top of which all applications can be built. Even though there are
many books in the series, I still think it is the
minimal foundation. Indeed, it is hard to remove any of those programs
because they depend on each other.
%
First, you need to rely on a kernel (hence the name), but
the shell and the C library are also essential. 
However, because those programs
are coded in C and assembly, you also need a C compiler and
an assembler, and because source code is usually split in many
files you also need a linker.
%The C compiler itself usually uses DSLs like Lex and Yacc.
%\l which are themselves written in C and actually use also lex and yacc themselves
To write all this code in the first place you need an editor.
Then, with so many source files you need
a build system to automate and optimize the compilation process.
Because the programs I just mentioned inevitably have bugs
or non optimal parts, you will need a debugger and a profiler.
\l maybe skip profiler
Finally, nowadays
it is inconceivable to not use a graphical user interface
and to not work with multiple windows opened at the same time.
In the same way it is also not conceivable to work
in isolation; programmers collaborate with each other,
especially via the Web.
This means you need a graphical and networking stack as well
as a windowing system.
% and a Web browser.
As I said earlier, it is hard to remove any of the programs
from the series.


\section{Literate programming crash course}
\label{sec:lp}

%dup: Principia.nw
Source code is the ultimate explanation for what a program does.
%
However, showing pages and pages of listings in an appendix, 
as done for instance in the Minix book~\cite{minix},
even when this appendix is preceded by documentation chapters, is arguably not
the best way to explain code. The code and its documentation
should be mixed together, as done for instance in the Xinu book~\cite{xinu},
so one does not have to switch back and forth between
an appendix and multiple chapters.


%dup: Principia.nw
{\em Literate programming}~\cite{lp-book} is a technique invented by
Donald Knuth to make it easy to mix code and documentation
in a document in order to better explain programs (and arguably also
to better develop programs). 
Such documents are called {\em literate programs}. All \principia
programs are literate programs.

Table~\ref{tab:foo.nw} presents a toy literate program on the
left and its rendered \LaTeX{} output on the right to illustrate the
main features of Noweb~\cite{noweb-ieee}\footnote{
Noweb is available at \url{http://www.cs.tufts.edu/~nr/noweb/}.
The No-web name comes from Knuth's tool [[cweb]]
which processed ``a {\bf web} of C chunks'',
and because Noweb is
{\bf no}t language specific like [[cweb]] but can work for many
programming languages, hence the no part.
},
the literate programming tool used for \principia.
%said-later: really syncweb now
The \verb+<<...>>=+ syntax allows to define a {\em chunk} 
that can be referenced before or after its definition in other
chunks using the \verb+<<...>>+ syntax.
%
Noweb is similar to macroprocessing languages such as
[[cpp]] or [[m4]]; chunks are the equivalent of macro constants.
However, with Noweb one can also define a macro in multiple parts,
for example [[initializations]] in Table~\ref{tab:foo.nw}.
%
Outside code chunks, one can write explanations using \LaTeX{} commands
or regular text.

\begin{table}[htbp]
\centering
%old: 0.48\textwidth
\begin{tabular}{|p{7cm}|p{7cm}|}
\hline
[[ToyKernel.nw]] excerpt & \textbf{\LaTeX~output} \\
\hline
\begin{minipage}[t]{\linewidth}
\VerbatimInput{ToyKernel.nw}
\end{minipage}
&
\begin{minipage}[t]{\linewidth}
#include "ToyKernel.nw"
\end{minipage}
\\
\hline
\end{tabular}
\caption{Noweb Source Example and Rendered Output.}
\label{tab:foo.nw}
\end{table}
% not shown, [ < foo ()> ] syncweb refs, but let's keep it simpler

Note that literate programming is different from using 
{API documentation generators} such as 
javadoc\furl{http://www.oracle.com/technetwork/articles/java/index-jsp-135444.html} or 
doxygen\furl{http://www.stack.nl/~dimitri/doxygen/}.
%
Noweb does not provide the same kind of services. 
%
Indeed, literate programming allows programmers to explain their
code in the order they think the flow of their thoughts and their code would
be best understood, rather than the order imposed by the compiler.

Literate programming
allows, among other things, to explain the code piece by piece, with the
possibility to present a high-level view first of the code,
to switch between {top-down} and {bottom-up} explanations, and to
separate {concerns}.
%
For instance, the [[Proc]] data structure of the \plan kernel, which
represents some information about a process, is a huge structure
with more than 90 fields. Many of those fields are used only
for advanced features of the kernel. The C compiler imposes
to define this structure in one place, however
Noweb allows to present this structure piece by piece, gradually, in different
chapters, as illustrated in Table~\ref{tab:foo.nw} although on
extremely simplified code.
%
One can show first the code of the structure with 
the most important fields,
and delay the exposition of other fields to advanced chapters.
This greatly facilitates the understanding of the code, by not
submerging the reader with too much details first.

In the same way, the [[main()]] function in most
programs is rather large and mixes together many concerns:
command line processing,
error management,
debugging output,
optimizations,
and usually a call to the main algorithm.
%
Showing in one listing the whole function would hide behind
noise this call to the main algorithm. The main flow of the program though
is arguably the most important thing to understand first.
Using literate programming,
one can show code where the most important parts are highlighted, and where
other concerns are hidden and presented later,
as illustrated again in Table~\ref{tab:foo.nw} on a simplified case.

In fact, I spent lots of time during the writing of the \principia books
in transforming the \plan programs in literate
programs, and in reorganizing again and again the \plan code to find
the best way, the best order, the best separation of concerns
to make it easier for the reader to understand the code.

Finally, here are the Noweb commands\footnote{
The [[notangle]] name comes again from Knuth's literate programming
terminology in which one ``tangles'' or ``weaves'' a web of chunks.
}
illustrating how to automatically generate the code
from the literate program:

\begin{verbatim}
$ notangle -Rproc.h ToyKernel.nw > proc.h
$ notangle -Rkernel.c ToyKernel.nw > kernel.c
$ cat proc.h
struct Proc {
  int pid;
  Lock l;
  int cnt;
} 
\end{verbatim}
%said-later: actually syncweb


\section{Yet another \plan fork}
\label{sec:new-fork}
%alt: actually forks, there is also the one for goken

To create the [[.nw]] Noweb literate programs out of the original
4th edition \plan code,
I first had to copy and split this code in separate functions
and data structures.
%said-later? used tool, lpizer to assist, but no time/space to explain
Then, I distributed those functions and data structures
in different chapters and sections and further split big functions
and data structures in smaller pieces again and again until the code
was simple enough that it could be more easily explained.
%
As I was trying to understand the code (in order to explain it in the books),
I sometimes
renamed unclear entities, 
removed what appeared to be dead code,
% dead functions, dead constants, dead fields, dead data structures
or even sometimes fixed bugs\footnote{See \url{http://9legacy.org/9legacy/patch/9-newseg-mapsize.diff} for an example of bugfix as well as other
entries with my name at \url{http://9legacy.org/patch.html}
} while I was trying hard to explain something that could not be explained
(because it was actually buggy).
% easier to fix and simplify that keep old code that require complicated
% actually lots of soft-eng cleaning (see modern practice section)
%
I ended up also renaming many C files from \plan and introduced
additional C files and header files to better separate concerns.
%for instance, to move 
%the globals of a program in a separate [[globals.c]] module and tried to reduce
%the number of globals needed.

The result of all this work is available on Github in a new \plan fork
at: \url{https://github.com/aryx/principia-softwarica}.
%Yet another fork. 
%said-later: git is good
%
I also reorganized the original \plan hierarchy to make the
source of the programs more easily discoverable; it is
arguably easier (and faster) to find the code for the assembler in
[[assemblers/5a/]] than in [[sys/src/cmd/5a]].
%
Figure~\ref{fig:repo-layout} presents the toplevel layout of the
\principia repository.
%
Note that this fork is a subset of \plan with only the code
derived from the \principia literate programs (no applications)
and with only the code to support the ARM and x86 architectures.


The repository contains not only the
literate program files (e.g., [[Assembler.nw]]) but
also the generated code (e.g., [[assemblers/5a/]]) as shown in
Figure~\ref{fig:repo-layout}, which seems redundant.
%
This helps though to remove the need for the student to install Noweb
to see (or compile) the generated code.


In fact, the code is not generated by Noweb as said previously but by
a separate tool I made called
[[syncweb]]\furl{https://github.com/aryx/syncweb} that introduces
special start and end {\em mark} comments in the generated code and remember
the [[md5sum]] of chunk content in a separate auxillary file as shown
by the commands below:

\VerbatimInput{syncweb_command.trace}
%old: content was inlined here, but then needed to cheat because
% the [[Proc]] was interpreted by syncweb -to_tex so I was using [Proc] instead

Thanks to those marks and checksums, 
one can modify the generated code or the literate program
and propagate (``synchronize'') automatically the modifications to
respectively the literate program or the code.
%
The checksum acts as a ``proxy'' to date a chunk.
%
Indeed, if one modifies one chunk in a file [[proc.h]], and another
chunk in the literate program [[ToyKernel.nw]], looking just at the
dates of those two files it is impossible to know which file has
the latest content (actually both files have parts of the latest content).
%
With the chunk checksums saved in the auxilary file, [[syncweb]]
can recompute the [[md5sum]]s of the chunks in the modified
files ([[proc.h]] and [[ToyKernel.nw]]) and detect for each chunk
separately which chunk was modified last and from which file,
and automatically merge the changes.
%
However, if one modifies the same chunk in both the generated code and
the literate program, [[syncweb]] can instead detect the conflict
and prompt the user to manually merge the changes.
% similar to modify a db view and get modif propagated to original table


The ability to modify either the code or the
literate program is a huge help during development and [[syncweb]]
solved one of the biggest complain people had about the use
of literate programming in a project.
\l cite? or it's just me?
\l also syncweb nice automatic crossref index, nice -lpizer


\begin{figure}
\centering
{\footnotesize
\begin{verbatim}
.
|-- assemblers/            |-- kernel/                  |-- mkfiles/
|   |-- 5a/                |   |-- arch/                |   |-- 386/
|   |-- 8a/                |   |-- concurrency/         |   |-- arm/
|   |-- data2s.c           |   |-- console/             |   |-- mkdirs
|   |-- Assembler.nw       |   |-- devices/             |   |-- mkfile.proto
|   `-- mkfile             |   |-- files/               |   |-- mklib
|-- builders/              |   |-- filesystems/         |   `-- mkone
|   |-- Builder.nw         |   |-- init/                |-- mkfile-target-pc
|   |-- mk/                |   |-- interrupts/          |-- mkfile-target-pi
|   `-- mkfile             |   |-- Kernel.nw            |-- networking/
|-- compilers/             |   |-- mkfile               |   |-- arp/
|   |-- 5c/                |   |-- network/             |   |-- dhcp/
|   |-- 8c/                |   |-- processes/           |   |-- ftp/
|   |-- cc/                |   `-- syscalls/            |   |-- http/
|   |-- Compiler.nw        |-- lib_core/                |   |-- ip/
|   |-- cpp/               |   |-- libbio/              |   |-- mkfile
|   `-- mkfile             |   |-- libc/                |   |-- ndb/
|-- debuggers/             |   |-- Libcore.nw           |   |-- Network.nw
|   |-- acid/              |   |-- libthread/           |   |-- snoopy/
|   |-- db/                |   `-- mkfile               |   `-- telnet/
|   |-- Debugger.nw        |-- lib_graphics/            |-- profilers/
|   |-- libmach/           |   |-- Graphics.nw          |   |-- iostats/
|   `-- mkfile             |   |-- libdraw/             |   |-- mkfile
|-- Dockerfile             |   |-- libimg/              |   `-- Profiler.nw
|-- docs/                  |   |-- libmemdraw/          |-- shells/         
|   |-- articles/          |   |-- libmemlayer/         |   |-- mkfile      
|   `-- iwp9/              |   `-- mkfile               |   |-- rc/         
|-- dosdisk.img            |-- lib_networking/          |   `-- Shell.nw    
|-- editors/               |   |-- lib9p/               |-- utilities/      
|   |-- ed/                |   |-- libip/               |   |-- archive/    
|   `-- mkfile             |   `-- mkfile               |   |-- byte/       
|-- include/               |-- lib_strings/             |   |-- calc/       
|   |-- libc.h             |   |-- libflate/            |   |-- compare/    
|   |-- ...                |   |-- libregexp/           |   |-- files/      
|   `-- u.h                |   |-- libstring/           |   |-- mkfile      
|-- linkers/               |   `-- mkfile               |   |-- pipe/       
|   |-- 5l/                |-- Makefile                 |   |-- process/    
|   |-- 8l/                |-- mkconfig.pc              |   |-- text/       
|   |-- Linker.nw          |-- mkconfig.pi              |   |-- time/       
|   `-- mkfile             |-- mkfile                   |   `-- Utilities.nw
|-- readme.txt             |-- mkfile-host-Cygwin       `-- ROOT/       
`-- windows/               |-- mkfile-host-Linux            |-- 386/    
    |-- libcomplete/       |-- mkfile-host-macOS            |-- arm/    
    |-- libplumb/          `-- mkfile-host-Plan9            |-- lib/    
    |-- mkfile                                              |-- rc/     
    |-- rio/                                                |-- tests/  
    `-- Windows.nw                                          `-- usr/    
\end{verbatim}
%cheat: a few dirs have been removed to simplify things
%origin: the content above was generated by chatGPT using some copy-pasted
% output from tree -L 2 and asking to re-layout in 3 columns
%miss Editor.nw, but because efuns actually, and in ed.nw is in docs/principia/
}
\caption{%
\url{https://github.com/aryx/principia-softwarica} Layout.\\
(as generated by [[tree -L 2 -F]] and spread in 3 columns)
}
\label{fig:repo-layout}
\end{figure}

%   Diff with 9front, plan9port, etc. is focus on code explanations!


\section{A new \plan distribution}
\label{sec:new-distribution}

The original \plan\furl{https://9p.io/plan9/download.html},
as well as forks such
as [[9legacy]]\furl{http://9legacy.org/download.html}
or [[9front]]\furl{https://9front.org/releases/},
can easily be tested thanks
to downloadable ISO CD images for PCs
or bootable SD card images for Raspberry Pi.
%
You can even run \plan virtualized
via QEMU\furl{https://www.qemu.org/} from Linux, macOS, and even Windows.
% cite qemu too at usenix original paper?
%
% What is the problem with current situation and classic plan9 distros?
But what if a student wants to modify the underlying \plan code?
How to rebuild and make an ISO CD image or SD card image from
this modified code?
How to compile the modified code from Linux, macOS, or Windows?
% can do from plan9 but not trivial from mainstream OSes.
% Got trouble 10 years ago to test, with ISO CDs, qemu pbs. slow down adoption.
% Now qemu friendly, and raspberry pi friendly! but again not easy to test
% currently. Fragmented.

The teaching context of \principia imposed to create a new way
to build and distribute \plan so that a student can easily modify
the code, rebuild, and play with it, from whatever mainstream OS.
%
It is not without challenge though.
%
Indeed, building \plan from another OS is not trivial because \plan
is written in a dialect of C with extensions supported only by the
\plan C compiler; one can not use [[gcc]] or [[clang]] to compile it.
%
In the same way, \plan contains also assembly code using a syntax that
is only supported by the \plan assembler.


Fortunately, thanks to porting work done for
Inferno\furl{https://github.com/inferno-os/inferno-os/tree/master/utils},
% cite inferno? add to Principia.bib
as well as work in
plan9port\furl{https://9fans.github.io/plan9port/},
it is possible to {\em compile} first some slightly modified
versions of the \plan C compiler and assembler (where the use
of special C extensions have been removed) using [[gcc]] or [[clang]]
% gcc-binutils and clang-llvm
from Linux, macOS, or Windows. Then, one can use those compiled
\plan compiler and assembler to {\em cross-compile} \plan itself.
%
For \principia I use my own fork (again) of code derived from the
Inferno toolchain in a new project called \goken\furl{https://github.com/aryx/goken9cc}
(I submitted a separate work-in-progress paper to this workshop
describing the motivations and features of \goken), which
contains the code of \plan compilers and assemblers that can
build on Linux, macOS, or Windows, and either on amd64 or arm64
machines (most modern machines fit the requirements).
% why not plan9port? can also build on Linux/macOS/... but no compiler in it!
% why not inferno-os kencc? well yes, that's what I did, but needed an update.

Once all the \plan binaries, including the kernel, have been cross-compiled
(from Linux, macOS, or Windows), it remains to {\em package} them
together in a form that can be booted and run.
%
With \principia, one can build a disk image using the VFAT (DOS)
filesystem (see the [[dosdisk.img]] file in Figure~\ref{fig:repo-layout}),
and either use QEMU to run it, or copy the image on a SD card to run
on the Raspberry Pi (hence the [[mkconfig.pc]] and [[mkconfig.pi]]
in Figure~\ref{fig:repo-layout}).

The motivation for choosing VFAT is that it is a filesystem
supported by all the mainstream OSes, making it easy
for students to explore the generated artifact.
%
This requires to build a kernel with the [[dossrv]] program embedded
in the kernel binary.


The whole build and packaging process for both \goken and \principia
can be succintely specified by the two Docker\furl{https://www.docker.com/}
self-explanatory
configuration files in Table~\ref{tab:dockerfiles}.
%(see one of those two [[Dockerfile]] in Figure~\ref{fig:repo-layout}) 
%
Thanks to those Docker files, the following
commands are then the only commands needed to build from scratch everything
and run \plan with QEMU:


\begin{table}[htbp]
\centering
%old: 0.48\textwidth
\begin{tabular}{|p{7cm}|p{7cm}|}
\hline
Goken [[Dockerfile]] excerpt & \principia~[[Dockerfile]] \\
\hline
\begin{minipage}[t]{\linewidth}
\begin{verbatim}
# Build goken on Ubuntu using
# gcc/binutils (and mk/rc)
FROM ubuntu:24.04

# Setup a basic C dev environment
RUN apt-get install -y gcc libc6-dev \
     byacc

# Now let's build from source
WORKDIR /src
COPY . .

# Small shell script (not GNU autoconf)
# to detect arch and generate
# ./mkconfig
RUN ./configure

# The script below obviously builds
# 'mk' (without needing mk) but also:
# - 'rc', which is called by 'mk'
# - 'ed', for the mkenam script
#    run during the build
RUN ./scripts/build-mk.sh

# copy ./ROOT/<arch>/bin/{mk,rc,ed}
# to ./bin/
RUN ./scripts/promote-mk.sh

# make mk and rc accessible
ENV PATH="/src/bin:${PATH}"
ENV MKSHELL="/src/bin/rc"

# Let's build goken (using mk/rc built
# in the previous step)
RUN mk
RUN mk install

ENV PATH="/src/ROOT/amd64/bin:\
          /src/ROOT/arm64/bin:\
          ${PATH}"
\end{verbatim}
%cheat: a few lines have been removed such as apt-get update, GOOS, NPROC, etc.
% and some flags such as --no-install-recommends removed
%note: works for both amd64 and arm64
\end{minipage}
&
\begin{minipage}[t]{\linewidth}
\begin{verbatim}
# Build principia on Ubuntu
# Linux (amd64 or arm64)
# for 386 (pc) and arm (pi)
FROM padator/goken

WORKDIR /principia
COPY . .

# 386
RUN cp mkconfig.pc mkconfig
RUN mk && mk install 
RUN mk kernel
# arm
RUN cp mkconfig.pi mkconfig
RUN mk && mk install
RUN mk kernel

# VFAT tools
RUN apt-get install -y dosfstools \
      mtools

# making dosdisk.img
RUN dd if=/dev/zero of=tmp.img \
        bs=1M count=512
RUN mkfs.vfat tmp.img
RUN mcopy -i tmp.img -s -o \
      ROOT/* ::
RUN cat MISC/pc/bootsector tmp.img \
     > dosdisk.img
RUN rm -f tmp.img
\end{verbatim}
%cheat: it is mk disk but inlined to simplify
\end{minipage}
\\
\hline
\end{tabular}
\caption{\principia Build and Packaging Process.}
\label{tab:dockerfiles}
\end{table}
%TODO: build singe disk image that can work both for PC and PI!

\begin{verbatim}
# --- build goken ---
$ git clone https://github.com/aryx/goken9cc
$ cd goken9cc
$ docker build -t "padator/goken" -f Dockerfile .
$ docker push "padator/goken" # requires credentials via docker login
$ cd ..

# --- build principia ---
$ git clone https://github.com/aryx/principia-softwarica
$ cd principia-softwarica
$ docker build -t "principia" -f Dockerfile .

# --- extract 9qemu and dosdisk.img and run ---
$ docker run -u $(id -u):$(id -g) --rm -v "$PWD:/out" principia \
   sh -c "cp kernel/COMPILE/9/pc/9qemu /out && cp dosdisk.img /out"
$ qemu-system-i386 -smp 4 -m 512 -kernel ./9qemu -hda ./dosdisk.img
\end{verbatim}
%cheat: removed arm64/amd64 thing

It is an incredible feeling to modify 
in [[lib_core/libc/9syscall/sys.h]]
one of the \plan system call, for instance
\verb+#define OPEN 6+ to 42, and recompile {\em everything} from
scratch using some of the commands above, and get the compilation done in
less than 10 seconds on a modern machine.
%
I have no idea which files in the Linux kernel or GNU C library
I would have to modify to have the same effect, and how much
time I would need to wait to recompile the Linux/GNU/Xorg
OS from scratch.
% let alone how to repackage something that can be booted

Note that it is not necessary to use a Docker container
to build \principia; one can also compile Principia on
``native'' Linux, macOS, or Windows. However, Docker files are
a convenient and portable way to specify the commands to run
in a fresh environment to build a project.
%
Moreover, the Github continuous integration (CI) system
Github Actions\furl{https://github.com/features/actions}
can also run Docker containers; this is used in the \goken
and \principia repositories to automatically check for build
and test regressions after each commit.
% talk about docker, say actually would be trivial
% to implement docker-like in plan9 thx to namespace, more elegant
% but what we have in the cloud, in github, infra is Linux
% and macOS and Windows, and they all can talk docker and it's
% quite convenient to have reproducible builds and tests!

%said-later:
% Not trivial, 64bits new arch, long vs int, but separate paper.
% Share code with principia but
% separate package because as its own use.


%\section{Modern practice}
%alt: Quality of life improvements:
%alt: A modern distribution

% Modern does not always mean better, and very often old plan9
% was better than modern tools, still there are a few things
% that are convenient nowadays, even if arguably not most elegant:

% git and github! easy collaboration (9front too).
%said-before:
% docker build infra.
%said-before:
% GHA (just run docker build)

% C ref_xxx, error1, abuse int/string.
% less globals, better split files, less mutual deps (thx codegraph)

\section{Future work}
\label{sec:future-work}

%dup: bookset section
As I mentioned earlier in Section~\ref{sec:bookset}, an important
future work is to add lines of explanations to many of the
books in Table~\ref{tab:books} to reach a LOE/LOC ratio close to 1.


Another future work is to make \principia{} {self-hosted}; right now
one can build Principia from the mainstream OSes (which is very useful
in a teaching context) but not from the produced and booted VFAT image itself.


Finally, it will be cleaner to merge some of the code in 
the \principia and \goken repositories. The code in the
two projects was forked from different \plan derivatives, respectively
\plan 4th edition and Kencc\furl{https://code.google.com/p/ken-cc/} 
(which itself
derived from the Inferno and \plan toolchain), because the
motivations and needs for the two projects were different.
However, the code could probably now be merged back together.


\section{Related work}
\label{sec:related-work}

%dup: Principia.nw
There are already lots of books explaining how computers work, explaining
the concepts, theories, and algorithms behind programs such as
kernels or compilers. There are also a few books about debuggers. However,
all those books rarely explain everything with full details, which
is what source code is all about. 
%
There are a few books that
include the whole source code of the program
described, for instance, the books about 
Minix~\cite{minix}, XINU~\cite{xinu}, or LCC~\cite{lcc}.
%said-later: Oberon, Software Tools, with LP: Tex the program, mmixware, 
However, those books cover only
a few essential programs, and mostly always either
the kernel or the compiler. Moreover, they do not form a coherent set
like in \principia.


Here are a few teaching operating systems that I originally considered
possible candidates for \principia, but which I ultimately discarded:
\n helps to appreciate even more how good our plan9 choice is


%dup: Principia.nw
\begin{itemize}

%history:
\item \unix 
V6\furl{http://minnie.tuhs.org/cgi-bin/utree.pl}
(Ken Thompson et al.),
\n tuhs = the unix heritage society
fully commented in the infamous book by John Lions~\cite{lions},
or its modern incarnation
xv6\furl{http://pdos.csail.mit.edu/6.828/2014/xv6.html},
are great resources to fully understand a \unix kernel.
However, this kernel is too simple; there is no support for graphics
or networking for instance.
\l it's a full OS, see unix-history-repo, 165 000 LOC for V7 in /sys/src

\item XINU\furl{http://minnie.tuhs.org/cgi-bin/utree.pl?file=Xinu7}
(Douglas Comer),
fully documented in two books~\cite{xinu,xinu2},
has a network stack, but the kernel
is still too simple with no virtual memory for instance.
\l and has multi processor support?
\t new edition in 2015! seems to have virtual memory now

\item Minix\furl{http://minnie.tuhs.org/cgi-bin/utree.pl?file=Minix1.1}
(Andrew Tannenbaum et al.),
also fully documented~\cite{minix},
is fairly small, but it is just a kernel. Minix does not provide
for instance its own windowing system; it relies instead on X Window,
which is far more complicated than the \plan windowing system.
\l it has a compiler though, ack, but was open sourced only in 2003

\item Hack\furl{http://www.nand2tetris.org/}
(Noam Nisan and Shimon Shocken)
is a toy computer introduced in the excellent
book {\em The Elements of Computing Systems}~\cite{tecs}.
This book is great for understanding processors, assemblers,
and even compilers, but the kernel part is really too simple.
\n and actually no code, only code is the different emulators and debuggers

%\item MMIX\furl{http://www-cs-faculty.stanford.edu/~uno/mmix-news.html}
% (Donald Knuth)
%and its ancestor MIX
%are computers designed by Donald Knuth
%and used in his classic book series 
%{\em The Art of Computer Programming}~\cite{taocp}.
%Donald Knuth also wrote a book using literate programming, 
%{\em MMIXware}~\cite{mmixware},
%to explain the full code of the MMIX simulator and assembler.
%%
%However, similar to Hack, very few programs have been written for
%this machine. For instance, the book assumes the presence of a kernel
%called NNIX, but nobody has ever written it.
%\n but as Knuth told me, gcc has been ported to mmix as well as Linux

%\item STEPS (Alan Kay et al.)
%%TODO: \cite{} the first and last paper
%\furl{http://vpri.org/html/writings.php}
%was a project to reinvent from scratch programming.
%It had a far more ambitious goal than \principia:
%write a full OS in 20 000 LOC. It was
%unfortunately never finished.
%\l our kernel book is 30 000 LOC, and it's just the kernel (and actually not all of it), so hmmm
%\l seems more oriented to apps though, minimal OS, and single lang I think
%%\item Squeak (Alan Kay et al.),%\furl{http://squeak.org/}


\item Oberon\furl{http://www.projectoberon.com/} 
(Niklaus Wirth et al.)
is a kernel,
compiler, 
and windowing system 
designed from scratch. It is
a great OS, very compact, and fully documented in a 
book~\cite{project-oberon}.
%
However, it imposes strong restrictions on the programmer:
only applications written in the Oberon programming language
can be run. 
This simplifies many things, 
\n singularity was a bit like that too
but OS like
\unix (and \plan) are more universal; they
can run any program in any language, as long as the program 
can be interpreted or compiled into a binary.
\l  javascript emulator: http://schierlm.github.io/OberonEmulator/
\n new edition in 2014!
\t see recent articles about SmallTalk/Unix reunion in my Download

\item TempleOS\furl{http://www.templeos.org/} (Terry A. Davis)
is an OS
single handedly created over a decade. It contains a kernel,
a windowing system, a compiler for a dialect of C, and
even some games.
It has graphics capabilities but there is no network support.

\end{itemize}

Another candidate for \principia was the combination of
the GNU system\furl{http://www.gnu.org/}, 
\l (Richard Stallman et al.)
the Linux kernel\furl{http://www.kernel.org/}, and
the X Window graphical user interface Xorg\furl{http://www.freedesktop.org}.
%
However, GNU/Linux/Xorg together is far bigger than \plan.
If you take the source code of 
the Linux kernel, 
the GNU C library ([[glibc]]),
the [[bash]] shell,
the GNU C compiler ([[gcc]]),
the GNU assembler ([[gas]]) and linker ([[ld]]) part of the [[binutils]] package,
%the GNU Lex and Yacc clones ([[flex]] and [[bison]]),
the Emacs editor, 
GNU [[make]],
the GNU debugger ([[gdb]]), 
the GNU profiler ([[gprof]]),
and the X Window system ([[Xorg]]), you will get
orders of magnitude more source code than \plan, even though
\plan provides in essence the same core services.
\l see Linux From Scratch excellent book showing how to install from
\l  source all those packages (and also Beyond Linux From Scratch)
%
In fact, almost all of the programs above use {\em individually}
more source code than the {\em whole} \plan system.

Of course,
the Linux kernel contains thousands of specific device drivers,
[[gcc]] handles a multitude of different architectures, and 
[[Xorg]] supports lots of graphic cards.
%
All of those things could be discarded when
presenting the {\em core} of those programs. However, their core is still far
bigger than the equivalent core in \plan programs.
\n Linux 1991, Plan9 1992 (but open source only in 2000), paper in 1990
%TODO: show gcc for instance with arm- part already 50 000 LOC!
% same for clang/llvm, take comparison from Linker.nw or Assembler.nw
%
% bash maybe smallest? or make? bash already 180 000 LOC I think
% you will need books of 20000 pages, not 400, averega for principia



% too many other to list here, see the comment in Principia.nw
% in Related work section



\section{Conclusion}
\label{sec:conclusion}

%dup: Principia.nw
I hope the \principia books will greatly consolidate
student computer science knowledge and give students a better and
more complete picture of what is going on in a computer.
%
Hopefully, it will also give more exposure to the hidden gem
that is \plan code; this code is small, elegant, powerful, open source, and
was written by incredible programmers who deserve to have
their art pieces (their programs) fully exposed.
% actually we can learn from the master! study the code of the masters!

%dup: Principia.nw
I hope those books will answer many students questions, 
even those that seem very simple at first
such as ``What happens when the user type [[ls]] in a terminal window?''.
% or what happens when type C-d in program reading stdin (e.g., ocaml interpreter)
The answer to this question
involves many software layers (the shell,
the C library, the kernel, the graphics stack, and the windowing system)
and involves actually a non-trivial amount of code.
% still, doable! On Linux would be super complicated with pty, tty
%TODO: copy discussion of Principia.nw on pty and elegance of plan9!
% or too big for conclusion?

\bibliography{../latex/Principia}
\bibliographystyle{alpha}

\end{document}
