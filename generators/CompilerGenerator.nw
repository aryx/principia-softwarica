\documentclass[twocolumn]{report}

%******************************************************************************
% Prelude
%******************************************************************************
\newif\iffinal
\newif\ifverbose
\finaltrue\verbosefalse % see also other newif in Macros.tex

%------------------------------------------------------------------------------
%history: 
%------------------------------------------------------------------------------

%thx to LP, changed for the better a few things:

%thx to codemap/codegraph:

%thx to this manual, better understand lex/yacc:

%history LP-ization:
% - skeleton, mostly copy paste of Template.nw skeleton
% - put all content of files in the Extra section, via 'pfff -lpize'
%   which also now split in chunks!
%    * function, global, struct, enum, constant, macro(actually function)
% - TODO read Extra section, identify concepts, first TOC
% - TODO distribute parts of the file before
% - TODO nullify, boolify, typeify,    scheckify
% - TODO aspecify advanced features!
% - TODO add figures
% - TODO add explanations

%------------------------------------------------------------------------------
% Packages
%------------------------------------------------------------------------------

\usepackage{../docs/latex/noweb}
 \noweboptions{footnotesizecode,nomargintag}
 %note: allow chunk on different pages, less white space at bottom of pages
 \def\nwendcode{\endtrivlist \endgroup}
 \let\nwdocspar=\par
\usepackage{xspace}
\usepackage{verbatim}
%note: required by noweblatexpad for the \t \l \n in this file
\usepackage{fancyvrb}
\usepackage{url}
\iffinal
\usepackage{hyperref}
 \hypersetup{colorlinks=true}
\fi
\usepackage[pageref]{backref}
 \def\backref{{\footnotesize cited page(s)}~}
\usepackage{cleveref} %\cref
\usepackage{multirow}
\usepackage{booktabs} 
 \newcommand{\otoprule}{\midrule[\heavyrulewidth]}
\usepackage{graphicx}
 %\usepackage[margin=0.5in]{geometry}
 %  but eat the bottom when very low
 %\usepackage{fullpage} is deprecated 
 % => do the more manual below:
 \addtolength{\oddsidemargin}{-.850in}
 \addtolength{\evensidemargin}{-.850in}
 \addtolength{\textwidth}{1.70in}
 \addtolength{\topmargin}{-.850in}
 \addtolength{\textheight}{1.70in}
%\usepackage{minitoc}

%------------------------------------------------------------------------------
% Macros
%------------------------------------------------------------------------------
\input{../docs/latex/Macros}

%------------------------------------------------------------------------------
% Config
%------------------------------------------------------------------------------
\allcodefalse
% used for?

%\setcounter{tocdepth}{1}

%******************************************************************************
% Title
%******************************************************************************

\begin{document}

\title{
{\Huge 
Principia Softwarica: Lex and Yacc
}\\
{version 0.1}
}
% for once it's not really Plan9, the lex and yacc seems really to
% come from Unix with a small port to Plan9, as opposed to the other
% document I've already written which are really Plan9 software

\author{
Yoann Padioleau\\
\texttt{yoann.padioleau@gmail.com}\\
\\
with code from\\
Xavier Leroy and ?
}
% Erik scmidt, xavier leroy
% jonhson, corbett

\maketitle 
\onecolumn
\hrule
\begin{quote}
    Copyright \copyright{} 2015 Yoann Padioleau \\
    Permission is granted to copy, distribute and/or modify this document,
    except all the source code it contains, under the terms of the GNU Free
    Documentation License, Version 1.3.
\end{quote}
\hrule

%CONFIG: \dominitoc

\iffinal
\begingroup
\hypersetup{linkcolor=blue}
% need to s/onecolumn/twocolumn in report.cls :) for \tableofcontents
\twocolumn
\tableofcontents
\endgroup
\else
\tableofcontents
\fi

%******************************************************************************
% Body
%******************************************************************************

\chapter{Introduction}

\section{Motivations}

The goal of this book is to present in full details the source code of
Lex and Yacc.
Why? Because I think it makes you a better programmer if
you fully understand how things work under the hood.

Here are other candidates that were considered but ultimately discarded:
\begin{itemize}
\item Flex and Bison
\item Antlr
\end{itemize}
% Prolog DCG
% EDSL for regexps or grammar?
% Many lexers are handwritten (e.g. in Plan9), and many grammars
% are also handwritten (seems to have some advantages for IDE, e.g.
% jetbrains does this, but also now hack/flow)

% Berkeley Yacc
% Unix Lex?
% go-yacc! a port of yacc.c to Go by Go authors, might be interesting
% to contrast C and Go.

% ocamllex: 1000 LOC vs the 3000 of plan9 lex
% ocamlyacc: 6000 LOC, vs the 3000 of plan9 yacc
% menhir? 21 000 LOC of ocaml, hmm bigger than my fork-ocaml, lol

% see also sed/awk? 

\section{Getting started}

\section{Requirements}

% you need to know about lex & yacc :) because we gonne use
% lex and yacc to explain the code of lex and yacc. hmm.

%(* To generate directly a NFA from a regular expression.
%   Confer Aho-Sethi-Ullman, dragon book, chap. 3 *)

\section{About this document}
#include "../docs/latex/About.nw"

\section{Copyright}

Most of this document is actually source code from Plan9, so
those parts are copyright by Lucent Technologies Inc.
The prose is mine and is licensed under the GNU Free Documentation
License.

<<copyright ocamllex>>=
(***********************************************************************)
(*                                                                     *)
(*                           Objective Caml                            *)
(*                                                                     *)
(*            Xavier Leroy, projet Cristal, INRIA Rocquencourt         *)
(*                                                                     *)
(*  Copyright 1996 Institut National de Recherche en Informatique et   *)
(*  Automatique.  Distributed only by permission.                      *)
(*                                                                     *)
(***********************************************************************)
@

\section{Acknowledgments}


\chapter{Overview}

\section{Lex and yacc principles}

% DSL! The first ones!

% there is usually a lex/yacc interface they need to agree on,
% but actually lex can also work without yacc, and yacc without lex.

\section{[[lex]] services}

<<[[Main.main()]] print lex usage if wrong number of arguments>>=
if Array.length Sys.argv != 2 then begin
  prerr_endline "Usage: ocamllex <input file>";
  exit 2
end;
@
% .mll to .ml

\section{[[yacc]] services}

\section{Toy calculator example}

\section{Code organization}

\section{Software architecture}

\section{Bootstrapping}

%###############################################################################

\chapter{Lex}

% essentially a DSL to go from regexp -> automaton (NFA)

\section{Core data structures}

\subsection{Regular expression}

<<type Syntax.regular_expression>>=
type regular_expression =
    Epsilon
  | Characters of int list
  | Sequence of regular_expression * regular_expression
  | Alternative of regular_expression * regular_expression
  | Repetition of regular_expression
@

% Characters could be a combination of Alternative and Char 
% but maybe useful optimisation?

% so 'X?' is really 'X | Epsilon' and 'X+' is 'X X*'


\subsection{Abstract syntax tree}

<<type Syntax.location>>=
type location =
    Location of int * int
@
% header/trailer/action slice, int = charpos

<<type Syntax.lexer_definition>>=
type lexer_definition =
    { header: location;
      entrypoints: (string * (regular_expression * location) list) list;
      trailer: location 
    }
@
% have more power than just regexps when can have different entry points! can
% actually do stuff that count! see comments below.

\subsection{Intermediate regexp format}

<<type Lexgen.regexp>>=
(* Deep abstract syntax for regular expressions *)

type regexp =
    Empty
  | Chars of int
  | Action of int
  | Seq of regexp * regexp
  | Alt of regexp * regexp
  | Star of regexp
@
% Chars??

<<type Lexgen.lexer_entry>>=
type lexer_entry =
  { lex_name: string;
    lex_regexp: regexp;
    lex_actions: (int * Syntax.location) list }
@

\subsection{Automata}

<<type Lexgen.automata_entry>>=
(* Representation of entry points *)

type automata_entry =
  { auto_name: string;
    auto_initial_state: int;
    auto_actions: (int * Syntax.location) list }
@

<<type Lexgen.automata>>=
type automata =
    Perform of int
  | Shift of automata_trans * automata_move array
@
% usually automata array!
%val make_dfa: Syntax.lexer_definition -> automata_entry list * automata array
% so array of array when have Shift.

<<type Lexgen.automata_trans>>=
and automata_trans =
    No_remember
  | Remember of int
@

<<type Lexgen.automata_move>>=
and automata_move =
    Backtrack
  | Goto of int
@

%TODO: dump example for calc/lexer.mll

\subsection{Optimised compacted automata}

<<type Compact.lex_tables>>=
(* Compaction of an automata *)

type lex_tables =
  { tbl_base: int array;                 (* Perform / Shift *)
    tbl_backtrk: int array;              (* No_remember / Remember *)
    tbl_default: int array;              (* Default transition *)
    tbl_trans: int array;                (* Transitions (compacted) *)
    tbl_check: int array }               (* Check (compacted) *)
@
%val compact_tables: Lexgen.automata array -> lex_tables

\subsection{Runtime lexbuf}

% useful to understand how things work internally,
% but also because the action will need to understand partially lexbuf
% (at least use the Lexeme.xxx API functions)

<<type Lexing.lexbuf>>=
(* The run-time library for lexers generated by camllex *)
(* The type of lexer buffers. A lexer buffer is the argument passed
   to the scanning functions defined by the generated scanners.
   The lexer buffer holds the current state of the scanner, plus
   a function to refill the buffer from the input. *)
type lexbuf =
  { refill_buff : lexbuf -> unit;

    mutable lex_buffer : string;
    mutable lex_buffer_len : int;

    mutable lex_abs_pos : int;
    mutable lex_start_pos : int;
    mutable lex_curr_pos : int;
    mutable lex_last_pos : int;

    mutable lex_last_action : int;
    mutable lex_eof_reached : bool 
  }
@

%less: put the Lexeme.xxx API functions here?

\section{[[main()]]}

<<toplevel Main._1>>=
let _ = 
  Printexc.catch main (); 
  exit 0
@


<<function Main.main>>=
let main () =
  <<[[Main.main()]] print lex usage if wrong number of arguments>>
  let source_name = Sys.argv.(1) in
  let dest_name =
    if Filename.check_suffix source_name ".mll" 
    then Filename.chop_suffix source_name ".mll" ^ ".ml"
    else source_name ^ ".ml" 
  in
  let ic = open_in source_name in
  let oc = open_out dest_name in
  let lexbuf = Lexing.from_channel ic in

  (* parsing *)
  let def =
    try
      Parser.lexer_definition Lexer.main lexbuf
    with exn ->
      close_out oc;
      Sys.remove dest_name;
       <<[[Main.main()]] report error exn>>
      exit 2 
  in
  (* compiling *)
  let (entries, transitions) = Lexgen.make_dfa def in
  (* optimizing *)
  let tables = Compact.compact_tables transitions in
  (* generating *)
  Output.output_lexdef ic oc def.header tables entries def.trailer;

  close_in ic;
  close_out oc
@

% Lexer.main :) in next section

% Parser.lexer_definition in next next section

<<signature Lexgen.make_dfa>>=
(* The entry point *)

val make_dfa: Syntax.lexer_definition -> automata_entry list * automata array
@

<<signature Compact.compact_tables>>=
val compact_tables: Lexgen.automata array -> lex_tables
@


<<signature Output.output_lexdef>>=
(* Output the DFA tables and its entry points *)

val output_lexdef:
      in_channel -> out_channel ->
      Syntax.location (* header *) ->
      Compact.lex_tables ->
      Lexgen.automata_entry list ->
      Syntax.location (* trailer *) ->
      unit
@


\section{Lexing}
% :) self-reference
% another example of use after calc/lexer.mll :)

<<lex/lexer.mll>>=
<<copyright ocamllex>>
(* The lexical analyzer for lexer definitions. Bootstrapped! *)

{
open Syntax
open Parser

exception Lexical_error of string

(* Auxiliaries for the lexical analyzer *)
<<Lexer helper functions and globals>>
}

<<rule Lexer.main>>

<<rule Lexer.action>>

<<rule Lexer.string>>
      
<<rule Lexer.comment>>
@
% can see header, rules, and here actually no trailer.
%    { header: location;
%      entrypoints: (string * (regular_expression * location) list) list;
%      trailer: location 
%less: type rule = (regular_expression * location) list?
%less: no 'let', could be nice to use to illustrate all features :)


% the tokens are defined in parser.mly, but they could be defined in the header
% (like we do in efuns/prog_modes/ocaml_mode.mll for instance)
<<type Parser.token>>=
%token Trule Tparse Tand
%token <int> Tchar
%token <string> Tstring
%token Tstar Tmaybe Tplus Tor Tlparen Trparen 
%token Tlbracket Trbracket Tcaret Tdash 
%token Tunderscore Teof
%token <Syntax.location> Taction
%token Tlet Tequal 
%token <string> Tident
%token Tend  
@
% see parser.mly, it expands in a type token = TIdent of string | ...

<<rule Lexer.main>>=
rule main = parse
  <<[[Lexer.main()]] space case>>
  <<[[Lexer.main()]] comment case>>
  <<[[Lexer.main()]] keyword or identifier case>>
  <<[[Lexer.main()]] string start case>>
  <<[[Lexer.main()]] character cases>>
  <<[[Lexer.main()]] operator cases>>
  <<[[Lexer.main()]] action case>>
  | eof  { Tend }
  | _
    { raise(Lexical_error
             ("illegal character " ^ String.escaped(Lexing.lexeme lexbuf))) }
@

% can see the list inner list
%      entrypoints: (string * (regular_expression * location) list) list;



\subsection{Comments}

<<[[Lexer.main()]] space case>>=
  [' ' '\010' '\013' '\009' '\012' ] + 
  { main lexbuf }
@
% C-j, C-m, tab, C-l

<<[[Lexer.main()]] comment case>>=
| "(*" 
  { comment_depth := 1;
    comment lexbuf;
    main lexbuf }
@

<<Lexer helper functions and globals>>=
let comment_depth = ref 0
@

% regexps can't count, but by playing with multiple
% rules and recursion you can! so good that lex
% is not just about regexps.

<<rule Lexer.comment>>=
and comment = parse
    "(*" 
    { incr comment_depth; comment lexbuf }
  | "*)" 
    { decr comment_depth;
      if !comment_depth == 0 then () else comment lexbuf }

  | eof 
    { raise(Lexical_error "unterminated comment") }
  | _ 
    { comment lexbuf }
@


% not sure we need that. Actually I find it annoying
%  | '"' 
%    { reset_string_buffer();
%      string lexbuf;
%      reset_string_buffer();
%      comment lexbuf }
%  | "''"
%      { comment lexbuf }
%  | "'" [^ '\\' '\''] "'"
%      { comment lexbuf }
%  | "'\\" ['\\' '\'' 'n' 't' 'b' 'r'] "'"
%      { comment lexbuf }
%  | "'\\" ['0'-'9'] ['0'-'9'] ['0'-'9'] "'"
%      { comment lexbuf }



\subsection{Keywords and identifiers}

<<[[Lexer.main()]] keyword or identifier case>>=
| ['A'-'Z' 'a'-'z'] ['A'-'Z' 'a'-'z' '\'' '_' '0'-'9'] *
  { match Lexing.lexeme lexbuf with
    | "rule" -> Trule
    | "parse" -> Tparse
    | "and" -> Tand
    | "eof" -> Teof
    | "let" -> Tlet
    | s -> Tident s 
   }
@
% have seen all the keywords (except let)

% there is a 'shortest' keyword in recent ocamllex, probably
% it's like the ? perl greedy operator

%TODO: 'as' is useful

\subsection{Operators}

<<[[Lexer.main()]] operator cases>>=
| '*'  { Tstar }
| '|'  { Tor }
@
% regexp! see AST correspondance!
% how is sorted priority? in ab*, the star is just for 'b' or for ab?

<<[[Lexer.main()]] operator cases>>=
| '?'  { Tmaybe }
| '+'  { Tplus }
@
% desugared in a a* and  epsilon | a

<<[[Lexer.main()]] operator cases>>=
| '('  { Tlparen }
| ')'  { Trparen }
@



<<[[Lexer.main()]] operator cases>>=
| '['  { Tlbracket }
| ']'  { Trbracket }
| '-'  { Tdash }
| '^'  { Tcaret }
@
% range

<<[[Lexer.main()]] operator cases>>=
| '_'  { Tunderscore }
@



<<[[Lexer.main()]] operator cases>>=
| '='  { Tequal }
@
% for naming

\subsection{Strings}

<<[[Lexer.main()]] string start case>>=
| '"' 
  { reset_string_buffer();
    string lexbuf;
    Tstring(get_stored_string()) }
@

% use Buffer instead? or just use ^ like I was doing originally?
% actually that's what they do in recent ocamllex

<<Lexer helper functions and globals>>=
let initial_string_buffer = String.create 256
let string_buff = ref initial_string_buffer
let string_index = ref 0
@

<<Lexer helper functions and globals>>=
let reset_string_buffer () =
  string_buff := initial_string_buffer;
  string_index := 0
@

<<Lexer helper functions and globals>>=
let get_stored_string () =
  String.sub !string_buff 0 !string_index
@


<<rule Lexer.string>>=
and string = parse
    '"' 
    { () }
  | '\\' [' ' '\010' '\013' '\009' '\026' '\012'] +
    { string lexbuf }
  | '\\' ['\\' '"' 'n' 't' 'b' 'r'] 
    { store_string_char(char_for_backslash(Lexing.lexeme_char lexbuf 1));
      string lexbuf }
  | '\\' ['0'-'9'] ['0'-'9'] ['0'-'9'] 
    { store_string_char(char_for_decimal_code lexbuf 1);
      string lexbuf }
  | eof 
    { raise(Lexical_error "unterminated string") }
  | _ 
    { store_string_char(Lexing.lexeme_char lexbuf 0);
      string lexbuf }
@
% multiline handling with the \ <RETURN> ?
% 10 = C-j, 13 = C-m, 12 = C-l (used in emacs somtimes, supposed to be
%  a page limit or something)
%less: need %9 = Tab? need 12? 26 = C-z, need?

<<Lexer helper functions and globals>>=
let store_string_char c =
  if !string_index >= String.length !string_buff then begin
    let new_buff = String.create (String.length !string_buff * 2) in
    String.blit !string_buff 0 new_buff 0 (String.length !string_buff);
    string_buff := new_buff
  end;
  !string_buff.[!string_index] <- c;
  incr string_index
@


<<Lexer helper functions and globals>>=
let char_for_backslash = function
    'n' -> '\n'
  | 't' -> '\t'
  | 'b' -> '\b'
  | 'r' -> '\r'
  | c   -> c
@

<<Lexer helper functions and globals>>=
let char_for_decimal_code lexbuf i =
  Char.chr(100 * (Char.code(Lexing.lexeme_char lexbuf i) - 48) +
            10 * (Char.code(Lexing.lexeme_char lexbuf (i+1)) - 48) +
                 (Char.code(Lexing.lexeme_char lexbuf (i+2)) - 48))
@

\subsection{Characters}

<<[[Lexer.main()]] character cases>>=
| "'" [^ '\\'] "'" 
  { Tchar(Char.code(Lexing.lexeme_char lexbuf 1)) }
| "'" '\\' ['\\' '\'' 'n' 't' 'b' 'r'] "'" 
  { Tchar(Char.code(char_for_backslash (Lexing.lexeme_char lexbuf 2))) }
| "'" '\\' ['0'-'9'] ['0'-'9'] ['0'-'9'] "'" 
  { Tchar(Char.code(char_for_decimal_code lexbuf 2)) }
@

\subsection{Actions}

<<[[Lexer.main()]] action case>>=
| '{' 
  { let n1 = Lexing.lexeme_end lexbuf in
    brace_depth := 1;
    let n2 = action lexbuf in
    Taction(Location(n1, n2)) }
@


<<Lexer helper functions and globals>>=
let brace_depth = ref 0
@
% need to count them, because the code in action can use '}'

<<rule Lexer.action>>=
and action = parse
    '{' 
    { incr brace_depth;
      action lexbuf }
  | '}' 
    { decr brace_depth;
      if !brace_depth == 0 then Lexing.lexeme_start lexbuf else action lexbuf }
  | "(*" 
    { comment_depth := 1;
      comment lexbuf;
      action lexbuf }
  | eof 
    { raise (Lexical_error "unterminated action") }
  | _ 
    { action lexbuf }
@

% not sure you need that, there is anyway no } in strings usually.
%  | '"' 
%    { reset_string_buffer();
%      string lexbuf;
%      reset_string_buffer();
%      action lexbuf }
%  | "'" [^ '\\'] "'" 
%    { action lexbuf }
%  | "'" '\\' ['\\' '\'' 'n' 't' 'b' 'r'] "'" 
%    { action lexbuf }
%  | "'" '\\' ['0'-'9'] ['0'-'9'] ['0'-'9'] "'" 
%    { action lexbuf }

% we need the comment stuff though because the action token is
% also used for the header/trailer and they may contain some comments
% with some } we don't want to consider as the end of the action.

\section{Parsing}

% good exercise for next chapter :)

\subsection{Overview}

<<lex/parser.mly>>=
<<copyright ocamllex bis>>

/* The grammar for lexer definitions */

%{
open Syntax

(* Auxiliaries for the parser. *)
<<parser helper functions and globals>>

%}

/* Tokens */

<<type Parser.token>>

/* Precedences and associativities. Lower precedences come first. */

<<Parser precedences and associativities>>

/* Entry points */

<<Parser entry points types>>

%%

<<grammar>>

%%
@
% header, rules, trailer


<<grammar>>=
<<lex top rule>>

<<lex header rule>>

<<lex named regexp rule>>

<<lex rule rule>>

<<lex regexp rule>>

@

% rule rule :) meta

% no action rule as it's actually just a token

\subsection{Lexer definition entry point}

<<Parser entry points types>>=
%start lexer_definition
%type <Syntax.lexer_definition> lexer_definition
@

<<lex top rule>>=
lexer_definition:
    header named_regexps Trule definition other_definitions header Tend
        { { header = $1;
            entrypoints = $4 :: List.rev $5;
            trailer = $6
           } 
         }
;
@

\subsection{Header and trailer}

<<lex header rule>>=
header:
    Taction      { $1 }
  | /*epsilon*/  { Location(0,0) }
;
@
%$ 
%an action! something between { }

\subsection{Rule}

% rule X = parse ...
<<lex rule rule>>=
definition:
    Tident Tequal entry
        { ($1,$3) }
;
entry:
    Tparse case rest_of_entry
        { $2::List.rev $3 }
  | Tparse rest_of_entry
        { List.rev $2 }
;
case:
    regexp Taction
        { ($1,$2) }
;


other_definitions:
    other_definitions Tand definition
        { $3::$1 }
  | /*epsilon*/
        { [] }
;
rest_of_entry:
    rest_of_entry Tor case
        { $3::$1 }
  | /*epsilon*/
        { [] }
;
@
%$


%less: in recent ocamllex the lexer can take more arguments, which is
% useful, for instance in my PHP lexer I pass the beginning symbol
% so that the rule called know what to look for to close it.

\subsection{Regexps}

\subsubsection{Basic regexps}

<<Parser precedences and associativities>>=
%left Tor
%left CONCAT
%nonassoc Tmaybe
%left Tstar
%left Tplus
@

% left Tstar?? but it's not a binary operator

<<lex regexp rule>>=
regexp:
    Tchar
        { Characters [$1] }
  | regexp Tstar
        { Repetition $1 }
  | regexp Tor regexp
        { Alternative($1,$3) }
  | regexp regexp %prec CONCAT
        { Sequence($1,$2) }
  | Tlparen regexp Trparen
        { $2 }
  <<rule regexp cases>>
;
@
%$

\subsubsection{Sugar regexps}

<<rule regexp cases>>=
  | regexp Tmaybe
        { Alternative($1, Epsilon) }
  | regexp Tplus
        { Sequence($1, Repetition $1) }
@
%$

\subsubsection{Range regexps}

<<rule regexp cases>>=
  | Tlbracket char_class Trbracket
        { Characters $2 }
@

<<lex regexp rule>>=
char_class:
    Tcaret char_class1
        { subtract all_chars $2 }
  | char_class1
        { $1 }
;
char_class1:
    Tchar Tdash Tchar
        { char_class $1 $3 }
  | Tchar
        { [$1] }
  | char_class1 char_class1 %prec CONCAT
        { $1 @ $2 }
;

@
%$

<<parser helper functions and globals>>=
let char_class c1 c2 =
  let rec cl n =
    if n > c2 then [] else n :: cl(succ n)
  in cl c1
@

<<parser helper functions and globals>>=
let all_chars = char_class 0 255
@

<<parser helper functions and globals>>=
let rec subtract l1 l2 =
  match l1 with
    [] -> []
  | a::r -> if List.mem a l2 then subtract r l2 else a :: subtract r l2
@


\subsubsection{Named regexps}
% aliases

<<parser helper functions and globals>>=
let named_regexps =
  (Hashtbl.create 13 : (string, regular_expression) Hashtbl.t)
@

<<rule regexp cases>>=
  | Tident
        { try
            Hashtbl.find named_regexps $1
          with Not_found ->
            prerr_string "Reference to unbound regexp name `";
            prerr_string $1;
            prerr_string "' at char ";
            prerr_int (Parsing.symbol_start());
            prerr_newline();
            exit 2 }
@

<<lex named regexp rule>>=
named_regexps:
    named_regexps Tlet Tident Tequal regexp
        { Hashtbl.add named_regexps $3 $5 }
  | /*epsilon*/
        { () }
;
@


\subsubsection{Other regexps}

<<rule regexp cases>>=
  | Tstring
        { regexp_for_string $1 }
@
%$

<<parser helper functions and globals>>=
let regexp_for_string s =
  let rec re_string n =
    if n >= String.length s then Epsilon
    else if succ n = String.length s then Characters([Char.code (s.[n])])
    else Sequence(Characters([Char.code (s.[n])]), re_string (succ n))
  in re_string 0
@

<<rule regexp cases>>=
  | Tunderscore
        { Characters all_chars }
  | Teof
        { Characters [256] }
@


\section{Compiling}

% make_dfa

\section{Optimizing}

% compact_tables

\section{Generating}

% output_lexdef

%TODO: how hard to generate instead for C code? so can replace
% Unix lex!

\section{Running}











%###############################################################################


\chapter{Yacc}

% essentially a DSL to go from grammar -> pushdown automaton

\chapter{Conclusion}

\appendix

\chapter{Debugging}

\chapter{Error Managment}

<<[[Main.main()]] report error exn>>=
(match exn with
  Parsing.Parse_error ->
    prerr_string "Syntax error around char ";
    prerr_int (Lexing.lexeme_start lexbuf);
    prerr_endline "."
| Lexer.Lexical_error s ->
    prerr_string "Lexical error around char ";
    prerr_int (Lexing.lexeme_start lexbuf);
    prerr_string ": ";
    prerr_string s;
    prerr_endline "."
| _ -> raise exn
);
@

\chapter{Libc}

\chapter{Extra Code}

\ifallcode
#include "CompilerGenerator_extra.nw"
\fi

\chapter{Changelog}
\label{sec:changelog}

\chapter{Glossary}
\label{sec:glossary}

\begin{verbatim}
\end{verbatim}

\chapter*{Indexes}
\addcontentsline{toc}{section}{Index}

%\chapter{References} 
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{99}

\bibitem[1]{wp-literate-programming} Donald Knuth,,
{\em Literate Programming}, 
\url{http://en.wikipedia.org/wiki/Literate\_Program}

\bibitem[2]{noweb} Norman Ramsey,
{\em Noweb}, 
\url{http://www.cs.tufts.edu/~nr/noweb/}

\bibitem[3]{syncweb} Yoann Padioleau,
{\em Syncweb, literate programming meets unison}, 
\url{http://padator.org/software/project-syncweb/readme.txt}

\end{thebibliography}

%******************************************************************************
% Postlude
%******************************************************************************

\end{document}
