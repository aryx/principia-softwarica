\documentclass[twocolumn]{report}

%******************************************************************************
% Prelude
%******************************************************************************
\newif\iffinal
\newif\ifverbose
\finaltrue\verbosefalse % see also other newif in Macros.tex

%------------------------------------------------------------------------------
%history: 
%------------------------------------------------------------------------------

%thx to LP, changed for the better a few things:
% - use more type aliases instead of (ab)using 'int' everywhere
%   which is not very informative and actually confusing

%thx to codemap/codegraph:

%thx to this manual, better understand lex/yacc:
% - priority of actions when two case matches (e.g kwd prio over ident 
%   if case is before)

%history LP-ization:
% - skeleton, mostly copy paste of Template.nw skeleton
% - put all content of files in the Extra section, via 'pfff -lpize'
%   which also now split in chunks!
%    * function, global, struct, enum, constant, macro(actually function)
% - read Extra section, identify concepts, first TOC
% - SEMI distribute parts of the file before
% - TODO nullify, boolify, typeify,    scheckify
% - TODO aspecify advanced features!
% - TODO add figures
% - TODO add explanations

%------------------------------------------------------------------------------
% Packages
%------------------------------------------------------------------------------

\usepackage{../docs/latex/noweb}
 \noweboptions{footnotesizecode,nomargintag}
 %note: allow chunk on different pages, less white space at bottom of pages
 \def\nwendcode{\endtrivlist \endgroup}
 \let\nwdocspar=\par
\usepackage{xspace}
\usepackage{verbatim}
%note: required by noweblatexpad for the \t \l \n in this file
\usepackage{fancyvrb}
\usepackage{url}
\iffinal
\usepackage{hyperref}
 \hypersetup{colorlinks=true}
\fi
\usepackage[pageref]{backref}
 \def\backref{{\footnotesize cited page(s)}~}
\usepackage{cleveref} %\cref
\usepackage{multirow}
\usepackage{booktabs} 
 \newcommand{\otoprule}{\midrule[\heavyrulewidth]}
\usepackage{graphicx}
 %\usepackage[margin=0.5in]{geometry}
 %  but eat the bottom when very low
 %\usepackage{fullpage} is deprecated 
 % => do the more manual below:
 \addtolength{\oddsidemargin}{-.850in}
 \addtolength{\evensidemargin}{-.850in}
 \addtolength{\textwidth}{1.70in}
 \addtolength{\topmargin}{-.850in}
 \addtolength{\textheight}{1.70in}
%\usepackage{minitoc}

%------------------------------------------------------------------------------
% Macros
%------------------------------------------------------------------------------
\input{../docs/latex/Macros}

%------------------------------------------------------------------------------
% Config
%------------------------------------------------------------------------------
\allcodefalse
% used for?

%\setcounter{tocdepth}{1}

%******************************************************************************
% Title
%******************************************************************************

\begin{document}

\title{
{\Huge 
Principia Softwarica: Lex and Yacc
}\\
{version 0.1}
}

\author{
Yoann Padioleau\\
\texttt{yoann.padioleau@gmail.com}\\
\\
with code from\\
Xavier Leroy and ?
}
% Erik scmidt, xavier leroy
% jonhson, corbett

\maketitle 
\onecolumn
\hrule
\begin{quote}
    Copyright \copyright{} 2015 Yoann Padioleau \\
    Permission is granted to copy, distribute and/or modify this document,
    except all the source code it contains, under the terms of the GNU Free
    Documentation License, Version 1.3.
\end{quote}
\hrule

%CONFIG: \dominitoc

\iffinal
\begingroup
\hypersetup{linkcolor=blue}
% need to s/onecolumn/twocolumn in report.cls :) for \tableofcontents
\twocolumn
\tableofcontents
\endgroup
\else
\tableofcontents
\fi

%******************************************************************************
% Body
%******************************************************************************

\chapter{Introduction}

\section{Motivations}

The goal of this book is to present in full details the source code of
Lex and Yacc.
Why? Because I think it makes you a better programmer if
you fully understand how things work under the hood.

% We gonna present ocamllex actually. Shorter code and easier
% to explain than the original lex.
% And for yacc?? ml-yacc port?

%% For once it's not really Plan9, the lex and yacc seems really to
%% come from Unix with a small port to Plan9, as opposed to the other
%% document I've already written which are really Plan9 software.

Here are other candidates that were considered but ultimately discarded:
\begin{itemize}
\item Flex and Bison
% LOC?
\item Antlr
% LOC?
\item Ometa
% ??
\end{itemize}

% Prolog DCG
% EDSL for regexps or grammar?

% Berkeley Yacc
% Unix Lex?
% go-yacc! a port of yacc.c to Go by Go authors, might be interesting
%   to contrast C and Go. 3464 LOC

% ocamllex: 1000 LOC vs the 3000 of plan9 lex
% ocamlyacc: 6000 LOC, vs the 3000 of plan9 yacc
% byacc:??
% menhir? 21 000 LOC of ocaml, hmm bigger than my fork-ocaml, lol


% Many lexers are actually handwritten (e.g. in Plan9), and many grammars
% are also handwritten (seems to have some advantages for IDE, e.g.
% jetbrains does this, but also now hack/flow)

% see also sed/awk? 

\section{Getting started}

\section{Requirements}

% you need to know about lex & yacc :) because we gonne use
% lex and yacc to explain the code of lex and yacc. hmm.

% Dragon book also useful.
%(* To generate directly a NFA from a regular expression.
%   Confer Aho-Sethi-Ullman, dragon book, chap. 3 *)

\section{About this document}
#include "../docs/latex/About.nw"

\section{Copyright}

Most of this document is actually source code from OCaml, so
those parts are copyright by INRIA.
The prose is mine and is licensed under the GNU Free Documentation
License.

<<copyright ocamllex>>=
(***********************************************************************)
(*                                                                     *)
(*                           Objective Caml                            *)
(*                                                                     *)
(*            Xavier Leroy, projet Cristal, INRIA Rocquencourt         *)
(*                                                                     *)
(*  Copyright 1996 Institut National de Recherche en Informatique et   *)
(*  Automatique.  Distributed only by permission.                      *)
(*                                                                     *)
(***********************************************************************)
@

\section{Acknowledgments}


\chapter{Overview}

\section{Lex and yacc principles}

% DSL! The first ones!
% Helpers for compiler writers (or nowadays for any tools working
% on source code, e.g. linters, refactorers, smart completion backend, 
% code indexers, etc).

% There is usually a lex/yacc interface they need to agree on,
% but actually lex can also work without yacc (give type of tokens)
% and yacc without lex (write a yylex() function).

\section{[[lex]] services}

<<[[Main.main()]] print lex usage if wrong number of arguments>>=
if Array.length Sys.argv != 2 then begin
  prerr_endline "Usage: ocamllex <input file>";
  exit 2
end;
@
% .mll to .ml

\section{[[yacc]] services}

\section{Toy calculator example}

% show generated code too? Can have toy generated code?

\section{Code organization}

\section{Software architecture}

\section{Bootstrapping}

%###############################################################################

\chapter{Lex}

% Essentially a DSL to go from regexps -> automatons (DFA)
% Automaton so given a character at a time it transitions to
% the right state and entering certain (final) states triggers
% a special action (usually returning a token constructor to be then
% consumed by the yacc parser).

% Why use regexps and not automaton directly? Because regexps are more
% convenient, more declarative! And they can be then compiled 
% to efficient code.


\section{Core data structures}

\subsection{Regular expression}

<<type Syntax.regular_expression>>=
type regular_expression =
    Epsilon
  | Characters of char_ list
  | Sequence of regular_expression * regular_expression
  | Alternative of regular_expression * regular_expression
  | Repetition of regular_expression
@

% Characters could be a combination of Alternative and Char 
% but maybe useful optimisation?

% so 'X?' is really 'X | Epsilon' and 'X+' is 'X X*'

<<type Syntax.char_>>=
type char_ = int
@
%pad: I added this type.
% why 'int list'? why not 'char list'? because we will use
% eof as a special marker with a special value (256). 
% (But maybe could have used 'char option' where None = eof)

%TODO: toy example? and its dump? unit test (parse string) = AST value?

\subsection{Abstract syntax tree}

<<type Syntax.lexer_definition>>=
type lexer_definition =
    { header: location;
      entrypoints: rule list;
      trailer: location 
    }
@
% have more power than just regexps when can have different entry points! can
% actually do stuff that count! see comments below.
% the list is a kind of Alternative, but it's alternatives with different
% actions.

% in the original lex the rule list is implemented via different
% "prefix" e.g. <COMMENT>/ ... INITIAL/... but ocamllex way
% is cleaner I think. Just reuse a concept we already know, function!

<<type Syntax.location>>=
type location =
    Location of charpos * charpos
@
% header/trailer/action slice
%(there are so many use of int that it's better to differentiate them)

<<type Syntax.charpos>>=
type charpos = int
@
%pad: I added this


<<type Syntax.rule>>=
type rule = string * (regular_expression * action) list
@
%pad: I added this
%less: could have type case = regular_regular * action and so case list

<<type Syntax.action>>=
type action = location
@
%pad: I added this


%TODO: toy example? and its dump? unit test (parse string) = AST value?


\subsection{Automata}

% An automata is a set of initial states corresponding to the
% different rules, and a shared transition table, which
% is a kind of matrix

<<type Lexgen.automata_entry>>=
(* Representation of entry points *)

type automata_entry =
  { auto_name: string;
    auto_initial_state: int;
    auto_actions: (action_id * Syntax.action) list;
  }
@
%less: automata_rule? instead of entry? 

% could do type state_id = int; but by now
% we have refined all int types so the remaining ints
% should be all about automaton state ids.

<<type Lexgen.action_id>>=
type action_id = int
@
%pad: I added this

<<type Lexgen.automata_matrix>>=
(* indexed by state number *)
type automata_matrix = automata_row array
@
%pad: I added this

<<type Lexgen.automata>>=
type automata_row =
    Perform of action_id
  (* indexed by an integer between 0 and 256(eof), that is a char_ *)
  | Shift of automata_trans * automata_move array 
@
%old: was automata, but automata_row is more accurate
%val make_dfa: Syntax.lexer_definition -> automata_entry list * automata_matrix

% So array of array when have Shift. Kinda of a matrix with
% row = state and col = character.

%FIGURE: TODO where show example of matrix for a simple regexp/automata

<<type Lexgen.automata_trans>>=
and automata_trans =
    No_remember
  | Remember of action_id
@
% when there is a rule like "if" { Tif } | ['a'-'z']+ { Tident }
% then once you've read "if", you don't know yet, you still
% need to "shift" but remember that there was a possible
% action if we have to backtrack

<<type Lexgen.automata_move>>=
and automata_move =
    Backtrack
  | Goto of int
@
% int is another state here, and the index in automata_move array
% is character


% need backtrak, because need lookahead(1) because when
% see "then" you don't know yet if you want to trigger the
% action related to 'then' or look for another character because it
% could be an identifier like 'thenbla'

%TODO: dump example for calc/lexer.mll


\subsection{Runtime lexbuf}

% useful to understand how things work internally,
% but also because the action will need to understand partially lexbuf
% (at least use the Lexeme.xxx API functions)

<<type Lexing.lexbuf>>=
(* The run-time library for lexers generated by camllex *)
(* The type of lexer buffers. A lexer buffer is the argument passed
   to the scanning functions defined by the generated scanners.
   The lexer buffer holds the current state of the scanner, plus
   a function to refill the buffer from the input. *)
type lexbuf =
  { refill_buff : lexbuf -> unit;

    mutable lex_buffer : string;
    mutable lex_buffer_len : int;

    mutable lex_abs_pos : int;
    mutable lex_start_pos : int;
    mutable lex_curr_pos : int;
    mutable lex_last_pos : int;

    mutable lex_last_action : int;
    mutable lex_eof_reached : bool 
  }
@

%less: put the Lexeme.xxx API functions here?

\section{[[main()]]}

<<toplevel Main._1>>=
let _ = 
  Printexc.catch main (); 
  exit 0
@


<<function Main.main>>=
let main () =
  <<[[Main.main()]] print lex usage if wrong number of arguments>>
  let source_name = Sys.argv.(1) in
  let dest_name =
    if Filename.check_suffix source_name ".mll" 
    then Filename.chop_suffix source_name ".mll" ^ ".ml"
    else source_name ^ ".ml" 
  in
  let ic = open_in source_name in
  let oc = open_out dest_name in
  let lexbuf = Lexing.from_channel ic in

  (* parsing *)
  let def =
    try
      Parser.lexer_definition Lexer.main lexbuf
    with exn ->
      close_out oc;
      Sys.remove dest_name;
       <<[[Main.main()]] report error exn>>
      exit 2 
  in
  (* compiling *)
  let (entries, transitions) = Lexgen.make_dfa def in
  (* optimizing *)
  let tables = Compact.compact_tables transitions in
  (* generating *)
  Output.output_lexdef ic oc def.header tables entries def.trailer;

  close_in ic;
  close_out oc
@

% Lexer.main :) in next section

% Parser.lexer_definition in next next section

<<signature Lexgen.make_dfa>>=
(* The entry point *)

val make_dfa: Syntax.lexer_definition -> automata_entry list * automata_matrix
@

<<signature Compact.compact_tables>>=
val compact_tables: Lexgen.automata_matrix -> lex_tables
@
% will see lex_tables later


<<signature Output.output_lexdef>>=
(* Output the DFA tables and its entry points *)

val output_lexdef:
      in_channel -> out_channel ->
      Syntax.location (* header *) ->
      Compact.lex_tables ->
      Lexgen.automata_entry list ->
      Syntax.location (* trailer *) ->
      unit
@


\section{Lexing}
% :) self-reference
% another example of use after calc/lexer.mll :)

%less: might have been better to not use lex so at least
% can see an alternative way to write lexer, that is by hand
% which can help also to understand and better appreciate
% certain things.

% this is the lexer definition to lex lexer definitions ... hmmm

<<lex/lexer.mll>>=
<<copyright ocamllex>>
(* The lexical analyzer for lexer definitions. Bootstrapped! *)

{
open Syntax
open Parser

exception Lexical_error of string

(* Auxiliaries for the lexical analyzer *)
<<Lexer helper functions and globals>>
}

<<rule Lexer.main>>

<<rule Lexer.action>>

<<rule Lexer.string>>
      
<<rule Lexer.comment>>
@
% can see header, rules, and here actually no trailer.
%    { header: location;
%      entrypoints: (string * rule) list;
%      trailer: location 
%less: no 'let', could be nice to use to illustrate all features :)


% the tokens are defined in parser.mly, but they could be defined in the header
% (like we do in efuns/prog_modes/ocaml_mode.mll for instance)
<<type Parser.token>>=
%token Trule Tparse Tand
%token <int> Tchar
%token <string> Tstring
%token Tstar Tmaybe Tplus Tor Tlparen Trparen 
%token Tlbracket Trbracket Tcaret Tdash 
%token Tunderscore Teof
%token <Syntax.location> Taction
%token Tlet Tequal 
%token <string> Tident
%token Tend  
@
% see parser.mly, it expands in a type token = TIdent of string | ...

<<rule Lexer.main>>=
rule main = parse
  <<[[Lexer.main()]] space case>>
  <<[[Lexer.main()]] comment case>>
  <<[[Lexer.main()]] keyword or identifier case>>
  <<[[Lexer.main()]] string start case>>
  <<[[Lexer.main()]] character cases>>
  <<[[Lexer.main()]] operator cases>>
  <<[[Lexer.main()]] action case>>
  | eof  { Tend }
  | _
    { raise(Lexical_error
             ("illegal character " ^ String.escaped(Lexing.lexeme lexbuf))) }
@

% can see the list inner list
%      entrypoints: (string * (regular_expression * location) list) list;



\subsection{Comments}

<<[[Lexer.main()]] space case>>=
  [' ' '\010' '\013' '\009' '\012' ] + 
  { main lexbuf }
@
% C-j, C-m, tab, C-l

<<[[Lexer.main()]] comment case>>=
| "(*" 
  { comment_depth := 1;
    comment lexbuf;
    main lexbuf }
@

<<Lexer helper functions and globals>>=
let comment_depth = ref 0
@

% regexps can't count, but by playing with multiple
% rules and recursion you can! so good that lex
% is not just about regexps.

<<rule Lexer.comment>>=
and comment = parse
    "(*" 
    { incr comment_depth; comment lexbuf }
  | "*)" 
    { decr comment_depth;
      if !comment_depth == 0 then () else comment lexbuf }

  | eof 
    { raise(Lexical_error "unterminated comment") }
  | _ 
    { comment lexbuf }
@


% not sure we need that. Actually I find it annoying
%  | '"' 
%    { reset_string_buffer();
%      string lexbuf;
%      reset_string_buffer();
%      comment lexbuf }
%  | "''"
%      { comment lexbuf }
%  | "'" [^ '\\' '\''] "'"
%      { comment lexbuf }
%  | "'\\" ['\\' '\'' 'n' 't' 'b' 'r'] "'"
%      { comment lexbuf }
%  | "'\\" ['0'-'9'] ['0'-'9'] ['0'-'9'] "'"
%      { comment lexbuf }



\subsection{Keywords and identifiers}

<<[[Lexer.main()]] keyword or identifier case>>=
| ['A'-'Z' 'a'-'z'] ['A'-'Z' 'a'-'z' '\'' '_' '0'-'9'] *
  { match Lexing.lexeme lexbuf with
    | "rule" -> Trule
    | "parse" -> Tparse
    | "and" -> Tand
    | "eof" -> Teof
    | "let" -> Tlet
    | s -> Tident s 
   }
@
% have seen all the keywords (except let)

% could have also actually written as
% | "rule" -> TRule
% ...
% | ['A'-'Z' 'a'-'z'] ...
% but this generates far more states (but maybe it's also more efficient)

%later: there is a 'shortest' keyword in recent ocamllex, probably
% it's like the ? perl greedy operator

%TODO: 'as' is useful

\subsection{Operators}

<<[[Lexer.main()]] operator cases>>=
| '*'  { Tstar }
| '|'  { Tor }
@
% regexp! see AST correspondance!
% how is sorted priority? in ab*, the star is just for 'b' or for ab?

<<[[Lexer.main()]] operator cases>>=
| '?'  { Tmaybe }
| '+'  { Tplus }
@
% desugared in a a* and  epsilon | a

<<[[Lexer.main()]] operator cases>>=
| '('  { Tlparen }
| ')'  { Trparen }
@



<<[[Lexer.main()]] operator cases>>=
| '['  { Tlbracket }
| ']'  { Trbracket }
| '-'  { Tdash }
| '^'  { Tcaret }
@
% range

<<[[Lexer.main()]] operator cases>>=
| '_'  { Tunderscore }
@



<<[[Lexer.main()]] operator cases>>=
| '='  { Tequal }
@
% for naming

\subsection{Strings}

<<[[Lexer.main()]] string start case>>=
| '"' 
  { reset_string_buffer();
    string lexbuf;
    Tstring(get_stored_string()) }
@

% use Buffer instead? or just use ^ like I was doing originally?
% actually that's what they do in recent ocamllex

<<Lexer helper functions and globals>>=
let initial_string_buffer = String.create 256
let string_buff = ref initial_string_buffer
let string_index = ref 0
@

<<Lexer helper functions and globals>>=
let reset_string_buffer () =
  string_buff := initial_string_buffer;
  string_index := 0
@

<<Lexer helper functions and globals>>=
let get_stored_string () =
  String.sub !string_buff 0 !string_index
@


<<rule Lexer.string>>=
and string = parse
    '"' 
    { () }
  | '\\' [' ' '\010' '\013' '\009' '\026' '\012'] +
    { string lexbuf }
  | '\\' ['\\' '"' 'n' 't' 'b' 'r'] 
    { store_string_char(char_for_backslash(Lexing.lexeme_char lexbuf 1));
      string lexbuf }
  | '\\' ['0'-'9'] ['0'-'9'] ['0'-'9'] 
    { store_string_char(char_for_decimal_code lexbuf 1);
      string lexbuf }
  | eof 
    { raise(Lexical_error "unterminated string") }
  | _ 
    { store_string_char(Lexing.lexeme_char lexbuf 0);
      string lexbuf }
@
% multiline handling with the \ <RETURN> ?
% 10 = C-j, 13 = C-m, 12 = C-l (used in emacs somtimes, supposed to be
%  a page limit or something)
%less: need %9 = Tab? need 12? 26 = C-z, need?

<<Lexer helper functions and globals>>=
let store_string_char c =
  if !string_index >= String.length !string_buff then begin
    let new_buff = String.create (String.length !string_buff * 2) in
    String.blit !string_buff 0 new_buff 0 (String.length !string_buff);
    string_buff := new_buff
  end;
  !string_buff.[!string_index] <- c;
  incr string_index
@


<<Lexer helper functions and globals>>=
let char_for_backslash = function
    'n' -> '\n'
  | 't' -> '\t'
  | 'b' -> '\b'
  | 'r' -> '\r'
  | c   -> c
@

<<Lexer helper functions and globals>>=
let char_for_decimal_code lexbuf i =
  Char.chr(100 * (Char.code(Lexing.lexeme_char lexbuf i) - 48) +
            10 * (Char.code(Lexing.lexeme_char lexbuf (i+1)) - 48) +
                 (Char.code(Lexing.lexeme_char lexbuf (i+2)) - 48))
@

\subsection{Characters}

<<[[Lexer.main()]] character cases>>=
| "'" [^ '\\'] "'" 
  { Tchar(Char.code(Lexing.lexeme_char lexbuf 1)) }
| "'" '\\' ['\\' '\'' 'n' 't' 'b' 'r'] "'" 
  { Tchar(Char.code(char_for_backslash (Lexing.lexeme_char lexbuf 2))) }
| "'" '\\' ['0'-'9'] ['0'-'9'] ['0'-'9'] "'" 
  { Tchar(Char.code(char_for_decimal_code lexbuf 2)) }
@

\subsection{Actions}

<<[[Lexer.main()]] action case>>=
| '{' 
  { let n1 = Lexing.lexeme_end lexbuf in
    brace_depth := 1;
    let n2 = action lexbuf in
    Taction(Location(n1, n2)) }
@


<<Lexer helper functions and globals>>=
let brace_depth = ref 0
@
% need to count them, because the code in action can use '}'

<<rule Lexer.action>>=
and action = parse
    '{' 
    { incr brace_depth;
      action lexbuf }
  | '}' 
    { decr brace_depth;
      if !brace_depth == 0 then Lexing.lexeme_start lexbuf else action lexbuf }
  | "(*" 
    { comment_depth := 1;
      comment lexbuf;
      action lexbuf }
  | eof 
    { raise (Lexical_error "unterminated action") }
  | _ 
    { action lexbuf }
@

% not sure you need that, there is anyway no } in strings usually.
%  | '"' 
%    { reset_string_buffer();
%      string lexbuf;
%      reset_string_buffer();
%      action lexbuf }
%  | "'" [^ '\\'] "'" 
%    { action lexbuf }
%  | "'" '\\' ['\\' '\'' 'n' 't' 'b' 'r'] "'" 
%    { action lexbuf }
%  | "'" '\\' ['0'-'9'] ['0'-'9'] ['0'-'9'] "'" 
%    { action lexbuf }

% we need the comment stuff though because the action token is
% also used for the header/trailer and they may contain some comments
% with some } we don't want to consider as the end of the action.

\section{Parsing}

% good exercise for next chapter :)

\subsection{Overview}

<<lex/parser.mly>>=
<<copyright ocamllex bis>>

/* The grammar for lexer definitions */

%{
open Syntax

(* Auxiliaries for the parser. *)
<<parser helper functions and globals>>

%}

/* Tokens */

<<type Parser.token>>

/* Precedences and associativities. Lower precedences come first. */

<<Parser precedences and associativities>>

/* Entry points */

<<Parser entry points types>>

%%

<<grammar>>

%%
@
% header, rules, trailer


<<grammar>>=
<<lex top rule>>

<<lex header rule>>

<<lex named regexp rule>>

<<lex rule rule>>

<<lex regexp rule>>

@

% rule rule :) meta

% no action rule as it's actually just a token

\subsection{Lexer definition entry point}

<<Parser entry points types>>=
%start lexer_definition
%type <Syntax.lexer_definition> lexer_definition
@

<<lex top rule>>=
lexer_definition:
    header named_regexps Trule definition other_definitions header Tend
        { { header = $1;
            entrypoints = $4 :: List.rev $5;
            trailer = $6
           } 
         }
;
@

\subsection{Header and trailer}

<<lex header rule>>=
header:
    Taction      { $1 }
  | /*epsilon*/  { Location(0,0) }
;
@
%$ 
%an action! something between { }

\subsection{Rule}

% rule X = parse ...
<<lex rule rule>>=
definition:
    Tident Tequal entry
        { ($1,$3) }
;
entry:
    Tparse case rest_of_entry
        { $2::List.rev $3 }
  | Tparse rest_of_entry
        { List.rev $2 }
;
case:
    regexp Taction
        { ($1,$2) }
;


other_definitions:
    other_definitions Tand definition
        { $3::$1 }
  | /*epsilon*/
        { [] }
;
rest_of_entry:
    rest_of_entry Tor case
        { $3::$1 }
  | /*epsilon*/
        { [] }
;
@
%$


%less: in recent ocamllex the lexer can take more arguments, which is
% useful, for instance in my PHP lexer I pass the beginning symbol
% so that the rule called know what to look for to close it.

\subsection{Regexps}

\subsubsection{Basic regexps}

<<Parser precedences and associativities>>=
%left Tor
%left CONCAT
%nonassoc Tmaybe
%left Tstar
%left Tplus
@

% left Tstar?? but it's not a binary operator

<<lex regexp rule>>=
regexp:
    Tchar
        { Characters [$1] }
  | regexp Tstar
        { Repetition $1 }
  | regexp Tor regexp
        { Alternative($1,$3) }
  | regexp regexp %prec CONCAT
        { Sequence($1,$2) }
  | Tlparen regexp Trparen
        { $2 }
  <<rule regexp cases>>
;
@
%$
% need this %prec?

\subsubsection{Sugar regexps}

<<rule regexp cases>>=
  | regexp Tmaybe
        { Alternative($1, Epsilon) }
  | regexp Tplus
        { Sequence($1, Repetition $1) }
@
%$

\subsubsection{Range regexps}

<<rule regexp cases>>=
  | Tlbracket char_class Trbracket
        { Characters $2 }
@

<<lex regexp rule>>=
char_class:
    Tcaret char_class1
        { subtract all_chars $2 }
  | char_class1
        { $1 }
;
char_class1:
    Tchar Tdash Tchar
        { char_class $1 $3 }
  | Tchar
        { [$1] }
  | char_class1 char_class1 %prec CONCAT
        { $1 @ $2 }
;

@
%$

<<parser helper functions and globals>>=
let char_class c1 c2 =
  let rec cl n =
    if n > c2 then [] else n :: cl(succ n)
  in cl c1
@

<<parser helper functions and globals>>=
let all_chars = char_class 0 255
@
% but not eof!

<<parser helper functions and globals>>=
let rec subtract l1 l2 =
  match l1 with
    [] -> []
  | a::r -> if List.mem a l2 then subtract r l2 else a :: subtract r l2
@


\subsubsection{Named regexps}
% aliases

<<parser helper functions and globals>>=
let named_regexps =
  (Hashtbl.create 13 : (string, regular_expression) Hashtbl.t)
@

<<rule regexp cases>>=
  | Tident
        { try
            Hashtbl.find named_regexps $1
          with Not_found ->
            prerr_string "Reference to unbound regexp name `";
            prerr_string $1;
            prerr_string "' at char ";
            prerr_int (Parsing.symbol_start());
            prerr_newline();
            exit 2 }
@

<<lex named regexp rule>>=
named_regexps:
    named_regexps Tlet Tident Tequal regexp
        { Hashtbl.add named_regexps $3 $5 }
  | /*epsilon*/
        { () }
;
@


\subsubsection{Other regexps}

<<rule regexp cases>>=
  | Tstring
        { regexp_for_string $1 }
@
%$

<<parser helper functions and globals>>=
let regexp_for_string s =
  let rec re_string n =
    if n >= String.length s then Epsilon
    else if succ n = String.length s then Characters([Char.code (s.[n])])
    else Sequence(Characters([Char.code (s.[n])]), re_string (succ n))
  in re_string 0
@

<<rule regexp cases>>=
  | Tunderscore
        { Characters all_chars }
  | Teof
        { Characters [256] }
@
% which is why we use 'int' and not 'char'

\section{Compiling}

% ok so now have the AST of a lexer definition, which are
% multiple "entries" with different cases with regexps and actions.

<<function Lexgen.make_dfa>>=
let make_dfa lexdef =
  let (charsets, lex_entries) = 
    encode_lexdef lexdef in
  let (automata_entries, automata_transitions) = 
    encode_lexentries charsets lex_entries in
  automata_entries, automata_transitions
@
%old: was actions ... but confusing with Syntax.action

% each "entry" gets a different automata_entry, but they all share
% the same automata transitions (=~ matrix).

\subsection{Normalized regexps}
% Normalized ... hmmm

% goal here is to generate a single regexp for the
% different cases of a single rule, and to put the actions
% in this regexp.


\subsubsection{Intermediate regexp format}

<<type Lexgen.regexp>>=
(* Deep abstract syntax for regular expressions *)

type regexp =
    Empty
  | Chars of charset_id
  | Action of action_id
  | Seq of regexp * regexp
  | Alt of regexp * regexp
  | Star of regexp
@
% diff with regular_expression? This is one "unified" regexp where
% the different cases of a rule has been merged and
% where the actions are also merged in! One step closer to
% the unified automaton and action dispatcher we will see later.

<<type Lexgen.lexer_entry>>=
type lexer_entry =
  { lex_name: string;
    lex_regexp: regexp;
    lex_actions: (action_id * Syntax.action) list;
  }
@

<<type Lexgen.charset_id>>=
type charset_id = int
@
%pad: I added this


<<constant Lexgen.chars>>=
let chars = ref ([] : char_ list list)
@
<<constant Lexgen.chars_count>>=
let chars_count = ref (0: charset_id)
@


<<constant Lexgen.actions>>=
let actions = ref ([] : (action_id * Syntax.location) list)
@
% mapping from id to action. really an assoc.
<<constant Lexgen.actions_count>>=
let actions_count = ref (0: action_id)
@
% as analyze the casedef with the alternatives,
% need to remember the action to run when reach a certain
% "state"

\subsubsection{[[encode_lexdef()]]}

<<function Lexgen.encode_lexdef>>=
let encode_lexdef def =
  chars := [];
  chars_count := 0;
  let entries =
    def.entrypoints |> List.map (fun (entry_name, casedef) ->
        actions := [];
        actions_count := 0;
        let re = encode_casedef casedef in
        { lex_name = entry_name;
          lex_regexp = re;
          lex_actions = List.rev !actions }
     )
  in
  (* map a charset_id to a charset *)
  let charsets = Array.of_list (List.rev !chars) in
  (charsets, entries)
@
%old: at the end, but redundant I think
%  chars := [];
%  actions := []; (* needed?? *)

% a bit ugly those globals, maybe the function could return
% multiple things

% so will return different entries which contain some
% references to actions in lex_actions or charsets in charsets


\subsubsection{[[encode_casedef()]]}

<<function Lexgen.encode_casedef>>=
let encode_casedef casedef =
  casedef |> List.fold_left (fun reg (re, action) ->
     let act_num = !actions_count in
     incr actions_count;
     actions := (act_num, action) :: !actions;
     Alt(reg, Seq(encode_regexp re, Action act_num))
  ) Empty
@

\subsubsection{[[encode_regexp()]]}

<<function Lexgen.encode_regexp>>=
let rec encode_regexp = function
    Epsilon -> Empty
  | Characters cl ->
      let n = !chars_count in
      incr chars_count;
      chars := cl :: !chars;
      Chars(n)
  | Sequence(r1,r2) ->
      Seq(encode_regexp r1, encode_regexp r2)
  | Alternative(r1,r2) ->
      Alt(encode_regexp r1, encode_regexp r2)
  | Repetition r ->
      Star (encode_regexp r)
@
% cl are normalized? sorted?

\subsection{DFA}

% Deterministic Finite Automaton.
% Idea is that have states, and edges/transitions.

%(* To generate directly a NFA from a regular expression.
%   Confer Aho-Sethi-Ullman, dragon book, chap. 3 *)
%apparently in latest ocamllex they optimized even more and use 
% "Extension to tagged automata - Ville Larikari"
%less: bad comment? he means DFA not NFA?


<<type Lexgen.transition>>=
type transition =
    OnChars of charset_id
  | ToAction of action_id
@

<<type Lexgen.state>>=
type state = transition Set.t
@
% so a state is a place where you are ready to transition over lots 
% of charsets and to perform different actions, and so it's actually
% represented as a set of transition you can do from.
% A macrostate. Remember the NFA to DFA algorithm where you
% merge different state in a union state.
% Convenient because then to know the transition to do from there,
% just look at the value of the state.


<<function Lexgen.reset_state_mem>>=
let reset_state_mem () =
  state_map := Map.empty;
  Stack.clear todo;
  next_state_num := 0
@

<<constant Lexgen.state_map>>=
let state_map = ref (Map.empty: (state, int) Map.t)
@



% idea of NFA to DFA is to create macrostates so if
% can from the start either go to state 1 or state 2 reading 'a'
% then you create a macro state 1_and_2.
% similar here?

<<constant Lexgen.todo>>=
let todo = (Stack.create() : (state * int) Stack.t)
@

<<constant Lexgen.next_state_num>>=
let next_state_num = ref 0
@



<<function Lexgen.encode_lexentries>>=
let encode_lexentries charsets lexentries =
  reset_state_mem();

  (* pass 1 on lexentries, get the initial states *)
  let automata_entries =
    lexentries |> List.map (fun entry ->
        { auto_name    = entry.lex_name;
          auto_actions = entry.lex_actions;
          (* get_state() will populate todo and state_map globals *)
          auto_initial_state = get_state (firstpos entry.lex_regexp);
        }
     )
  in
  (* pass 2 on lexentries, get the transitions *)

  let follow = followpos (Array.length charsets) lexentries in
  let states = 
    map_on_all_states (fun st -> translate_state charsets follow st) 
  in
  let transitions = Array.create !next_state_num (Perform 0) in
  states |> List.iter (fun (act, i) -> transitions.(i) <- act);
  (automata_entries, transitions)
@
%note: was 'chars' but prefer charsets
%old:  reset_state_mem(); at the end, but redundant I think
%old: was actions, but again confusing

\subsubsection{Initial states}

% pos? meh
% but Dragon book used those function names

% remember that lexdef is a unified regexp so all the cases
% have been merged as an Alt so firstpos will return
% a big set for the beginning.

<<function Lexgen.firstpos>>=
let rec firstpos = function
    Empty      -> Set.empty
  | Chars pos  -> Set.add (OnChars pos) Set.empty
  | Action act -> Set.add (ToAction act) Set.empty
  | Seq(r1,r2) -> if nullable r1
                  then Set.union (firstpos r1) (firstpos r2)
                  else firstpos r1
  | Alt(r1,r2) -> Set.union (firstpos r1) (firstpos r2)
  | Star r     -> firstpos r
@

<<function Lexgen.nullable>>=
let rec nullable = function
    Empty      -> true
  | Chars _    -> false
  | Action _   -> false
  | Seq(r1,r2) -> nullable r1 && nullable r2
  | Alt(r1,r2) -> nullable r1 || nullable r2
  | Star r     -> true
@

<<function Lexgen.get_state>>=
let get_state st = 
  try
    Map.find st !state_map
  with Not_found ->
    let num = !next_state_num in
    incr next_state_num;
    state_map := Map.add st num !state_map;
    Stack.push (st, num) todo;
    num
@

% so if regexp = "ab"
% then firstpos = OnChars 'a' which will then be mapped to
% a state 0?

% what todo is for? see below.

\subsubsection{Transitions}

<<function Lexgen.map_on_all_states>>=
let map_on_all_states f =
  let res = ref [] in
  begin try
    while true do
      let (st, i) = Stack.pop todo in
      let r = f st in
      res := (r, i) :: !res
    done
  with Stack.Empty -> ()
  end;
  !res
@

<<function Lexgen.translate_state>>=
let translate_state charsets follow = 
 fun state ->
  match split_trans_set state with
    (n, []) -> Perform n
  | (n, ps) -> Shift((if n = no_action then No_remember else Remember n),
                     transition_from charsets follow ps)
@

% split transition set, one action and a union of charsets
<<function Lexgen.split_trans_set>>=
let split_trans_set trans_set =
  Set.fold (fun trans (act, pos_set as act_pos_set) ->
      match trans with
        OnChars pos -> (act, pos :: pos_set)
      | ToAction act1 -> if act1 < act then (act1, pos_set) else act_pos_set
   ) trans_set (no_action, [])
@
% first action has priority!!! lexing rule! the order matters!
% so if do  "if" and later an ident rule, then the first
% action will be executed

<<constant Lexgen.no_action>>=
let no_action = (max_int: action_id)
@
% use max_int because do < above.


% the meat! finally!
% why 257? 0-255 for chars, then 256 for eof! so length = 257.
<<function Lexgen.transition_from>>=
let transition_from charsets follow pos_set = 
  let tr    = Array.create 257 Set.empty in
  pos_set |> List.iter (fun pos ->
     charsets.(pos) |> List.iter (fun c ->
           tr.(c) <- Set.union tr.(c) follow.(pos)
      )
  );

  let shift = Array.create 257 Backtrack in
  for i = 0 to 256 do
    shift.(i) <- goto_state tr.(i)
  done;
  shift
@

<<function Lexgen.goto_state>>=
let goto_state st =
  if Set.is_empty st 
  then Backtrack 
  (* can create a new state in todo *)
  else Goto (get_state st)
@


<<function Lexgen.followpos>>=
let followpos size_charsets entries =
  let v = Array.create size_charsets Set.empty in
  let fill_pos first = function
      OnChars pos -> v.(pos) <- Set.union first v.(pos)
    | ToAction _  -> () 
  in
  let rec fill = function
      Seq(r1,r2) ->
        fill r1; fill r2;
        Set.iter (fill_pos (firstpos r2)) (lastpos r1)
    | Alt(r1,r2) ->
        fill r1; fill r2
    | Star r ->
        fill r;
        Set.iter (fill_pos (firstpos r)) (lastpos r)
    | _ -> () in
  entries |> List.iter (fun entry -> fill entry.lex_regexp);
  v
@
%TODOL ??? need the Dragon Book I think.


<<function Lexgen.lastpos>>=
let rec lastpos = function
    Empty      -> Set.empty
  | Chars pos  -> Set.add (OnChars pos) Set.empty
  | Action act -> Set.add (ToAction act) Set.empty
  | Seq(r1,r2) -> if nullable r2
                  then Set.union (lastpos r1) (lastpos r2)
                  else lastpos r2
  | Alt(r1,r2) -> Set.union (lastpos r1) (lastpos r2)
  | Star r     -> lastpos r
@


\section{Generating}

%val output_lexdef:
%      in_channel -> out_channel ->
%      Syntax.location (* header *) ->
%      Compact.lex_tables ->
%      Lexgen.automata_entry list ->
%      Syntax.location (* trailer *) ->
%      unit

%TODO: how hard to generate instead for C code? so can replace
% Unix lex!
%TODO: can simplify and use automata_matrix instead of compacted
% lex_tables? and use ocaml code to run the engine?

% pretty simple, header, shared tables, rules, trailer
<<function Output.output_lexdef>>=
(* Main output function *)

let output_lexdef ic oc header tables entry_points trailer =
  <<[[Output.output_lexdef()]] print statistics>>
  copy_chunk ic oc header;
  output_tables oc tables;
  <<[[Output.output_lexdef()]] generate entry points>>
  copy_chunk ic oc trailer
@


<<[[Output.output_lexdef()]] print statistics>>=
Printf.printf "%d states, %d transitions, table size %d bytes\n"
  (Array.length tables.tbl_base)
  (Array.length tables.tbl_trans)
  (2 * (Array.length tables.tbl_base + Array.length tables.tbl_backtrk +
        Array.length tables.tbl_default + Array.length tables.tbl_trans +
        Array.length tables.tbl_check));
flush stdout;
@

\subsection{Entry points}

<<[[Output.output_lexdef()]] generate entry points>>=
(match entry_points with
  [] -> ()
| entry1 :: entries ->
    output_string oc "let rec "; 
    output_entry ic oc entry1;
    entries |> List.iter (fun e -> 
      output_string oc "and "; 
      output_entry ic oc e
    )
);
@

<<function Output.output_entry>>=
(* Output the entries *)

let output_entry ic oc e =
  fprintf oc "%s lexbuf = %s_rec lexbuf %d\n"
          e.auto_name e.auto_name e.auto_initial_state;
  fprintf oc "and %s_rec lexbuf state =\n" e.auto_name;
  fprintf oc "  match Lexing.engine lex_tables state lexbuf with\n    ";
  let first = ref true in
  e.auto_actions |> List.iter (fun (num, loc_action) ->
      if !first 
      then first := false 
      else fprintf oc "  | ";
      fprintf oc "%d -> (" num;
      copy_chunk ic oc loc_action;
      fprintf oc ")\n"
  );
  fprintf oc "  | n -> lexbuf.Lexing.refill_buff lexbuf; %s_rec lexbuf n\n\n"
          e.auto_name
@

\subsection{Shared transition table}

<<function Output.output_tables>>=
(* Output the tables *)

let output_tables oc tbl =
  output_string oc "let lex_tables = {\n";
  fprintf oc "  Lexing.lex_base = \n%a;\n" output_array tbl.tbl_base;
  fprintf oc "  Lexing.lex_backtrk = \n%a;\n" output_array tbl.tbl_backtrk;
  fprintf oc "  Lexing.lex_default = \n%a;\n" output_array tbl.tbl_default;
  fprintf oc "  Lexing.lex_trans = \n%a;\n" output_array tbl.tbl_trans;
  fprintf oc "  Lexing.lex_check = \n%a\n" output_array tbl.tbl_check;
  output_string oc "}\n\n"
@

<<function Output.output_array>>=
let output_array oc v =
  output_string oc "   \"";
  for i = 0 to Array.length v - 1 do
    output_byte oc (v.(i) land 0xFF);
    output_byte oc ((v.(i) asr 8) land 0xFF);
    if i land 7 = 7 then output_string oc "\\\n    "
  done;
  output_string oc "\""
@

<<function Output.output_byte>>=
(* To output an array of short ints, encoded as a string *)

let output_byte oc b =
  output_char oc '\\';
  output_char oc (Char.chr(48 + b / 100));
  output_char oc (Char.chr(48 + (b / 10) mod 10));
  output_char oc (Char.chr(48 + b mod 10))
@

\subsection{Chunks}

<<function Output.copy_chunk>>=
let copy_chunk ic oc (Location(start,stop)) =
  seek_in ic start;
  let n = ref (stop - start) in
  while !n > 0 do
    let m = input ic copy_buffer 0 (min !n 1024) in
    output oc copy_buffer 0 m;
    n := !n - m
  done
@

<<constant Output.copy_buffer>>=
(* To copy the ML code fragments *)

let copy_buffer = String.create 1024
@


%<<constant Output.copy_chunk>>=
%let copy_chunk =
%  match Sys.os_type with
%  | _       -> copy_chunk_unix
%@


\section{Running}











%###############################################################################


\chapter{Yacc}

% essentially a DSL to go from grammar -> pushdown automaton

\chapter{Advanced Features}

\section{[[left]], [[right]], [[assoc]]}

\chapter{Optimisations}

\section{Lex}

\subsection{Compacted automata}

<<type Compact.lex_tables>>=
(* Compaction of an automata *)

type lex_tables =
  { tbl_base: int array;                 (* Perform / Shift *)
    tbl_backtrk: int array;              (* No_remember / Remember *)
    tbl_default: int array;              (* Default transition *)

    tbl_trans: int array;                (* Transitions (compacted) *)
    tbl_check: int array }               (* Check (compacted) *)
@
%val compact_tables: Lexgen.automata_matrix -> lex_tables

\subsection{[[compact_tables()]]}


<<function Compact.compact_tables>>=
let compact_tables state_v =
  let n = Array.length state_v in
  let base = Array.create n 0 in
  let backtrk = Array.create n (-1) in
  let default = Array.create n 0 in
  for i = 0 to n - 1 do
    match state_v.(i) with
      Perform n ->
        base.(i) <- -(n+1)
    | Shift(trans, move) ->
        (match trans with
          No_remember -> ()
        | Remember n -> backtrk.(i) <- n
        );
        let (b, d) = pack_moves i move in
        base.(i) <- b;
        default.(i) <- d
  done;
  { tbl_base = base;
    tbl_backtrk = backtrk;
    tbl_default = default;
    tbl_trans = Array.sub !trans 0 !last_used;
    tbl_check = Array.sub !check 0 !last_used }
@


<<function Compact.pack_moves>>=
let pack_moves state_num move_t =
  let move_v = Array.create 257 0 in
  for i = 0 to 256 do
    move_v.(i) <-
      (match move_t.(i) with
        Backtrack -> -1
      | Goto n -> n)
  done;
  let default = most_frequent_elt move_v in
  let nondef = non_default_elements default move_v in
  let rec pack_from b =
    while b + 257 > Array.length !trans do grow_transitions() done;
    let rec try_pack = function
      [] -> b
    | (pos, v) :: rem ->
        if !check.(b + pos) = -1 then try_pack rem else pack_from (b+1) in
    try_pack nondef in
  let base = pack_from 0 in
  List.iter
    (fun (pos, v) ->
      !trans.(base + pos) <- v;
      !check.(base + pos) <- state_num)
    nondef;
  if base + 257 > !last_used then last_used := base + 257;
  (base, default)
@

<<function Compact.grow_transitions>>=
let grow_transitions () =
  let old_trans = !trans
  and old_check = !check in
  let n = Array.length old_trans in
  trans := Array.create (2*n) 0;
  Array.blit old_trans 0 !trans 0 !last_used;
  check := Array.create (2*n) (-1);
  Array.blit old_check 0 !check 0 !last_used
@

<<function Compact.non_default_elements>>=
(* Transform an array into a list of (position, non-default element) *)

let non_default_elements def v =
  let rec nondef i =
    if i >= Array.length v then [] else begin
      let e = v.(i) in
      if e = def then nondef(i+1) else (i, e) :: nondef(i+1)
    end in
  nondef 0
@

<<function Compact.most_frequent_elt>>=
(* Determine the integer occurring most frequently in an array *)

let most_frequent_elt v =
  let frequencies = Hashtbl.create 17 in
  let max_freq = ref 0 in
  let most_freq = ref (v.(0)) in
  for i = 0 to Array.length v - 1 do
    let e = v.(i) in
    let r =
      try
        Hashtbl.find frequencies e
      with Not_found ->
        let r = ref 1 in Hashtbl.add frequencies e r; r in
    incr r;
    if !r > !max_freq then begin max_freq := !r; most_freq := e end
  done;
  !most_freq
@

\subsection{Output compacted tables}

\subsection{C transition engine}

\section{Yacc}

\chapter{Advanced Topics}

\section{Unicode}



\chapter{Conclusion}

\appendix

\chapter{Debugging}

\chapter{Error Managment}

<<[[Main.main()]] report error exn>>=
(match exn with
  Parsing.Parse_error ->
    prerr_string "Syntax error around char ";
    prerr_int (Lexing.lexeme_start lexbuf);
    prerr_endline "."
| Lexer.Lexical_error s ->
    prerr_string "Lexical error around char ";
    prerr_int (Lexing.lexeme_start lexbuf);
    prerr_string ": ";
    prerr_string s;
    prerr_endline "."
| _ -> raise exn
);
@

\chapter{Libc}

\chapter{Extra Code}

\ifallcode
#include "CompilerGenerator_extra.nw"
\fi

\chapter{Changelog}
\label{sec:changelog}

\chapter{Glossary}
\label{sec:glossary}

\begin{verbatim}
\end{verbatim}

\chapter*{Indexes}
\addcontentsline{toc}{section}{Index}

%\chapter{References} 
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{99}

\bibitem[1]{wp-literate-programming} Donald Knuth,,
{\em Literate Programming}, 
\url{http://en.wikipedia.org/wiki/Literate\_Program}

\bibitem[2]{noweb} Norman Ramsey,
{\em Noweb}, 
\url{http://www.cs.tufts.edu/~nr/noweb/}

\bibitem[3]{syncweb} Yoann Padioleau,
{\em Syncweb, literate programming meets unison}, 
\url{http://padator.org/software/project-syncweb/readme.txt}

\end{thebibliography}

%******************************************************************************
% Postlude
%******************************************************************************

\end{document}
