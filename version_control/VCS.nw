\documentclass[twocolumn, landscape]{report}

%******************************************************************************
% Prelude
%******************************************************************************
\newif\iffinal
\newif\ifverbose
\finaltrue\verbosefalse % see also other newif in Macros.tex

%------------------------------------------------------------------------------
%history: 
%------------------------------------------------------------------------------

%thx to LP, I changed for the better a few things:

%thx to codemap/codegraph/scheck:
% - TODO use cg to reduce backward deps, introduce globals.c, utils.c,
%   (harder to understand non layered code)
% - TODO use scheck to remove deadcode, dead prototypes, useless export
%   or mv as forward decl
%   (harder to understand big interface files)
% - TODO use cg to reduce number of globals by moving them closer to the
%   relevant file (or even function), better cluster the code
%   (harder to understand non functional code using lots of globals)

%thx to this manual, better understand VCS?:

%history LP-ization:
% - skeleton, mostly copy paste of Template.nw skeleton
% - put all content of files in the Extra section, via 'pfff -lpize'
%   which also now split in chunks!
%    * function, global, struct, enum, constant, macro(actually function)
%    * TODO ctor/dtor, dumper
%    * TODO [[xxx]] other fields, [[xxx]] extra fields
% - TODO read Extra section, identify concepts, first TOC
% - TODO distribute parts of the Extra section in the main file
% - TODO understand main(), LP split main, improve TOC
% - TODO understand main functions, LP split, cluster, improve TOC
% - TODO LP split the structures, use datalog for flow to field info
% - TODO nullify, boolify, errorify, enumify,  typeify,    scheckify, plan9ify
% - TODO aspecify advanced features! remove useless features
% - TODO add figures
% - TODO add explanations

%------------------------------------------------------------------------------
% Packages
%------------------------------------------------------------------------------

\usepackage{../docs/latex/noweb}
 \noweboptions{footnotesizecode,nomargintag}
 %note: allow chunk on different pages, less white space at bottom of pages
 \def\nwendcode{\endtrivlist \endgroup}
 \let\nwdocspar=\par
\usepackage{xspace}
\usepackage{verbatim}
%note: required by noweblatexpad for the \t \l \n in this file
\usepackage{fancyvrb}
\usepackage{url}
\usepackage{hyperref}
 \hypersetup{colorlinks=true}
\usepackage[pageref]{backref}
 \def\backref{{\footnotesize cited page(s)}~}
\usepackage{booktabs} 
 \newcommand{\otoprule}{\midrule[\heavyrulewidth]}
\usepackage{graphicx}

%------------------------------------------------------------------------------
% Macros
%------------------------------------------------------------------------------
\input{../docs/latex/Macros}

%------------------------------------------------------------------------------
% Config
%------------------------------------------------------------------------------
\allcodefalse
% used for forward decl, pragmas, func decl, extern decl, stats, #ifdef,
% debugging macros

\addtolength{\topmargin}{-.850in}
\addtolength{\textheight}{1.70in}


\begin{document}
%******************************************************************************
% Title
%******************************************************************************
\title{
{\Huge 
Principia Softwarica: The Version Control System [[git]]
}\\
Dulwich (clone of [[git]] in Python) edition\\
{version 0.1}
}

\author{
Yoann Padioleau\\
\texttt{yoann.padioleau@gmail.com}\\
\\
with code from\\
Jelmer Vernooij
}
\maketitle 

\onecolumn
\hrule
\input{../docs/latex/Copyright}
%\input{../docs/latex/CopyrightPlan9}
\hrule
\twocolumn

\begingroup
\hypersetup{linkcolor=blue}
% need to s/onecolumn/twocolumn in report.cls :) for \tableofcontents
\tableofcontents
\endgroup

%******************************************************************************
% Body
%******************************************************************************

\chapter{Introduction}

\section{Motivations}

The goal of this book is to present in full details the source code of
a version control system.
Why? Because I think you are a better programmer if
you fully understand how things work under the hood.

Here are a few questions I hope this book will answer:
\begin{itemize}
\end{itemize}

% Stored as diffs? apparently not. Just compressed and indexed by sha.
% If same content, then same id.

\section{The version control system [[git]] ([[dulwich]])}

% Chose camp, a mini darcs. Simple? Elegant? Arguably more powerful.
% < 10 000 LOC.
%% Can be good opportunity also to read some haskell code. Maybe can then
%% port the code to OCaml.

\section{Other version control systems}

Here are other candidates that were considered but ultimately discarded:
\begin{itemize}

\item RCS
% 24~000 LOC, but too limited, just one file, no branch, no concurrent work
% (and already 24 000 LOC, insane)

\item CVS
% 160 000 LOC, with rcs.c at 8000 LOC, hmm,
% concurrent, but branch is really tedious

\item git
% 489 000 LOC
% very good, but big codebase in C, and complicated (e.g. rebase)
% has an article in AOSA book 1 or 2
% first commit was small:
%  https://github.com/git/git/commit/e83c5163316f89bfbde7d9ab23ca2e25604af290
% also libgit2, LOC? share code with git?

\item mercurial
%mercurial (147 000 LOC, core = 84 000 LOC)
%   core of 1.0 is? maybe not that bad

\end{itemize}

% In the end I picked a mix between git and mercurial: git, but
% implemented in the language used for mercurial (Python).

%https://building-git.launchrock.com/
%https://blog.jcoglan.com/2017/02/12/the-myers-diff-algorithm-part-1/


%industry:
% - perforce
% - sourcesafe
% - bitkeeper
%other:
% - subversion (630 000 LOC, hmm)
%other DVCSs:
% - darcs 80 000 LOC (using literate haskell), maybe good candidate, simpler
%   model, arguably simpler than git (rebase for free?), some patch theory.
%   But code looks actually awful. Lots of boilerplate. Huge types.
%   Camp is a simplified version at 6300 LOC
% - gnu arch (aka tla 273 000 LOC)
% - bazaar (477 000 LOC)
%   https://www.jelmer.uk/pages/bzr-a-retrospective.html
% - monotone (99 000 LOC for 1.1)
%git alternate porcelain:
% - gitless, a better design for git, great paper:
%   http://people.csail.mit.edu/sperezde/pre-print-oopsla16.pdf
%   (can implement this design with dulwich?)
%mini:
% - http://benhoyt.com/writings/pygit/ 500 LOC to push to github
% - http://gitlet.maryrosecook.com/ mini git in Javascript, heavily
%   commented (but LP would be better)
% - pijul, initially in ocaml (3000 LOC), then in Rust, but seems to have 
%   a slow development
% - gg 7000 LOC, but no locking, no push, and requires external diff.
%   http://www-cs-students.stanford.edu/~blynn/gg/
%   this guy also wrote a book on Git "Git Magic"
%clones in other languages (all clones of git actually):
% - dulwich: git written in python (15000 LOC (without tests))
%   which includes some porcelain now (there is also another project
%   called gittle which is porcelain for dulwich).
%   Used by Google for some projects to provide bridge between
%   mercurial and git.
%   Originally created to offer bridge between bazaar and git by one
%   of bazaar maintainer. Based on python-git hack by James Westby.
% - git-go: git written in Go (but seems limited to archeology command)
% - JGit: git in Java
% - libgit: git in C, but reuse git code or new implem?
% - gat: git clone in haskell http://evan-tech.livejournal.com/254793.html
%   another one:
%   http://stefan.saasen.me/articles/git-clone-in-haskell-from-the-bottom-up/
% - ocaml-git: 15 000 LOC, but  heavily functorized (to support unix 
%   but also mirage), many dependencies (mstructs, cstruct, topkg, logs)
%   and seems to be only an API to access data. No support
%   for diffs; no merge; no porcelain.

%https://en.wikipedia.org/wiki/List_of_revision_control_software
%http://www.catb.org/esr/writings/version-control/version-control.html
%https://codewords.recurse.com/issues/two/git-from-the-inside-out
%http://eagain.net/articles/git-for-computer-scientists/
%https://stevebennett.me/2012/02/24/10-things-i-hate-about-git/
%Bram cohen vs torvalds on deep algo in git:
%http://www.wincent.com/a/about/wincent/weblog/archives/2007/07/a_look_back_bra.php

%future:
% - my semantic-vcs proposal!
%   related: https://www.semanticmerge.com/
% - gitless design  http://people.csail.mit.edu/sperezde/pre-print-oopsla16.pdf

%http://manishearth.github.io/blog/2017/03/05/understanding-git-filter-branch/


\section{Getting started}

\section{Requirements}

\section{About this document}
#include "docs/latex/About.nw"

\section{Copyright}

Most of this document is actually source code from [[dulwich]], so
those parts are copyright by Jelmer Vernooij.
The prose is mine and is licensed under the GNU Free Documentation
License.

<<dulwich license>>=
# Dulwich is dual-licensed under the Apache License, Version 2.0 and the GNU
# General Public License as public by the Free Software Foundation; version 2.0
# or (at your option) any later version. You can redistribute it and/or
# modify it under the terms of either of these two licenses.
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# You should have received a copy of the licenses; if not, see
# <http://www.gnu.org/licenses/> for a copy of the GNU General Public License
# and <http://www.apache.org/licenses/LICENSE-2.0> for a copy of the Apache
# License, Version 2.0.
#
@

\section{Acknowledgments}






\chapter{Overview}

\section{VCS principles}

% See Misfits paper, great summary of essential purposes.

% First, store past versions. Can go back in time if made a mistake.
% Could get that with copy, but better organized, and storing
% deltas has efficiency implications. 
%update: actually git does not really use delta. really more a content tracker.

% Also by having delta
% and message associated with it help explore history. Git log, git
% blame are fantastic tool, even in single-user mode. To know
% code related to a line, the message of this patch, etc.
% Also in multi-user mode good to know the author, test plan, test
% files, coupled code, etc.
% I used it a lot to understand code at Facebook.

% Then when work in groups, useful to have way to work concurrently,
% to not pass his copy and wait for the "token". Concurrent
% techniques are lock, or better optimistic and later merge.

% Branches are also very useful. Work on different tasks, different
% branches. Can juggle between those.

% What brings distributed then? More convenient than centralized.
% Less imposing. Nice to have everything locally. Just more general.
% Can do centralized with distributed (e.g. "main" repo on github
% is the main thing to pull from).

% Finally github is fantastic. Easy to setup and start sharing work.

% ten innovations in the history of vcs:
% http://www.flourish.org/blog/?p=397
% http://www.flourish.org/2011/12/astonishments-ten-in-the-history-of-version-control/
% free easy hosting is in it? a la github?

% see Mercurial chapter in AOSA book.

% Some stuff in dulwich/docs/ but very limited.

\section{[[git]] services}

\section{Repository format, [[.git/]]}

% objects/!!
% branches/
% hooks/
% refs/


% encoding UTF8 for filenames.
% https://github.com/git/git/blob/master/Documentation/i18n.txt

\section{A toy session}

% git init
% git add
% git commit
% git log

% git status
% git diff

% git branch
% git checkout

% git clone
% git pull
% git push

\section{Code organization}

\section{Software architecture}

% plumbing vs porcelain

\section{Book structure}

%###############################################################################

\chapter{Core Data Structures}

% see 'dulwich in 5 min' pdf:
%  - basic obj read/write (tree, blog, tag, commit)
%  - pack (read/write)
%  - smart server/client???
%  - index (read/write)
%  - patch parsing/generation
%  - fastimport/fastexport

\section{SHA-1}

% used for consistency checking, for identifying, for indexing, for folders.
% SHA underlies lots of things in git.

%for more information on SHA1:
% http://www-cs-students.stanford.edu/~blynn/gitmagic/ch08.html


<<function sha_to_hex>>=
def sha_to_hex(sha):
    """Takes a string and returns the hex of the sha within"""
    hexsha = binascii.hexlify(sha)
    assert len(hexsha) == 40, "Incorrect length of sha1 string: %d" % hexsha
    return hexsha
@

<<function hex_to_sha>>=
def hex_to_sha(hex):
    """Takes a hex sha and returns a binary sha"""
    assert len(hex) == 40, "Incorrect length of hexsha: %s" % hex
    try:
        return binascii.unhexlify(hex)
    except TypeError as exc:
        if not isinstance(hex, bytes):
            raise
        raise ValueError(exc.args[0])
@

<<function valid_hexsha>>=
def valid_hexsha(hex):
    if len(hex) != 40:
        return False
    try:
        binascii.unhexlify(hex)
    except (TypeError, binascii.Error):
        return False
    else:
        return True
@



\section{Repository}

<<class Repo>>=
class Repo(BaseRepo):
    """A git repository backed by local disk.

    To open an existing repository, call the contructor with
    the path of the repository.

    To create a new repository, use the Repo.init class method.
    """

    <<[[Repo]] methods>>
@


<<class BaseRepo>>=
class BaseRepo(object):
    """Base class for a git repository.

    :ivar object_store: Dictionary-like object for accessing
        the objects
    :ivar refs: Dictionary-like object with the refs in this
        repository
    """

    <<[[BaseRepo]] methods>>
@
% split in 2 classes for MemRepo mock.


<<constant repo.CONTROLDIR>>=
CONTROLDIR = '.git'
@
% also called commondir when non-bare repo.

<<constant repo.BASE_DIRECTORIES>>=
BASE_DIRECTORIES = [
    ["branches"],
    [REFSDIR],
    [REFSDIR, REFSDIR_TAGS],
    [REFSDIR, REFSDIR_HEADS],
    ["hooks"],
    ["info"]
    ]
@

<<constant repo.REFSDIR>>=
REFSDIR = 'refs'
@
<<constant repo.REFSDIR_TAGS>>=
REFSDIR_TAGS = 'tags'
@
<<constant repo.REFSDIR_HEADS>>=
REFSDIR_HEADS = 'heads'
@

% Tags later. Hooks later.
% info??

<<constant repo.OBJECTDIR>>=
OBJECTDIR = 'objects'
@
% why not in BASE_DIRECTORIES?

<<function hex_to_filename>>=
def hex_to_filename(path, hex):
    """Takes a hex sha and returns its filename relative to the given path."""
    # os.path.join accepts bytes or unicode, but all args must be of the same
    # type. Make sure that hex which is expected to be bytes, is the same type
    # as path.
    if getattr(path, 'encode', None) is not None:
        hex = hex.decode('ascii')
    dir = hex[:2]
    file = hex[2:]
    # Check from object dir
    return os.path.join(path, dir, file)

@

<<function filename_to_hex>>=
def filename_to_hex(filename):
    """Takes an object filename and returns its corresponding hex sha."""
    # grab the last (up to) two path components
    names = filename.rsplit(os.path.sep, 2)[-2:]
    errmsg = "Invalid object filename: %s" % filename
    assert len(names) == 2, errmsg
    base, rest = names
    assert len(base) == 2 and len(rest) == 38, errmsg
    hex = (base + rest).encode('ascii')
    hex_to_sha(hex)
    return hex
@



<<[[Repo]] methods>>=
def __init__(self, root):
    hidden_path = os.path.join(root, CONTROLDIR)
    if os.path.isdir(os.path.join(hidden_path, OBJECTDIR)):
        self.bare = False
        self._controldir = hidden_path
    <<[[Repo.__init__()]] else if no [[.git/objects/]] directory>>
    else:
        raise NotGitRepository(
            "No git repository was found at %(path)s" % dict(path=root)
        )

    <<[[Repo.__init__()]] commondir>>

    self.path = root
    object_store = DiskObjectStore(
        os.path.join(self.commondir(), OBJECTDIR))
    refs = DiskRefsContainer(self.commondir(), self._controldir)

    BaseRepo.__init__(self, object_store, refs)

    <<[[Repo.__init__()]] grafts>>
    <<[[Repo.__init__()]] hooks>>
@


<<[[BaseRepo]] methods>>=
def __init__(self, object_store, refs):
    """Open a repository.

    This shouldn't be called directly, but rather through one of the
    base classes, such as MemoryRepo or Repo.

    :param object_store: Object store to use
    :param refs: Refs container to use
    """
    self.object_store = object_store
    self.refs = refs

    <<[[BaseRepo.__init__()]] grafts>>
    <<[[BaseRepo.__init__()]] hooks>>
@


% repo = object store and refs in object store.
% In fact can use r[xx] to access transparently either:

<<[[BaseRepo]] methods>>=
def __getitem__(self, name):
    """Retrieve a Git object by SHA1 or ref.

    :param name: A Git object SHA1 or a ref name
    :return: A `ShaFile` object, such as a Commit or Blob
    :raise KeyError: when the specified ref or object does not exist
    """
    if not isinstance(name, bytes):
        raise TypeError("'name' must be bytestring, not %.80s" %
                        type(name).__name__)
    if len(name) in (20, 40):
        try:
            return self.object_store[name]
        except (KeyError, ValueError):
            pass
    try:
        return self.object_store[self.refs[name]]
    except RefFormatError:
        raise KeyError(name)
@

% can do r[xxx] or r.object_store[xxx]
% self.refs and self.object_store are themselvesdictionary classes 

<<[[BaseRepo]] methods>>=
def __setitem__(self, name, value):
    """Set a ref.

    :param name: ref name
    :param value: Ref value - either a ShaFile object, or a hex sha
    """
    if name.startswith(b"refs/") or name == b'HEAD':
        if isinstance(value, ShaFile):
            self.refs[name] = value.id
        elif isinstance(value, bytes):
            self.refs[name] = value
        else:
            raise TypeError(value)
    else:
        raise ValueError(name)

@

<<[[BaseRepo]] methods>>=
def __delitem__(self, name):
    """Remove a ref.

    :param name: Name of the ref to remove
    """
    if name.startswith(b"refs/") or name == b"HEAD":
        del self.refs[name]
    else:
        raise ValueError(name)
@



\section{Object store}

<<class DiskObjectStore>>=
class DiskObjectStore(PackBasedObjectStore):
    """Git-style object store that exists on disk."""

    <<[[DiskObjectStore]] methods>>
@

<<class PackBasedObjectStore>>=
class PackBasedObjectStore(BaseObjectStore):

    <<[[PackBasedObjectStore]] methods>>
@
% See Pack in adv topics.

<<class BaseObjectStore>>=
class BaseObjectStore(object):
    """Object store interface."""

    <<[[BaseObjectStore]] methods>>
@


<<[[DiskObjectStore]] methods>>=
def __init__(self, path):
    """Open an object store.

    :param path: Path of the object store.
    """
    super(DiskObjectStore, self).__init__()
    self.path = path

    <<[[DiskObjectStore.__init__()]] set pack fields>>
    <<[[DiskObjectStore.__init__()]] set alternates>>
@


<<[[BaseObjectStore]] methods>>=
def __getitem__(self, sha):
    """Obtain an object by SHA1."""
    type_num, uncomp = self.get_raw(sha)
    return ShaFile.from_raw_string(type_num, uncomp, sha=sha)
@
% but already deserialize in get_raw, so why parse again?
% or optimized?

<<[[BaseObjectStore]] methods>>=
def get_raw(self, name):
    """Obtain the raw text for an object.

    :param name: sha for the object.
    :return: tuple with numeric type and object contents.
    """
    raise NotImplementedError(self.get_raw)
@


<<[[PackBasedObjectStore]] methods>>=
def get_raw(self, name):
    """Obtain the raw text for an object.

    :param name: sha for the object.
    :return: tuple with numeric type and object contents.
    """
    if len(name) == 40:
        sha = hex_to_sha(name)
        hexsha = name
    elif len(name) == 20:
        sha = name
        hexsha = None
    else:
        raise AssertionError("Invalid object name %r" % name)
    <<[[PackBasedObjectStore.get_raw()]] look in packs>>
    if hexsha is None:
        hexsha = sha_to_hex(name)

    ret = self._get_loose_object(hexsha)
    if ret is not None:
        return ret.type_num, ret.as_raw_string()

    <<[[PackBasedObjectStore.get_raw()]] look in alternates>>
    raise KeyError(hexsha)

@

% so as_raw_string, and later from_fraw_string. useless a bit

<<[[ShaFile]] methods>>=
def as_raw_string(self):
    """Return raw string with serialization of the object.

    :return: String object
    """
    return b''.join(self.as_raw_chunks())

@

% Actually this will call serialize, so get_raw
% actually call serialize 1 time and deserialize 2 times?

\section{Objects}

\subsection{[[ShaFile]]}

<<class ShaFile>>=
class ShaFile(object):
    """A git SHA file."""

    __slots__ = ('_chunked_text', '_sha', '_needs_serialization')

    <<[[ShaFile]] methods>>
@

<<[[ShaFile]] methods>>=
def __init__(self):
    """Don't call this directly"""
    self._sha = None
    self._chunked_text = []
    self._needs_serialization = True

@
% _sha is set on demand when ask for self.id which calls self.sha
% _needs_serialization is to know if _chunked_text is set which means
%  a call to _serialize() has been done already and cached in _chunked_text.
%  So set to True here because need call _serialize to set _chunked_text.
% Maybe could rename needs_serialization to 'modified'

% ShaFile.id -> <>
<<[[ShaFile]] methods>>=
def sha(self):
    """The SHA1 object that is the name of this object."""
    if self._sha is None or self._needs_serialization:
        # this is a local because as_raw_chunks() overwrites self._sha
        new_sha = sha1()
        new_sha.update(self._header())
        for chunk in self.as_raw_chunks():
            new_sha.update(chunk)
        self._sha = new_sha
    return self._sha

@

<<[[ShaFile]] methods>>=
def _header(self):
    return object_header(self.type, self.raw_length())

@


<<[[ShaFile]] methods>>=
def check(self):
    """Check this object for internal consistency.

    :raise ObjectFormatException: if the object is malformed in some way
    :raise ChecksumMismatch: if the object was created with a SHA that does
        not match its contents
    """
    # TODO: if we find that error-checking during object parsing is a
    # performance bottleneck, those checks should be moved to the class's
    # check() method during optimization so we can still check the object
    # when necessary.
    old_sha = self.id
    try:
        self._deserialize(self.as_raw_chunks())
        self._sha = None
        new_sha = self.id
    except Exception as e:
        raise ObjectFormatException(e)
    if old_sha != new_sha:
        raise ChecksumMismatch(new_sha, old_sha)

@

<<[[ShaFile]] methods>>=
@property
def id(self):
    """The hex SHA of this object."""
    return self.sha().hexdigest().encode('ascii')

@

% ShaFile.as_raw_string | ShaFile.check | ??  -> <>
<<[[ShaFile]] methods>>=
def as_raw_chunks(self):
    """Return chunks with serialization of the object.

    :return: List of strings, not necessarily one per line
    """
    if self._needs_serialization:
        self._sha = None
        self._chunked_text = self._serialize()
        self._needs_serialization = False
    return self._chunked_text

@
% why chunks? why not single chunk? easier in writer
% because metadata is really a list of things.

% serialize and deserialize later


<<constant OBJECT_CLASSES>>=
OBJECT_CLASSES = (
    Commit,
    Tree,
    Blob,
    Tag,
    )
@

<<global _TYPE_MAP>>=
_TYPE_MAP = {}
@

<<[[objects.py]] toplevel>>=
for cls in OBJECT_CLASSES:
    _TYPE_MAP[cls.type_name] = cls
    _TYPE_MAP[cls.type_num] = cls
@

<<function serializable_property>>=
def serializable_property(name, docstring=None):
    """A property that helps tracking whether serialization is necessary.
    """
    def set(obj, value):
        setattr(obj, "_"+name, value)
        obj._needs_serialization = True

    def get(obj):
        return getattr(obj, "_"+name)
    return property(get, set, doc=docstring)
@

\subsection{[[Commit]]}

<<class Commit>>=
class Commit(ShaFile):
    """A git commit object"""

    type_name = b'commit'
    type_num = 1

    __slots__ = ('_parents', '_encoding', '_extra', '_author_timezone_neg_utc',
                 '_commit_timezone_neg_utc', '_commit_time',
                 '_author_time', '_author_timezone', '_commit_timezone',
                 '_author', '_committer', '_tree', '_message',
                 '_mergetag', '_gpgsig')

    <<[[Commit]] methods>>
@

<<[[Commit]] methods>>=
def __init__(self):
    super(Commit, self).__init__()
    self._parents = []
    self._encoding = None
    self._mergetag = []
    self._gpgsig = None
    self._extra = []
    self._author_timezone_neg_utc = False
    self._commit_timezone_neg_utc = False
@

<<[[Commit]] methods>>=
tree = serializable_property(
    "tree", "Tree that is the state of this commit")

@
<<[[Commit]] methods>>=
<<method Common._get_parents>>
<<method Common._set_parents>>
parents = property(_get_parents, _set_parents,
                   doc="Parents of this commit, by their SHA1.")
@

<<method Common._get_parents>>=
def _get_parents(self):
    """Return a list of parents of this commit."""
    return self._parents
@

<<method Common._set_parents>>=
def _set_parents(self, value):
    """Set a list of parents of this commit."""
    self._needs_serialization = True
    self._parents = value
@

<<[[Commit]] methods>>=
author = serializable_property("author",
    "The name of the author of the commit")
@

<<[[Commit]] methods>>=
committer = serializable_property("committer",
    "The name of the committer of the commit")

@

<<[[Commit]] methods>>=
message = serializable_property(
    "message", "The commit message")

@

<<[[Commit]] methods>>=
commit_time = serializable_property("commit_time",
    "The timestamp of the commit. As the number of seconds since the epoch.")

@

<<[[Commit]] methods>>=
commit_timezone = serializable_property("commit_timezone",
    "The zone the commit time is in")

@

<<[[Commit]] methods>>=
author_time = serializable_property("author_time",
    "The timestamp the commit was written. As the number of "
    "seconds since the epoch.")

@

<<[[Commit]] methods>>=
author_timezone = serializable_property(
    "author_timezone", "Returns the zone the author time is in.")

@

<<[[Commit]] methods>>=
encoding = serializable_property(
    "encoding", "Encoding of the commit message.")

@

<<[[Commit]] methods>>=
mergetag = serializable_property(
    "mergetag", "Associated signed tag.")

@

<<[[Commit]] methods>>=
gpgsig = serializable_property(
    "gpgsig", "GPG Signature.")
@


<<method Commit._get_extra>>=
def _get_extra(self):
    """Return extra settings of this commit."""
    return self._extra

@

<<[[Commit]] methods>>=
<<method Commit._get_extra>>
extra = property(_get_extra,
    doc="Extra header fields not understood (presumably added in a "
        "newer version of git). Kept verbatim so the object can "
        "be correctly reserialized. For private commit metadata, use "
        "pseudo-headers in Commit.message, rather than this field.")

@


<<[[Commit]] methods>>=
def check(self):
    """Check this object for internal consistency.

    :raise ObjectFormatException: if the object is malformed in some way
    """
    super(Commit, self).check()
    self._check_has_member("_tree", "missing tree")
    self._check_has_member("_author", "missing author")
    self._check_has_member("_committer", "missing committer")
    # times are currently checked when set

    for parent in self._parents:
        check_hexsha(parent, "invalid parent sha")
    check_hexsha(self._tree, "invalid tree sha")

    check_identity(self._author, "invalid author")
    check_identity(self._committer, "invalid committer")

    <<[[Commit.check()]] check message>>
    # TODO: optionally check for duplicate parents

@


<<[[ShaFile]] methods>>=
def _check_has_member(self, member, error_msg):
    """Check that the object has a given member variable.

    :param member: the member variable to check for
    :param error_msg: the message for an error if the member is missing
    :raise ObjectFormatException: with the given error_msg if member is
        missing or is None
    """
    if getattr(self, member, None) is None:
        raise ObjectFormatException(error_msg)

@

<<function objects.check_hexsha>>=
def check_hexsha(hex, error_msg):
    """Check if a string is a valid hex sha string.

    :param hex: Hex string to check
    :param error_msg: Error message to use in exception
    :raise ObjectFormatException: Raised when the string is not valid
    """
    if not valid_hexsha(hex):
        raise ObjectFormatException("%s %s" % (error_msg, hex))
@

<<function objects.check_identity>>=
def check_identity(identity, error_msg):
    """Check if the specified identity is valid.

    This will raise an exception if the identity is not valid.

    :param identity: Identity string
    :param error_msg: Error message to use in exception
    """
    email_start = identity.find(b'<')
    email_end = identity.find(b'>')
    if (email_start < 0 or email_end < 0 or email_end <= email_start
            or identity.find(b'<', email_start + 1) >= 0
            or identity.find(b'>', email_end + 1) >= 0
            or not identity.endswith(b'>')):
        raise ObjectFormatException(error_msg)
@

<<[[Commit.check()]] check message>>=
last = None
for field, _ in _parse_message(self._chunked_text):
    if field == _TREE_HEADER and last is not None:
        raise ObjectFormatException("unexpected tree")
    elif field == _PARENT_HEADER and last not in (_PARENT_HEADER,
                                                  _TREE_HEADER):
        raise ObjectFormatException("unexpected parent")
    elif field == _AUTHOR_HEADER and last not in (_TREE_HEADER,
                                                  _PARENT_HEADER):
        raise ObjectFormatException("unexpected author")
    elif field == _COMMITTER_HEADER and last != _AUTHOR_HEADER:
        raise ObjectFormatException("unexpected committer")
    elif field == _ENCODING_HEADER and last != _COMMITTER_HEADER:
        raise ObjectFormatException("unexpected encoding")
    last = field
@


\subsection{[[Tree]]}

<<class Tree>>=
class Tree(ShaFile):
    """A Git tree object"""

    type_name = b'tree'
    type_num = 2

    __slots__ = ('_entries')

    <<[[Tree]] methods>>
@

<<[[Tree]] methods>>=
def __init__(self):
    super(Tree, self).__init__()
    self._entries = {}
@




<<[[Tree]] methods>>=
def check(self):
    """Check this object for internal consistency.

    :raise ObjectFormatException: if the object is malformed in some way
    """
    super(Tree, self).check()
    last = None
    allowed_modes = (stat.S_IFREG | 0o755, stat.S_IFREG | 0o644,
                     stat.S_IFLNK, stat.S_IFDIR, S_IFGITLINK,
                     # TODO: optionally exclude as in git fsck --strict
                     stat.S_IFREG | 0o664)
    for name, mode, sha in parse_tree(b''.join(self._chunked_text),
                                      True):
        check_hexsha(sha, 'invalid sha %s' % sha)
        if b'/' in name or name in (b'', b'.', b'..'):
            raise ObjectFormatException('invalid name %s' % name)

        if mode not in allowed_modes:
            raise ObjectFormatException('invalid mode %06o' % mode)

        entry = (name, (mode, sha))
        if last:
            if key_entry(last) > key_entry(entry):
                raise ObjectFormatException('entries not sorted')
            if name == last[0]:
                raise ObjectFormatException('duplicate entry %s' % name)
        last = entry

@


<<[[Tree]] methods>>=
def add(self, name, mode, hexsha):
    """Add an entry to the tree.

    :param mode: The mode of the entry as an integral type. Not all
        possible modes are supported by git; see check() for details.
    :param name: The name of the entry, as a string.
    :param hexsha: The hex SHA of the entry as a string.
    """
    if isinstance(name, int) and isinstance(mode, bytes):
        (name, mode) = (mode, name)
        warnings.warn(
            "Please use Tree.add(name, mode, hexsha)",
            category=DeprecationWarning, stacklevel=2)
    self._entries[name] = mode, hexsha
    self._needs_serialization = True

@


<<[[Tree]] methods>>=
def __getitem__(self, name):
    return self._entries[name]

@

<<[[Tree]] methods>>=
def __setitem__(self, name, value):
    """Set a tree entry by name.

    :param name: The name of the entry, as a string.
    :param value: A tuple of (mode, hexsha), where mode is the mode of the
        entry as an integral type and hexsha is the hex SHA of the entry as
        a string.
    """
    mode, hexsha = value
    self._entries[name] = (mode, hexsha)
    self._needs_serialization = True

@

<<[[Tree]] methods>>=
def __delitem__(self, name):
    del self._entries[name]
    self._needs_serialization = True

@


\subsection{[[Blob]]}

<<class Blob>>=
class Blob(ShaFile):
    """A Git Blob object."""

    __slots__ = ()

    type_name = b'blob'
    type_num = 3

    <<[[Blob]] methods>>
@

<<[[Blob]] methods>>=
<<method Blob._get_data>>
<<method Blob._set_data>>
data = property(_get_data, _set_data,
                "The text contained within the blob object.")

@
<<method Blob._get_data>>=
def _get_data(self):
    return self.as_raw_string()
@
<<method Blob._set_data>>=
def _set_data(self, data):
    self.set_raw_string(data)
@


<<[[Blob]] methods>>=
<<method Blob._get_chunked>>
<<method Blob._set_chunked>>
chunked = property(
    _get_chunked, _set_chunked,
    "The text within the blob object, as chunks (not necessarily lines).")
@
<<method Blob._get_chunked>>=
def _get_chunked(self):
    return self._chunked_text
@

<<method Blob._set_chunked>>=
def _set_chunked(self, chunks):
    self._chunked_text = chunks
@




<<[[Blob]] methods>>=
def __init__(self):
    super(Blob, self).__init__()
    self._chunked_text = []
    self._needs_serialization = False
@

<<[[Blob]] methods>>=
def check(self):
    """Check this object for internal consistency.

    :raise ObjectFormatException: if the object is malformed in some way
    """
    super(Blob, self).check()

@


\section{References}

<<class DiskRefsContainer>>=
class DiskRefsContainer(RefsContainer):
    """Refs container that reads refs from disk."""

    <<[[DiskRefsContainer]] methods>>
@

<<class RefsContainer>>=
class RefsContainer(object):
    """A container for refs."""

    <<[[RefsContainer]] methods>>
@

<<[[DiskRefsContainer]] methods>>=
def __init__(self, path, worktree_path=None):
    self.path = path
    self.worktree_path = worktree_path or path
    <<[[DiskRefsContainer.__init__()]] set pack fields>>
@


<<[[RefsContainer]] methods>>=
def __getitem__(self, name):
    """Get the SHA1 for a reference name.

    This method follows all symbolic references.
    """
    _, sha = self.follow(name)
    if sha is None:
        raise KeyError(name)
    return sha

@

% follow() later

\section{The index}
% Staging area

<<constant repo.INDEX_FILENAME>>=
INDEX_FILENAME = "index"
@

% .git/index

<<class Index>>=
class Index(object):
    """A Git Index file."""

    <<[[Index]] methods>>
@

<<[[Index]] methods>>=
def __init__(self, filename):
    """Open an index file.

    :param filename: Path to the index file
    """
    self._filename = filename
    self.clear()
    self.read()
@

<<[[Index]] methods>>=
def clear(self):
    """Remove all contents from this index."""
    self._byname = {}

@

% Index.read() will be in Parsing chapter.

<<type IndexEntry>>=
IndexEntry = collections.namedtuple(
    'IndexEntry', [
        'ctime', 'mtime', 'dev', 'ino', 'mode', 'uid', 'gid', 'size', 'sha',
        'flags'])
@


<<[[Index]] methods>>=
def __getitem__(self, name):
    """Retrieve entry by relative path.

    :return: tuple with (ctime, mtime, dev, ino, mode, uid, gid, size, sha, flags)
    """
    return self._byname[name]

@

<<[[Index]] methods>>=
def __setitem__(self, name, x):
    assert isinstance(name, bytes)
    assert len(x) == 10
    # Remove the old entry if any
    self._byname[name] = x

@

<<[[Index]] methods>>=
def __delitem__(self, name):
    assert isinstance(name, bytes)
    del self._byname[name]

@


\section{[[GitFile]]}

% in Reading chapter will see that most DS are backed up by
% simple file on the disk. When read/write those files,
% use GitFile. Git locks those files so sure consistent repo.

<<function GitFile>>=
def GitFile(filename, mode='rb', bufsize=-1):
    """Create a file object that obeys the git file locking protocol.

    :return: a builtin file object or a _GitFile object

    :note: See _GitFile for a description of the file locking protocol.

    Only read-only and write-only (binary) modes are supported; r+, w+, and a
    are not.  To read and write from the same file, you can take advantage of
    the fact that opening a file for write does not actually open the file you
    request.
    """
    if 'a' in mode:
        raise IOError('append mode not supported for Git files')
    if '+' in mode:
        raise IOError('read/write mode not supported for Git files')
    if 'b' not in mode:
        raise IOError('text mode not supported for Git files')
    if 'w' in mode:
        return _GitFile(filename, mode, bufsize)
    else:
        return io.open(filename, mode, bufsize)
@

% if read, then no lock.

% if write, then lock! so atomic!
<<class _GitFile>>=
class _GitFile(object):
    """File that follows the git locking protocol for writes.

    All writes to a file foo will be written into foo.lock in the same
    directory, and the lockfile will be renamed to overwrite the original file
    on close.

    :note: You *must* call close() or abort() on a _GitFile for the lock to be
        released. Typically this will happen in a finally block.
    """

    PROXY_PROPERTIES = set(['closed', 'encoding', 'errors', 'mode', 'name',
                            'newlines', 'softspace'])
    PROXY_METHODS = ('__iter__', 'flush', 'fileno', 'isatty', 'read',
                     'readline', 'readlines', 'seek', 'tell',
                     'truncate', 'write', 'writelines')

    <<[[_GitFile]] methods>>
@

<<[[_GitFile]] methods>>=
def __init__(self, filename, mode, bufsize):
    self._filename = filename
    self._lockfilename = '%s.lock' % self._filename
    fd = os.open(
        self._lockfilename,
        os.O_RDWR | os.O_CREAT | os.O_EXCL | getattr(os, "O_BINARY", 0))
    self._file = os.fdopen(fd, mode, bufsize)
    self._closed = False

    for method in self.PROXY_METHODS:
        setattr(self, method, getattr(self._file, method))
@

<<[[_GitFile]] methods>>=
def close(self):
    """Close this file, saving the lockfile over the original.

    :note: If this method fails, it will attempt to delete the lockfile.
        However, it is not guaranteed to do so (e.g. if a filesystem
        becomes suddenly read-only), which will prevent future writes to
        this file until the lockfile is removed manually.
    :raises OSError: if the original file could not be overwritten. The
        lock file is still closed, so further attempts to write to the same
        file object will raise ValueError.
    """
    if self._closed:
        return
    self._file.close()
    try:
        try:
            os.rename(self._lockfilename, self._filename)
        except OSError as e:
                raise
    finally:
        self.abort()
@
%win32:
%            if sys.platform == 'win32' and e.errno == errno.EEXIST:
%                # Windows versions prior to Vista don't support atomic
%                # renames
%                _fancy_rename(self._lockfilename, self._filename)
%            else:
%win32:
%def _fancy_rename(oldname, newname):
%    """Rename file with temporary backup file to rollback if rename fails"""
%    if not os.path.exists(newname):
%        try:
%            os.rename(oldname, newname)
%        except OSError:
%            raise
%        return
%
%    # destination file exists
%    try:
%        (fd, tmpfile) = tempfile.mkstemp(".tmp", prefix=oldname+".", dir=".")
%        os.close(fd)
%        os.remove(tmpfile)
%    except OSError:
%        # either file could not be created (e.g. permission problem)
%        # or could not be deleted (e.g. rude virus scanner)
%        raise
%    try:
%        os.rename(newname, tmpfile)
%    except OSError:
%        raise   # no rename occurred
%    try:
%        os.rename(oldname, newname)
%    except OSError:
%        os.rename(tmpfile, newname)
%        raise
%    os.remove(tmpfile)


<<[[_GitFile]] methods>>=
def abort(self):
    """Close and discard the lockfile without overwriting the target.

    If the file is already closed, this is a no-op.
    """
    if self._closed:
        return
    self._file.close()
    try:
        os.remove(self._lockfilename)
        self._closed = True
    except OSError as e:
        # The file may have been removed already, which is ok.
        if e.errno != errno.ENOENT:
            raise
        self._closed = True
@



<<[[_GitFile]] methods>>=
def __enter__(self):
    return self
@
%???

<<[[_GitFile]] methods>>=
def __exit__(self, exc_type, exc_val, exc_tb):
    self.close()
@
%???

<<[[_GitFile]] methods>>=
def __getattr__(self, name):
    """Proxy property calls to the underlying file."""
    if name in self.PROXY_PROPERTIES:
        return getattr(self._file, name)
    raise AttributeError(name)
@

\section{Client}

% for git://, ssh://, but also for local.
% Abstracted behind Client interface.

<<class GitClient>>=
# TODO(durin42): this doesn't correctly degrade if the server doesn't
# support some capabilities. This should work properly with servers
# that don't support multi_ack.
class GitClient(object):
    """Git smart server client.

    """

    <<[[GitClient]] methods>>
@

<<class LocalGitClient>>=
class LocalGitClient(GitClient):
    """Git Client that just uses a local Repo."""

    <<[[LocalGitClient]] methods>>
@

<<class default_local_git_client_cls>>=
# What Git client to use for local access
default_local_git_client_cls = LocalGitClient
@


% porcelain.clone | porcelain.push | porcelain.pull -> <>
<<function client.get_transport_and_path>>=
def get_transport_and_path(location, **kwargs):
    """Obtain a git client from a URL.

    :param location: URL or path (a string)
    :param config: Optional config object
    :param thin_packs: Whether or not thin packs should be retrieved
    :param report_activity: Optional callback for reporting transport
        activity.
    :return: Tuple with client instance and relative path.
    """
    <<[[get_transport_and_path()]] try parse location as a URL>>
    # Otherwise, assume it's a local path.
    return default_local_git_client_cls(**kwargs), location
@
%win32:
%    if (sys.platform == 'win32' and
%            location[0].isalpha() and location[1:3] == ':\\'):
%        # Windows local path
%        return default_local_git_client_cls(**kwargs), location


<<[[GitClient]] methods>>=
def __init__(self, thin_packs=True, report_activity=None, quiet=False):
    """Create a new GitClient instance.

    :param thin_packs: Whether or not thin packs should be retrieved
    :param report_activity: Optional callback for reporting transport
        activity.
    """
    self._report_activity = report_activity
    self._report_status_parser = None
    <<[[GitClient.__init__()]] set capabilities>>
    <<[[GitClient.__init__()]] adjust capabilities>>
@

<<[[LocalGitClient]] methods>>=
def __init__(self, thin_packs=True, report_activity=None):
    """Create a new LocalGitClient instance.

    :param thin_packs: Whether or not thin packs should be retrieved
    :param report_activity: Optional callback for reporting transport
        activity.
    """
    self._report_activity = report_activity
    # Ignore the thin_packs argument
@



\chapter{Main Functions}

\section{[[commands]]}

<<class Command>>=
class Command(object):
    """A Dulwich subcommand."""

    def run(self, args):
        """Run the command."""
        raise NotImplementedError(self.run)
@
%ocaml: just make it a function ...

<<constant commands>>=
commands = {
    "add": cmd_add,
    "archive": cmd_archive,
    "clone": cmd_clone,
    "commit": cmd_commit,
    "commit-tree": cmd_commit_tree,
    "daemon": cmd_daemon,
    "diff": cmd_diff,
    "diff-tree": cmd_diff_tree,
    "dump-pack": cmd_dump_pack,
    "dump-index": cmd_dump_index,
    "fetch-pack": cmd_fetch_pack,
    "fetch": cmd_fetch,
    "help": cmd_help,
    "init": cmd_init,
    "log": cmd_log,
    "ls-remote": cmd_ls_remote,
    "ls-tree": cmd_ls_tree,
    "pack-objects": cmd_pack_objects,
    "pull": cmd_pull,
    "receive-pack": cmd_receive_pack,
    "remote": cmd_remote,
    "repack": cmd_repack,
    "reset": cmd_reset,
    "rev-list": cmd_rev_list,
    "rm": cmd_rm,
    "show": cmd_show,
    "status": cmd_status,
    "symbolic-ref": cmd_symbolic_ref,
    "tag": cmd_tag,
    "update-server-info": cmd_update_server_info,
    "upload-pack": cmd_upload_pack,
    "web-daemon": cmd_web_daemon,
    }
@

\section{[[git help]]}

<<function cmd_help>>=
class cmd_help(Command):

    def run(self, args):
        parser = optparse.OptionParser()
        parser.add_option("-a", "--all", dest="all",
                          action="store_true",
                          help="List all commands.")
        options, args = parser.parse_args(args)

        if options.all:
            print('Available commands:')
            for cmd in sorted(commands):
                print('  %s' % cmd)
        else:
            print("""\
The dulwich command line tool is currently a very basic frontend for the
Dulwich python module. For full functionality, please see the API reference.

For a list of supported commands, see 'dulwich help -a'.
""")
@

\section{[[main()]]}

<<toplevel main>>=
<<sanity check [[sys.argv]]>>
cmd = sys.argv[1]
try:
    cmd_kls = commands[cmd]
except KeyError:
    print("No such subcommand: %s" % cmd)
    sys.exit(1)

# TODO(jelmer): Return non-0 on errors
cmd_kls().run(sys.argv[2:])
@

<<sanity check [[sys.argv]]>>=
if len(sys.argv) < 2:
    print("Usage: %s <%s> [OPTIONS...]" % (sys.argv[0], "|".join(commands.keys())))
    sys.exit(1)
@

\section{[[Porcelain]]}

<<function porcelain.open_repo_closing>>=
def open_repo_closing(path_or_repo):
    """Open an argument that can be a repository or a path for a repository.
    returns a context manager that will close the repo on exit if the argument
    is a path, else does nothing if the argument is a repo.
    """
    if isinstance(path_or_repo, BaseRepo):
        return _noop_context_manager(path_or_repo)
    return closing(Repo(path_or_repo))
@
% closing(): 'from contextlib import closing'

<<function _noop_context_manager>>=
@contextmanager
def _noop_context_manager(obj):
    """Context manager that has the same api as closing but does nothing."""
    yield obj
@


\chapter{Initializing a Repository}

\section{Creating a fresh repository, [[git init]]}

<<function cmd_init>>=
class cmd_init(Command):

    def run(self, args):
        opts, args = getopt(args, "", ["bare"])
        opts = dict(opts)

        if args == []:
            path = os.getcwd()
        else:
            path = args[0]

        porcelain.init(path, bare=("--bare" in opts))
@

<<function porcelain.init>>=
def init(path=".", bare=False):
    """Create a new git repository.

    :param path: Path to repository.
    :param bare: Whether to create a bare repository.
    :return: A Repo instance
    """
    if not os.path.exists(path):
        os.mkdir(path)

    <<[[porcelain.init()]] if bare>>
    else:
        return Repo.init(path)
@
% this mkdir can be done in Repo.init if pass true as second argument


<<[[Repo]] methods>>=
@classmethod
def init(cls, path, mkdir=False):
    """Create a new repository.

    :param path: Path in which to create the repository
    :param mkdir: Whether to create the directory
    :return: `Repo` instance
    """
    if mkdir:
        os.mkdir(path)
    controldir = os.path.join(path, CONTROLDIR)
    os.mkdir(controldir)
    cls._init_maybe_bare(controldir, False)
    return cls(path)
@
% useless cls(path) here, done already in _init_maybe_bare


<<[[Repo]] methods>>=
@classmethod
def _init_maybe_bare(cls, path, bare):
    for d in BASE_DIRECTORIES:
        os.mkdir(os.path.join(path, *d))

    DiskObjectStore.init(os.path.join(path, OBJECTDIR))
    ret = cls(path)

    ret.refs.set_symbolic_ref(b'HEAD', DEFAULT_REF)
    ret._init_files(bare)

    return ret
@
%ocaml: why so many _initxxx, just put everything in one function


\t who initialize the index?


\subsection{Common directory}

% To abstract diff between bare and normal repository

<<[[Repo.__init__()]] commondir>>=
<<[[Repo.__init__()]] if commondir>>
else:
    self._commondir = self._controldir
@

<<[[Repo]] methods>>=
def commondir(self):
    """Return the path of the common directory.

    For a main working tree, it is identical to controldir().

    For a linked working tree, it is the control directory of the
    main working tree."""

    return self._commondir
@

\subsection{[[DiskObjectStore]]}

<<[[DiskObjectStore]] methods>>=
@classmethod
def init(cls, path):
    try:
        os.mkdir(path)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise
    os.mkdir(os.path.join(path, "info"))
    <<[[DiskObjectStore.init()]] create pack dir>>
    return cls(path)

@
% should use INFODIR





\subsection{[[DiskRefsContainer]]}

<<constant repo.DEFAULT_REF>>=
DEFAULT_REF = b'refs/heads/master'
@

<<[[RefsContainer]] methods>>=
def set_symbolic_ref(self, name, other):
    """Make a ref point at another ref.

    :param name: Name of the ref to set
    :param other: Name of the ref to point at
    """
    raise NotImplementedError(self.set_symbolic_ref)
@

% could be put in IO?
<<[[DiskRefsContainer]] methods>>=
def set_symbolic_ref(self, name, other):
    """Make a ref point at another ref.

    :param name: Name of the ref to set
    :param other: Name of the ref to point at
    """
    self._check_refname(name)
    self._check_refname(other)
    filename = self.refpath(name)
    try:
        f = GitFile(filename, 'wb')
        try:
            f.write(SYMREF + other + b'\n')
        except (IOError, OSError):
            f.abort()
            raise
    finally:
        f.close()

@

<<constant refs.SYMREF>>=
SYMREF = b'ref: '
@





<<[[DiskRefsContainer]] methods>>=
def refpath(self, name):
    """Return the disk path of a ref.

    """
    if getattr(self.path, "encode", None) and getattr(name, "decode", None):
        name = name.decode(sys.getfilesystemencoding())
    if os.path.sep != "/":
        name = name.replace("/", os.path.sep)
    # TODO: as the 'HEAD' reference is working tree specific, it
    # should actually not be a part of RefsContainer
    if name == 'HEAD':
        return os.path.join(self.worktree_path, name)
    else:
        return os.path.join(self.path, name)

@




\subsection{Named files}

<<[[BaseRepo]] methods>>=
def _init_files(self, bare):
    """Initialize a default set of named files."""
    from dulwich.config import ConfigFile
    self._put_named_file('description', b"Unnamed repository")
    f = BytesIO()
    cf = ConfigFile()
    cf.set(b"core", b"repositoryformatversion", b"0")
    if self._determine_file_mode():
        cf.set(b"core", b"filemode", True)
    else:
        cf.set(b"core", b"filemode", False)

    cf.set(b"core", b"bare", bare)
    cf.set(b"core", b"logallrefupdates", True)
    cf.write_to_file(f)
    self._put_named_file('config', f.getvalue())
    self._put_named_file(os.path.join('info', 'exclude'), b'')

@

% see config file later


<<[[Repo]] methods>>=
def _put_named_file(self, path, contents):
    """Write a file to the control dir with the given name and contents.

    :param path: The path to the file, relative to the control dir.
    :param contents: A string to write to the file.
    """
    path = path.lstrip(os.path.sep)
    with GitFile(os.path.join(self.controldir(), path), 'wb') as f:
        f.write(contents)
@

%\subsection{[[GitFile]]}


\section{Cloning a repository, [[git clone]]}

<<function cmd_clone>>=
class cmd_clone(Command):

    def run(self, args):
        opts, args = getopt(args, "", ["bare"])
        opts = dict(opts)

        if args == []:
            print("usage: dulwich clone host:path [PATH]")
            sys.exit(1)

        source = args.pop(0)
        if len(args) > 0:
            target = args.pop(0)
        else:
            target = None

        porcelain.clone(source, target, bare=("--bare" in opts))
@

<<function porcelain.clone>>=
def clone(source, target=None, bare=False, checkout=None,
          errstream=default_bytes_err_stream, outstream=None,
          origin=b"origin"):
    """Clone a local or remote git repository.

    :param source: Path or URL for source repository
    :param target: Path to target repository (optional)
    :param bare: Whether or not to create a bare repository
    :param checkout: Whether or not to check-out HEAD after cloning
    :param errstream: Optional stream to write progress to
    :param outstream: Optional stream to write progress to (deprecated)
    :return: The new repository
    """
    if outstream is not None:
        import warnings
        warnings.warn(
            "outstream= has been deprecated in favour of errstream=.",
            DeprecationWarning, stacklevel=3)
        errstream = outstream

    if checkout is None:
        checkout = (not bare)
    if checkout and bare:
        raise ValueError("checkout and bare are incompatible")
    client, host_path = get_transport_and_path(source)

    if target is None:
        target = host_path.split("/")[-1]

    if not os.path.exists(target):
        os.mkdir(target)

    <<[[porcelain.clone()]] if bare>>
    else:
        r = Repo.init(target)
    try:
        remote_refs = client.fetch(
            host_path, r, determine_wants=r.object_store.determine_wants_all,
            progress=errstream.write)
        r.refs.import_refs(
            b'refs/remotes/' + origin,
            {n[len(b'refs/heads/'):]: v for (n, v) in remote_refs.items()
                if n.startswith(b'refs/heads/')})
        r.refs.import_refs(
            b'refs/tags',
            {n[len(b'refs/tags/'):]: v for (n, v) in remote_refs.items()
                if n.startswith(b'refs/tags/') and
                not n.endswith(ANNOTATED_TAG_SUFFIX)})
        if b"HEAD" in remote_refs and not bare:
            # TODO(jelmer): Support symref capability,
            # https://github.com/jelmer/dulwich/issues/485
            r[b"HEAD"] = remote_refs[b"HEAD"]

        target_config = r.get_config()
        if not isinstance(source, bytes):
            source = source.encode(DEFAULT_ENCODING)
        target_config.set((b'remote', b'origin'), b'url', source)
        target_config.set(
            (b'remote', b'origin'), b'fetch',
            b'+refs/heads/*:refs/remotes/origin/*')
        target_config.write_to_path()

        if checkout and b"HEAD" in r.refs:
            errstream.write(b'Checking out HEAD\n')
            r.reset_index()
    except:
        r.close()
        raise

    return r
@


<<[[GitClient]] methods>>=
def fetch(self, path, target, determine_wants=None, progress=None):
    """Fetch into a target repository.

    :param path: Path to fetch from (as bytestring)
    :param target: Target repository to fetch into
    :param determine_wants: Optional function to determine what refs
        to fetch. Receives dictionary of name->sha, should return
        list of shas to fetch. Defaults to all shas.
    :param progress: Optional progress function
    :return: Dictionary with all remote refs (not just those fetched)
    """
    if determine_wants is None:
        determine_wants = target.object_store.determine_wants_all

    <<[[GitClient.fetch()]] if thin pack capability>>
    else:
        f, commit, abort = target.object_store.add_pack()
    try:
        result = self.fetch_pack(
            path, determine_wants, target.get_graph_walker(), f.write,
            progress)
    except:
        abort()
        raise
    else:
        commit()
    return result

@


<<[[GitClient]] methods>>=
def get_refs(self, path):
    """Retrieve the current refs from a git smart server.

    :param path: Path to the repo to fetch from. (as bytestring)
    """
    raise NotImplementedError(self.get_refs)

@

<<[[LocalGitClient]] methods>>=
def fetch(self, path, target, determine_wants=None, progress=None):
    """Fetch into a target repository.

    :param path: Path to fetch from (as bytestring)
    :param target: Target repository to fetch into
    :param determine_wants: Optional function determine what refs
        to fetch. Receives dictionary of name->sha, should return
        list of shas to fetch. Defaults to all shas.
    :param progress: Optional progress function
    :return: Dictionary with all remote refs (not just those fetched)
    """
    with self._open_repo(path) as r:
        return r.fetch(target, determine_wants=determine_wants,
                       progress=progress)
@


<<[[LocalGitClient]] methods>>=
def get_refs(self, path):
    """Retrieve the current refs from a git smart server."""

    with self._open_repo(path) as target:
        return target.get_refs()
@

<<[[LocalGitClient]] methods>>=
@classmethod
def _open_repo(cls, path):
    from dulwich.repo import Repo
    if not isinstance(path, str):
        path = path.decode(sys.getfilesystemencoding())
    return closing(Repo(path))
@



<<[[BaseObjectStore]] methods>>=
def determine_wants_all(self, refs):
    return [sha for (ref, sha) in refs.items()
            if sha not in self and not ref.endswith(b"^{}") and
            not sha == ZERO_SHA]
@





\chapter{Reading from a Repository}

\section{Objects}

% Will go from 
% sha -> path -> GitFile -> header -> compressed -> raw -> deserialize
% Many intermediate functions below.

% BaseObjectStore.__get__item -> PackBasedObjectStore.get_raw -> <>
<<[[DiskObjectStore]] methods>>=
def _get_loose_object(self, sha):
    path = self._get_shafile_path(sha)
    try:
        return ShaFile.from_path(path)
    except (OSError, IOError) as e:
        if e.errno == errno.ENOENT:
            return None
        raise
@


<<[[DiskObjectStore]] methods>>=
def _get_shafile_path(self, sha):
    # Check from object dir
    return hex_to_filename(self.path, sha)

@


<<[[ShaFile]] methods>>=
@classmethod
def from_path(cls, path):
    """Open a SHA file from disk."""
    with GitFile(path, 'rb') as f:
        return cls.from_file(f)

@


<<[[ShaFile]] methods>>=
@classmethod
def from_file(cls, f):
    """Get the contents of a SHA file on disk."""
    try:
        obj = cls._parse_file(f)
        obj._sha = None
        return obj
    except (IndexError, ValueError):
        raise ObjectFormatException("invalid object header")

@

<<[[ShaFile]] methods>>=
@classmethod
def _parse_file(cls, f):
    map = f.read()
    <<[[ShaFile._parse_file()]] if legacy object>>
    else:
        obj = cls._parse_object_header(map, f)
        obj._parse_object(map)
    return obj

@

<<[[ShaFile]] methods>>=
@staticmethod
def _parse_object_header(magic, f):
    """Parse a new style object, creating it but not reading the file."""
    num_type = (ord(magic[0:1]) >> 4) & 7
    obj_class = object_class(num_type)
    if not obj_class:
        raise ObjectFormatException("Not a known type %d" % num_type)
    return obj_class()

@

<<function objects.object_class>>=
def object_class(type):
    """Get the object class corresponding to the given type.

    :param type: Either a type name string or a numeric type.
    :return: The ShaFile subclass corresponding to the given type, or None if
        type is not a valid type name/number.
    """
    return _TYPE_MAP.get(type, None)
@

% old format actually where read the whole content
<<[[ShaFile]] methods>>=
def _parse_object(self, map):
    """Parse a new style object, setting self._text."""
    # skip type and size; type must have already been determined, and
    # we trust zlib to fail if it's otherwise corrupted
    byte = ord(map[0:1])
    used = 1
    while (byte & 0x80) != 0:
        byte = ord(map[used:used+1])
        used += 1
    raw = map[used:]
    self.set_raw_string(_decompress(raw))

@
%bug: comment is wrong, it's set self._chunked_text

<<function objects._decompress>>=
def _decompress(string):
    dcomp = zlib.decompressobj()
    dcomped = dcomp.decompress(string)
    dcomped += dcomp.flush()
    return dcomped

@


<<[[ShaFile]] methods>>=
def set_raw_string(self, text, sha=None):
    """Set the contents of this object from a serialized string."""
    if not isinstance(text, bytes):
        raise TypeError('Expected bytes for text, got %r' % text)
    self.set_raw_chunks([text], sha)

@


<<[[ShaFile]] methods>>=
def set_raw_chunks(self, chunks, sha=None):
    """Set the contents of this object from a list of chunks."""
    self._chunked_text = chunks
    self._deserialize(chunks)
    if sha is None:
        self._sha = None
    else:
        self._sha = FixedSha(sha)
    self._needs_serialization = False

@
% raw but actually call deserialize! so not really raw.
% In fact ShaFile does call againt ShaFile.from_raw_string later

% deserialize!!! Finally. .... long chain ... 

% (BxxxStore.__getitem__ -> PxxxStore.get_raw -> DxxxStore._get_loose_object ->
%  S.from_path -> S.from_file -> S._parse_file -> S._parse_object -> 
%  S.set_raw_string -> S.set_raw_chunks) 
% | ShaFile.check 
% -> <>
<<[[ShaFile]] methods>>=
def _deserialize(self, chunks):
    raise NotImplementedError(self._deserialize)

@


% BaseObjectStore.__getitem__ -> <>
<<[[ShaFile]] methods>>=
@staticmethod
def from_raw_string(type_num, string, sha=None):
    """Creates an object of the indicated type from the raw string given.

    :param type_num: The numeric type of the object.
    :param string: The raw uncompressed contents.
    :param sha: Optional known sha for the object
    """
    obj = object_class(type_num)()
    obj.set_raw_string(string, sha)
    return obj

@
%bug? so call deserialize 2 times!!



% porcelain.branch_create | porcelain.tag_create -> <>
<<function objectspec.parse_object>>=
def parse_object(repo, objectish):
    """Parse a string referring to an object.

    :param repo: A `Repo` object
    :param objectish: A string referring to an object
    :return: A git object
    :raise KeyError: If the object can not be found
    """
    objectish = to_bytes(objectish)
    return repo[objectish]
@
% objectish is ref or sha1. use of repo[xxx] here

<<function objectspec.to_bytes>>=
def to_bytes(text):
    if getattr(text, "encode", None) is not None:
        text = text.encode('ascii')
    return text
@

\subsection{Commit}

% ?? -> <>
<<[[Commit]] methods>>=
@classmethod
def from_path(cls, path):
    commit = ShaFile.from_path(path)
    if not isinstance(commit, cls):
        raise NotCommitError(path)
    return commit

@
%dead? just for API?

% ShaFile.set_raw_chunks | ShaFile.check -> <>
<<[[Commit]] methods>>=
def _deserialize(self, chunks):
    (self._tree, self._parents, author_info, commit_info, self._encoding,
            self._mergetag, self._gpgsig, self._message, self._extra) = (
                    parse_commit(chunks))
    (self._author, self._author_time, (self._author_timezone,
         self._author_timezone_neg_utc)) = author_info
    (self._committer, self._commit_time, (self._commit_timezone,
         self._commit_timezone_neg_utc)) = commit_info

@

<<function objects.parse_commit>>=
def parse_commit(chunks):
    """Parse a commit object from chunks.

    :param chunks: Chunks to parse
    :return: Tuple of (tree, parents, author_info, commit_info,
        encoding, mergetag, gpgsig, message, extra)
    """
    parents = []
    extra = []
    tree = None
    author_info = (None, None, (None, None))
    commit_info = (None, None, (None, None))
    encoding = None
    mergetag = []
    message = None
    gpgsig = None

    for field, value in _parse_message(chunks):
        # TODO(jelmer): Enforce ordering
        if field == _TREE_HEADER:
            tree = value
        elif field == _PARENT_HEADER:
            parents.append(value)
        elif field == _AUTHOR_HEADER:
            author, timetext, timezonetext = value.rsplit(b' ', 2)
            author_time = int(timetext)
            author_info = (author, author_time, parse_timezone(timezonetext))
        elif field == _COMMITTER_HEADER:
            committer, timetext, timezonetext = value.rsplit(b' ', 2)
            commit_time = int(timetext)
            commit_info = (committer, commit_time, parse_timezone(timezonetext))
        elif field == _ENCODING_HEADER:
            encoding = value
        elif field == _MERGETAG_HEADER:
            mergetag.append(Tag.from_string(value + b'\n'))
        elif field == _GPGSIG_HEADER:
            gpgsig = value
        elif field is None:
            message = value
        else:
            extra.append((field, value))
    return (tree, parents, author_info, commit_info, encoding, mergetag,
            gpgsig, message, extra)

@

% Commit.check | ... -> <>
<<function objects._parse_message>>=
def _parse_message(chunks):
    """Parse a message with a list of fields and a body.

    :param chunks: the raw chunks of the tag or commit object.
    :return: iterator of tuples of (field, value), one per header line, in the
        order read from the text, possibly including duplicates. Includes a
        field named None for the freeform tag/commit text.
    """
    f = BytesIO(b''.join(chunks))
    k = None
    v = ""
    eof = False

    def _strip_last_newline(value):
        """Strip the last newline from value"""
        if value and value.endswith(b'\n'):
            return value[:-1]
        return value

    # Parse the headers
    #
    # Headers can contain newlines. The next line is indented with a space.
    # We store the latest key as 'k', and the accumulated value as 'v'.
    for l in f:
        if l.startswith(b' '):
            # Indented continuation of the previous line
            v += l[1:]
        else:
            if k is not None:
                # We parsed a new header, return its value
                yield (k, _strip_last_newline(v))
            if l == b'\n':
                # Empty line indicates end of headers
                break
            (k, v) = l.split(b' ', 1)

    else:
        # We reached end of file before the headers ended. We still need to
        # return the previous header, then we need to return a None field for
        # the text.
        eof = True
        if k is not None:
            yield (k, _strip_last_newline(v))
        yield (None, None)

    if not eof:
        # We didn't reach the end of file while parsing headers. We can return
        # the rest of the file as a message.
        yield (None, f.read())

    f.close()
@


\subsection{Blob}

<<[[Blob]] methods>>=
@classmethod
def from_path(cls, path):
    blob = ShaFile.from_path(path)
    if not isinstance(blob, cls):
        raise NotBlobError(path)
    return blob
@
%dead? just for API?

% ShaFile.set_raw_chunks | ShaFile.check -> <>
<<[[Blob]] methods>>=
def _deserialize(self, chunks):
    self._chunked_text = chunks
@

% ?? -> <>
<<function index.blob_from_path_and_stat>>=
def blob_from_path_and_stat(fs_path, st):
    """Create a blob from a path and a stat object.

    :param fs_path: Full file system path to file
    :param st: A stat object
    :return: A `Blob` object
    """
    assert isinstance(fs_path, bytes)
    blob = Blob()
    if not stat.S_ISLNK(st.st_mode):
        with open(fs_path, 'rb') as f:
            blob.data = f.read()
    else:
            blob.data = os.readlink(fs_path)
    return blob
@

%win32:
%        if sys.platform == 'win32' and sys.version_info[0] == 3:
%            # os.readlink on Python3 on Windows requires a unicode string.
%            # TODO(jelmer): Don't assume tree_encoding == fs_encoding
%            tree_encoding = sys.getfilesystemencoding()
%            fs_path = fs_path.decode(tree_encoding)
%            blob.data = os.readlink(fs_path).encode(tree_encoding)
%        else:


\subsection{Tree}

<<[[Tree]] methods>>=
@classmethod
def from_path(cls, filename):
    tree = ShaFile.from_path(filename)
    if not isinstance(tree, cls):
        raise NotTreeError(filename)
    return tree
@
%dead? just for API?

% ShaFile.set_raw_chunks | ShaFile.check -> <>
<<[[Tree]] methods>>=
def _deserialize(self, chunks):
    """Grab the entries in the tree"""
    try:
        parsed_entries = parse_tree(b''.join(chunks))
    except ValueError as e:
        raise ObjectFormatException(e)
    # TODO: list comprehension is for efficiency in the common (small)
    # case; if memory efficiency in the large case is a concern, use a genexp.
    self._entries = dict([(n, (m, s)) for n, m, s in parsed_entries])

@

% Tree.check | Tree._deserialize -> <>
<<function object.parse_tree>>=
def parse_tree(text, strict=False):
    """Parse a tree text.

    :param text: Serialized text to parse
    :return: iterator of tuples of (name, mode, sha)
    :raise ObjectFormatException: if the object was malformed in some way
    """
    count = 0
    l = len(text)
    while count < l:
        mode_end = text.index(b' ', count)
        mode_text = text[count:mode_end]
        if strict and mode_text.startswith(b'0'):
            raise ObjectFormatException("Invalid mode '%s'" % mode_text)
        try:
            mode = int(mode_text, 8)
        except ValueError:
            raise ObjectFormatException("Invalid mode '%s'" % mode_text)
        name_end = text.index(b'\0', mode_end)
        name = text[mode_end+1:name_end]
        count = name_end+21
        sha = text[name_end+1:count]
        if len(sha) != 20:
            raise ObjectFormatException("Sha has invalid length")
        hexsha = sha_to_hex(sha)
        yield (name, mode, hexsha)
@


\section{Index}

% porcelain.add -> <>
<<[[Repo]] methods>>=
def open_index(self):
    """Open the index for this repository.

    :raise NoIndexPresent: If no index is present
    :return: The matching `Index`
    """
    from dulwich.index import Index
    if not self.has_index():
        raise NoIndexPresent()
    return Index(self.index_path())
@

<<[[Repo]] methods>>=
def has_index(self):
    """Check if an index is present."""
    # Bare repos must never have index files; non-bare repos may have a
    # missing index file, which is treated as empty.
    return not self.bare
@


<<[[Repo]] methods>>=
def index_path(self):
    """Return path to the index file."""
    return os.path.join(self.controldir(), INDEX_FILENAME)
@

% Index.__init__ -> <>
<<[[Index]] methods>>=
def read(self):
    """Read current contents of index from disk."""
    if not os.path.exists(self._filename):
        return
    f = GitFile(self._filename, 'rb')
    try:
        f = SHA1Reader(f)
        for x in read_index(f):
            self[x[0]] = IndexEntry(*x[1:])
        # FIXME: Additional data?
        f.read(os.path.getsize(self._filename)-f.tell()-20)
        f.check_sha()
    finally:
        f.close()

@

<<class SHA1Reader>>=
class SHA1Reader(object):
    """Wrapper around a file-like object that remembers the SHA1 of its data."""

    def __init__(self, f):
        self.f = f
        self.sha1 = sha1(b'')

    def read(self, num=None):
        data = self.f.read(num)
        self.sha1.update(data)
        return data

    def check_sha(self):
        stored = self.f.read(20)
        if stored != self.sha1.digest():
            raise ChecksumMismatch(self.sha1.hexdigest(), sha_to_hex(stored))

    def close(self):
        return self.f.close()

    def tell(self):
        return self.f.tell()
@

<<function read_index>>=
def read_index(f):
    """Read an index file, yielding the individual entries."""
    header = f.read(4)
    if header != b'DIRC':
        raise AssertionError("Invalid index file header: %r" % header)
    (version, num_entries) = struct.unpack(b'>LL', f.read(4 * 2))
    assert version in (1, 2)
    for i in range(num_entries):
        yield read_cache_entry(f)
@

<<function index.read_cache_entry>>=
def read_cache_entry(f):
    """Read an entry from a cache file.

    :param f: File-like object to read from
    :return: tuple with: device, inode, mode, uid, gid, size, sha, flags
    """
    beginoffset = f.tell()
    ctime = read_cache_time(f)
    mtime = read_cache_time(f)
    (dev, ino, mode, uid, gid, size, sha, flags, ) = \
        struct.unpack(">LLLLLL20sH", f.read(20 + 4 * 6 + 2))
    name = f.read((flags & 0x0fff))
    # Padding:
    real_size = ((f.tell() - beginoffset + 8) & ~7)
    f.read((beginoffset + real_size) - f.tell())
    return (name, ctime, mtime, dev, ino, mode, uid, gid, size,
            sha_to_hex(sha), flags & ~0x0fff)
@

\section{Refs}

% RefsContainer.__getitem__ -> <>
<<[[RefsContainer]] methods>>=
def follow(self, name):
    """Follow a reference name.

    :return: a tuple of (refnames, sha), wheres refnames are the names of
        references in the chain
    """
    contents = SYMREF + name
    depth = 0
    refnames = []
    while contents.startswith(SYMREF):
        refname = contents[len(SYMREF):]
        refnames.append(refname)
        contents = self.read_ref(refname)
        if not contents:
            break
        depth += 1
        if depth > 5:
            raise KeyError(name)
    return refnames, contents

@

<<[[RefsContainer]] methods>>=
def read_ref(self, refname):
    """Read a reference without following any references.

    :param refname: The name of the reference
    :return: The contents of the ref file, or None if it does
        not exist.
    """
    contents = self.read_loose_ref(refname)
    if not contents:
        contents = self.get_packed_refs().get(refname, None)
    return contents

@

<<[[RefsContainer]] methods>>=
def read_loose_ref(self, name):
    """Read a loose reference and return its contents.

    :param name: the refname to read
    :return: The contents of the ref file, or None if it does
        not exist.
    """
    raise NotImplementedError(self.read_loose_ref)
@


<<[[DiskRefsContainer]] methods>>=
def read_loose_ref(self, name):
    """Read a reference file and return its contents.

    If the reference file a symbolic reference, only read the first line of
    the file. Otherwise, only read the first 40 bytes.

    :param name: the refname to read, relative to refpath
    :return: The contents of the ref file, or None if the file does not
        exist.
    :raises IOError: if any other error occurs
    """
    filename = self.refpath(name)
    try:
        with GitFile(filename, 'rb') as f:
            header = f.read(len(SYMREF))
            if header == SYMREF:
                # Read only the first line
                return header + next(iter(f)).rstrip(b'\r\n')
            else:
                # Read only the first 40 bytes
                return header + f.read(40 - len(SYMREF))
    except IOError as e:
        if e.errno == errno.ENOENT:
            return None
        raise
@


<<[[RefsContainer]] methods>>=
def allkeys(self):
    """All refs present in this container."""
    raise NotImplementedError(self.allkeys)

@

<<[[DiskRefsContainer]] methods>>=
def allkeys(self):
    allkeys = set()
    if os.path.exists(self.refpath(b'HEAD')):
        allkeys.add(b'HEAD')
    path = self.refpath(b'')
    for root, dirs, files in os.walk(self.refpath(b'refs')):
        dir = root[len(path):].strip(os.path.sep).replace(os.path.sep, "/")
        for filename in files:
            refname = ("%s/%s" % (dir, filename)).encode(sys.getfilesystemencoding())
            if check_ref_format(refname):
                allkeys.add(refname)
    <<[[DiskRefsContainer.allkeys()]] look in packed refs>>
    return allkeys

@


\chapter{Writing to a Repository}

\section{Objects}


<<[[BaseObjectStore]] methods>>=
def add_object(self, obj):
    """Add a single object to this object store.

    """
    raise NotImplementedError(self.add_object)

@

% Repo.stage  | BaseRepo.do_commit-> <>
<<[[DiskObjectStore]] methods>>=
def add_object(self, obj):
    """Add a single object to this object store.

    :param obj: Object to add
    """
    path = self._get_shafile_path(obj.id)
    dir = os.path.dirname(path)
    try:
        os.mkdir(dir)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise
    if os.path.exists(path):
        return  # Already there, no need to write again
    with GitFile(path, 'wb') as f:
        f.write(obj.as_legacy_object())
@
% so it uses the legacy format?

% ShaFile.as_raw_chunks -> <>
<<[[ShaFile]] methods>>=
def _serialize(self):
    raise NotImplementedError(self._serialize)

@


\subsection{Commit}

<<[[Commit]] methods>>=
def _serialize(self):
    chunks = []
    tree_bytes = self._tree.id if isinstance(self._tree, Tree) else self._tree
    chunks.append(git_line(_TREE_HEADER, tree_bytes))
    for p in self._parents:
        chunks.append(git_line(_PARENT_HEADER, p))
    chunks.append(git_line(
        _AUTHOR_HEADER, self._author, str(self._author_time).encode('ascii'),
        format_timezone(self._author_timezone,
                        self._author_timezone_neg_utc)))
    chunks.append(git_line(
        _COMMITTER_HEADER, self._committer, str(self._commit_time).encode('ascii'),
        format_timezone(self._commit_timezone,
                        self._commit_timezone_neg_utc)))
    if self.encoding:
        chunks.append(git_line(_ENCODING_HEADER, self.encoding))
    for mergetag in self.mergetag:
        mergetag_chunks = mergetag.as_raw_string().split(b'\n')

        chunks.append(git_line(_MERGETAG_HEADER, mergetag_chunks[0]))
        # Embedded extra header needs leading space
        for chunk in mergetag_chunks[1:]:
            chunks.append(b' ' + chunk + b'\n')

        # No trailing empty line
        if chunks[-1].endswith(b' \n'):
            chunks[-1] = chunks[-1][:-2]
    for k, v in self.extra:
        if b'\n' in k or b'\n' in v:
            raise AssertionError(
                "newline in extra data: %r -> %r" % (k, v))
        chunks.append(git_line(k, v))
    if self.gpgsig:
        sig_chunks = self.gpgsig.split(b'\n')
        chunks.append(git_line(_GPGSIG_HEADER, sig_chunks[0]))
        for chunk in sig_chunks[1:]:
            chunks.append(git_line(b'',  chunk))
    chunks.append(b'\n')  # There must be a new line after the headers
    chunks.append(self._message)
    return chunks

@


\subsection{Blob}

<<[[Blob]] methods>>=
def _serialize(self):
    return self._chunked_text

@

\subsection{Tree}


<<[[Tree]] methods>>=
def _serialize(self):
    return list(serialize_tree(self.iteritems()))
@

<<function object.serialize_tree>>=
def serialize_tree(items):
    """Serialize the items in a tree to a text.

    :param items: Sorted iterable over (name, mode, sha) tuples
    :return: Serialized tree text as chunks
    """
    for name, mode, hexsha in items:
        yield ("%04o" % mode).encode('ascii') + b' ' + name + b'\0' + hex_to_sha(hexsha)
@

\section{Index}

% porcelain.rm -> ()
<<[[Index]] methods>>=
def write(self):
    """Write current contents of index to disk."""
    f = GitFile(self._filename, 'wb')
    try:
        f = SHA1Writer(f)
        write_index_dict(f, self._byname)
    finally:
        f.close()

@

<<class SHA1Writer>>=
class SHA1Writer(object):
    """Wrapper around a file-like object that remembers the SHA1 of its data."""

    def __init__(self, f):
        self.f = f
        self.length = 0
        self.sha1 = sha1(b'')

    def write(self, data):
        self.sha1.update(data)
        self.f.write(data)
        self.length += len(data)

    def write_sha(self):
        sha = self.sha1.digest()
        assert len(sha) == 20
        self.f.write(sha)
        self.length += len(sha)
        return sha

    def close(self):
        sha = self.write_sha()
        self.f.close()
        return sha

    def offset(self):
        return self.length

    def tell(self):
        return self.f.tell()
@

<<function write_index_dict>>=
def write_index_dict(f, entries):
    """Write an index file based on the contents of a dictionary.

    """
    entries_list = []
    for name in sorted(entries):
        entries_list.append((name,) + tuple(entries[name]))
    write_index(f, entries_list)
@

<<function write_index>>=
def write_index(f, entries):
    """Write an index file.

    :param f: File-like object to write to
    :param entries: Iterable over the entries to write
    """
    f.write(b'DIRC')
    f.write(struct.pack(b'>LL', 2, len(entries)))
    for x in entries:
        write_cache_entry(f, x)
@

<<function index.write_cache_entry>>=
def write_cache_entry(f, entry):
    """Write an index entry to a file.

    :param f: File object
    :param entry: Entry to write, tuple with:
        (name, ctime, mtime, dev, ino, mode, uid, gid, size, sha, flags)
    """
    beginoffset = f.tell()
    (name, ctime, mtime, dev, ino, mode, uid, gid, size, sha, flags) = entry
    write_cache_time(f, ctime)
    write_cache_time(f, mtime)
    flags = len(name) | (flags &~ 0x0fff)
    f.write(struct.pack(b'>LLLLLL20sH', dev & 0xFFFFFFFF, ino & 0xFFFFFFFF, mode, uid, gid, size, hex_to_sha(sha), flags))
    f.write(name)
    real_size = ((f.tell() - beginoffset + 8) & ~7)
    f.write(b'\0' * ((beginoffset + real_size) - f.tell()))
@

\section{Refs}

\chapter{Staging a Diff}

\section{Adding files, [[git add]]}

<<function cmd_add>>=
class cmd_add(Command):

    def run(self, args):
        opts, args = getopt(args, "", [])

        porcelain.add(".", paths=args)
@

<<function porcelain.add>>=
def add(repo=".", paths=None):
    """Add files to the staging area.

    :param repo: Repository for the files
    :param paths: Paths to add.  No value passed stages all modified files.
    """
    with open_repo_closing(repo) as r:
        if not paths:
            paths = list(get_untracked_paths(os.getcwd(), r.path,
                r.open_index()))
        # TODO(jelmer): Possibly allow passing in absolute paths?
        relpaths = []
        if not isinstance(paths, list):
            paths = [paths]
        for p in paths:
            # FIXME: Support patterns, directories.
            if os.path.isabs(p) and p.startswith(repo.path):
                relpath = os.path.relpath(p, repo.path)
            else:
                relpath = p
            relpaths.append(relpath)
        r.stage(relpaths)
@




<<function porcelain.get_untracked_paths>>=
def get_untracked_paths(frompath, basepath, index):
    """Get untracked paths.

    ;param frompath: Path to walk
    :param basepath: Path to compare to
    :param index: Index to check against
    """
    # If nothing is specified, add all non-ignored files.
    for dirpath, dirnames, filenames in os.walk(frompath):
        # Skip .git and below.
        if '.git' in dirnames:
            dirnames.remove('.git')
        for filename in filenames:
            p = os.path.join(dirpath[len(basepath)+1:], filename)
            if p not in index:
                yield p
@

\section{Removing files, [[git rm]]}

<<function cmd_rm>>=
class cmd_rm(Command):

    def run(self, args):
        opts, args = getopt(args, "", [])

        porcelain.rm(".", paths=args)
@

<<function porcelain.rm>>=
def rm(repo=".", paths=None):
    """Remove files from the staging area.

    :param repo: Repository for the files
    :param paths: Paths to remove
    """
    with open_repo_closing(repo) as r:
        index = r.open_index()
        for p in paths:
            del index[p.encode(sys.getfilesystemencoding())]
        index.write()
@

\section{[[Repo.stage()]]}

<<[[Repo]] methods>>=
def stage(self, fs_paths):
    """Stage a set of paths.

    :param fs_paths: List of paths, relative to the repository path
    """

    root_path_bytes = self.path.encode(sys.getfilesystemencoding())

    if not isinstance(fs_paths, list):
        fs_paths = [fs_paths]
    from dulwich.index import (
        blob_from_path_and_stat,
        index_entry_from_stat,
        _fs_to_tree_path,
        )
    index = self.open_index()
    for fs_path in fs_paths:
        if not isinstance(fs_path, bytes):
            fs_path = fs_path.encode(sys.getfilesystemencoding())
        if os.path.isabs(fs_path):
            raise ValueError(
                "path %r should be relative to "
                "repository root, not absolute" % fs_path)
        tree_path = _fs_to_tree_path(fs_path)
        full_path = os.path.join(root_path_bytes, fs_path)
        try:
            st = os.lstat(full_path)
        except OSError:
            # File no longer exists
            try:
                del index[tree_path]
            except KeyError:
                pass  # already removed
        else:
            blob = blob_from_path_and_stat(full_path, st)
            self.object_store.add_object(blob)
            index[tree_path] = index_entry_from_stat(st, blob.id, 0)
    index.write()
@


\section{[[git reset]]}

<<function cmd_reset>>=
class cmd_reset(Command):

    def run(self, args):
        opts, args = getopt(args, "", ["hard", "soft", "mixed"])
        opts = dict(opts)

        mode = ""
        if "--hard" in opts:
            mode = "hard"
        elif "--soft" in opts:
            mode = "soft"
        elif "--mixed" in opts:
            mode = "mixed"

        porcelain.reset('.', mode=mode, *args)

@

<<function porcelain.reset>>=
def reset(repo, mode, committish="HEAD"):
    """Reset current HEAD to the specified state.

    :param repo: Path to repository
    :param mode: Mode ("hard", "soft", "mixed")
    """

    if mode != "hard":
        raise ValueError("hard is the only mode currently supported")

    with open_repo_closing(repo) as r:
        tree = r[committish].tree
        r.reset_index(tree)
@

% porcelain.reset | porcelain.clone -> <>
<<[[Repo]] methods>>=
def reset_index(self, tree=None):
    """Reset the index back to a specific tree.

    :param tree: Tree SHA to reset to, None for current HEAD tree.
    """
    from dulwich.index import (
        build_index_from_tree,
        validate_path_element_default,
        validate_path_element_ntfs,
        )
    if tree is None:
        tree = self[b'HEAD'].tree
    config = self.get_config()
    honor_filemode = config.get_boolean(
        'core', 'filemode', os.name != "nt")
    validate_path_element = validate_path_element_default
    return build_index_from_tree(
        self.path, self.index_path(), self.object_store, tree,
        honor_filemode=honor_filemode,
        validate_path_element=validate_path_element)
@

%win32:
%    if config.get_boolean('core', 'core.protectNTFS', os.name == "nt"):
%        validate_path_element = validate_path_element_ntfs
%    else:

%def validate_path_element_ntfs(element):
%    stripped = element.rstrip(b". ").lower()
%    if stripped in INVALID_DOTNAMES:
%        return False
%    if stripped == b"git~1":
%        return False
%    return True


<<function build_index_from_tree>>=
def build_index_from_tree(root_path, index_path, object_store, tree_id,
                          honor_filemode=True,
                          validate_path_element=validate_path_element_default):
    """Generate and materialize index from a tree

    :param tree_id: Tree to materialize
    :param root_path: Target dir for materialized index files
    :param index_path: Target path for generated index
    :param object_store: Non-empty object store holding tree contents
    :param honor_filemode: An optional flag to honor core.filemode setting in
        config file, default is core.filemode=True, change executable bit
    :param validate_path_element: Function to validate path elements to check out;
        default just refuses .git and .. directories.

    :note:: existing index is wiped and contents are not merged
        in a working dir. Suitable only for fresh clones.
    """

    index = Index(index_path)
    if not isinstance(root_path, bytes):
        root_path = root_path.encode(sys.getfilesystemencoding())

    for entry in object_store.iter_tree_contents(tree_id):
        if not validate_path(entry.path, validate_path_element):
            continue
        full_path = _tree_to_fs_path(root_path, entry.path)

        if not os.path.exists(os.path.dirname(full_path)):
            os.makedirs(os.path.dirname(full_path))

        # TODO(jelmer): Merge new index into working tree
        if S_ISGITLINK(entry.mode):
            if not os.path.isdir(full_path):
                os.mkdir(full_path)
            st = os.lstat(full_path)
            # TODO(jelmer): record and return submodule paths
        else:
            obj = object_store[entry.sha]
            st = build_file_from_blob(
                obj, entry.mode, full_path, honor_filemode=honor_filemode)
        # Add file to index
        if not honor_filemode or S_ISGITLINK(entry.mode):
            # we can not use tuple slicing to build a new tuple,
            # because on windows that will convert the times to
            # longs, which causes errors further along
            st_tuple = (entry.mode, st.st_ino, st.st_dev, st.st_nlink,
                        st.st_uid, st.st_gid, st.st_size, st.st_atime,
                        st.st_mtime, st.st_ctime)
            st = st.__class__(st_tuple)
        index[entry.path] = index_entry_from_stat(st, entry.sha, 0)

    index.write()
@

<<function build_file_from_blob>>=
def build_file_from_blob(blob, mode, target_path, honor_filemode=True):
    """Build a file or symlink on disk based on a Git object.

    :param obj: The git object
    :param mode: File mode
    :param target_path: Path to write to
    :param honor_filemode: An optional flag to honor core.filemode setting in
        config file, default is core.filemode=True, change executable bit
    :return: stat object for the file
    """
    try:
        oldstat = os.lstat(target_path)
    except OSError as e:
        if e.errno == errno.ENOENT:
            oldstat = None
        else:
            raise
    contents = blob.as_raw_string()
    if stat.S_ISLNK(mode):
        # FIXME: This will fail on Windows. What should we do instead?
        if oldstat:
            os.unlink(target_path)
        os.symlink(contents, target_path)
    else:
        if oldstat is not None and oldstat.st_size == len(contents):
            with open(target_path, 'rb') as f:
                if f.read() == contents:
                    return oldstat

        with open(target_path, 'wb') as f:
            # Write out file
            f.write(contents)

        if honor_filemode:
            os.chmod(target_path, mode)

    return os.lstat(target_path)
@

%win32:
%        if sys.platform == 'win32' and sys.version_info[0] == 3:
%            # os.readlink on Python3 on Windows requires a unicode string.
%            # TODO(jelmer): Don't assume tree_encoding == fs_encoding
%            tree_encoding = sys.getfilesystemencoding()
%            contents = contents.decode(tree_encoding)
%            target_path = target_path.decode(tree_encoding)

\chapter{Committing a Diff}


\section{Commiting files, [[git commit]]}

<<function cmd_commit>>=
class cmd_commit(Command):

    def run(self, args):
        opts, args = getopt(args, "", ["message"])
        opts = dict(opts)

        porcelain.commit(".", message=opts["--message"])
@

<<function porcelain.commit>>=
def commit(repo=".", message=None, author=None, committer=None):
    """Create a new commit.

    :param repo: Path to repository
    :param message: Optional commit message
    :param author: Optional author name and email
    :param committer: Optional committer name and email
    :return: SHA1 of the new commit
    """
    # FIXME: Support --all argument
    # FIXME: Support --signoff argument

    with open_repo_closing(repo) as r:
        return r.do_commit(message=message, author=author, committer=committer)
@



<<[[BaseRepo]] methods>>=
def do_commit(self, message=None, committer=None,
              author=None, commit_timestamp=None,
              commit_timezone=None, author_timestamp=None,
              author_timezone=None, tree=None, encoding=None,
              ref=b'HEAD', merge_heads=None):
    """Create a new commit.

    :param message: Commit message
    :param committer: Committer fullname
    :param author: Author fullname (defaults to committer)
    :param commit_timestamp: Commit timestamp (defaults to now)
    :param commit_timezone: Commit timestamp timezone (defaults to GMT)
    :param author_timestamp: Author timestamp (defaults to commit
        timestamp)
    :param author_timezone: Author timestamp timezone
        (defaults to commit timestamp timezone)
    :param tree: SHA1 of the tree root to use (if not specified the
        current index will be committed).
    :param encoding: Encoding
    :param ref: Optional ref to commit to (defaults to current branch)
    :param merge_heads: Merge heads (defaults to .git/MERGE_HEADS)
    :return: New commit SHA1
    """
    import time
    c = Commit()
    if tree is None:
        index = self.open_index()
        c.tree = index.commit(self.object_store)
    else:
        if len(tree) != 40:
            raise ValueError("tree must be a 40-byte hex sha string")
        c.tree = tree

    <<[[BaseRepo.do_commit()]] execute pre-commit hook>>

    if merge_heads is None:
        # FIXME: Read merge heads from .git/MERGE_HEADS
        merge_heads = []
    if committer is None:
        # FIXME: Support GIT_COMMITTER_NAME/GIT_COMMITTER_EMAIL environment
        # variables
        committer = self._get_user_identity()
    c.committer = committer
    if commit_timestamp is None:
        # FIXME: Support GIT_COMMITTER_DATE environment variable
        commit_timestamp = time.time()
    c.commit_time = int(commit_timestamp)
    if commit_timezone is None:
        # FIXME: Use current user timezone rather than UTC
        commit_timezone = 0
    c.commit_timezone = commit_timezone
    if author is None:
        # FIXME: Support GIT_AUTHOR_NAME/GIT_AUTHOR_EMAIL environment
        # variables
        author = committer
    c.author = author
    if author_timestamp is None:
        # FIXME: Support GIT_AUTHOR_DATE environment variable
        author_timestamp = commit_timestamp
    c.author_time = int(author_timestamp)
    if author_timezone is None:
        author_timezone = commit_timezone
    c.author_timezone = author_timezone
    if encoding is not None:
        c.encoding = encoding
    if message is None:
        # FIXME: Try to read commit message from .git/MERGE_MSG
        raise ValueError("No commit message specified")

    <<[[BaseRepo.do_commit()]] execute commit-msg hook>>

    if ref is None:
        # Create a dangling commit
        c.parents = merge_heads
        self.object_store.add_object(c)
    else:
        try:
            old_head = self.refs[ref]
            c.parents = [old_head] + merge_heads
            self.object_store.add_object(c)
            ok = self.refs.set_if_equals(ref, old_head, c.id)
        except KeyError:
            c.parents = merge_heads
            self.object_store.add_object(c)
            ok = self.refs.add_if_new(ref, c.id)
        if not ok:
            # Fail if the atomic compare-and-swap failed, leaving the
            # commit and all its objects as garbage.
            raise CommitError("%s changed during commit" % (ref,))

    <<[[BaseRepo.do_commit()]] execute post-commit hook>>

    return c.id
@



<<[[Index]] methods>>=
def commit(self, object_store):
    """Create a new tree from an index.

    :param object_store: Object store to save the tree in
    :return: Root tree SHA
    """
    return commit_tree(object_store, self.iterblobs())
@

<<function index.commit_tree>>=
def commit_tree(object_store, blobs):
    """Commit a new tree.

    :param object_store: Object store to add trees to
    :param blobs: Iterable over blob path, sha, mode entries
    :return: SHA1 of the created tree.
    """

    trees = {b'': {}}

    def add_tree(path):
        if path in trees:
            return trees[path]
        dirname, basename = pathsplit(path)
        t = add_tree(dirname)
        assert isinstance(basename, bytes)
        newtree = {}
        t[basename] = newtree
        trees[path] = newtree
        return newtree

    for path, sha, mode in blobs:
        tree_path, basename = pathsplit(path)
        tree = add_tree(tree_path)
        tree[basename] = (mode, sha)

    def build_tree(path):
        tree = Tree()
        for basename, entry in trees[path].items():
            if isinstance(entry, dict):
                mode = stat.S_IFDIR
                sha = build_tree(pathjoin(path, basename))
            else:
                (mode, sha) = entry
            tree.add(basename, mode, sha)
        object_store.add_object(tree)
        return tree.id
    return build_tree(b'')
@

\chapter{Branching}
% purpose: parallel development
% gitless records working version of files, so can easily switch branch!
% no problem if got uncommitted stuff in current dir or untracked file
% that conflicts with tracked file in another branch, it will be saved!
% in gitless, it really is like you had 2 separate dirs with 2
%  separate working versions!

\section{Listing branches, [[git branch]]}

% ??? -> <>
<<function porcelain.branch_list>>=
def branch_list(repo):
    """List all branches.

    :param repo: Path to the repository
    """
    with open_repo_closing(repo) as r:
        return r.refs.keys(base=b"refs/heads/")

@

<<[[RefsContainer]] methods>>=
def keys(self, base=None):
    """Refs present in this container.

    :param base: An optional base to return refs under.
    :return: An unsorted set of valid refs in this container, including
        packed refs.
    """
    if base is not None:
        return self.subkeys(base)
    else:
        return self.allkeys()

@


<<[[RefsContainer]] methods>>=
def subkeys(self, base):
    """Refs present in this container under a base.

    :param base: The base to return refs under.
    :return: A set of valid refs in this container under the base; the base
        prefix is stripped from the ref names returned.
    """
    keys = set()
    base_len = len(base) + 1
    for refname in self.allkeys():
        if refname.startswith(base):
            keys.add(refname[base_len:])
    return keys

@

% allkeys in Parsing chapter


\section{Creating branches, [[git checkout -b]]}

% ??? -> <>
<<function porcelain.branch_create>>=
def branch_create(repo, name, objectish=None, force=False):
    """Create a branch.

    :param repo: Path to the repository
    :param name: Name of the new branch
    :param objectish: Target object to point new branch at (defaults to HEAD)
    :param force: Force creation of branch, even if it already exists
    """
    with open_repo_closing(repo) as r:
        if objectish is None:
            objectish = "HEAD"
        object = parse_object(r, objectish)
        refname = b"refs/heads/" + name
        if refname in r.refs and not force:
            raise KeyError("Branch with name %s already exists." % name)
        r.refs[refname] = object.id
@

% and then usually checkout!

\section{Deleting branches, [[git branch -d]]}

<<function porcelain.branch_delete>>=
def branch_delete(repo, name):
    """Delete a branch.

    :param repo: Path to the repository
    :param name: Name of the branch
    """
    with open_repo_closing(repo) as r:
        if isinstance(name, bytes):
            names = [name]
        elif isinstance(name, list):
            names = name
        else:
            raise TypeError("Unexpected branch name type %r" % name)
        for name in names:
            del r.refs[b"refs/heads/" + name]
@


\chapter{Pulling and Pushing}


\section{[[git pull]]}

<<function cmd_pull>>=
class cmd_pull(Command):

    def run(self, args):
        parser = optparse.OptionParser()
        options, args = parser.parse_args(args)
        try:
            from_location = args[0]
        except IndexError:
            from_location = None

        porcelain.pull('.', from_location)
@

<<function porcelain.pull>>=
def pull(repo, remote_location=None, refspecs=None,
         outstream=default_bytes_out_stream,
         errstream=default_bytes_err_stream):
    """Pull from remote via dulwich.client

    :param repo: Path to repository
    :param remote_location: Location of the remote
    :param refspec: refspecs to fetch
    :param outstream: A stream file to write to output
    :param errstream: A stream file to write to errors
    """
    # Open the repo
    with open_repo_closing(repo) as r:
        if remote_location is None:
            # TODO(jelmer): Lookup 'remote' for current branch in config
            raise NotImplementedError(
                "looking up remote from branch config not supported yet")
        if refspecs is None:
            refspecs = [b"HEAD"]
        selected_refs = []

        def determine_wants(remote_refs):
            selected_refs.extend(
                parse_reftuples(remote_refs, r.refs, refspecs))
            return [remote_refs[lh] for (lh, rh, force) in selected_refs]
        client, path = get_transport_and_path(remote_location)
        remote_refs = client.fetch(
            path, r, progress=errstream.write, determine_wants=determine_wants)
        for (lh, rh, force) in selected_refs:
            r.refs[rh] = remote_refs[lh]
        if selected_refs:
            r[b'HEAD'] = remote_refs[selected_refs[0][1]]

        # Perform 'git checkout .' - syncs staged changes
        tree = r[b"HEAD"].tree
        r.reset_index()
@

\section{[[git push]]}

<<function porcelain.push>>=
def push(repo, remote_location, refspecs,
         outstream=default_bytes_out_stream,
         errstream=default_bytes_err_stream):
    """Remote push with dulwich via dulwich.client

    :param repo: Path to repository
    :param remote_location: Location of the remote
    :param refspecs: Refs to push to remote
    :param outstream: A stream file to write output
    :param errstream: A stream file to write errors
    """

    # Open the repo
    with open_repo_closing(repo) as r:

        # Get the client and path
        client, path = get_transport_and_path(remote_location)

        selected_refs = []

        def update_refs(refs):
            selected_refs.extend(parse_reftuples(r.refs, refs, refspecs))
            new_refs = {}
            # TODO: Handle selected_refs == {None: None}
            for (lh, rh, force) in selected_refs:
                if lh is None:
                    new_refs[rh] = ZERO_SHA
                else:
                    new_refs[rh] = r.refs[lh]
            return new_refs

        err_encoding = getattr(errstream, 'encoding', None) or DEFAULT_ENCODING
        remote_location_bytes = client.get_url(path).encode(err_encoding)
        try:
            client.send_pack(
                path, update_refs, r.object_store.generate_pack_contents,
                progress=errstream.write)
            errstream.write(
                b"Push to " + remote_location_bytes + b" successful.\n")
        except (UpdateRefsError, SendPackError) as e:
            errstream.write(b"Push to " + remote_location_bytes +
                            b" failed -> " + e.message.encode(err_encoding) +
                            b"\n")
@

\chapter{Inspecting}

\section{[[git log]]}

<<function cmd_log>>=
class cmd_log(Command):

    def run(self, args):
        parser = optparse.OptionParser()
        parser.add_option("--reverse", dest="reverse", action="store_true",
                          help="Reverse order in which entries are printed")
        parser.add_option("--name-status", dest="name_status", action="store_true",
                          help="Print name/status for each changed file")
        options, args = parser.parse_args(args)

        porcelain.log(".", paths=args, reverse=options.reverse,
                      name_status=options.name_status,
                      outstream=sys.stdout)
@

<<function porcelain.log>>=
def log(repo=".", paths=None, outstream=sys.stdout, max_entries=None,
        reverse=False, name_status=False):
    """Write commit logs.

    :param repo: Path to repository
    :param paths: Optional set of specific paths to print entries for
    :param outstream: Stream to write log output to
    :param reverse: Reverse order in which entries are printed
    :param name_status: Print name status
    :param max_entries: Optional maximum number of entries to display
    """
    with open_repo_closing(repo) as r:
        walker = r.get_walker(
            max_entries=max_entries, paths=paths, reverse=reverse)
        for entry in walker:
            def decode(x):
                return commit_decode(entry.commit, x)
            print_commit(entry.commit, decode, outstream)
            if name_status:
                outstream.writelines(
                    [l+'\n' for l in print_name_status(entry.changes())])
@


\section{Walker}

\section{[[git show]]}

<<function cmd_show>>=
class cmd_show(Command):

    def run(self, args):
        opts, args = getopt(args, "", [])
        porcelain.show(".", args)
@

<<function porcelain.show>>=
# TODO(jelmer): better default for encoding?
def show(repo=".", objects=None, outstream=sys.stdout,
         default_encoding=DEFAULT_ENCODING):
    """Print the changes in a commit.

    :param repo: Path to repository
    :param objects: Objects to show (defaults to [HEAD])
    :param outstream: Stream to write to
    :param default_encoding: Default encoding to use if none is set in the
        commit
    """
    if objects is None:
        objects = ["HEAD"]
    if not isinstance(objects, list):
        objects = [objects]
    with open_repo_closing(repo) as r:
        for objectish in objects:
            o = parse_object(r, objectish)
            if isinstance(o, Commit):
                def decode(x):
                    return commit_decode(o, x, default_encoding)
            else:
                def decode(x):
                    return x.decode(default_encoding)
            show_object(r, o, decode, outstream)
@

\section{[[git diff]]}

<<function cmd_diff>>=
class cmd_diff(Command):

    def run(self, args):
        opts, args = getopt(args, "", [])

        if args == []:
            print("Usage: dulwich diff COMMITID")
            sys.exit(1)

        r = Repo(".")
        commit_id = args[0]
        commit = r[commit_id]
        parent_commit = r[commit.parents[0]]

        write_tree_diff(sys.stdout, r.object_store, parent_commit.tree, commit.tree)

@

\section{[[git status]]}

<<function cmd_status>>=
class cmd_status(Command):

    def run(self, args):
        parser = optparse.OptionParser()
        options, args = parser.parse_args(args)
        if len(args) >= 1:
            gitdir = args[0]
        else:
            gitdir = '.'

        status = porcelain.status(gitdir)
        if any(names for (kind, names) in status.staged.items()):
            sys.stdout.write("Changes to be committed:\n\n")
            for kind, names in status.staged.items():
                for name in names:
                    sys.stdout.write("\t%s: %s\n" % (
                        kind, name.decode(sys.getfilesystemencoding())))
            sys.stdout.write("\n")
        if status.unstaged:
            sys.stdout.write("Changes not staged for commit:\n\n")
            for name in status.unstaged:
                sys.stdout.write("\t%s\n" %
                        name.decode(sys.getfilesystemencoding()))
            sys.stdout.write("\n")
        if status.untracked:
            sys.stdout.write("Untracked files:\n\n")
            for name in status.untracked:
                sys.stdout.write("\t%s\n" % name)
            sys.stdout.write("\n")
@

<<function porcelain.status>>=
def status(repo="."):
    """Returns staged, unstaged, and untracked changes relative to the HEAD.

    :param repo: Path to repository or repository object
    :return: GitStatus tuple,
        staged -    list of staged paths (diff index/HEAD)
        unstaged -  list of unstaged paths (diff index/working-tree)
        untracked - list of untracked, un-ignored & non-.git paths
    """
    with open_repo_closing(repo) as r:
        # 1. Get status of staged
        tracked_changes = get_tree_changes(r)
        # 2. Get status of unstaged
        index = r.open_index()
        unstaged_changes = list(get_unstaged_changes(index, r.path))
        untracked_changes = list(get_untracked_paths(r.path, r.path, index))
        return GitStatus(tracked_changes, unstaged_changes, untracked_changes)
@

<<type GitStatus>>=
# Module level tuple definition for status output
GitStatus = namedtuple('GitStatus', 'staged unstaged untracked')
@

\section{[[git grep]]}

% pretty useful

\chapter{Networking}

% Distributed VCS! 

% should not need that code below. Should mount fs with NFS or sshfs
% or plan9 fs or whatever and then simply use LocalGitClient!

<<[[get_transport_and_path()]] try parse location as a URL>>=
# First, try to parse it as a URL
try:
    return get_transport_and_path_from_url(location, **kwargs)
except ValueError:
    pass
<<[[get_transport_and_path()]] try parse location as a ssh URL>>
@

<<function get_transport_and_path_from_url>>=
def get_transport_and_path_from_url(url, config=None, **kwargs):
    """Obtain a git client from a URL.

    :param url: URL to open (a unicode string)
    :param config: Optional config object
    :param thin_packs: Whether or not thin packs should be retrieved
    :param report_activity: Optional callback for reporting transport
        activity.
    :return: Tuple with client instance and relative path.
    """
    parsed = urlparse.urlparse(url)
    if parsed.scheme == 'git':
        return (TCPGitClient.from_parsedurl(parsed, **kwargs),
                parsed.path)
    <<[[get_transport_and_path_from_url()]] elif ssh>>
    <<[[get_transport_and_path_from_url()]] elif http>>
    elif parsed.scheme == 'file':
        return default_local_git_client_cls.from_parsedurl(
            parsed, **kwargs), parsed.path

    raise ValueError("unknown scheme '%s'" % parsed.scheme)
@



\section{[[git://]] protocol}

\section{Capabilities}

<<[[GitClient.__init__()]] set capabilities>>=
self._fetch_capabilities = set(FETCH_CAPABILITIES)
self._fetch_capabilities.add(capability_agent())
self._send_capabilities = set(SEND_CAPABILITIES)
self._send_capabilities.add(capability_agent())
@

<<constant client.FETCH_CAPABILITIES>>=
FETCH_CAPABILITIES = ([CAPABILITY_THIN_PACK, 
                       CAPABILITY_MULTI_ACK,
                       CAPABILITY_MULTI_ACK_DETAILED] +
                      COMMON_CAPABILITIES)
@
% see Adv topics.

<<constant client.COMMON_CAPABILITIES>>=
COMMON_CAPABILITIES = [CAPABILITY_OFS_DELTA, CAPABILITY_SIDE_BAND_64K]
@

<<constant client.SEND_CAPABILITIES>>=
SEND_CAPABILITIES = [CAPABILITY_REPORT_STATUS] + COMMON_CAPABILITIES
@

\section{Client}

<<[[GitClient]] methods>>=
def get_url(self, path):
    """Retrieves full url to given path.

    :param path: Repository path (as string)
    :return: Url to path (as string)
    """
    raise NotImplementedError(self.get_url)

@

<<[[LocalGitClient]] methods>>=
def get_url(self, path):
    return urlparse.urlunsplit(('file', '', path, '', ''))

@


<<[[GitClient]] methods>>=
@classmethod
def from_parsedurl(cls, parsedurl, **kwargs):
    """Create an instance of this client from a urlparse.parsed object.

    :param parsedurl: Result of urlparse.urlparse()
    :return: A `GitClient` object
    """
    raise NotImplementedError(cls.from_parsedurl)
@

<<[[LocalGitClient]] methods>>=
@classmethod
def from_parsedurl(cls, parsedurl, **kwargs):
    return cls(**kwargs)

@


<<class TraditionalGitClient>>=
class TraditionalGitClient(GitClient):
    """Traditional Git client."""

    DEFAULT_ENCODING = 'utf-8'

    <<[[TraditionalGitClient]] methods>>
@


<<[[TraditionalGitClient]] methods>>=
def __init__(self, path_encoding=DEFAULT_ENCODING, **kwargs):
    self._remote_path_encoding = path_encoding
    super(TraditionalGitClient, self).__init__(**kwargs)

def _connect(self, cmd, path):
    """Create a connection to the server.

    This method is abstract - concrete implementations should
    implement their own variant which connects to the server and
    returns an initialized Protocol object with the service ready
    for use and a can_read function which may be used to see if
    reads would block.

    :param cmd: The git service name to which we should connect.
    :param path: The path we should pass to the service. (as bytestirng)
    """
    raise NotImplementedError()

def send_pack(self, path, update_refs, generate_pack_contents,
              progress=None, write_pack=write_pack_objects):
    """Upload a pack to a remote repository.

    :param path: Repository path (as bytestring)
    :param update_refs: Function to determine changes to remote refs.
        Receive dict with existing remote refs, returns dict with
        changed refs (name -> sha, where sha=ZERO_SHA for deletions)
    :param generate_pack_contents: Function that can return a sequence of
        the shas of the objects to upload.
    :param progress: Optional callback called with progress updates
    :param write_pack: Function called with (file, iterable of objects) to
        write the objects returned by generate_pack_contents to the server.

    :raises SendPackError: if server rejects the pack data
    :raises UpdateRefsError: if the server supports report-status
                             and rejects ref updates
    :return: new_refs dictionary containing the changes that were made
        {refname: new_ref}, including deleted refs.
    """
    proto, unused_can_read = self._connect(b'receive-pack', path)
    with proto:
        old_refs, server_capabilities = read_pkt_refs(proto)
        negotiated_capabilities = (
            self._send_capabilities & server_capabilities)

        if CAPABILITY_REPORT_STATUS in negotiated_capabilities:
            self._report_status_parser = ReportStatusParser()
        report_status_parser = self._report_status_parser

        try:
            new_refs = orig_new_refs = update_refs(dict(old_refs))
        except:
            proto.write_pkt_line(None)
            raise

        if CAPABILITY_DELETE_REFS not in server_capabilities:
            # Server does not support deletions. Fail later.
            new_refs = dict(orig_new_refs)
            for ref, sha in orig_new_refs.items():
                if sha == ZERO_SHA:
                    if CAPABILITY_REPORT_STATUS in negotiated_capabilities:
                        report_status_parser._ref_statuses.append(
                            b'ng ' + sha +
                            b' remote does not support deleting refs')
                        report_status_parser._ref_status_ok = False
                    del new_refs[ref]

        if new_refs is None:
            proto.write_pkt_line(None)
            return old_refs

        if len(new_refs) == 0 and len(orig_new_refs):
            # NOOP - Original new refs filtered out by policy
            proto.write_pkt_line(None)
            if report_status_parser is not None:
                report_status_parser.check()
            return old_refs

        (have, want) = self._handle_receive_pack_head(
            proto, negotiated_capabilities, old_refs, new_refs)
        if (not want and
                set(new_refs.items()).issubset(set(old_refs.items()))):
            return new_refs
        objects = generate_pack_contents(have, want)

        dowrite = len(objects) > 0
        dowrite = dowrite or any(old_refs.get(ref) != sha
                                 for (ref, sha) in new_refs.items()
                                 if sha != ZERO_SHA)
        if dowrite:
            write_pack(proto.write_file(), objects)

        self._handle_receive_pack_tail(
            proto, negotiated_capabilities, progress)
        return new_refs

def fetch_pack(self, path, determine_wants, graph_walker, pack_data,
               progress=None):
    """Retrieve a pack from a git smart server.

    :param path: Remote path to fetch from
    :param determine_wants: Function determine what refs
        to fetch. Receives dictionary of name->sha, should return
        list of shas to fetch.
    :param graph_walker: Object with next() and ack().
    :param pack_data: Callback called for each bit of data in the pack
    :param progress: Callback for progress reports (strings)
    :return: Dictionary with all remote refs (not just those fetched)
    """
    proto, can_read = self._connect(b'upload-pack', path)
    with proto:
        refs, server_capabilities = read_pkt_refs(proto)
        negotiated_capabilities = (
            self._fetch_capabilities & server_capabilities)

        if refs is None:
            proto.write_pkt_line(None)
            return refs

        try:
            wants = determine_wants(refs)
        except:
            proto.write_pkt_line(None)
            raise
        if wants is not None:
            wants = [cid for cid in wants if cid != ZERO_SHA]
        if not wants:
            proto.write_pkt_line(None)
            return refs
        self._handle_upload_pack_head(
            proto, negotiated_capabilities, graph_walker, wants, can_read)
        self._handle_upload_pack_tail(
            proto, negotiated_capabilities, graph_walker, pack_data,
            progress)
        return refs

def get_refs(self, path):
    """Retrieve the current refs from a git smart server."""
    # stock `git ls-remote` uses upload-pack
    proto, _ = self._connect(b'upload-pack', path)
    with proto:
        refs, _ = read_pkt_refs(proto)
        proto.write_pkt_line(None)
        return refs

def archive(self, path, committish, write_data, progress=None,
            write_error=None):
    proto, can_read = self._connect(b'upload-archive', path)
    with proto:
        proto.write_pkt_line(b"argument " + committish)
        proto.write_pkt_line(None)
        pkt = proto.read_pkt_line()
        if pkt == b"NACK\n":
            return
        elif pkt == b"ACK\n":
            pass
        elif pkt.startswith(b"ERR "):
            raise GitProtocolError(pkt[4:].rstrip(b"\n"))
        else:
            raise AssertionError("invalid response %r" % pkt)
        ret = proto.read_pkt_line()
        if ret is not None:
            raise AssertionError("expected pkt tail")
        self._read_side_band64k_data(proto, {
            SIDE_BAND_CHANNEL_DATA: write_data,
            SIDE_BAND_CHANNEL_PROGRESS: progress,
            SIDE_BAND_CHANNEL_FATAL: write_error})
@


<<class TCPGitClient>>=
class TCPGitClient(TraditionalGitClient):
    """A Git Client that works over TCP directly (i.e. git://)."""

    <<[[TCPGitClient]] methods>>
@


<<[[TCPGitClient]] methods>>=
def __init__(self, host, port=None, **kwargs):
    if port is None:
        port = TCP_GIT_PORT
    self._host = host
    self._port = port
    super(TCPGitClient, self).__init__(**kwargs)
@

<<[[TCPGitClient]] methods>>=
@classmethod
def from_parsedurl(cls, parsedurl, **kwargs):
    return cls(parsedurl.hostname, port=parsedurl.port, **kwargs)

@

<<[[TCPGitClient]] methods>>=
def get_url(self, path):
    netloc = self._host
    if self._port is not None and self._port != TCP_GIT_PORT:
        netloc += ":%d" % self._port
    return urlparse.urlunsplit(("git", netloc, path, '', ''))

@

<<[[TCPGitClient]] methods>>=
def _connect(self, cmd, path):
    if not isinstance(cmd, bytes):
        raise TypeError(cmd)
    if not isinstance(path, bytes):
        path = path.encode(self._remote_path_encoding)
    sockaddrs = socket.getaddrinfo(
        self._host, self._port, socket.AF_UNSPEC, socket.SOCK_STREAM)
    s = None
    err = socket.error("no address found for %s" % self._host)
    for (family, socktype, proto, canonname, sockaddr) in sockaddrs:
        s = socket.socket(family, socktype, proto)
        s.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
        try:
            s.connect(sockaddr)
            break
        except socket.error as err:
            if s is not None:
                s.close()
            s = None
    if s is None:
        raise err
    # -1 means system default buffering
    rfile = s.makefile('rb', -1)
    # 0 means unbuffered
    wfile = s.makefile('wb', 0)

    def close():
        rfile.close()
        wfile.close()
        s.close()

    proto = Protocol(rfile.read, wfile.write, close,
                     report_activity=self._report_activity)
    if path.startswith(b"/~"):
        path = path[1:]
    # TODO(jelmer): Alternative to ascii?
    proto.send_cmd(
        b'git-' + cmd, path, b'host=' + self._host.encode('ascii'))
    return proto, lambda: _fileno_can_read(s)
@

\section{Server}

\chapter{Advanced Topics}

\section{Tags}

% why need tags? not simply refs?

<<class Tag>>=
class Tag(ShaFile):
    """A Git Tag object."""

    type_name = b'tag'
    type_num = 4

    __slots__ = ('_tag_timezone_neg_utc', '_name', '_object_sha',
                 '_object_class', '_tag_time', '_tag_timezone',
                 '_tagger', '_message')
    <<[[Tag]] methods>>

@

<<[[Tag]] methods>>=
def __init__(self):
    super(Tag, self).__init__()
    self._tagger = None
    self._tag_time = None
    self._tag_timezone = None
    self._tag_timezone_neg_utc = False
@

<<[[Tag]] methods>>=
<<method Tag._get_object>>
<<method Tag._set_object>>
object = property(_get_object, _set_object)
@

<<method Tag._get_object>>=
def _get_object(self):
    """Get the object pointed to by this tag.

    :return: tuple of (object class, sha).
    """
    return (self._object_class, self._object_sha)
@

<<method Tag._set_object>>=
def _set_object(self, value):
    (self._object_class, self._object_sha) = value
    self._needs_serialization = True

@

<<[[Tag]] methods>>=
name = serializable_property("name", "The name of this tag")
@

<<[[Tag]] methods>>=
tagger = serializable_property("tagger",
    "Returns the name of the person who created this tag")
@

<<[[Tag]] methods>>=
tag_time = serializable_property("tag_time",
    "The creation timestamp of the tag.  As the number of seconds "
    "since the epoch")
@

<<[[Tag]] methods>>=
tag_timezone = serializable_property("tag_timezone",
    "The timezone that tag_time is in.")
@

<<[[Tag]] methods>>=
message = serializable_property(
    "message", "The message attached to this tag")
@



<<[[Tag]] methods>>=
def check(self):
    """Check this object for internal consistency.

    :raise ObjectFormatException: if the object is malformed in some way
    """
    super(Tag, self).check()
    self._check_has_member("_object_sha", "missing object sha")
    self._check_has_member("_object_class", "missing object type")
    self._check_has_member("_name", "missing tag name")

    if not self._name:
        raise ObjectFormatException("empty tag name")

    check_hexsha(self._object_sha, "invalid object sha")

    if getattr(self, "_tagger", None):
        check_identity(self._tagger, "invalid tagger")

    last = None
    for field, _ in _parse_message(self._chunked_text):
        if field == _OBJECT_HEADER and last is not None:
            raise ObjectFormatException("unexpected object")
        elif field == _TYPE_HEADER and last != _OBJECT_HEADER:
            raise ObjectFormatException("unexpected type")
        elif field == _TAG_HEADER and last != _TYPE_HEADER:
            raise ObjectFormatException("unexpected tag name")
        elif field == _TAGGER_HEADER and last != _TAG_HEADER:
            raise ObjectFormatException("unexpected tagger")
        last = field

@



<<[[Tag]] methods>>=
def _deserialize(self, chunks):
    """Grab the metadata attached to the tag"""
    self._tagger = None
    self._tag_time = None
    self._tag_timezone = None
    self._tag_timezone_neg_utc = False
    for field, value in _parse_message(chunks):
        if field == _OBJECT_HEADER:
            self._object_sha = value
        elif field == _TYPE_HEADER:
            obj_class = object_class(value)
            if not obj_class:
                raise ObjectFormatException("Not a known type: %s" % value)
            self._object_class = obj_class
        elif field == _TAG_HEADER:
            self._name = value
        elif field == _TAGGER_HEADER:
            try:
                sep = value.index(b'> ')
            except ValueError:
                self._tagger = value
                self._tag_time = None
                self._tag_timezone = None
                self._tag_timezone_neg_utc = False
            else:
                self._tagger = value[0:sep+1]
                try:
                    (timetext, timezonetext) = value[sep+2:].rsplit(b' ', 1)
                    self._tag_time = int(timetext)
                    self._tag_timezone, self._tag_timezone_neg_utc = \
                            parse_timezone(timezonetext)
                except ValueError as e:
                    raise ObjectFormatException(e)
        elif field is None:
            self._message = value
        else:
            raise ObjectFormatException("Unknown field %s" % field)

@

<<[[Tag]] methods>>=
def _serialize(self):
    chunks = []
    chunks.append(git_line(_OBJECT_HEADER, self._object_sha))
    chunks.append(git_line(_TYPE_HEADER, self._object_class.type_name))
    chunks.append(git_line(_TAG_HEADER, self._name))
    if self._tagger:
        if self._tag_time is None:
            chunks.append(git_line(_TAGGER_HEADER, self._tagger))
        else:
            chunks.append(git_line(
                _TAGGER_HEADER, self._tagger,
                str(self._tag_time).encode('ascii'),
                format_timezone(self._tag_timezone, self._tag_timezone_neg_utc)))
    if self._message is not None:
        chunks.append(b'\n') # To close headers
        chunks.append(self._message)
    return chunks

@


\subsection{[[git tag]]}

<<function cmd_tag>>=
class cmd_tag(Command):

    def run(self, args):
        opts, args = getopt(args, '', [])
        if len(args) < 2:
            print('Usage: dulwich tag NAME')
            sys.exit(1)

        porcelain.tag('.', args[0])
@
% should call tag_create here instead

<<function porcelain.tag>>=
def tag(*args, **kwargs):
    import warnings
    warnings.warn("tag has been deprecated in favour of tag_create.",
                  DeprecationWarning)
    return tag_create(*args, **kwargs)

@

<<function porcelain.tag_create>>=
def tag_create(
        repo, tag, author=None, message=None, annotated=False,
        objectish="HEAD", tag_time=None, tag_timezone=None):
    """Creates a tag in git via dulwich calls:

    :param repo: Path to repository
    :param tag: tag string
    :param author: tag author (optional, if annotated is set)
    :param message: tag message (optional)
    :param annotated: whether to create an annotated tag
    :param objectish: object the tag should point at, defaults to HEAD
    :param tag_time: Optional time for annotated tag
    :param tag_timezone: Optional timezone for annotated tag
    """

    with open_repo_closing(repo) as r:
        object = parse_object(r, objectish)

        if annotated:
            # Create the tag object
            tag_obj = Tag()
            if author is None:
                # TODO(jelmer): Don't use repo private method.
                author = r._get_user_identity()
            tag_obj.tagger = author
            tag_obj.message = message
            tag_obj.name = tag
            tag_obj.object = (type(object), object.id)
            if tag_time is None:
                tag_time = int(time.time())
            tag_obj.tag_time = tag_time
            if tag_timezone is None:
                # TODO(jelmer) Use current user timezone rather than UTC
                tag_timezone = 0
            elif isinstance(tag_timezone, str):
                tag_timezone = parse_timezone(tag_timezone)
            tag_obj.tag_timezone = tag_timezone
            r.object_store.add_object(tag_obj)
            tag_id = tag_obj.id
        else:
            tag_id = object.id

        r.refs[b'refs/tags/' + tag] = tag_id
@

<<function porcelain.tag_list>>=
def tag_list(repo, outstream=sys.stdout):
    """List all tags.

    :param repo: Path to repository
    :param outstream: Stream to write tags to
    """
    with open_repo_closing(repo) as r:
        tags = sorted(r.refs.as_dict(b"refs/tags"))
        return tags
@

<<function porcelain.tag_delete>>=
def tag_delete(repo, name):
    """Remove a tag.

    :param repo: Path to repository
    :param name: Name of tag to remove
    """
    with open_repo_closing(repo) as r:
        if isinstance(name, bytes):
            names = [name]
        elif isinstance(name, list):
            names = name
        else:
            raise TypeError("Unexpected tag name type %r" % name)
        for name in names:
            del r.refs[b"refs/tags/" + name]
@

\section{Reflog}

<<function read_reflog>>=
def read_reflog(f):
    """Read reflog.

    :param f: File-like object
    :returns: Iterator over Entry objects
    """
    for l in f:
        yield parse_reflog_line(l)
@

<<type reflog.Entry>>=
Entry = collections.namedtuple(
    'Entry', ['old_sha', 'new_sha', 'committer', 'timestamp', 'timezone',
              'message'])

@

<<function parse_reflog_line>>=
def parse_reflog_line(line):
    """Parse a reflog line.

    :param line: Line to parse
    :return: Tuple of (old_sha, new_sha, committer, timestamp, timezone,
        message)
    """
    (begin, message) = line.split(b'\t', 1)
    (old_sha, new_sha, rest) = begin.split(b' ', 2)
    (committer, timestamp_str, timezone_str) = rest.rsplit(b' ', 2)
    return Entry(old_sha, new_sha, committer, int(timestamp_str),
                 parse_timezone(timezone_str)[0], message)
@

<<function format_reflog_line>>=
def format_reflog_line(old_sha, new_sha, committer, timestamp, timezone,
                       message):
    """Generate a single reflog line.

    :param old_sha: Old Commit SHA
    :param new_sha: New Commit SHA
    :param committer: Committer name and e-mail
    :param timestamp: Timestamp
    :param timezone: Timezone
    :param message: Message
    """
    if old_sha is None:
        old_sha = ZERO_SHA
    return (old_sha + b' ' + new_sha + b' ' + committer + b' ' +
            str(timestamp).encode('ascii') + b' ' +
            format_timezone(timezone) + b'\t' + message)

@

\section{Pack}
% kinda an optimization

<<constant object_store.PACKDIR>>=
PACKDIR = 'pack'
@

\subsection{Modifications}

\subsubsection{[[ObjectStore]]}

% path at this point is objects dir?
<<[[DiskObjectStore.init()]] create pack dir>>=
os.mkdir(os.path.join(path, PACKDIR))
@

<<[[DiskObjectStore.__init__()]] set pack fields>>=
self.pack_dir = os.path.join(self.path, PACKDIR)
self._pack_cache_time = 0
self._pack_cache = {}
@


<<[[BaseObjectStore]] methods>>=
@property
def packs(self):
    """Iterable of pack objects."""
    raise NotImplementedError

@

<<[[BaseObjectStore]] methods>>=
def contains_packed(self, sha):
    """Check if a particular object is present by SHA1 and is packed."""
    raise NotImplementedError(self.contains_packed)

@


<<[[PackBasedObjectStore]] methods>>=
def contains_packed(self, sha):
    """Check if a particular object is present by SHA1 and is packed.

    This does not check alternates.
    """
    for pack in self.packs:
        if sha in pack:
            return True
    return False

@

<<[[PackBasedObjectStore]] methods>>=
def __contains__(self, sha):
    """Check if a particular object is present by SHA1.

    This method makes no distinction between loose and packed objects.
    """
    if self.contains_packed(sha) or self.contains_loose(sha):
        return True
    for alternate in self.alternates:
        if sha in alternate:
            return True
    return False

@

<<[[PackBasedObjectStore]] methods>>=
def _pack_cache_stale(self):
    """Check whether the pack cache is stale."""
    raise NotImplementedError(self._pack_cache_stale)

@

<<[[PackBasedObjectStore]] methods>>=
def _add_known_pack(self, base_name, pack):
    """Add a newly appeared pack to the cache by path.

    """
    self._pack_cache[base_name] = pack

@

<<[[PackBasedObjectStore]] methods>>=
def close(self):
    pack_cache = self._pack_cache
    self._pack_cache = {}
    while pack_cache:
        (name, pack) = pack_cache.popitem()
        pack.close()

@

<<[[PackBasedObjectStore]] methods>>=
@property
def packs(self):
    """List with pack objects."""
    if self._pack_cache is None or self._pack_cache_stale():
        self._update_pack_cache()

    return self._pack_cache.values()

@

<<[[PackBasedObjectStore]] methods>>=
def _iter_alternate_objects(self):
    """Iterate over the SHAs of all the objects in alternate stores."""
    for alternate in self.alternates:
        for alternate_object in alternate:
            yield alternate_object

@

<<[[PackBasedObjectStore]] methods>>=
def _iter_loose_objects(self):
    """Iterate over the SHAs of all loose objects."""
    raise NotImplementedError(self._iter_loose_objects)

@

<<[[PackBasedObjectStore]] methods>>=
def _get_loose_object(self, sha):
    raise NotImplementedError(self._get_loose_object)

@

<<[[PackBasedObjectStore]] methods>>=
def _remove_loose_object(self, sha):
    raise NotImplementedError(self._remove_loose_object)

@

<<[[PackBasedObjectStore]] methods>>=
def pack_loose_objects(self):
    """Pack loose objects.

    :return: Number of objects packed
    """
    objects = set()
    for sha in self._iter_loose_objects():
        objects.add((self._get_loose_object(sha), None))
    self.add_objects(list(objects))
    for obj, path in objects:
        self._remove_loose_object(obj.id)
    return len(objects)

@


<<[[BaseObjectStore]] methods>>=
def add_objects(self, objects):
    """Add a set of objects to this object store.

    :param objects: Iterable over a list of (object, path) tuples
    """
    raise NotImplementedError(self.add_objects)
@

<<[[PackBasedObjectStore]] methods>>=
def add_objects(self, objects):
    """Add a set of objects to this object store.

    :param objects: Iterable over (object, path) tuples, should support
        __len__.
    :return: Pack object of the objects written.
    """
    if len(objects) == 0:
        # Don't bother writing an empty pack file
        return
    f, commit, abort = self.add_pack()
    try:
        write_pack_objects(f, objects)
    except:
        abort()
        raise
    else:
        return commit()

@


<<[[PackBasedObjectStore]] methods>>=
def __iter__(self):
    """Iterate over the SHAs that are present in this store."""
    iterables = (list(self.packs) + [self._iter_loose_objects()] +
                 [self._iter_alternate_objects()])
    return chain(*iterables)

@

<<[[PackBasedObjectStore]] methods>>=
def contains_loose(self, sha):
    """Check if a particular object is present by SHA1 and is loose.

    This does not check alternates.
    """
    return self._get_loose_object(sha) is not None

@

<<[[PackBasedObjectStore.get_raw()]] look in packs>>=
for pack in self.packs:
    try:
        return pack.get_raw(sha)
    except KeyError:
        pass
@







<<[[DiskObjectStore]] methods>>=
def _update_pack_cache(self):
    try:
        pack_dir_contents = os.listdir(self.pack_dir)
    except OSError as e:
        if e.errno == errno.ENOENT:
            self._pack_cache_time = 0
            self.close()
            return
        raise
    self._pack_cache_time = os.stat(self.pack_dir).st_mtime
    pack_files = set()
    for name in pack_dir_contents:
        assert isinstance(name, basestring if sys.version_info[0] == 2 else str)
        if name.startswith("pack-") and name.endswith(".pack"):
            # verify that idx exists first (otherwise the pack was not yet
            # fully written)
            idx_name = os.path.splitext(name)[0] + ".idx"
            if idx_name in pack_dir_contents:
                pack_name = name[:-len(".pack")]
                pack_files.add(pack_name)

    # Open newly appeared pack files
    for f in pack_files:
        if f not in self._pack_cache:
            self._pack_cache[f] = Pack(os.path.join(self.pack_dir, f))
    # Remove disappeared pack files
    for f in set(self._pack_cache) - pack_files:
        self._pack_cache.pop(f).close()

@

<<[[DiskObjectStore]] methods>>=
def _pack_cache_stale(self):
    try:
        return os.stat(self.pack_dir).st_mtime > self._pack_cache_time
    except OSError as e:
        if e.errno == errno.ENOENT:
            return True
        raise

@

<<[[DiskObjectStore]] methods>>=
def _get_pack_basepath(self, entries):
    suffix = iter_sha1(entry[0] for entry in entries)
    # TODO: Handle self.pack_dir being bytes
    suffix = suffix.decode('ascii')
    return os.path.join(self.pack_dir, "pack-" + suffix)

@



<<[[DiskObjectStore]] methods>>=
def move_in_pack(self, path):
    """Move a specific file containing a pack into the pack directory.

    :note: The file should be on the same file system as the
        packs directory.

    :param path: Path to the pack file.
    """
    with PackData(path) as p:
        entries = p.sorted_entries()
        basename = self._get_pack_basepath(entries)
        with GitFile(basename+".idx", "wb") as f:
            write_pack_index_v2(f, entries, p.get_stored_checksum())
    os.rename(path, basename + ".pack")
    final_pack = Pack(basename)
    self._add_known_pack(basename, final_pack)
    return final_pack

@

<<[[DiskObjectStore]] methods>>=
def add_pack(self):
    """Add a new pack to this object store.

    :return: Fileobject to write to, a commit function to
        call when the pack is finished and an abort
        function.
    """
    fd, path = tempfile.mkstemp(dir=self.pack_dir, suffix=".pack")
    f = os.fdopen(fd, 'wb')

    def commit():
        os.fsync(fd)
        f.close()
        if os.path.getsize(path) > 0:
            return self.move_in_pack(path)
        else:
            os.remove(path)
            return None

    def abort():
        f.close()
        os.remove(path)
    return f, commit, abort

@


<<[[BaseObjectStore]] methods>>=
def generate_pack_contents(self, have, want, progress=None):
    """Iterate over the contents of a pack file.

    :param have: List of SHA1s of objects that should not be sent
    :param want: List of SHA1s of objects that should be sent
    :param progress: Optional progress reporting method
    """
    return self.iter_shas(self.find_missing_objects(have, want, progress))

@

<<[[PackBasedObjectStore]] methods>>=
def __init__(self):
    self._pack_cache = {}
@

\subsubsection{[[RefsContainer]]}

<<[[DiskRefsContainer.__init__()]] set pack fields>>=
self._packed_refs = None
self._peeled_refs = None
@

<<[[DiskRefsContainer.subkeys()]] look in packed refs>>=
for key in self.get_packed_refs():
    if key.startswith(base):
        subkeys.add(key[len(base):].strip(b'/'))
@

<<[[DiskRefsContainer.allkeys()]] look in packed refs>>=
allkeys.update(self.get_packed_refs())
@

<<[[DiskRefsContainer]] methods>>=
def get_packed_refs(self):
    """Get contents of the packed-refs file.

    :return: Dictionary mapping ref names to SHA1s

    :note: Will return an empty dictionary when no packed-refs file is
        present.
    """
    # TODO: invalidate the cache on repacking
    if self._packed_refs is None:
        # set both to empty because we want _peeled_refs to be
        # None if and only if _packed_refs is also None.
        self._packed_refs = {}
        self._peeled_refs = {}
        path = os.path.join(self.path, 'packed-refs')
        try:
            f = GitFile(path, 'rb')
        except IOError as e:
            if e.errno == errno.ENOENT:
                return {}
            raise
        with f:
            first_line = next(iter(f)).rstrip()
            if (first_line.startswith(b'# pack-refs') and b' peeled' in
                    first_line):
                for sha, name, peeled in read_packed_refs_with_peeled(f):
                    self._packed_refs[name] = sha
                    if peeled:
                        self._peeled_refs[name] = peeled
            else:
                f.seek(0)
                for sha, name in read_packed_refs(f):
                    self._packed_refs[name] = sha
    return self._packed_refs

@

<<[[DiskRefsContainer]] methods>>=
def _remove_packed_ref(self, name):
    if self._packed_refs is None:
        return
    filename = os.path.join(self.path, 'packed-refs')
    # reread cached refs from disk, while holding the lock
    f = GitFile(filename, 'wb')
    try:
        self._packed_refs = None
        self.get_packed_refs()

        if name not in self._packed_refs:
            return

        del self._packed_refs[name]
        if name in self._peeled_refs:
            del self._peeled_refs[name]
        write_packed_refs(f, self._packed_refs, self._peeled_refs)
        f.close()
    finally:
        f.abort()

@

\subsubsection{[[GitClient]]}

<<[[GitClient]] methods>>=
def fetch_pack(self, path, determine_wants, graph_walker, pack_data,
               progress=None):
    """Retrieve a pack from a git smart server.

    :param path: Remote path to fetch from
    :param determine_wants: Function determine what refs
        to fetch. Receives dictionary of name->sha, should return
        list of shas to fetch.
    :param graph_walker: Object with next() and ack().
    :param pack_data: Callback called for each bit of data in the pack
    :param progress: Callback for progress reports (strings)
    :return: Dictionary with all remote refs (not just those fetched)
    """
    raise NotImplementedError(self.fetch_pack)

@

<<[[GitClient]] methods>>=
def send_pack(self, path, update_refs, generate_pack_contents,
              progress=None, write_pack=write_pack_objects):
    """Upload a pack to a remote repository.

    :param path: Repository path (as bytestring)
    :param update_refs: Function to determine changes to remote refs.
        Receive dict with existing remote refs, returns dict with
        changed refs (name -> sha, where sha=ZERO_SHA for deletions)
    :param generate_pack_contents: Function that can return a sequence of
        the shas of the objects to upload.
    :param progress: Optional progress function
    :param write_pack: Function called with (file, iterable of objects) to
        write the objects returned by generate_pack_contents to the server.

    :raises SendPackError: if server rejects the pack data
    :raises UpdateRefsError: if the server supports report-status
                             and rejects ref updates
    :return: new_refs dictionary containing the changes that were made
        {refname: new_ref}, including deleted refs.
    """
    raise NotImplementedError(self.send_pack)

@

<<[[GitClient]] methods>>=
def _handle_receive_pack_head(self, proto, capabilities, old_refs,
                              new_refs):
    """Handle the head of a 'git-receive-pack' request.

    :param proto: Protocol object to read from
    :param capabilities: List of negotiated capabilities
    :param old_refs: Old refs, as received from the server
    :param new_refs: Refs to change
    :return: (have, want) tuple
    """
    want = []
    have = [x for x in old_refs.values() if not x == ZERO_SHA]
    sent_capabilities = False

    for refname in new_refs:
        if not isinstance(refname, bytes):
            raise TypeError('refname is not a bytestring: %r' % refname)
        old_sha1 = old_refs.get(refname, ZERO_SHA)
        if not isinstance(old_sha1, bytes):
            raise TypeError('old sha1 for %s is not a bytestring: %r' %
                            (refname, old_sha1))
        new_sha1 = new_refs.get(refname, ZERO_SHA)
        if not isinstance(new_sha1, bytes):
            raise TypeError('old sha1 for %s is not a bytestring %r' %
                            (refname, new_sha1))

        if old_sha1 != new_sha1:
            if sent_capabilities:
                proto.write_pkt_line(old_sha1 + b' ' + new_sha1 + b' ' +
                                     refname)
            else:
                proto.write_pkt_line(
                    old_sha1 + b' ' + new_sha1 + b' ' + refname + b'\0' +
                    b' '.join(capabilities))
                sent_capabilities = True
        if new_sha1 not in have and new_sha1 != ZERO_SHA:
            want.append(new_sha1)
    proto.write_pkt_line(None)
    return (have, want)

@

<<[[GitClient]] methods>>=
def _handle_receive_pack_tail(self, proto, capabilities, progress=None):
    """Handle the tail of a 'git-receive-pack' request.

    :param proto: Protocol object to read from
    :param capabilities: List of negotiated capabilities
    :param progress: Optional progress reporting function
    """
    if b"side-band-64k" in capabilities:
        if progress is None:
            def progress(x):
                pass
        channel_callbacks = {2: progress}
        if CAPABILITY_REPORT_STATUS in capabilities:
            channel_callbacks[1] = PktLineParser(
                self._report_status_parser.handle_packet).parse
        self._read_side_band64k_data(proto, channel_callbacks)
    else:
        if CAPABILITY_REPORT_STATUS in capabilities:
            for pkt in proto.read_pkt_seq():
                self._report_status_parser.handle_packet(pkt)
    if self._report_status_parser is not None:
        self._report_status_parser.check()

@

<<[[GitClient]] methods>>=
def _handle_upload_pack_head(self, proto, capabilities, graph_walker,
                             wants, can_read):
    """Handle the head of a 'git-upload-pack' request.

    :param proto: Protocol object to read from
    :param capabilities: List of negotiated capabilities
    :param graph_walker: GraphWalker instance to call .ack() on
    :param wants: List of commits to fetch
    :param can_read: function that returns a boolean that indicates
        whether there is extra graph data to read on proto
    """
    assert isinstance(wants, list) and isinstance(wants[0], bytes)
    proto.write_pkt_line(COMMAND_WANT + b' ' + wants[0] + b' ' +
                         b' '.join(capabilities) + b'\n')
    for want in wants[1:]:
        proto.write_pkt_line(COMMAND_WANT + b' ' + want + b'\n')
    proto.write_pkt_line(None)
    have = next(graph_walker)
    while have:
        proto.write_pkt_line(COMMAND_HAVE + b' ' + have + b'\n')
        if can_read():
            pkt = proto.read_pkt_line()
            parts = pkt.rstrip(b'\n').split(b' ')
            if parts[0] == b'ACK':
                graph_walker.ack(parts[1])
                if parts[2] in (b'continue', b'common'):
                    pass
                elif parts[2] == b'ready':
                    break
                else:
                    raise AssertionError(
                        "%s not in ('continue', 'ready', 'common)" %
                        parts[2])
        have = next(graph_walker)
    proto.write_pkt_line(COMMAND_DONE + b'\n')

@

<<[[GitClient]] methods>>=
def _handle_upload_pack_tail(self, proto, capabilities, graph_walker,
                             pack_data, progress=None, rbufsize=_RBUFSIZE):
    """Handle the tail of a 'git-upload-pack' request.

    :param proto: Protocol object to read from
    :param capabilities: List of negotiated capabilities
    :param graph_walker: GraphWalker instance to call .ack() on
    :param pack_data: Function to call with pack data
    :param progress: Optional progress reporting function
    :param rbufsize: Read buffer size
    """
    pkt = proto.read_pkt_line()
    while pkt:
        parts = pkt.rstrip(b'\n').split(b' ')
        if parts[0] == b'ACK':
            graph_walker.ack(parts[1])
        if len(parts) < 3 or parts[2] not in (
                b'ready', b'continue', b'common'):
            break
        pkt = proto.read_pkt_line()
    if CAPABILITY_SIDE_BAND_64K in capabilities:
        if progress is None:
            # Just ignore progress data

            def progress(x):
                pass
        self._read_side_band64k_data(proto, {
            SIDE_BAND_CHANNEL_DATA: pack_data,
            SIDE_BAND_CHANNEL_PROGRESS: progress}
        )
    else:
        while True:
            data = proto.read(rbufsize)
            if data == b"":
                break
            pack_data(data)
@




<<[[LocalGitClient]] methods>>=
def send_pack(self, path, update_refs, generate_pack_contents,
              progress=None, write_pack=write_pack_objects):
    """Upload a pack to a remote repository.

    :param path: Repository path (as bytestring)
    :param update_refs: Function to determine changes to remote refs.
        Receive dict with existing remote refs, returns dict with
        changed refs (name -> sha, where sha=ZERO_SHA for deletions)
    :param generate_pack_contents: Function that can return a sequence of
        the shas of the objects to upload.
    :param progress: Optional progress function
    :param write_pack: Function called with (file, iterable of objects) to
        write the objects returned by generate_pack_contents to the server.

    :raises SendPackError: if server rejects the pack data
    :raises UpdateRefsError: if the server supports report-status
                             and rejects ref updates
    :return: new_refs dictionary containing the changes that were made
        {refname: new_ref}, including deleted refs.
    """
    if not progress:
        def progress(x):
            pass

    with self._open_repo(path) as target:
        old_refs = target.get_refs()
        new_refs = update_refs(dict(old_refs))

        have = [sha1 for sha1 in old_refs.values() if sha1 != ZERO_SHA]
        want = []
        for refname, new_sha1 in new_refs.items():
            if (new_sha1 not in have and
                    new_sha1 not in want and
                    new_sha1 != ZERO_SHA):
                want.append(new_sha1)

        if (not want and
                set(new_refs.items()).issubset(set(old_refs.items()))):
            return new_refs

        target.object_store.add_objects(generate_pack_contents(have, want))

        for refname, new_sha1 in new_refs.items():
            old_sha1 = old_refs.get(refname, ZERO_SHA)
            if new_sha1 != ZERO_SHA:
                if not target.refs.set_if_equals(
                        refname, old_sha1, new_sha1):
                    progress('unable to set %s to %s' %
                             (refname, new_sha1))
            else:
                if not target.refs.remove_if_equals(refname, old_sha1):
                    progress('unable to remove %s' % refname)

    return new_refs

@

<<[[LocalGitClient]] methods>>=
def fetch_pack(self, path, determine_wants, graph_walker, pack_data,
               progress=None):
    """Retrieve a pack from a git smart server.

    :param path: Remote path to fetch from
    :param determine_wants: Function determine what refs
        to fetch. Receives dictionary of name->sha, should return
        list of shas to fetch.
    :param graph_walker: Object with next() and ack().
    :param pack_data: Callback called for each bit of data in the pack
    :param progress: Callback for progress reports (strings)
    :return: Dictionary with all remote refs (not just those fetched)
    """
    with self._open_repo(path) as r:
        objects_iter = r.fetch_objects(
            determine_wants, graph_walker, progress)

        # Did the process short-circuit (e.g. in a stateless RPC call)?
        # Note that the client still expects a 0-object pack in most cases.
        if objects_iter is None:
            return
        write_pack_objects(ProtocolFile(None, pack_data), objects_iter)
        return r.get_refs()

@

\subsection{[[Pack]]}

<<class Pack>>=
class Pack(object):
    """A Git pack object."""

    <<[[Pack]] methods>>
@

<<[[Pack]] methods>>=
def __init__(self, basename, resolve_ext_ref=None):
    self._basename = basename
    self._data = None
    self._idx = None
    self._idx_path = self._basename + '.idx'
    self._data_path = self._basename + '.pack'
    self._data_load = lambda: PackData(self._data_path)
    self._idx_load = lambda: load_pack_index(self._idx_path)
    self.resolve_ext_ref = resolve_ext_ref
@

<<[[Pack]] methods>>=
@classmethod
def from_lazy_objects(self, data_fn, idx_fn):
    """Create a new pack object from callables to load pack data and
    index objects."""
    ret = Pack('')
    ret._data_load = data_fn
    ret._idx_load = idx_fn
    return ret

@

<<[[Pack]] methods>>=
@classmethod
def from_objects(self, data, idx):
    """Create a new pack object from pack data and index objects."""
    ret = Pack('')
    ret._data_load = lambda: data
    ret._idx_load = lambda: idx
    return ret

@

<<[[Pack]] methods>>=
def name(self):
    """The SHA over the SHAs of the objects in this pack."""
    return self.index.objects_sha1()

@

<<[[Pack]] methods>>=
@property
def data(self):
    """The pack data object being used."""
    if self._data is None:
        self._data = self._data_load()
        self._data.pack = self
        self.check_length_and_checksum()
    return self._data

@

<<[[Pack]] methods>>=
@property
def index(self):
    """The index being used.

    :note: This may be an in-memory index
    """
    if self._idx is None:
        self._idx = self._idx_load()
    return self._idx

@

<<[[Pack]] methods>>=
def close(self):
    if self._data is not None:
        self._data.close()
    if self._idx is not None:
        self._idx.close()

@

<<[[Pack]] methods>>=
def __enter__(self):
    return self

@

<<[[Pack]] methods>>=
def __exit__(self, exc_type, exc_val, exc_tb):
    self.close()

@

<<[[Pack]] methods>>=
def __eq__(self, other):
    return isinstance(self, type(other)) and self.index == other.index

@

<<[[Pack]] methods>>=
def __len__(self):
    """Number of entries in this pack."""
    return len(self.index)

@


<<[[Pack]] methods>>=
def __iter__(self):
    """Iterate over all the sha1s of the objects in this pack."""
    return iter(self.index)

@

<<[[Pack]] methods>>=
def check_length_and_checksum(self):
    """Sanity check the length and checksum of the pack index and data."""
    assert len(self.index) == len(self.data)
    idx_stored_checksum = self.index.get_pack_checksum()
    data_stored_checksum = self.data.get_stored_checksum()
    if idx_stored_checksum != data_stored_checksum:
        raise ChecksumMismatch(sha_to_hex(idx_stored_checksum),
                               sha_to_hex(data_stored_checksum))

@

<<[[Pack]] methods>>=
def check(self):
    """Check the integrity of this pack.

    :raise ChecksumMismatch: if a checksum for the index or data is wrong
    """
    self.index.check()
    self.data.check()
    for obj in self.iterobjects():
        obj.check()
    # TODO: object connectivity checks

@

<<[[Pack]] methods>>=
def get_stored_checksum(self):
    return self.data.get_stored_checksum()

@

<<[[Pack]] methods>>=
def __contains__(self, sha1):
    """Check whether this pack contains a particular SHA1."""
    try:
        self.index.object_index(sha1)
        return True
    except KeyError:
        return False

@

<<[[Pack]] methods>>=
def get_raw(self, sha1):
    offset = self.index.object_index(sha1)
    obj_type, obj = self.data.get_object_at(offset)
    type_num, chunks = self.data.resolve_object(offset, obj_type, obj)
    return type_num, b''.join(chunks)

@

<<[[Pack]] methods>>=
def __getitem__(self, sha1):
    """Retrieve the specified SHA1."""
    type, uncomp = self.get_raw(sha1)
    return ShaFile.from_raw_string(type, uncomp, sha=sha1)

@

<<[[Pack]] methods>>=
def iterobjects(self):
    """Iterate over the objects in this pack."""
    return iter(PackInflater.for_pack_data(
        self.data, resolve_ext_ref=self.resolve_ext_ref))

@

<<[[Pack]] methods>>=
def pack_tuples(self):
    """Provide an iterable for use with write_pack_objects.

    :return: Object that can iterate over (object, path) tuples
        and provides __len__
    """
    class PackTupleIterable(object):

        def __init__(self, pack):
            self.pack = pack

        def __len__(self):
            return len(self.pack)

        def __iter__(self):
            return ((o, None) for o in self.pack.iterobjects())

    return PackTupleIterable(self)

@

<<[[Pack]] methods>>=
def keep(self, msg=None):
    """Add a .keep file for the pack, preventing git from garbage collecting it.

    :param msg: A message written inside the .keep file; can be used later
        to determine whether or not a .keep file is obsolete.
    :return: The path of the .keep file, as a string.
    """
    keepfile_name = '%s.keep' % self._basename
    with GitFile(keepfile_name, 'wb') as keepfile:
        if msg:
            keepfile.write(msg)
            keepfile.write(b'\n')
    return keepfile_name
@

\subsection{[[PackData]]}

<<class PackData>>=
class PackData(object):
    """The data contained in a packfile.

    Pack files can be accessed both sequentially for exploding a pack, and
    directly with the help of an index to retrieve a specific object.

    The objects within are either complete or a delta against another.

    The header is variable length. If the MSB of each byte is set then it
    indicates that the subsequent byte is still part of the header.
    For the first byte the next MS bits are the type, which tells you the type
    of object, and whether it is a delta. The LS byte is the lowest bits of the
    size. For each subsequent byte the LS 7 bits are the next MS bits of the
    size, i.e. the last byte of the header contains the MS bits of the size.

    For the complete objects the data is stored as zlib deflated data.
    The size in the header is the uncompressed object size, so to uncompress
    you need to just keep feeding data to zlib until you get an object back,
    or it errors on bad data. This is done here by just giving the complete
    buffer from the start of the deflated object on. This is bad, but until I
    get mmap sorted out it will have to do.

    Currently there are no integrity checks done. Also no attempt is made to
    try and detect the delta case, or a request for an object at the wrong
    position.  It will all just throw a zlib or KeyError.
    """

    <<[[PackData]] methods>>
@

<<[[PackData]] methods>>=
def __init__(self, filename, file=None, size=None):
    """Create a PackData object representing the pack in the given filename.

    The file must exist and stay readable until the object is disposed of.
    It must also stay the same size. It will be mapped whenever needed.

    Currently there is a restriction on the size of the pack as the python
    mmap implementation is flawed.
    """
    self._filename = filename
    self._size = size
    self._header_size = 12
    if file is None:
        self._file = GitFile(self._filename, 'rb')
    else:
        self._file = file
    (version, self._num_objects) = read_pack_header(self._file.read)
    self._offset_cache = LRUSizeCache(
        1024*1024*20, compute_size=_compute_object_size)
    self.pack = None

@property
def filename(self):
    return os.path.basename(self._filename)

@classmethod
def from_file(cls, file, size):
    return cls(str(file), file=file, size=size)

@classmethod
def from_path(cls, path):
    return cls(filename=path)

def close(self):
    self._file.close()

def __enter__(self):
    return self

def __exit__(self, exc_type, exc_val, exc_tb):
    self.close()

def _get_size(self):
    if self._size is not None:
        return self._size
    self._size = os.path.getsize(self._filename)
    if self._size < self._header_size:
        errmsg = ('%s is too small for a packfile (%d < %d)' %
                  (self._filename, self._size, self._header_size))
        raise AssertionError(errmsg)
    return self._size

def __len__(self):
    """Returns the number of objects in this pack."""
    return self._num_objects

def calculate_checksum(self):
    """Calculate the checksum for this pack.

    :return: 20-byte binary SHA1 digest
    """
    return compute_file_sha(self._file, end_ofs=-20).digest()

def get_ref(self, sha):
    """Get the object for a ref SHA, only looking in this pack."""
    # TODO: cache these results
    if self.pack is None:
        raise KeyError(sha)
    try:
        offset = self.pack.index.object_index(sha)
    except KeyError:
        offset = None
    if offset:
        type, obj = self.get_object_at(offset)
    elif self.pack is not None and self.pack.resolve_ext_ref:
        type, obj = self.pack.resolve_ext_ref(sha)
    else:
        raise KeyError(sha)
    return offset, type, obj

def resolve_object(self, offset, type, obj, get_ref=None):
    """Resolve an object, possibly resolving deltas when necessary.

    :return: Tuple with object type and contents.
    """
    # Walk down the delta chain, building a stack of deltas to reach
    # the requested object.
    base_offset = offset
    base_type = type
    base_obj = obj
    delta_stack = []
    while base_type in DELTA_TYPES:
        prev_offset = base_offset
        if get_ref is None:
            get_ref = self.get_ref
        if base_type == OFS_DELTA:
            (delta_offset, delta) = base_obj
            # TODO: clean up asserts and replace with nicer error messages
            assert (
                isinstance(base_offset, int)
                or isinstance(base_offset, long))
            assert (
                isinstance(delta_offset, int)
                or isinstance(base_offset, long))
            base_offset = base_offset - delta_offset
            base_type, base_obj = self.get_object_at(base_offset)
            assert isinstance(base_type, int)
        elif base_type == REF_DELTA:
            (basename, delta) = base_obj
            assert isinstance(basename, bytes) and len(basename) == 20
            base_offset, base_type, base_obj = get_ref(basename)
            assert isinstance(base_type, int)
        delta_stack.append((prev_offset, base_type, delta))

    # Now grab the base object (mustn't be a delta) and apply the
    # deltas all the way up the stack.
    chunks = base_obj
    for prev_offset, delta_type, delta in reversed(delta_stack):
        chunks = apply_delta(chunks, delta)
        # TODO(dborowitz): This can result in poor performance if
        # large base objects are separated from deltas in the pack.
        # We should reorganize so that we apply deltas to all
        # objects in a chain one after the other to optimize cache
        # performance.
        if prev_offset is not None:
            self._offset_cache[prev_offset] = base_type, chunks
    return base_type, chunks

def iterobjects(self, progress=None, compute_crc32=True):
    self._file.seek(self._header_size)
    for i in range(1, self._num_objects + 1):
        offset = self._file.tell()
        unpacked, unused = unpack_object(
          self._file.read, compute_crc32=compute_crc32)
        if progress is not None:
            progress(i, self._num_objects)
        yield (offset, unpacked.pack_type_num, unpacked._obj(),
               unpacked.crc32)
        # Back up over unused data.
        self._file.seek(-len(unused), SEEK_CUR)

def _iter_unpacked(self):
    # TODO(dborowitz): Merge this with iterobjects, if we can change its
    # return type.
    self._file.seek(self._header_size)

    if self._num_objects is None:
        return

    for _ in range(self._num_objects):
        offset = self._file.tell()
        unpacked, unused = unpack_object(
          self._file.read, compute_crc32=False)
        unpacked.offset = offset
        yield unpacked
        # Back up over unused data.
        self._file.seek(-len(unused), SEEK_CUR)

def iterentries(self, progress=None):
    """Yield entries summarizing the contents of this pack.

    :param progress: Progress function, called with current and total
        object count.
    :return: iterator of tuples with (sha, offset, crc32)
    """
    num_objects = self._num_objects
    resolve_ext_ref = (
        self.pack.resolve_ext_ref if self.pack is not None else None)
    indexer = PackIndexer.for_pack_data(
        self, resolve_ext_ref=resolve_ext_ref)
    for i, result in enumerate(indexer):
        if progress is not None:
            progress(i, num_objects)
        yield result

def sorted_entries(self, progress=None):
    """Return entries in this pack, sorted by SHA.

    :param progress: Progress function, called with current and total
        object count
    :return: List of tuples with (sha, offset, crc32)
    """
    ret = sorted(self.iterentries(progress=progress))
    return ret

def create_index_v1(self, filename, progress=None):
    """Create a version 1 file for this data file.

    :param filename: Index filename.
    :param progress: Progress report function
    :return: Checksum of index file
    """
    entries = self.sorted_entries(progress=progress)
    with GitFile(filename, 'wb') as f:
        return write_pack_index_v1(f, entries, self.calculate_checksum())

def create_index_v2(self, filename, progress=None):
    """Create a version 2 index file for this data file.

    :param filename: Index filename.
    :param progress: Progress report function
    :return: Checksum of index file
    """
    entries = self.sorted_entries(progress=progress)
    with GitFile(filename, 'wb') as f:
        return write_pack_index_v2(f, entries, self.calculate_checksum())

def create_index(self, filename, progress=None,
                 version=2):
    """Create an  index file for this data file.

    :param filename: Index filename.
    :param progress: Progress report function
    :return: Checksum of index file
    """
    if version == 1:
        return self.create_index_v1(filename, progress)
    elif version == 2:
        return self.create_index_v2(filename, progress)
    else:
        raise ValueError('unknown index format %d' % version)

def get_stored_checksum(self):
    """Return the expected checksum stored in this pack."""
    self._file.seek(-20, SEEK_END)
    return self._file.read(20)

def check(self):
    """Check the consistency of this pack."""
    actual = self.calculate_checksum()
    stored = self.get_stored_checksum()
    if actual != stored:
        raise ChecksumMismatch(stored, actual)

def get_object_at(self, offset):
    """Given an offset in to the packfile return the object that is there.

    Using the associated index the location of an object can be looked up,
    and then the packfile can be asked directly for that object using this
    function.
    """
    try:
        return self._offset_cache[offset]
    except KeyError:
        pass
    assert offset >= self._header_size
    self._file.seek(offset)
    unpacked, _ = unpack_object(self._file.read)
    return (unpacked.pack_type_num, unpacked._obj())
@


\subsection{[[PackIndex]]}

<<class PackIndex>>=
class PackIndex(object):
    """An index in to a packfile.

    Given a sha id of an object a pack index can tell you the location in the
    packfile of that object if it has it.
    """

    def __eq__(self, other):
        if not isinstance(other, PackIndex):
            return False

        for (name1, _, _), (name2, _, _) in izip(self.iterentries(),
                                                 other.iterentries()):
            if name1 != name2:
                return False
        return True

    def __ne__(self, other):
        return not self.__eq__(other)

    def __len__(self):
        """Return the number of entries in this pack index."""
        raise NotImplementedError(self.__len__)

    def __iter__(self):
        """Iterate over the SHAs in this pack."""
        return imap(sha_to_hex, self._itersha())

    def iterentries(self):
        """Iterate over the entries in this pack index.

        :return: iterator over tuples with object name, offset in packfile and
            crc32 checksum.
        """
        raise NotImplementedError(self.iterentries)

    def get_pack_checksum(self):
        """Return the SHA1 checksum stored for the corresponding packfile.

        :return: 20-byte binary digest
        """
        raise NotImplementedError(self.get_pack_checksum)

    def object_index(self, sha):
        """Return the index in to the corresponding packfile for the object.

        Given the name of an object it will return the offset that object
        lives at within the corresponding pack file. If the pack file doesn't
        have the object then None will be returned.
        """
        if len(sha) == 40:
            sha = hex_to_sha(sha)
        return self._object_index(sha)

    def _object_index(self, sha):
        """See object_index.

        :param sha: A *binary* SHA string. (20 characters long)_
        """
        raise NotImplementedError(self._object_index)

    def objects_sha1(self):
        """Return the hex SHA1 over all the shas of all objects in this pack.

        :note: This is used for the filename of the pack.
        """
        return iter_sha1(self._itersha())

    def _itersha(self):
        """Yield all the SHA1's of the objects in the index, sorted."""
        raise NotImplementedError(self._itersha)
@


<<class FilePackIndex>>=
class FilePackIndex(PackIndex):
    """Pack index that is based on a file.

    To do the loop it opens the file, and indexes first 256 4 byte groups
    with the first byte of the sha id. The value in the four byte group indexed
    is the end of the group that shares the same starting byte. Subtract one
    from the starting byte and index again to find the start of the group.
    The values are sorted by sha id within the group, so do the math to find
    the start and end offset and then bisect in to find if the value is
    present.
    """

    def __init__(self, filename, file=None, contents=None, size=None):
        """Create a pack index object.

        Provide it with the name of the index file to consider, and it will map
        it whenever required.
        """
        self._filename = filename
        # Take the size now, so it can be checked each time we map the file to
        # ensure that it hasn't changed.
        if file is None:
            self._file = GitFile(filename, 'rb')
        else:
            self._file = file
        if contents is None:
            self._contents, self._size = _load_file_contents(self._file, size)
        else:
            self._contents, self._size = (contents, size)

    def __eq__(self, other):
        # Quick optimization:
        if (isinstance(other, FilePackIndex) and
                self._fan_out_table != other._fan_out_table):
            return False

        return super(FilePackIndex, self).__eq__(other)

    def close(self):
        self._file.close()
        if getattr(self._contents, "close", None) is not None:
            self._contents.close()

    def __len__(self):
        """Return the number of entries in this pack index."""
        return self._fan_out_table[-1]

    def _unpack_entry(self, i):
        """Unpack the i-th entry in the index file.

        :return: Tuple with object name (SHA), offset in pack file and CRC32
            checksum (if known).
        """
        raise NotImplementedError(self._unpack_entry)

    def _unpack_name(self, i):
        """Unpack the i-th name from the index file."""
        raise NotImplementedError(self._unpack_name)

    def _unpack_offset(self, i):
        """Unpack the i-th object offset from the index file."""
        raise NotImplementedError(self._unpack_offset)

    def _unpack_crc32_checksum(self, i):
        """Unpack the crc32 checksum for the ith object from the index file.
        """
        raise NotImplementedError(self._unpack_crc32_checksum)

    def _itersha(self):
        for i in range(len(self)):
            yield self._unpack_name(i)

    def iterentries(self):
        """Iterate over the entries in this pack index.

        :return: iterator over tuples with object name, offset in packfile and
            crc32 checksum.
        """
        for i in range(len(self)):
            yield self._unpack_entry(i)

    def _read_fan_out_table(self, start_offset):
        ret = []
        for i in range(0x100):
            fanout_entry = self._contents[
                start_offset+i*4:start_offset+(i+1)*4]
            ret.append(struct.unpack('>L', fanout_entry)[0])
        return ret

    def check(self):
        """Check that the stored checksum matches the actual checksum."""
        actual = self.calculate_checksum()
        stored = self.get_stored_checksum()
        if actual != stored:
            raise ChecksumMismatch(stored, actual)

    def calculate_checksum(self):
        """Calculate the SHA1 checksum over this pack index.

        :return: This is a 20-byte binary digest
        """
        return sha1(self._contents[:-20]).digest()

    def get_pack_checksum(self):
        """Return the SHA1 checksum stored for the corresponding packfile.

        :return: 20-byte binary digest
        """
        return bytes(self._contents[-40:-20])

    def get_stored_checksum(self):
        """Return the SHA1 checksum stored for this index.

        :return: 20-byte binary digest
        """
        return bytes(self._contents[-20:])

    def _object_index(self, sha):
        """See object_index.

        :param sha: A *binary* SHA string. (20 characters long)_
        """
        assert len(sha) == 20
        idx = ord(sha[:1])
        if idx == 0:
            start = 0
        else:
            start = self._fan_out_table[idx-1]
        end = self._fan_out_table[idx]
        i = bisect_find_sha(start, end, sha, self._unpack_name)
        if i is None:
            raise KeyError(sha)
        return self._unpack_offset(i)

@

\subsection{[[git pack-objects]]}

<<function cmd_pack_objects>>=
class cmd_pack_objects(Command):

    def run(self, args):
        opts, args = getopt(args, '', ['stdout'])
        opts = dict(opts)
        if len(args) < 1 and not '--stdout' in args:
            print('Usage: dulwich pack-objects basename')
            sys.exit(1)
        object_ids = [l.strip() for l in sys.stdin.readlines()]
        basename = args[0]
        if '--stdout' in opts:
            packf = getattr(sys.stdout, 'buffer', sys.stdout)
            idxf = None
            close = []
        else:
            packf = open(basename + '.pack', 'w')
            idxf = open(basename + '.idx', 'w')
            close = [packf, idxf]

        porcelain.pack_objects('.', object_ids, packf, idxf)
        for f in close:
            f.close()
@

<<function porcelain.pack_objects>>=
def pack_objects(repo, object_ids, packf, idxf, delta_window_size=None):
    """Pack objects into a file.

    :param repo: Path to the repository
    :param object_ids: List of object ids to write
    :param packf: File-like object to write to
    :param idxf: File-like object to write to (can be None)
    """
    with open_repo_closing(repo) as r:
        entries, data_sum = write_pack_objects(
            packf,
            r.object_store.iter_shas((oid, None) for oid in object_ids),
            delta_window_size=delta_window_size)
    if idxf is not None:
        entries = sorted([(k, v[0], v[1]) for (k, v) in entries.items()])
        write_pack_index(idxf, entries, data_sum)
@

\subsection{[[git repack]]}

<<function cmd_repack>>=
class cmd_repack(Command):

    def run(self, args):
        opts, args = getopt(args, "", [])
        opts = dict(opts)

        porcelain.repack('.')

@

<<function porcelain.repack>>=
def repack(repo):
    """Repack loose files in a repository.

    Currently this only packs loose objects.

    :param repo: Path to the repository
    """
    with open_repo_closing(repo) as r:
        r.object_store.pack_loose_objects()
@

\subsection{[[git fetch-pack]]}

<<function cmd_fetch_pack>>=
class cmd_fetch_pack(Command):

    def run(self, args):
        opts, args = getopt(args, "", ["all"])
        opts = dict(opts)
        client, path = get_transport_and_path(args.pop(0))
        r = Repo(".")
        if "--all" in opts:
            determine_wants = r.object_store.determine_wants_all
        else:
            determine_wants = lambda x: [y for y in args if not y in r.object_store]
        client.fetch(path, r, determine_wants)
@

\subsection{[[git receive-pack]]}

<<function cmd_receive_pack>>=
class cmd_receive_pack(Command):

    def run(self, args):
        parser = optparse.OptionParser()
        options, args = parser.parse_args(args)
        if len(args) >= 1:
            gitdir = args[0]
        else:
            gitdir = '.'
        porcelain.receive_pack(gitdir)

@

<<function porcelain.receive_pack>>=
def receive_pack(path=".", inf=None, outf=None):
    """Receive a pack file after negotiating its contents using smart protocol.

    :param path: Path to the repository
    :param inf: Input stream to communicate with client
    :param outf: Output stream to communicate with client
    """
    if outf is None:
        outf = getattr(sys.stdout, 'buffer', sys.stdout)
    if inf is None:
        inf = getattr(sys.stdin, 'buffer', sys.stdin)
    path = os.path.expanduser(path)
    backend = FileSystemBackend(path)

    def send_fn(data):
        outf.write(data)
        outf.flush()
    proto = Protocol(inf.read, send_fn)
    handler = ReceivePackHandler(backend, [path], proto)
    # FIXME: Catch exceptions and write a single-line summary to outf.
    handler.handle()
    return 0
@

\subsection{[[git upload-pack]]}

<<function cmd_upload_pack>>=
class cmd_upload_pack(Command):

    def run(self, args):
        parser = optparse.OptionParser()
        options, args = parser.parse_args(args)
        if len(args) >= 1:
            gitdir = args[0]
        else:
            gitdir = '.'
        porcelain.upload_pack(gitdir)
@

<<function porcelain.upload_pack>>=
def upload_pack(path=".", inf=None, outf=None):
    """Upload a pack file after negotiating its contents using smart protocol.

    :param path: Path to the repository
    :param inf: Input stream to communicate with client
    :param outf: Output stream to communicate with client
    """
    if outf is None:
        outf = getattr(sys.stdout, 'buffer', sys.stdout)
    if inf is None:
        inf = getattr(sys.stdin, 'buffer', sys.stdin)
    path = os.path.expanduser(path)
    backend = FileSystemBackend(path)

    def send_fn(data):
        outf.write(data)
        outf.flush()
    proto = Protocol(inf.read, send_fn)
    handler = UploadPackHandler(backend, [path], proto)
    # FIXME: Catch exceptions and write a single-line summary to outf.
    handler.handle()
    return 0
@

\section{Optimizations}

\subsection{LRU cache}

<<class _LRUNode>>=
class _LRUNode(object):
    """This maintains the linked-list which is the lru internals."""

    __slots__ = ('prev', 'next_key', 'key', 'value', 'cleanup', 'size')

    def __init__(self, key, value, cleanup=None):
        self.prev = None
        self.next_key = _null_key
        self.key = key
        self.value = value
        self.cleanup = cleanup
        # TODO: We could compute this 'on-the-fly' like we used to, and remove
        #       one pointer from this object, we just need to decide if it
        #       actually costs us much of anything in normal usage
        self.size = None

    def __repr__(self):
        if self.prev is None:
            prev_key = None
        else:
            prev_key = self.prev.key
        return '%s(%r n:%r p:%r)' % (self.__class__.__name__, self.key,
                                     self.next_key, prev_key)

    def run_cleanup(self):
        if self.cleanup is not None:
            self.cleanup(self.key, self.value)
        self.cleanup = None
        # Just make sure to break any refcycles, etc
        self.value = None
@

<<class LRUCache>>=
class LRUCache(object):
    """A class which manages a cache of entries, removing unused ones."""

    def __init__(self, max_cache=100, after_cleanup_count=None):
        self._cache = {}
        # The "HEAD" of the lru linked list
        self._most_recently_used = None
        # The "TAIL" of the lru linked list
        self._least_recently_used = None
        self._update_max_cache(max_cache, after_cleanup_count)

    def __contains__(self, key):
        return key in self._cache

    def __getitem__(self, key):
        cache = self._cache
        node = cache[key]
        # Inlined from _record_access to decrease the overhead of __getitem__
        # We also have more knowledge about structure if __getitem__ is
        # succeeding, then we know that self._most_recently_used must not be
        # None, etc.
        mru = self._most_recently_used
        if node is mru:
            # Nothing to do, this node is already at the head of the queue
            return node.value
        # Remove this node from the old location
        node_prev = node.prev
        next_key = node.next_key
        # benchmarking shows that the lookup of _null_key in globals is faster
        # than the attribute lookup for (node is self._least_recently_used)
        if next_key is _null_key:
            # 'node' is the _least_recently_used, because it doesn't have a
            # 'next' item. So move the current lru to the previous node.
            self._least_recently_used = node_prev
        else:
            node_next = cache[next_key]
            node_next.prev = node_prev
        node_prev.next_key = next_key
        # Insert this node at the front of the list
        node.next_key = mru.key
        mru.prev = node
        self._most_recently_used = node
        node.prev = None
        return node.value

    def __len__(self):
        return len(self._cache)

    def _walk_lru(self):
        """Walk the LRU list, only meant to be used in tests."""
        node = self._most_recently_used
        if node is not None:
            if node.prev is not None:
                raise AssertionError('the _most_recently_used entry is not'
                                     ' supposed to have a previous entry'
                                     ' %s' % (node,))
        while node is not None:
            if node.next_key is _null_key:
                if node is not self._least_recently_used:
                    raise AssertionError('only the last node should have'
                                         ' no next value: %s' % (node,))
                node_next = None
            else:
                node_next = self._cache[node.next_key]
                if node_next.prev is not node:
                    raise AssertionError('inconsistency found, node.next.prev'
                                         ' != node: %s' % (node,))
            if node.prev is None:
                if node is not self._most_recently_used:
                    raise AssertionError('only the _most_recently_used should'
                                         ' not have a previous node: %s'
                                         % (node,))
            else:
                if node.prev.next_key != node.key:
                    raise AssertionError('inconsistency found, node.prev.next'
                                         ' != node: %s' % (node,))
            yield node
            node = node_next

    def add(self, key, value, cleanup=None):
        """Add a new value to the cache.

        Also, if the entry is ever removed from the cache, call
        cleanup(key, value).

        :param key: The key to store it under
        :param value: The object to store
        :param cleanup: None or a function taking (key, value) to indicate
                        'value' should be cleaned up.
        """
        if key is _null_key:
            raise ValueError('cannot use _null_key as a key')
        if key in self._cache:
            node = self._cache[key]
            node.run_cleanup()
            node.value = value
            node.cleanup = cleanup
        else:
            node = _LRUNode(key, value, cleanup=cleanup)
            self._cache[key] = node
        self._record_access(node)

        if len(self._cache) > self._max_cache:
            # Trigger the cleanup
            self.cleanup()

    def cache_size(self):
        """Get the number of entries we will cache."""
        return self._max_cache

    def get(self, key, default=None):
        node = self._cache.get(key, None)
        if node is None:
            return default
        self._record_access(node)
        return node.value

    def keys(self):
        """Get the list of keys currently cached.

        Note that values returned here may not be available by the time you
        request them later. This is simply meant as a peak into the current
        state.

        :return: An unordered list of keys that are currently cached.
        """
        return self._cache.keys()

    def items(self):
        """Get the key:value pairs as a dict."""
        return dict((k, n.value) for k, n in self._cache.items())

    def cleanup(self):
        """Clear the cache until it shrinks to the requested size.

        This does not completely wipe the cache, just makes sure it is under
        the after_cleanup_count.
        """
        # Make sure the cache is shrunk to the correct size
        while len(self._cache) > self._after_cleanup_count:
            self._remove_lru()

    def __setitem__(self, key, value):
        """Add a value to the cache, there will be no cleanup function."""
        self.add(key, value, cleanup=None)

    def _record_access(self, node):
        """Record that key was accessed."""
        # Move 'node' to the front of the queue
        if self._most_recently_used is None:
            self._most_recently_used = node
            self._least_recently_used = node
            return
        elif node is self._most_recently_used:
            # Nothing to do, this node is already at the head of the queue
            return
        # We've taken care of the tail pointer, remove the node, and insert it
        # at the front
        # REMOVE
        if node is self._least_recently_used:
            self._least_recently_used = node.prev
        if node.prev is not None:
            node.prev.next_key = node.next_key
        if node.next_key is not _null_key:
            node_next = self._cache[node.next_key]
            node_next.prev = node.prev
        # INSERT
        node.next_key = self._most_recently_used.key
        self._most_recently_used.prev = node
        self._most_recently_used = node
        node.prev = None

    def _remove_node(self, node):
        if node is self._least_recently_used:
            self._least_recently_used = node.prev
        self._cache.pop(node.key)
        # If we have removed all entries, remove the head pointer as well
        if self._least_recently_used is None:
            self._most_recently_used = None
        node.run_cleanup()
        # Now remove this node from the linked list
        if node.prev is not None:
            node.prev.next_key = node.next_key
        if node.next_key is not _null_key:
            node_next = self._cache[node.next_key]
            node_next.prev = node.prev
        # And remove this node's pointers
        node.prev = None
        node.next_key = _null_key

    def _remove_lru(self):
        """Remove one entry from the lru, and handle consequences.

        If there are no more references to the lru, then this entry should be
        removed from the cache.
        """
        self._remove_node(self._least_recently_used)

    def clear(self):
        """Clear out all of the cache."""
        # Clean up in LRU order
        while self._cache:
            self._remove_lru()

    def resize(self, max_cache, after_cleanup_count=None):
        """Change the number of entries that will be cached."""
        self._update_max_cache(max_cache,
                               after_cleanup_count=after_cleanup_count)

    def _update_max_cache(self, max_cache, after_cleanup_count=None):
        self._max_cache = max_cache
        if after_cleanup_count is None:
            self._after_cleanup_count = self._max_cache * 8 / 10
        else:
            self._after_cleanup_count = min(after_cleanup_count,
                                            self._max_cache)
        self.cleanup()
@

<<class LRUSizeCache>>=
class LRUSizeCache(LRUCache):
    """An LRUCache that removes things based on the size of the values.

    This differs in that it doesn't care how many actual items there are,
    it just restricts the cache to be cleaned up after so much data is stored.

    The size of items added will be computed using compute_size(value), which
    defaults to len() if not supplied.
    """

    def __init__(self, max_size=1024*1024, after_cleanup_size=None,
                 compute_size=None):
        """Create a new LRUSizeCache.

        :param max_size: The max number of bytes to store before we start
            clearing out entries.
        :param after_cleanup_size: After cleaning up, shrink everything to this
            size.
        :param compute_size: A function to compute the size of the values. We
            use a function here, so that you can pass 'len' if you are just
            using simple strings, or a more complex function if you are using
            something like a list of strings, or even a custom object.
            The function should take the form "compute_size(value) => integer".
            If not supplied, it defaults to 'len()'
        """
        self._value_size = 0
        self._compute_size = compute_size
        if compute_size is None:
            self._compute_size = len
        self._update_max_size(max_size, after_cleanup_size=after_cleanup_size)
        LRUCache.__init__(self, max_cache=max(int(max_size/512), 1))

    def add(self, key, value, cleanup=None):
        """Add a new value to the cache.

        Also, if the entry is ever removed from the cache, call
        cleanup(key, value).

        :param key: The key to store it under
        :param value: The object to store
        :param cleanup: None or a function taking (key, value) to indicate
                        'value' should be cleaned up.
        """
        if key is _null_key:
            raise ValueError('cannot use _null_key as a key')
        node = self._cache.get(key, None)
        value_len = self._compute_size(value)
        if value_len >= self._after_cleanup_size:
            # The new value is 'too big to fit', as it would fill up/overflow
            # the cache all by itself
            if node is not None:
                # We won't be replacing the old node, so just remove it
                self._remove_node(node)
            if cleanup is not None:
                cleanup(key, value)
            return
        if node is None:
            node = _LRUNode(key, value, cleanup=cleanup)
            self._cache[key] = node
        else:
            self._value_size -= node.size
        node.size = value_len
        self._value_size += value_len
        self._record_access(node)

        if self._value_size > self._max_size:
            # Time to cleanup
            self.cleanup()

    def cleanup(self):
        """Clear the cache until it shrinks to the requested size.

        This does not completely wipe the cache, just makes sure it is under
        the after_cleanup_size.
        """
        # Make sure the cache is shrunk to the correct size
        while self._value_size > self._after_cleanup_size:
            self._remove_lru()

    def _remove_node(self, node):
        self._value_size -= node.size
        LRUCache._remove_node(self, node)

    def resize(self, max_size, after_cleanup_size=None):
        """Change the number of bytes that will be cached."""
        self._update_max_size(max_size, after_cleanup_size=after_cleanup_size)
        max_cache = max(int(max_size/512), 1)
        self._update_max_cache(max_cache)

    def _update_max_size(self, max_size, after_cleanup_size=None):
        self._max_size = max_size
        if after_cleanup_size is None:
            self._after_cleanup_size = self._max_size * 8 // 10
        else:
            self._after_cleanup_size = min(after_cleanup_size, self._max_size)
@

\subsection{fast Import/export}

<<class GitFastExporter>>=
class GitFastExporter(object):
    """Generate a fast-export output stream for Git objects."""

    def __init__(self, outf, store):
        self.outf = outf
        self.store = store
        self.markers = {}
        self._marker_idx = 0

    def print_cmd(self, cmd):
        self.outf.write(getattr(cmd, "__bytes__", cmd.__repr__)() + b"\n")

    def _allocate_marker(self):
        self._marker_idx+=1
        return ("%d" % (self._marker_idx,)).encode('ascii')

    def _export_blob(self, blob):
        marker = self._allocate_marker()
        self.markers[marker] = blob.id
        return (commands.BlobCommand(marker, blob.data), marker)

    def emit_blob(self, blob):
        (cmd, marker) = self._export_blob(blob)
        self.print_cmd(cmd)
        return marker

    def _iter_files(self, base_tree, new_tree):
        for ((old_path, new_path), (old_mode, new_mode),
            (old_hexsha, new_hexsha)) in \
                self.store.tree_changes(base_tree, new_tree):
            if new_path is None:
                yield commands.FileDeleteCommand(old_path)
                continue
            if not stat.S_ISDIR(new_mode):
                blob = self.store[new_hexsha]
                marker = self.emit_blob(blob)
            if old_path != new_path and old_path is not None:
                yield commands.FileRenameCommand(old_path, new_path)
            if old_mode != new_mode or old_hexsha != new_hexsha:
                prefixed_marker = b':' + marker
                yield commands.FileModifyCommand(
                    new_path, new_mode, prefixed_marker, None
                )

    def _export_commit(self, commit, ref, base_tree=None):
        file_cmds = list(self._iter_files(base_tree, commit.tree))
        marker = self._allocate_marker()
        if commit.parents:
            from_ = commit.parents[0]
            merges = commit.parents[1:]
        else:
            from_ = None
            merges = []
        author, author_email = split_email(commit.author)
        committer, committer_email = split_email(commit.committer)
        cmd = commands.CommitCommand(ref, marker,
            (author, author_email, commit.author_time, commit.author_timezone),
            (committer, committer_email, commit.commit_time,
                commit.commit_timezone),
            commit.message, from_, merges, file_cmds)
        return (cmd, marker)

    def emit_commit(self, commit, ref, base_tree=None):
        cmd, marker = self._export_commit(commit, ref, base_tree)
        self.print_cmd(cmd)
        return marker
@


<<class GitImportProcessor>>=
class GitImportProcessor(processor.ImportProcessor):
    """An import processor that imports into a Git repository using Dulwich.

    """
    # FIXME: Batch creation of objects?

    def __init__(self, repo, params=None, verbose=False, outf=None):
        processor.ImportProcessor.__init__(self, params, verbose)
        self.repo = repo
        self.last_commit = None
        self.markers = {}
        self._contents = {}

    def import_stream(self, stream):
        p = parser.ImportParser(stream)
        self.process(p.iter_commands)
        return self.markers

    def blob_handler(self, cmd):
        """Process a BlobCommand."""
        blob = Blob.from_string(cmd.data)
        self.repo.object_store.add_object(blob)
        if cmd.mark:
            self.markers[cmd.mark] = blob.id

    def checkpoint_handler(self, cmd):
        """Process a CheckpointCommand."""
        pass

    def commit_handler(self, cmd):
        """Process a CommitCommand."""
        commit = Commit()
        if cmd.author is not None:
            author = cmd.author
        else:
            author = cmd.committer
        (author_name, author_email, author_timestamp, author_timezone) = author
        (committer_name, committer_email, commit_timestamp,
            commit_timezone) = cmd.committer
        commit.author = author_name + b" <" + author_email + b">"
        commit.author_timezone = author_timezone
        commit.author_time = int(author_timestamp)
        commit.committer = committer_name + b" <" + committer_email + b">"
        commit.commit_timezone = commit_timezone
        commit.commit_time = int(commit_timestamp)
        commit.message = cmd.message
        commit.parents = []
        if cmd.from_:
            self._reset_base(cmd.from_)
        for filecmd in cmd.iter_files():
            if filecmd.name == b"filemodify":
                if filecmd.data is not None:
                    blob = Blob.from_string(filecmd.data)
                    self.repo.object_store.add(blob)
                    blob_id = blob.id
                else:
                    assert filecmd.dataref.startswith(b":"), \
                        "non-marker refs not supported yet (%r)" % filecmd.dataref
                    blob_id = self.markers[filecmd.dataref[1:]]
                self._contents[filecmd.path] = (filecmd.mode, blob_id)
            elif filecmd.name == b"filedelete":
                del self._contents[filecmd.path]
            elif filecmd.name == b"filecopy":
                self._contents[filecmd.dest_path] = self._contents[
                    filecmd.src_path]
            elif filecmd.name == b"filerename":
                self._contents[filecmd.new_path] = self._contents[
                    filecmd.old_path]
                del self._contents[filecmd.old_path]
            elif filecmd.name == b"filedeleteall":
                self._contents = {}
            else:
                raise Exception("Command %s not supported" % filecmd.name)
        commit.tree = commit_tree(self.repo.object_store,
            ((path, hexsha, mode) for (path, (mode, hexsha)) in
                self._contents.items()))
        if self.last_commit is not None:
            commit.parents.append(self.last_commit)
        commit.parents += cmd.merges
        self.repo.object_store.add_object(commit)
        self.repo[cmd.ref] = commit.id
        self.last_commit = commit.id
        if cmd.mark:
            self.markers[cmd.mark] = commit.id

    def progress_handler(self, cmd):
        """Process a ProgressCommand."""
        pass

    def _reset_base(self, commit_id):
        if self.last_commit == commit_id:
            return
        self.last_commit = commit_id
        self._contents = {}
        tree_id = self.repo[commit_id].tree
        for (path, mode, hexsha) in (
                self.repo.object_store.iter_tree_contents(tree_id)):
            self._contents[path] = (mode, hexsha)

    def reset_handler(self, cmd):
        """Process a ResetCommand."""
        self._reset_base(cmd.from_)
        self.repo.refs[cmd.ref] = cmd.from_

    def tag_handler(self, cmd):
        """Process a TagCommand."""
        tag = Tag()
        tag.tagger = cmd.tagger
        tag.message = cmd.message
        tag.name = cmd.tag
        self.repo.add_object(tag)
        self.repo.refs["refs/tags/" + tag.name] = tag.id

    def feature_handler(self, cmd):
        """Process a FeatureCommand."""
        raise fastimport_errors.UnknownFeature(cmd.feature_name)
@

\section{Advanced client/server capabilities}

\subsection{Quiet}

<<[[GitClient.__init__()]] adjust capabilities>>=
if quiet:
    self._send_capabilities.add(CAPABILITY_QUIET)
@

\subsection{Thin pack}

<<[[GitClient.__init__()]] adjust capabilities>>=
if not thin_packs:
    self._fetch_capabilities.remove(CAPABILITY_THIN_PACK)
@

<<[[GitClient.fetch()]] if thin pack capability>>=
if CAPABILITY_THIN_PACK in self._fetch_capabilities:
    # TODO(jelmer): Avoid reading entire file into memory and
    # only processing it after the whole file has been fetched.
    f = BytesIO()

    def commit():
        if f.tell():
            f.seek(0)
            target.object_store.add_thin_pack(f.read, None)

    def abort():
        pass
@

<<[[DiskObjectStore]] methods>>=
def add_thin_pack(self, read_all, read_some):
    """Add a new thin pack to this object store.

    Thin packs are packs that contain deltas with parents that exist
    outside the pack. They should never be placed in the object store
    directly, and always indexed and completed as they are copied.

    :param read_all: Read function that blocks until the number of
        requested bytes are read.
    :param read_some: Read function that returns at least one byte, but may
        not return the number of bytes requested.
    :return: A Pack object pointing at the now-completed thin pack in the
        objects/pack directory.
    """
    fd, path = tempfile.mkstemp(dir=self.path, prefix='tmp_pack_')
    with os.fdopen(fd, 'w+b') as f:
        indexer = PackIndexer(f, resolve_ext_ref=self.get_raw)
        copier = PackStreamCopier(read_all, read_some, f,
                                  delta_iter=indexer)
        copier.verify()
        return self._complete_thin_pack(f, path, copier, indexer)

@


<<[[DiskObjectStore]] methods>>=
def _complete_thin_pack(self, f, path, copier, indexer):
    """Move a specific file containing a pack into the pack directory.

    :note: The file should be on the same file system as the
        packs directory.

    :param f: Open file object for the pack.
    :param path: Path to the pack file.
    :param copier: A PackStreamCopier to use for writing pack data.
    :param indexer: A PackIndexer for indexing the pack.
    """
    entries = list(indexer)

    # Update the header with the new number of objects.
    f.seek(0)
    write_pack_header(f, len(entries) + len(indexer.ext_refs()))

    # Must flush before reading (http://bugs.python.org/issue3207)
    f.flush()

    # Rescan the rest of the pack, computing the SHA with the new header.
    new_sha = compute_file_sha(f, end_ofs=-20)

    # Must reposition before writing (http://bugs.python.org/issue3207)
    f.seek(0, os.SEEK_CUR)

    # Complete the pack.
    for ext_sha in indexer.ext_refs():
        assert len(ext_sha) == 20
        type_num, data = self.get_raw(ext_sha)
        offset = f.tell()
        crc32 = write_pack_object(f, type_num, data, sha=new_sha)
        entries.append((ext_sha, offset, crc32))
    pack_sha = new_sha.digest()
    f.write(pack_sha)
    f.close()

    # Move the pack in.
    entries.sort()
    pack_base_name = self._get_pack_basepath(entries)

    os.rename(path, pack_base_name + '.pack')

    # Write the index.
    index_file = GitFile(pack_base_name + '.idx', 'wb')
    try:
        write_pack_index_v2(index_file, entries, pack_sha)
        index_file.close()
    finally:
        index_file.abort()

    # Add the pack to the store and return it.
    final_pack = Pack(pack_base_name)
    final_pack.check_length_and_checksum()
    self._add_known_pack(pack_base_name, final_pack)
    return final_pack
@

%win32:
%    if sys.platform == 'win32':
%        try:
%            os.rename(path, pack_base_name + '.pack')
%        except WindowsError:
%            os.remove(pack_base_name + '.pack')
%            os.rename(path, pack_base_name + '.pack')
%    else:


\subsection{Side band 64k}

<<[[GitClient]] methods>>=
def _read_side_band64k_data(self, proto, channel_callbacks):
    """Read per-channel data.

    This requires the side-band-64k capability.

    :param proto: Protocol object to read from
    :param channel_callbacks: Dictionary mapping channels to packet
        handlers to use. None for a callback discards channel data.
    """
    for pkt in proto.read_pkt_seq():
        channel = ord(pkt[:1])
        pkt = pkt[1:]
        try:
            cb = channel_callbacks[channel]
        except KeyError:
            raise AssertionError('Invalid sideband channel %d' % channel)
        else:
            if cb is not None:
                cb(pkt)

@

\section{Grafts}

<<[[BaseRepo.__init__()]] grafts>>=
self._graftpoints = {}
@

<<[[Repo.__init__()]] grafts>>=
self._graftpoints = {}
graft_file = self.get_named_file(os.path.join("info", "grafts"),
                                 basedir=self.commondir())
if graft_file:
    with graft_file:
        self._graftpoints.update(parse_graftpoints(graft_file))
graft_file = self.get_named_file("shallow",
                                 basedir=self.commondir())
if graft_file:
    with graft_file:
        self._graftpoints.update(parse_graftpoints(graft_file))
@


<<[[BaseRepo]] methods>>=
def _add_graftpoints(self, updated_graftpoints):
    """Add or modify graftpoints

    :param updated_graftpoints: Dict of commit shas to list of parent shas
    """

    # Simple validation
    for commit, parents in updated_graftpoints.items():
        for sha in [commit] + parents:
            check_hexsha(sha, 'Invalid graftpoint')

    self._graftpoints.update(updated_graftpoints)

@

<<[[BaseRepo]] methods>>=
def _remove_graftpoints(self, to_remove=[]):
    """Remove graftpoints

    :param to_remove: List of commit shas
    """
    for sha in to_remove:
        del self._graftpoints[sha]

@


<<function parse_graftpoints>>=
def parse_graftpoints(graftpoints):
    """Convert a list of graftpoints into a dict

    :param graftpoints: Iterator of graftpoint lines

    Each line is formatted as:
        <commit sha1> <parent sha1> [<parent sha1>]*

    Resulting dictionary is:
        <commit sha1>: [<parent sha1>*]

    https://git.wiki.kernel.org/index.php/GraftPoint
    """
    grafts = {}
    for l in graftpoints:
        raw_graft = l.split(None, 1)

        commit = raw_graft[0]
        if len(raw_graft) == 2:
            parents = raw_graft[1].split()
        else:
            parents = []

        for sha in [commit] + parents:
            check_hexsha(sha, 'Invalid graftpoint')

        grafts[commit] = parents
    return grafts
@

<<function serialize_graftpoints>>=
def serialize_graftpoints(graftpoints):
    """Convert a dictionary of grafts into string

    The graft dictionary is:
        <commit sha1>: [<parent sha1>*]

    Each line is formatted as:
        <commit sha1> <parent sha1> [<parent sha1>]*

    https://git.wiki.kernel.org/index.php/GraftPoint

    """
    graft_lines = []
    for commit, parents in graftpoints.items():
        if parents:
            graft_lines.append(commit + b' ' + b' '.join(parents))
        else:
            graft_lines.append(commit)
    return b'\n'.join(graft_lines)

@

\section{Submodules}

<<constant S_IFGITLINK>>=
S_IFGITLINK = 0o160000
@

<<function objects.S_ISGITLINK>>=
def S_ISGITLINK(m):
    """Check if a mode indicates a submodule.

    :param m: Mode to check
    :return: a ``boolean``
    """
    return (stat.S_IFMT(m) == S_IFGITLINK)
@
% `` ``

<<class StackedConfig>>=
class StackedConfig(Config):
    """Configuration which reads from multiple config files.."""

    <<[[StackedConfig]] methods>>
@

<<[[StackedConfig]] methods>>=
def __init__(self, backends, writable=None):
    self.backends = backends
    self.writable = writable
@


<<[[StackedConfig]] methods>>=
@classmethod
def default_backends(cls):
    """Retrieve the default configuration.

    See git-config(1) for details on the files searched.
    """
    paths = []
    paths.append(os.path.expanduser("~/.gitconfig"))

    xdg_config_home = os.environ.get(
        "XDG_CONFIG_HOME", os.path.expanduser("~/.config/"),
    )
    paths.append(os.path.join(xdg_config_home, "git", "config"))

    if "GIT_CONFIG_NOSYSTEM" not in os.environ:
        paths.append("/etc/gitconfig")

    backends = []
    for path in paths:
        try:
            cf = ConfigFile.from_path(path)
        except (IOError, OSError) as e:
            if e.errno != errno.ENOENT:
                raise
            else:
                continue
        backends.append(cf)
    return backends

@

<<[[StackedConfig]] methods>>=
def get(self, section, name):
    for backend in self.backends:
        try:
            return backend.get(section, name)
        except KeyError:
            pass
    raise KeyError(name)

@

<<[[StackedConfig]] methods>>=
def set(self, section, name, value):
    if self.writable is None:
        raise NotImplementedError(self.set)
    return self.writable.set(section, name, value)
@

<<function config.parse_submodules>>=
def parse_submodules(config):
    """Parse a gitmodules GitConfig file, returning submodules.

   :param config: A `ConfigFile`
   :return: list of tuples (submodule path, url, name),
       where name is quoted part of the section's name.
    """
    for section in config.keys():
        section_kind, section_name = section
        if section_kind == b'submodule':
            sm_path = config.get(section, b'path')
            sm_url = config.get(section, b'url')
            yield (sm_path, sm_url, section_name)
@

\section{Legacy objects}

<<[[ShaFile._parse_file()]] if legacy object>>=
if cls._is_legacy_object(map):
    raise AssertionError('use legacy format')
    obj = cls._parse_legacy_object_header(map, f)
    obj._parse_legacy_object(map)
@

<<[[ShaFile]] methods>>=
@classmethod
def _is_legacy_object(cls, magic):
    b0 = ord(magic[0:1])
    b1 = ord(magic[1:2])
    word = (b0 << 8) + b1
    return (b0 & 0x8F) == 0x08 and (word % 31) == 0
@

<<[[ShaFile]] methods>>=
@staticmethod
def _parse_legacy_object_header(magic, f):
    """Parse a legacy object, creating it but not reading the file."""
    bufsize = 1024
    decomp = zlib.decompressobj()
    header = decomp.decompress(magic)
    start = 0
    end = -1
    while end < 0:
        extra = f.read(bufsize)
        header += decomp.decompress(extra)
        magic += extra
        end = header.find(b'\0', start)
        start = len(header)
    header = header[:end]
    type_name, size = header.split(b' ', 1)
    size = int(size)  # sanity check
    obj_class = object_class(type_name)
    if not obj_class:
        raise ObjectFormatException("Not a known type: %s" % type_name)
    return obj_class()
@

<<[[ShaFile]] methods>>=
def _parse_legacy_object(self, map):
    """Parse a legacy object, setting the raw string."""
    text = _decompress(map)
    header_end = text.find(b'\0')
    if header_end < 0:
        raise ObjectFormatException("Invalid object header, no \\0")
    self.set_raw_string(text[header_end+1:])

@


<<[[ShaFile]] methods>>=
def as_legacy_object(self):
    """Return string representing the object in the experimental format.
    """
    return b''.join(self.as_legacy_object_chunks())
@

<<[[ShaFile]] methods>>=
def as_legacy_object_chunks(self):
    """Return chunks representing the object in the experimental format.

    :return: List of strings
    """
    compobj = zlib.compressobj()
    yield compobj.compress(self._header())
    for chunk in self.as_raw_chunks():
        yield compobj.compress(chunk)
    yield compobj.flush()

@


\section{Alternates}

<<[[PackBasedObjectStore]] methods>>=
@property
def alternates(self):
    return []

@

<<[[DiskObjectStore.__init__()]] set alternates>>=
self._alternates = None
@

<<[[PackBasedObjectStore.get_raw()]] look in alternates>>=
for alternate in self.alternates:
    try:
        return alternate.get_raw(hexsha)
    except KeyError:
        pass
@


<<[[DiskObjectStore]] methods>>=
@property
def alternates(self):
    if self._alternates is not None:
        return self._alternates
    self._alternates = []
    for path in self._read_alternate_paths():
        self._alternates.append(DiskObjectStore(path))
    return self._alternates

@

<<[[DiskObjectStore]] methods>>=
def _read_alternate_paths(self):
    try:
        f = GitFile(os.path.join(self.path, INFODIR, "alternates"), 'rb')
    except (OSError, IOError) as e:
        if e.errno == errno.ENOENT:
            return
        raise
    with f:
        for l in f.readlines():
            l = l.rstrip(b"\n")
            if l[0] == b"#":
                continue
            if os.path.isabs(l):
                yield l.decode(sys.getfilesystemencoding())
            else:
                yield os.path.join(self.path, l).decode(
                    sys.getfilesystemencoding())

@

<<[[DiskObjectStore]] methods>>=
def add_alternate_path(self, path):
    """Add an alternate path to this object store.
    """
    try:
        os.mkdir(os.path.join(self.path, INFODIR))
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise
    alternates_path = os.path.join(self.path, INFODIR, "alternates")
    with GitFile(alternates_path, 'wb') as f:
        try:
            orig_f = open(alternates_path, 'rb')
        except (OSError, IOError) as e:
            if e.errno != errno.ENOENT:
                raise
        else:
            with orig_f:
                f.write(orig_f.read())
        f.write(path.encode(sys.getfilesystemencoding()) + b"\n")

    if not os.path.isabs(path):
        path = os.path.join(self.path, path)
    self.alternates.append(DiskObjectStore(path))

@

\section{Peeled}

<<[[BaseObjectStore]] methods>>=
def peel_sha(self, sha):
    """Peel all tags from a SHA.

    :param sha: The object SHA to peel.
    :return: The fully-peeled SHA1 of a tag object, after peeling all
        intermediate tags; if the original ref does not point to a tag,
        this will equal the original SHA1.
    """
    obj = self[sha]
    obj_class = object_class(obj.type_name)
    while obj_class is Tag:
        obj_class, sha = obj.object
        obj = self[sha]
    return obj

@

<<[[RefsContainer]] methods>>=
def get_peeled(self, name):
    """Return the cached peeled value of a ref, if available.

    :param name: Name of the ref to peel
    :return: The peeled value of the ref. If the ref is known not point to a
        tag, this will be the SHA the ref refers to. If the ref may point to
        a tag, but no cached information is available, None is returned.
    """
    return None
@

<<[[BaseRepo]] methods>>=
def get_peeled(self, ref):
    """Get the peeled value of a ref.

    :param ref: The refname to peel.
    :return: The fully-peeled SHA1 of a tag object, after peeling all
        intermediate tags; if the original ref does not point to a tag,
        this will equal the original SHA1.
    """
    cached = self.refs.get_peeled(ref)
    if cached is not None:
        return cached
    return self.object_store.peel_sha(self.refs[ref]).id
@

<<[[DiskRefsContainer]] methods>>=
def get_peeled(self, name):
    """Return the cached peeled value of a ref, if available.

    :param name: Name of the ref to peel
    :return: The peeled value of the ref. If the ref is known not point to a
        tag, this will be the SHA the ref refers to. If the ref may point to
        a tag, but no cached information is available, None is returned.
    """
    self.get_packed_refs()
    if self._peeled_refs is None or name not in self._packed_refs:
        # No cache: no peeled refs were read, or this ref is loose
        return None
    if name in self._peeled_refs:
        return self._peeled_refs[name]
    else:
        # Known not peelable
        return self[name]

@


\section{Configuration file, [[.git/config]]}


<<class ConfigFile>>=
class ConfigFile(ConfigDict):
    """A Git configuration file, like .git/config or ~/.gitconfig.
    """
    <<[[ConfigFile]] methods>>
@

<<class ConfigDict>>=
class ConfigDict(Config, MutableMapping):
    """Git configuration stored in a dictionary."""

    <<[[ConfigDict]] methods>>
@

<<class Config>>=
class Config(object):
    """A Git configuration."""

    <<[[Config]] methods>>
@



<<[[Config]] methods>>=
def get(self, section, name):
    """Retrieve the contents of a configuration setting.

    :param section: Tuple with section name and optional subsection namee
    :param subsection: Subsection name
    :return: Contents of the setting
    :raise KeyError: if the value is not set
    """
    raise NotImplementedError(self.get)
@

<<[[Config]] methods>>=
def get_boolean(self, section, name, default=None):
    """Retrieve a configuration setting as boolean.

    :param section: Tuple with section name and optional subsection namee
    :param name: Name of the setting, including section and possible
        subsection.
    :return: Contents of the setting
    :raise KeyError: if the value is not set
    """
    try:
        value = self.get(section, name)
    except KeyError:
        return default
    if value.lower() == b"true":
        return True
    elif value.lower() == b"false":
        return False
    raise ValueError("not a valid boolean string: %r" % value)
@

<<[[Config]] methods>>=
def set(self, section, name, value):
    """Set a configuration value.

    :param section: Tuple with section name and optional subsection namee
    :param name: Name of the configuration value, including section
        and optional subsection
    :param: Value of the setting
    """
    raise NotImplementedError(self.set)
@

<<[[Config]] methods>>=
def iteritems(self, section):
    """Iterate over the configuration pairs for a specific section.

    :param section: Tuple with section name and optional subsection namee
    :return: Iterator over (name, value) pairs
    """
    raise NotImplementedError(self.iteritems)
@

<<[[Config]] methods>>=
def itersections(self):
    """Iterate over the sections.

    :return: Iterator over section tuples
    """
    raise NotImplementedError(self.itersections)
@

<<[[Config]] methods>>=
def has_section(self, name):
    """Check if a specified section exists.

    :param name: Name of section to check for
    :return: boolean indicating whether the section exists
    """
    return (name in self.itersections())
@

<<[[ConfigFile]] methods>>=
@classmethod
def from_file(cls, f):
    """Read configuration from a file-like object."""
    ret = cls()
    section = None
    setting = None
    for lineno, line in enumerate(f.readlines()):
        line = line.lstrip()
        if setting is None:
            # Parse section header ("[bla]")
            if len(line) > 0 and line[:1] == b"[":
                line = _strip_comments(line).rstrip()
                last = line.index(b"]")
                if last == -1:
                    raise ValueError("expected trailing ]")
                pts = line[1:last].split(b" ", 1)
                line = line[last+1:]
                pts[0] = pts[0].lower()
                if len(pts) == 2:
                    if pts[1][:1] != b"\"" or pts[1][-1:] != b"\"":
                        raise ValueError(
                            "Invalid subsection %r" % pts[1])
                    else:
                        pts[1] = pts[1][1:-1]
                    if not _check_section_name(pts[0]):
                        raise ValueError("invalid section name %r" %
                                         pts[0])
                    section = (pts[0], pts[1])
                else:
                    if not _check_section_name(pts[0]):
                        raise ValueError(
                            "invalid section name %r" % pts[0])
                    pts = pts[0].split(b".", 1)
                    if len(pts) == 2:
                        section = (pts[0], pts[1])
                    else:
                        section = (pts[0], )
                ret._values[section] = OrderedDict()
            if _strip_comments(line).strip() == b"":
                continue
            if section is None:
                raise ValueError("setting %r without section" % line)
            try:
                setting, value = line.split(b"=", 1)
            except ValueError:
                setting = line
                value = b"true"
            setting = setting.strip().lower()
            if not _check_variable_name(setting):
                raise ValueError("invalid variable name %s" % setting)
            if value.endswith(b"\\\n"):
                continuation = value[:-2]
            else:
                continuation = None
                value = _parse_string(value)
                ret._values[section][setting] = value
                setting = None
        else:  # continuation line
            if line.endswith(b"\\\n"):
                continuation += line[:-2]
            else:
                continuation += line
                value = _parse_string(continuation)
                ret._values[section][setting] = value
                continuation = None
                setting = None
    return ret
@

<<[[ConfigFile]] methods>>=
@classmethod
def from_path(cls, path):
    """Read configuration from a file on disk."""
    with GitFile(path, 'rb') as f:
        ret = cls.from_file(f)
        ret.path = path
        return ret
@

<<[[ConfigFile]] methods>>=
def write_to_path(self, path=None):
    """Write configuration to a file on disk."""
    if path is None:
        path = self.path
    with GitFile(path, 'wb') as f:
        self.write_to_file(f)
@

<<[[ConfigFile]] methods>>=
def write_to_file(self, f):
    """Write configuration to a file-like object."""
    for section, values in self._values.items():
        try:
            section_name, subsection_name = section
        except ValueError:
            (section_name, ) = section
            subsection_name = None
        if subsection_name is None:
            f.write(b"[" + section_name + b"]\n")
        else:
            f.write(b"[" + section_name +
                    b" \"" + subsection_name + b"\"]\n")
        for key, value in values.items():
            if value is True:
                value = b"true"
            elif value is False:
                value = b"false"
            else:
                value = _format_string(value)
            f.write(b"\t" + key + b" = " + value + b"\n")
@



<<[[ConfigDict]] methods>>=
def __init__(self, values=None):
    """Create a new ConfigDict."""
    if values is None:
        values = OrderedDict()
    self._values = values
@


<<[[ConfigDict]] methods>>=
def __eq__(self, other):
    return (
        isinstance(other, self.__class__) and
        other._values == self._values)

@

<<[[ConfigDict]] methods>>=
def __getitem__(self, key):
    return self._values.__getitem__(key)

@

<<[[ConfigDict]] methods>>=
def __setitem__(self, key, value):
    return self._values.__setitem__(key, value)

@

<<[[ConfigDict]] methods>>=
def __delitem__(self, key):
    return self._values.__delitem__(key)

@

<<[[ConfigDict]] methods>>=
def __iter__(self):
    return self._values.__iter__()

@

<<[[ConfigDict]] methods>>=
def __len__(self):
    return self._values.__len__()

@

<<[[ConfigDict]] methods>>=
@classmethod
def _parse_setting(cls, name):
    parts = name.split(".")
    if len(parts) == 3:
        return (parts[0], parts[1], parts[2])
    else:
        return (parts[0], None, parts[1])

@

<<[[ConfigDict]] methods>>=
def get(self, section, name):
    if not isinstance(section, tuple):
        section = (section, )
    if len(section) > 1:
        try:
            return self._values[section][name]
        except KeyError:
            pass
    return self._values[(section[0],)][name]

@

<<[[ConfigDict]] methods>>=
def set(self, section, name, value):
    if not isinstance(section, tuple):
        section = (section, )
    if not isinstance(name, bytes):
        raise TypeError(name)
    if type(value) not in (bool, bytes):
        raise TypeError(value)
    self._values.setdefault(section, OrderedDict())[name] = value

@

<<[[ConfigDict]] methods>>=
def iteritems(self, section):
    return self._values.get(section, OrderedDict()).items()

@

<<[[ConfigDict]] methods>>=
def itersections(self):
    return self._values.keys()
@

\section{hooks, [[.git/hooks/]]}

<<[[BaseRepo.__init__()]] hooks>>=
self.hooks = {}
@

<<class Hook>>=
class Hook(object):
    """Generic hook object."""

    <<[[Hook]] methods>>
@

<<[[Hook]] methods>>=
def execute(self, *args):
    """Execute the hook with the given args

    :param args: argument list to hook
    :raise HookError: hook execution failure
    :return: a hook may return a useful value
    """
    raise NotImplementedError(self.execute)
@

<<class ShellHook>>=
class ShellHook(Hook):
    """Hook by executable file

    Implements standard githooks(5) [0]:

    [0] http://www.kernel.org/pub/software/scm/git/docs/githooks.html
    """

    <<[[ShellHook]] methods>>
@

<<[[ShellHook]] methods>>=
def __init__(self, name, path, numparam,
             pre_exec_callback=None, post_exec_callback=None):
    """Setup shell hook definition

    :param name: name of hook for error messages
    :param path: absolute path to executable file
    :param numparam: number of requirements parameters
    :param pre_exec_callback: closure for setup before execution
        Defaults to None. Takes in the variable argument list from the
        execute functions and returns a modified argument list for the
        shell hook.
    :param post_exec_callback: closure for cleanup after execution
        Defaults to None. Takes in a boolean for hook success and the
        modified argument list and returns the final hook return value
        if applicable
    """
    self.name = name
    self.filepath = path
    self.numparam = numparam

    self.pre_exec_callback = pre_exec_callback
    self.post_exec_callback = post_exec_callback
@

%win32:
%    if sys.version_info[0] == 2 and sys.platform == 'win32':
%        # Python 2 on windows does not support unicode file paths
%        # http://bugs.python.org/issue1759845
%        self.filepath = self.filepath.encode(sys.getfilesystemencoding())

<<[[ShellHook]] methods>>=
def execute(self, *args):
    """Execute the hook with given args"""

    if len(args) != self.numparam:
        raise HookError("Hook %s executed with wrong number of args. \
                        Expected %d. Saw %d. args: %s"
                        % (self.name, self.numparam, len(args), args))

    if (self.pre_exec_callback is not None):
        args = self.pre_exec_callback(*args)

    try:
        ret = subprocess.call([self.filepath] + list(args))
        if ret != 0:
            if (self.post_exec_callback is not None):
                self.post_exec_callback(0, *args)
            raise HookError("Hook %s exited with non-zero status"
                            % (self.name))
        if (self.post_exec_callback is not None):
            return self.post_exec_callback(1, *args)
    except OSError:  # no file. silent failure.
        if (self.post_exec_callback is not None):
            self.post_exec_callback(0, *args)
@


<<[[Repo.__init__()]] hooks>>=
self.hooks['pre-commit'] = PreCommitShellHook(self.controldir())
self.hooks['commit-msg'] = CommitMsgShellHook(self.controldir())
self.hooks['post-commit'] = PostCommitShellHook(self.controldir())
@


<<class PreCommitShellHook>>=
class PreCommitShellHook(ShellHook):
    """pre-commit shell hook"""

    def __init__(self, controldir):
        filepath = os.path.join(controldir, 'hooks', 'pre-commit')

        ShellHook.__init__(self, 'pre-commit', filepath, 0)
@

<<class PostCommitShellHook>>=
class PostCommitShellHook(ShellHook):
    """post-commit shell hook"""

    def __init__(self, controldir):
        filepath = os.path.join(controldir, 'hooks', 'post-commit')

        ShellHook.__init__(self, 'post-commit', filepath, 0)
@

<<class CommitMsgShellHook>>=
class CommitMsgShellHook(ShellHook):
    """commit-msg shell hook

    :param args[0]: commit message
    :return: new commit message or None
    """

    def __init__(self, controldir):
        filepath = os.path.join(controldir, 'hooks', 'commit-msg')

        def prepare_msg(*args):
            (fd, path) = tempfile.mkstemp()

            with os.fdopen(fd, 'wb') as f:
                f.write(args[0])

            return (path,)

        def clean_msg(success, *args):
            if success:
                with open(args[0], 'rb') as f:
                    new_msg = f.read()
                os.unlink(args[0])
                return new_msg
            os.unlink(args[0])

        ShellHook.__init__(self, 'commit-msg', filepath, 1,
                           prepare_msg, clean_msg)
@




<<[[BaseRepo.do_commit()]] execute pre-commit hook>>=
try:
    self.hooks['pre-commit'].execute()
except HookError as e:
    raise CommitError(e)
except KeyError:  # no hook defined, silent fallthrough
    pass
@

<<[[BaseRepo.do_commit()]] execute commit-msg hook>>=
try:
    c.message = self.hooks['commit-msg'].execute(message)
    if c.message is None:
        c.message = message
except HookError as e:
    raise CommitError(e)
except KeyError:  # no hook defined, message not modified
    c.message = message
@

<<[[BaseRepo.do_commit()]] execute post-commit hook>>=
try:
    self.hooks['post-commit'].execute()
except HookError as e:  # silent failure
    warnings.warn("post-commit hook failed: %s" % e, UserWarning)
except KeyError:  # no hook defined, silent fallthrough
    pass
@

\section{Ignore files, [[.gitignore]]}

<<class IgnoreFilter>>=
class IgnoreFilter(object):

    def __init__(self, patterns):
        self._patterns = []
        for pattern in patterns:
            self.append_pattern(pattern)

    def append_pattern(self, pattern):
        """Add a pattern to the set."""
        self._patterns.append(pattern)

    def is_ignored(self, path):
        """Check whether a path is ignored.

        For directories, include a trailing slash.

        :return: None if file is not mentioned, True if it is included, False
            if it is explicitly excluded.
        """
        status = None
        for pattern in self._patterns:
            if pattern[0:1] == b'!':
                if match_pattern(path, pattern[1:]):
                    status = False
            else:
                if pattern[0:1] == b'\\':
                    pattern = pattern[1:]
                if match_pattern(path, pattern):
                    status = True
        return status

@

<<class IgnoreFilterStack>>=
class IgnoreFilterStack(object):
    """Check for ignore status in multiple filters."""

    def __init__(self, filters):
        self._filters = filters

    def is_ignored(self, path):
        """Check whether a path is explicitly included or excluded in ignores.

        :param path: Path to check
        :return: None if the file is not mentioned, True if it is included,
            False if it is explicitly excluded.
        """
        status = None
        for filter in self._filters:
            status = filter.is_ignored(path)
            if status is not None:
                return status
        return status
@

\section{Other clients}

\subsection{[[ssh://]] client}

<<[[get_transport_and_path_from_url()]] elif ssh>>=
elif parsed.scheme in ('git+ssh', 'ssh'):
    return SSHGitClient.from_parsedurl(parsed, **kwargs), parsed.path
@

<<[[get_transport_and_path()]] try parse location as a ssh URL>>=
# First, try to parse it as a URL
try:
    return get_transport_and_path_from_url(location, **kwargs)
except ValueError:
    pass

if ':' in location and '@' not in location:
    # SSH with no user@, zero or one leading slash.
    (hostname, path) = location.split(':', 1)
    return SSHGitClient(hostname, **kwargs), path
elif ':' in location:
    # SSH with user@host:foo.
    user_host, path = location.split(':', 1)
    if '@' in user_host:
        user, host = user_host.rsplit('@', 1)
    else:
        user = None
        host = user_host
    return SSHGitClient(host, username=user, **kwargs), path
@



<<class SSHGitClient>>=
class SSHGitClient(TraditionalGitClient):

    <<[[SSHGitClient]] methods>>
@

<<[[SSHGitClient]] methods>>=
def __init__(self, host, port=None, username=None, vendor=None, **kwargs):
    self.host = host
    self.port = port
    self.username = username
    super(SSHGitClient, self).__init__(**kwargs)
    self.alternative_paths = {}
    if vendor is not None:
        self.ssh_vendor = vendor
    else:
        self.ssh_vendor = get_ssh_vendor()

def get_url(self, path):
    netloc = self.host
    if self.port is not None:
        netloc += ":%d" % self.port

    if self.username is not None:
        netloc = urlquote(self.username, '@/:') + "@" + netloc

    return urlparse.urlunsplit(('ssh', netloc, path, '', ''))

@classmethod
def from_parsedurl(cls, parsedurl, **kwargs):
    return cls(host=parsedurl.hostname, port=parsedurl.port,
               username=parsedurl.username, **kwargs)

def _get_cmd_path(self, cmd):
    cmd = self.alternative_paths.get(cmd, b'git-' + cmd)
    assert isinstance(cmd, bytes)
    return cmd

def _connect(self, cmd, path):
    if not isinstance(cmd, bytes):
        raise TypeError(cmd)
    if isinstance(path, bytes):
        path = path.decode(self._remote_path_encoding)
    if path.startswith("/~"):
        path = path[1:]
    argv = (self._get_cmd_path(cmd).decode(self._remote_path_encoding) +
            " '" + path + "'")
    con = self.ssh_vendor.run_command(
        self.host, argv, port=self.port, username=self.username)
    return (Protocol(con.read, con.write, con.close,
                     report_activity=self._report_activity),
            con.can_read)
@

<<global get_ssh_vendor>>=
# Can be overridden by users
get_ssh_vendor = SubprocessSSHVendor
@

<<class SSHVendor>>=
class SSHVendor(object):
    """A client side SSH implementation."""

    def connect_ssh(self, host, command, username=None, port=None):
        # This function was deprecated in 0.9.1
        import warnings
        warnings.warn(
            "SSHVendor.connect_ssh has been renamed to SSHVendor.run_command",
            DeprecationWarning)
        return self.run_command(host, command, username=username, port=port)

    def run_command(self, host, command, username=None, port=None):
        """Connect to an SSH server.

        Run a command remotely and return a file-like object for interaction
        with the remote command.

        :param host: Host name
        :param command: Command to run (as argv array)
        :param username: Optional ame of user to log in as
        :param port: Optional SSH port to use
        """
        raise NotImplementedError(self.run_command)
@

<<class SubprocessSSHVendor>>=
class SubprocessSSHVendor(SSHVendor):
    """SSH vendor that shells out to the local 'ssh' command."""

    def run_command(self, host, command, username=None, port=None):
        # FIXME: This has no way to deal with passwords..
        args = ['ssh', '-x']
        if port is not None:
            args.extend(['-p', str(port)])
        if username is not None:
            host = '%s@%s' % (username, host)
        args.append(host)
        proc = subprocess.Popen(args + [command], bufsize=0,
                                stdin=subprocess.PIPE,
                                stdout=subprocess.PIPE)
        return SubprocessWrapper(proc)
@

<<class SubprocessWrapper>>=
class SubprocessWrapper(object):
    """A socket-like object that talks to a subprocess via pipes."""

    def __init__(self, proc):
        self.proc = proc
        if sys.version_info[0] == 2:
            self.read = proc.stdout.read
        else:
            self.read = BufferedReader(proc.stdout).read
        self.write = proc.stdin.write

    def can_read(self):
            return _fileno_can_read(self.proc.stdout.fileno())

    def close(self):
        self.proc.stdin.close()
        self.proc.stdout.close()
        if self.proc.stderr:
            self.proc.stderr.close()
        self.proc.wait()
@

%win32:
%        if sys.platform == 'win32':
%            from msvcrt import get_osfhandle
%            handle = get_osfhandle(self.proc.stdout.fileno())
%            return _win32_peek_avail(handle) != 0
%        else:

%def _win32_peek_avail(handle):
%    """Wrapper around PeekNamedPipe to check how many bytes are available."""
%    from ctypes import byref, wintypes, windll
%    c_avail = wintypes.DWORD()
%    c_message = wintypes.DWORD()
%    success = windll.kernel32.PeekNamedPipe(
%        handle, None, 0, None, byref(c_avail),
%        byref(c_message))
%    if not success:
%        raise OSError(wintypes.GetLastError())
%    return c_avail.value


\subsection{[[http://]] client}

<<[[get_transport_and_path_from_url()]] elif http>>=
elif parsed.scheme in ('http', 'https'):
    return HttpGitClient.from_parsedurl(
        parsed, config=config, **kwargs), parsed.path
@

<<class HttpGitClient>>=
class HttpGitClient(GitClient):

    def __init__(self, base_url, dumb=None, opener=None, config=None,
                 username=None, password=None, **kwargs):
        self._base_url = base_url.rstrip("/") + "/"
        self._username = username
        self._password = password
        self.dumb = dumb
        if opener is None:
            self.opener = default_urllib2_opener(config)
        else:
            self.opener = opener
        if username is not None:
            pass_man = urllib2.HTTPPasswordMgrWithDefaultRealm()
            pass_man.add_password(None, base_url, username, password)
            self.opener.add_handler(urllib2.HTTPBasicAuthHandler(pass_man))
        GitClient.__init__(self, **kwargs)

    def get_url(self, path):
        return self._get_url(path).rstrip("/")

    @classmethod
    def from_parsedurl(cls, parsedurl, **kwargs):
        auth, host = urllib2.splituser(parsedurl.netloc)
        password = parsedurl.password
        if password is not None:
            password = urlunquote(password)
        username = parsedurl.username
        if username is not None:
            username = urlunquote(username)
        # TODO(jelmer): This also strips the username
        parsedurl = parsedurl._replace(netloc=host)
        return cls(urlparse.urlunparse(parsedurl),
                   password=password, username=username, **kwargs)

    def __repr__(self):
        return "%s(%r, dumb=%r)" % (
            type(self).__name__, self._base_url, self.dumb)

    def _get_url(self, path):
        if not isinstance(path, str):
            # TODO(jelmer): this is unrelated to the local filesystem;
            # This is not necessarily the right encoding to decode the path
            # with.
            path = path.decode(sys.getfilesystemencoding())
        return urlparse.urljoin(self._base_url, path).rstrip("/") + "/"

    def _http_request(self, url, headers={}, data=None):
        req = urllib2.Request(url, headers=headers, data=data)
        try:
            resp = self.opener.open(req)
        except urllib2.HTTPError as e:
            if e.code == 404:
                raise NotGitRepository()
            if e.code != 200:
                raise GitProtocolError("unexpected http response %d" % e.code)
        return resp

    def _discover_references(self, service, url):
        assert url[-1] == "/"
        url = urlparse.urljoin(url, "info/refs")
        headers = {}
        if self.dumb is not False:
            url += "?service=%s" % service.decode('ascii')
            headers["Content-Type"] = "application/x-%s-request" % (
                service.decode('ascii'))
        resp = self._http_request(url, headers)
        try:
            content_type = resp.info().gettype()
        except AttributeError:
            content_type = resp.info().get_content_type()
        try:
            self.dumb = (not content_type.startswith("application/x-git-"))
            if not self.dumb:
                proto = Protocol(resp.read, None)
                # The first line should mention the service
                try:
                    [pkt] = list(proto.read_pkt_seq())
                except ValueError:
                    raise GitProtocolError(
                        "unexpected number of packets received")
                if pkt.rstrip(b'\n') != (b'# service=' + service):
                    raise GitProtocolError(
                        "unexpected first line %r from smart server" % pkt)
                return read_pkt_refs(proto)
            else:
                return read_info_refs(resp), set()
        finally:
            resp.close()

    def _smart_request(self, service, url, data):
        assert url[-1] == "/"
        url = urlparse.urljoin(url, service)
        headers = {
            "Content-Type": "application/x-%s-request" % service
        }
        resp = self._http_request(url, headers, data)
        try:
            content_type = resp.info().gettype()
        except AttributeError:
            content_type = resp.info().get_content_type()
        if content_type != (
                "application/x-%s-result" % service):
            raise GitProtocolError("Invalid content-type from server: %s"
                                   % content_type)
        return resp

    def send_pack(self, path, update_refs, generate_pack_contents,
                  progress=None, write_pack=write_pack_objects):
        """Upload a pack to a remote repository.

        :param path: Repository path (as bytestring)
        :param update_refs: Function to determine changes to remote refs.
            Receive dict with existing remote refs, returns dict with
            changed refs (name -> sha, where sha=ZERO_SHA for deletions)
        :param generate_pack_contents: Function that can return a sequence of
            the shas of the objects to upload.
        :param progress: Optional progress function
        :param write_pack: Function called with (file, iterable of objects) to
            write the objects returned by generate_pack_contents to the server.

        :raises SendPackError: if server rejects the pack data
        :raises UpdateRefsError: if the server supports report-status
                                 and rejects ref updates
        :return: new_refs dictionary containing the changes that were made
            {refname: new_ref}, including deleted refs.
        """
        url = self._get_url(path)
        old_refs, server_capabilities = self._discover_references(
            b"git-receive-pack", url)
        negotiated_capabilities = self._send_capabilities & server_capabilities

        if CAPABILITY_REPORT_STATUS in negotiated_capabilities:
            self._report_status_parser = ReportStatusParser()

        new_refs = update_refs(dict(old_refs))
        if new_refs is None:
            # Determine wants function is aborting the push.
            return old_refs
        if self.dumb:
            raise NotImplementedError(self.fetch_pack)
        req_data = BytesIO()
        req_proto = Protocol(None, req_data.write)
        (have, want) = self._handle_receive_pack_head(
            req_proto, negotiated_capabilities, old_refs, new_refs)
        if not want and set(new_refs.items()).issubset(set(old_refs.items())):
            return new_refs
        objects = generate_pack_contents(have, want)
        if len(objects) > 0:
            write_pack(req_proto.write_file(), objects)
        resp = self._smart_request("git-receive-pack", url,
                                   data=req_data.getvalue())
        try:
            resp_proto = Protocol(resp.read, None)
            self._handle_receive_pack_tail(
                resp_proto, negotiated_capabilities, progress)
            return new_refs
        finally:
            resp.close()

    def fetch_pack(self, path, determine_wants, graph_walker, pack_data,
                   progress=None):
        """Retrieve a pack from a git smart server.

        :param determine_wants: Callback that returns list of commits to fetch
        :param graph_walker: Object with next() and ack().
        :param pack_data: Callback called for each bit of data in the pack
        :param progress: Callback for progress reports (strings)
        :return: Dictionary with all remote refs (not just those fetched)
        """
        url = self._get_url(path)
        refs, server_capabilities = self._discover_references(
            b"git-upload-pack", url)
        negotiated_capabilities = (
            self._fetch_capabilities & server_capabilities)
        wants = determine_wants(refs)
        if wants is not None:
            wants = [cid for cid in wants if cid != ZERO_SHA]
        if not wants:
            return refs
        if self.dumb:
            raise NotImplementedError(self.send_pack)
        req_data = BytesIO()
        req_proto = Protocol(None, req_data.write)
        self._handle_upload_pack_head(
                req_proto, negotiated_capabilities, graph_walker, wants,
                lambda: False)
        resp = self._smart_request(
            "git-upload-pack", url, data=req_data.getvalue())
        try:
            resp_proto = Protocol(resp.read, None)
            self._handle_upload_pack_tail(
                resp_proto, negotiated_capabilities, graph_walker, pack_data,
                progress)
            return refs
        finally:
            resp.close()

    def get_refs(self, path):
        """Retrieve the current refs from a git smart server."""
        url = self._get_url(path)
        refs, _ = self._discover_references(
            b"git-upload-pack", url)
        return refs
@

\section{[[git://]] server}
% git smart http protocol

\subsection{[[git update-server-info]]}

<<function cmd_update_server_info>>=
class cmd_update_server_info(Command):

    def run(self, args):
        porcelain.update_server_info(".")

@

<<function porcelain.update_server_info>>=
def update_server_info(repo="."):
    """Update server info files for a repository.

    :param repo: path to the repository
    """
    with open_repo_closing(repo) as r:
        server_update_server_info(r)
@

\subsection{[[git daemon]]}

<<function cmd_daemon>>=
class cmd_daemon(Command):

    def run(self, args):
        from dulwich import log_utils
        from dulwich.protocol import TCP_GIT_PORT
        parser = optparse.OptionParser()
        parser.add_option("-l", "--listen_address", dest="listen_address",
                          default="localhost",
                          help="Binding IP address.")
        parser.add_option("-p", "--port", dest="port", type=int,
                          default=TCP_GIT_PORT,
                          help="Binding TCP port.")
        options, args = parser.parse_args(args)

        log_utils.default_logging_config()
        if len(args) >= 1:
            gitdir = args[0]
        else:
            gitdir = '.'
        from dulwich import porcelain
        porcelain.daemon(gitdir, address=options.listen_address,
                         port=options.port)

@

<<function porcelain.daemon>>=
def daemon(path=".", address=None, port=None):
    """Run a daemon serving Git requests over TCP/IP.

    :param path: Path to the directory to serve.
    :param address: Optional address to listen on (defaults to ::)
    :param port: Optional port to listen on (defaults to TCP_GIT_PORT)
    """
    # TODO(jelmer): Support git-daemon-export-ok and --export-all.
    backend = FileSystemBackend(path)
    server = TCPGitServer(backend, address, port)
    server.serve_forever()
@

\subsection{[[git web-daemon]]}

<<function cmd_web_daemon>>=
class cmd_web_daemon(Command):

    def run(self, args):
        from dulwich import log_utils
        parser = optparse.OptionParser()
        parser.add_option("-l", "--listen_address", dest="listen_address",
                          default="",
                          help="Binding IP address.")
        parser.add_option("-p", "--port", dest="port", type=int,
                          default=8000,
                          help="Binding TCP port.")
        options, args = parser.parse_args(args)

        log_utils.default_logging_config()
        if len(args) >= 1:
            gitdir = args[0]
        else:
            gitdir = '.'
        from dulwich import porcelain
        porcelain.web_daemon(gitdir, address=options.listen_address,
                             port=options.port)

@

<<function porcelain.web_daemon>>=
def web_daemon(path=".", address=None, port=None):
    """Run a daemon serving Git requests over HTTP.

    :param path: Path to the directory to serve
    :param address: Optional address to listen on (defaults to ::)
    :param port: Optional port to listen on (defaults to 80)
    """
    from dulwich.web import (
        make_wsgi_chain,
        make_server,
        WSGIRequestHandlerLogger,
        WSGIServerLogger)

    backend = FileSystemBackend(path)
    app = make_wsgi_chain(backend)
    server = make_server(address, port, app,
                         handler_class=WSGIRequestHandlerLogger,
                         server_class=WSGIServerLogger)
    server.serve_forever()
@

\section{Bare repository}

<<[[porcelain.init()]] if bare>>=
if bare:
    return Repo.init_bare(path)
@
% when git init --bare

<<[[porcelain.clone()]] if bare>>=
if bare:
    r = Repo.init_bare(target)
@

<<[[Repo.__init__()]] else if no [[.git/objects/]] directory>>=
elif (os.path.isdir(os.path.join(root, OBJECTDIR)) and
      os.path.isdir(os.path.join(root, REFSDIR))):
    self.bare = True
    self._controldir = root
<<[[Repo.__init__()]] else if [[.git]] is a file>>
@
% when objects/ directly at root, not under hidden_path (controldir).



<<[[Repo]] methods>>=
@classmethod
def init_bare(cls, path, mkdir=False):
    """Create a new bare repository.

    ``path`` should already exist and be an empty directory.

    :param path: Path to create bare repository in
    :return: a `Repo` instance
    """
    if mkdir:
        os.mkdir(path)
    return cls._init_maybe_bare(path, True)
@
%``

<<[[Repo]] methods>>=
create = init_bare
@

<<constant repo.COMMONDIR>>=
COMMONDIR = 'commondir'
@

<<[[Repo.__init__()]] if commondir>>=
commondir = self.get_named_file(COMMONDIR)
if commondir is not None:
    with commondir:
        self._commondir = os.path.join(
            self.controldir(),
            commondir.read().rstrip(b"\r\n").decode(
                sys.getfilesystemencoding()))
@

\section{[[.git]] file}
% linked working tree? see docstring of commondir()

<<[[Repo.__init__()]] else if [[.git]] is a file>>=
elif os.path.isfile(hidden_path):
    self.bare = False
    with open(hidden_path, 'r') as f:
        path = read_gitfile(f)
    self.bare = False
    self._controldir = os.path.join(root, path)
@

<<function repo.read_gitfile>>=
def read_gitfile(f):
    """Read a ``.git`` file.

    The first line of the file should start with "gitdir: "

    :param f: File-like object to read from
    :return: A path
    """
    cs = f.read()
    if not cs.startswith("gitdir: "):
        raise ValueError("Expected file to start with 'gitdir: '")
    return cs[len("gitdir: "):].rstrip("\n")
@


\section{Other commands}

\subsection{[[git fetch]]}

<<function cmd_fetch>>=
class cmd_fetch(Command):

    def run(self, args):
        opts, args = getopt(args, "", [])
        opts = dict(opts)
        client, path = get_transport_and_path(args.pop(0))
        r = Repo(".")
        if "--all" in opts:
            determine_wants = r.object_store.determine_wants_all
        refs = client.fetch(path, r, progress=sys.stdout.write)
        print("Remote refs:")
        for item in refs.items():
            print("%s -> %s" % item)
@


<<function porcelain.fetch>>=
def fetch(repo, remote_location, outstream=sys.stdout,
          errstream=default_bytes_err_stream):
    """Fetch objects from a remote server.

    :param repo: Path to the repository
    :param remote_location: String identifying a remote server
    :param outstream: Output stream (defaults to stdout)
    :param errstream: Error stream (defaults to stderr)
    :return: Dictionary with refs on the remote
    """
    with open_repo_closing(repo) as r:
        client, path = get_transport_and_path(remote_location)
        remote_refs = client.fetch(path, r, progress=errstream.write)
    return remote_refs
@

\subsection{[[git symbolic-ref]]}

<<function cmd_symbolic_ref>>=
class cmd_symbolic_ref(Command):

    def run(self, args):
        opts, args = getopt(args, "", ["ref-name", "force"])
        if not args:
            print("Usage: dulwich symbolic-ref REF_NAME [--force]")
            sys.exit(1)

        ref_name = args.pop(0)
        porcelain.symbolic_ref(".", ref_name=ref_name, force='--force' in args)

@

<<function porcelain.symbolic_ref>>=
def symbolic_ref(repo, ref_name, force=False):
    """Set git symbolic ref into HEAD.

    :param repo: path to the repository
    :param ref_name: short name of the new ref
    :param force: force settings without checking if it exists in refs/heads
    """
    with open_repo_closing(repo) as repo_obj:
        ref_path = b'refs/heads/' + ref_name
        if not force and ref_path not in repo_obj.refs.keys():
            raise ValueError('fatal: ref `%s` is not a ref' % ref_name)
        repo_obj.refs.set_symbolic_ref(b'HEAD', ref_path)

@


\subsection{[[git rev-list]]}

<<function cmd_rev_list>>=
class cmd_rev_list(Command):

    def run(self, args):
        opts, args = getopt(args, "", [])
        if len(args) < 1:
            print('Usage: dulwich rev-list COMMITID...')
            sys.exit(1)

        porcelain.rev_list('.', args)

@

<<function porcelain.rev_list>>=
def rev_list(repo, commits, outstream=sys.stdout):
    """Lists commit objects in reverse chronological order.

    :param repo: Path to repository
    :param commits: Commits over which to iterate
    :param outstream: Stream to write to
    """
    with open_repo_closing(repo) as r:
        for entry in r.get_walker(include=[r[c].id for c in commits]):
            outstream.write(entry.commit.id + b"\n")

@

\subsection{[[git diff-tree]]}

<<function cmd_diff_tree>>=
class cmd_diff_tree(Command):

    def run(self, args):
        opts, args = getopt(args, "", [])
        if len(args) < 2:
            print("Usage: dulwich diff-tree OLD-TREE NEW-TREE")
            sys.exit(1)
        porcelain.diff_tree(".", args[0], args[1])

@

<<function porcelain.diff_tree>>=
def diff_tree(repo, old_tree, new_tree, outstream=sys.stdout):
    """Compares the content and mode of blobs found via two tree objects.

    :param repo: Path to repository
    :param old_tree: Id of old tree
    :param new_tree: Id of new tree
    :param outstream: Stream to write to
    """
    with open_repo_closing(repo) as r:
        write_tree_diff(outstream, r.object_store, old_tree, new_tree)
@

\subsection{[[git commit-tree]]}

<<function cmd_commit_tree>>=
class cmd_commit_tree(Command):

    def run(self, args):
        opts, args = getopt(args, "", ["message"])
        if args == []:
            print("usage: dulwich commit-tree tree")
            sys.exit(1)
        opts = dict(opts)
        porcelain.commit_tree(".", tree=args[0], message=opts["--message"])
@

<<function porcelain.commit_tree>>=
def commit_tree(repo, tree, message=None, author=None, committer=None):
    """Create a new commit object.

    :param repo: Path to repository
    :param tree: An existing tree object
    :param author: Optional author name and email
    :param committer: Optional committer name and email
    """
    with open_repo_closing(repo) as r:
        return r.do_commit(
            message=message, tree=tree, committer=committer, author=author)
@

\subsection{[[git ls-tree]]}

<<function cmd_ls_tree>>=
class cmd_ls_tree(Command):

    def run(self, args):
        parser = optparse.OptionParser()
        parser.add_option("-r", "--recursive", action="store_true",
                          help="Recusively list tree contents.")
        parser.add_option("--name-only", action="store_true",
                          help="Only display name.")
        options, args = parser.parse_args(args)
        try:
            treeish = args.pop(0)
        except IndexError:
            treeish = None
        porcelain.ls_tree(
            '.', treeish, outstream=sys.stdout, recursive=options.recursive,
            name_only=options.name_only)
@

<<function porcelain.ls_tree>>=
def ls_tree(repo, tree_ish=None, outstream=sys.stdout, recursive=False,
            name_only=False):
    """List contents of a tree.

    :param repo: Path to the repository
    :param tree_ish: Tree id to list
    :param outstream: Output stream (defaults to stdout)
    :param recursive: Whether to recursively list files
    :param name_only: Only print item name
    """
    def list_tree(store, treeid, base):
        for (name, mode, sha) in store[treeid].iteritems():
            if base:
                name = posixpath.join(base, name)
            if name_only:
                outstream.write(name + b"\n")
            else:
                outstream.write(pretty_format_tree_entry(name, mode, sha))
            if stat.S_ISDIR(mode):
                list_tree(store, sha, name)
    if tree_ish is None:
        tree_ish = "HEAD"
    with open_repo_closing(repo) as r:
        c = r[tree_ish]
        treeid = c.tree
        list_tree(r.object_store, treeid, "")
@

\subsection{[[git remote add]]}

<<function cmd_remote>>=
class cmd_remote(Command):

    subcommands = {
        "add": cmd_remote_add,
    }

    def run(self, args):
        if not args:
            print("Supported subcommands: %s" % ', '.join(self.subcommands.keys()))
            return False
        cmd = args[0]
        try:
            cmd_kls = self.subcommands[cmd]
        except KeyError:
            print('No such subcommand: %s' % args[0])
            return False
        return cmd_kls(args[1:])

@

<<function cmd_remote_add>>=
class cmd_remote_add(Command):

    def run(self, args):
        parser = optparse.OptionParser()
        options, args = parser.parse_args(args)
        porcelain.remote_add('.', args[0], args[1])

@



<<function porcelain.remote_add>>=
def remote_add(repo, name, url):
    """Add a remote.

    :param repo: Path to the repository
    :param name: Remote name
    :param url: Remote URL
    """
    if not isinstance(name, bytes):
        name = name.encode(DEFAULT_ENCODING)
    if not isinstance(url, bytes):
        url = url.encode(DEFAULT_ENCODING)
    with open_repo_closing(repo) as r:
        c = r.get_config()
        section = (b'remote', name)
        if c.has_section(section):
            raise RemoteExists(section)
        c.set(section, b"url", url)
        c.write_to_path()
@

\subsection{[[git ls-remote]]}

<<function cmd_ls_remote>>=
class cmd_ls_remote(Command):

    def run(self, args):
        opts, args = getopt(args, '', [])
        if len(args) < 1:
            print('Usage: dulwich ls-remote URL')
            sys.exit(1)
        refs = porcelain.ls_remote(args[0])
        for ref in sorted(refs):
            sys.stdout.write("%s\t%s\n" % (ref, refs[ref]))
@

<<function porcelain.ls_remote>>=
def ls_remote(remote):
    """List the refs in a remote.

    :param remote: Remote repository location
    :return: Dictionary with remote refs
    """
    client, host_path = get_transport_and_path(remote)
    return client.get_refs(host_path)
@

\subsection{[[git archive]]}

% create tarball from git repo

<<function cmd_archive>>=
class cmd_archive(Command):

    def run(self, args):
        opts, args = getopt(args, "", [])
        client, path = get_transport_and_path(args.pop(0))
        location = args.pop(0)
        committish = args.pop(0)
        porcelain.archive(location, committish, outstream=sys.stdout,
            errstream=sys.stderr)
@

<<function porcelain.archive>>=
def archive(repo, committish=None, outstream=default_bytes_out_stream,
            errstream=default_bytes_err_stream):
    """Create an archive.

    :param repo: Path of repository for which to generate an archive.
    :param committish: Commit SHA1 or ref to use
    :param outstream: Output stream (defaults to stdout)
    :param errstream: Error stream (defaults to stderr)
    """

    if committish is None:
        committish = "HEAD"
    with open_repo_closing(repo) as repo_obj:
        c = repo_obj[committish]
        for chunk in tar_stream(
                repo_obj.object_store, repo_obj.object_store[c.tree],
                c.commit_time):
            outstream.write(chunk)
@


<<function archive.tar_stream>>=
def tar_stream(store, tree, mtime, format=''):
    """Generate a tar stream for the contents of a Git tree.

    Returns a generator that lazily assembles a .tar.gz archive, yielding it in
    pieces (bytestrings). To obtain the complete .tar.gz binary file, simply
    concatenate these chunks.

    :param store: Object store to retrieve objects from
    :param tree: Tree object for the tree root
    :param mtime: UNIX timestamp that is assigned as the modification time for
        all files
    :param format: Optional compression format for tarball
    :return: Bytestrings
    """
    buf = BytesIO()
    with closing(tarfile.open(None, "w:%s" % format, buf)) as tar:
        for entry_abspath, entry in _walk_tree(store, tree):
            try:
                blob = store[entry.sha]
            except KeyError:
                # Entry probably refers to a submodule, which we don't yet support.
                continue
            data = ChunkedBytesIO(blob.chunked)

            info = tarfile.TarInfo()
            info.name = entry_abspath.decode('ascii') # tarfile only works with ascii.
            info.size = blob.raw_length()
            info.mode = entry.mode
            info.mtime = mtime

            tar.addfile(info, data)
            yield buf.getvalue()
            buf.truncate(0)
            buf.seek(0)
    yield buf.getvalue()
@

<<class ChunkedBytesIO>>=
class ChunkedBytesIO(object):
    """Turn a list of bytestrings into a file-like object.

    This is similar to creating a `BytesIO` from a concatenation of the
    bytestring list, but saves memory by NOT creating one giant bytestring first::

        BytesIO(b''.join(list_of_bytestrings)) =~= ChunkedBytesIO(list_of_bytestrings)
    """
    def __init__(self, contents):
        self.contents = contents
        self.pos = (0, 0)

    def read(self, maxbytes=None):
        if maxbytes < 0:
            maxbytes = float('inf')

        buf = []
        chunk, cursor = self.pos

        while chunk < len(self.contents):
            if maxbytes < len(self.contents[chunk]) - cursor:
                buf.append(self.contents[chunk][cursor:cursor+maxbytes])
                cursor += maxbytes
                self.pos = (chunk, cursor)
                break
            else:
                buf.append(self.contents[chunk][cursor:])
                maxbytes -= len(self.contents[chunk]) - cursor
                chunk += 1
                cursor = 0
                self.pos = (chunk, cursor)
        return b''.join(buf)
@

<<function archive._walk_tree>>=
def _walk_tree(store, tree, root=b''):
    """Recursively walk a dulwich Tree, yielding tuples of
    (absolute path, TreeEntry) along the way.
    """
    for entry in tree.iteritems():
        entry_abspath = posixpath.join(root, entry.path)
        if stat.S_ISDIR(entry.mode):
            for _ in _walk_tree(store, store[entry.sha], entry_abspath):
                yield _
        else:
            yield (entry_abspath, entry)
@

\chapter{Conclusion}

\appendix

\chapter{Debugging}

\section{[[__repr__]]}

<<[[Repo]] methods>>=
def __repr__(self):
    return "<Repo at %r>" % self.path
@

<<[[DiskObjectStore]] methods>>=
def __repr__(self):
    return "<%s(%r)>" % (self.__class__.__name__, self.path)
@

<<[[DiskRefsContainer]] methods>>=
def __repr__(self):
    return "%s(%r)" % (self.__class__.__name__, self.path)
@

<<[[ShaFile]] methods>>=
def __repr__(self):
    return "<%s %s>" % (self.__class__.__name__, self.id)
@

<<[[Index]] methods>>=
def __repr__(self):
    return "%s(%r)" % (self.__class__.__name__, self._filename)
@

<<[[Pack]] methods>>=
def __repr__(self):
    return '%s(%r)' % (self.__class__.__name__, self._basename)
@

<<[[ConfigDict]] methods>>=
def __repr__(self):
    return "%s(%r)" % (self.__class__.__name__, self._values)
@

<<[[StackedConfig]] methods>>=
def __repr__(self):
    return "<%s for %r>" % (self.__class__.__name__, self.backends)
@



\section{Mocks}

<<class MemoryRepo>>=
class MemoryRepo(BaseRepo):
    """Repo that stores refs, objects, and named files in memory.

    MemoryRepos are always bare: they have no working tree and no index, since
    those have a stronger dependency on the filesystem.
    """

    def __init__(self):
        from dulwich.config import ConfigFile
        BaseRepo.__init__(self, MemoryObjectStore(), DictRefsContainer({}))
        self._named_files = {}
        self.bare = True
        self._config = ConfigFile()
        self._description = None

    def set_description(self, description):
        self._description = description

    def get_description(self):
        return self._description

    def _determine_file_mode(self):
        """Probe the file-system to determine whether permissions can be trusted.

        :return: True if permissions can be trusted, False otherwise.
        """
        return sys.platform != 'win32'

    def _put_named_file(self, path, contents):
        """Write a file to the control dir with the given name and contents.

        :param path: The path to the file, relative to the control dir.
        :param contents: A string to write to the file.
        """
        self._named_files[path] = contents

    def get_named_file(self, path):
        """Get a file from the control dir with a specific name.

        Although the filename should be interpreted as a filename relative to
        the control dir in a disk-baked Repo, the object returned need not be
        pointing to a file in that location.

        :param path: The path to the file, relative to the control dir.
        :return: An open file object, or None if the file does not exist.
        """
        contents = self._named_files.get(path, None)
        if contents is None:
            return None
        return BytesIO(contents)

    def open_index(self):
        """Fail to open index for this repo, since it is bare.

        :raise NoIndexPresent: Raised when no index is present
        """
        raise NoIndexPresent()

    def get_config(self):
        """Retrieve the config object.

        :return: `ConfigFile` object.
        """
        return self._config

    @classmethod
    def init_bare(cls, objects, refs):
        """Create a new bare repository in memory.

        :param objects: Objects for the new repository,
            as iterable
        :param refs: Refs as dictionary, mapping names
            to object SHA1s
        """
        ret = cls()
        for obj in objects:
            ret.object_store.add_object(obj)
        for refname, sha in refs.items():
            ret.refs[refname] = sha
        ret._init_files(bare=True)
        return ret
@


<<class MemoryObjectStore>>=
class MemoryObjectStore(BaseObjectStore):
    """Object store that keeps all objects in memory."""

    def __init__(self):
        super(MemoryObjectStore, self).__init__()
        self._data = {}

    def _to_hexsha(self, sha):
        if len(sha) == 40:
            return sha
        elif len(sha) == 20:
            return sha_to_hex(sha)
        else:
            raise ValueError("Invalid sha %r" % (sha,))

    def contains_loose(self, sha):
        """Check if a particular object is present by SHA1 and is loose."""
        return self._to_hexsha(sha) in self._data

    def contains_packed(self, sha):
        """Check if a particular object is present by SHA1 and is packed."""
        return False

    def __iter__(self):
        """Iterate over the SHAs that are present in this store."""
        return iter(self._data.keys())

    @property
    def packs(self):
        """List with pack objects."""
        return []

    def get_raw(self, name):
        """Obtain the raw text for an object.

        :param name: sha for the object.
        :return: tuple with numeric type and object contents.
        """
        obj = self[self._to_hexsha(name)]
        return obj.type_num, obj.as_raw_string()

    def __getitem__(self, name):
        return self._data[self._to_hexsha(name)].copy()

    def __delitem__(self, name):
        """Delete an object from this store, for testing only."""
        del self._data[self._to_hexsha(name)]

    def add_object(self, obj):
        """Add a single object to this object store.

        """
        self._data[obj.id] = obj.copy()

    def add_objects(self, objects):
        """Add a set of objects to this object store.

        :param objects: Iterable over a list of (object, path) tuples
        """
        for obj, path in objects:
            self.add_object(obj)

    def add_pack(self):
        """Add a new pack to this object store.

        Because this object store doesn't support packs, we extract and add the
        individual objects.

        :return: Fileobject to write to and a commit function to
            call when the pack is finished.
        """
        f = BytesIO()

        def commit():
            p = PackData.from_file(BytesIO(f.getvalue()), f.tell())
            f.close()
            for obj in PackInflater.for_pack_data(p, self.get_raw):
                self.add_object(obj)

        def abort():
            pass
        return f, commit, abort

    def _complete_thin_pack(self, f, indexer):
        """Complete a thin pack by adding external references.

        :param f: Open file object for the pack.
        :param indexer: A PackIndexer for indexing the pack.
        """
        entries = list(indexer)

        # Update the header with the new number of objects.
        f.seek(0)
        write_pack_header(f, len(entries) + len(indexer.ext_refs()))

        # Rescan the rest of the pack, computing the SHA with the new header.
        new_sha = compute_file_sha(f, end_ofs=-20)

        # Complete the pack.
        for ext_sha in indexer.ext_refs():
            assert len(ext_sha) == 20
            type_num, data = self.get_raw(ext_sha)
            write_pack_object(f, type_num, data, sha=new_sha)
        pack_sha = new_sha.digest()
        f.write(pack_sha)

    def add_thin_pack(self, read_all, read_some):
        """Add a new thin pack to this object store.

        Thin packs are packs that contain deltas with parents that exist
        outside the pack. Because this object store doesn't support packs, we
        extract and add the individual objects.

        :param read_all: Read function that blocks until the number of
            requested bytes are read.
        :param read_some: Read function that returns at least one byte, but may
            not return the number of bytes requested.
        """
        f, commit, abort = self.add_pack()
        try:
            indexer = PackIndexer(f, resolve_ext_ref=self.get_raw)
            copier = PackStreamCopier(read_all, read_some, f,
                                      delta_iter=indexer)
            copier.verify()
            self._complete_thin_pack(f, indexer)
        except:
            abort()
            raise
        else:
            commit()
@

<<class DictRefsContainer>>=
class DictRefsContainer(RefsContainer):
    """RefsContainer backed by a simple dict.

    This container does not support symbolic or packed references and is not
    threadsafe.
    """

    def __init__(self, refs):
        self._refs = refs
        self._peeled = {}

    def allkeys(self):
        return self._refs.keys()

    def read_loose_ref(self, name):
        return self._refs.get(name, None)

    def get_packed_refs(self):
        return {}

    def set_symbolic_ref(self, name, other):
        self._refs[name] = SYMREF + other

    def set_if_equals(self, name, old_ref, new_ref):
        if old_ref is not None and self._refs.get(name, ZERO_SHA) != old_ref:
            return False
        realnames, _ = self.follow(name)
        for realname in realnames:
            self._check_refname(realname)
            self._refs[realname] = new_ref
        return True

    def add_if_new(self, name, ref):
        if name in self._refs:
            return False
        self._refs[name] = ref
        return True

    def remove_if_equals(self, name, old_ref):
        if old_ref is not None and self._refs.get(name, ZERO_SHA) != old_ref:
            return False
        try:
            del self._refs[name]
        except KeyError:
            pass
        return True

    def get_peeled(self, name):
        return self._peeled.get(name)

    def _update(self, refs):
        """Update multiple refs; intended only for testing."""
        # TODO(dborowitz): replace this with a public function that uses
        # set_if_equal.
        self._refs.update(refs)

    def _update_peeled(self, peeled):
        """Update cached peeled refs; intended only for testing."""
        self._peeled.update(peeled)
@

<<class MemoryPackIndex>>=
class MemoryPackIndex(PackIndex):
    """Pack index that is stored entirely in memory."""

    def __init__(self, entries, pack_checksum=None):
        """Create a new MemoryPackIndex.

        :param entries: Sequence of name, idx, crc32 (sorted)
        :param pack_checksum: Optional pack checksum
        """
        self._by_sha = {}
        for name, idx, crc32 in entries:
            self._by_sha[name] = idx
        self._entries = entries
        self._pack_checksum = pack_checksum

    def get_pack_checksum(self):
        return self._pack_checksum

    def __len__(self):
        return len(self._entries)

    def _object_index(self, sha):
        return self._by_sha[sha][0]

    def _itersha(self):
        return iter(self._by_sha)

    def iterentries(self):
        return iter(self._entries)
@

\section{[[git dump-index]]}

% not a git command
<<function cmd_dump_index>>=
class cmd_dump_index(Command):

    def run(self, args):
        opts, args = getopt(args, "", [])

        if args == []:
            print("Usage: dulwich dump-index FILENAME")
            sys.exit(1)

        filename = args[0]
        idx = Index(filename)

        for o in idx:
            print(o, idx[o])
@

\section{[[git dump-pack]]}

<<function cmd_dump_pack>>=
class cmd_dump_pack(Command):

    def run(self, args):
        opts, args = getopt(args, "", [])

        if args == []:
            print("Usage: dulwich dump-pack FILENAME")
            sys.exit(1)

        basename, _ = os.path.splitext(args[0])
        x = Pack(basename)
        print("Object names checksum: %s" % x.name())
        print("Checksum: %s" % sha_to_hex(x.get_stored_checksum()))
        if not x.check():
            print("CHECKSUM DOES NOT MATCH")
        print("Length: %d" % len(x))
        for name in x:
            try:
                print("\t%s" % x[name])
            except KeyError as k:
                print("\t%s: Unable to resolve base %s" % (name, k))
            except ApplyDeltaError as e:
                print("\t%s: Unable to apply delta: %r" % (name, e))
@

\chapter{Profiling}

\chapter{Error Management}


<<errors>>=
class ChecksumMismatch(Exception):
    """A checksum didn't match the expected contents."""

    def __init__(self, expected, got, extra=None):
        if len(expected) == 20:
            expected = binascii.hexlify(expected)
        if len(got) == 20:
            got = binascii.hexlify(got)
        self.expected = expected
        self.got = got
        self.extra = extra
        if self.extra is None:
            Exception.__init__(self,
                "Checksum mismatch: Expected %s, got %s" % (expected, got))
        else:
            Exception.__init__(self,
                "Checksum mismatch: Expected %s, got %s; %s" %
                (expected, got, extra))
@

<<errors>>=
class MissingCommitError(Exception):
    """Indicates that a commit was not found in the repository"""

    def __init__(self, sha, *args, **kwargs):
        self.sha = sha
        Exception.__init__(self, "%s is not in the revision store" % sha)
@

<<errors>>=
class ObjectMissing(Exception):
    """Indicates that a requested object is missing."""

    def __init__(self, sha, *args, **kwargs):
        Exception.__init__(self, "%s is not in the pack" % sha)


@

<<errors>>=
class ApplyDeltaError(Exception):
    """Indicates that applying a delta failed."""

    def __init__(self, *args, **kwargs):
        Exception.__init__(self, *args, **kwargs)


@

<<errors>>=
class NotGitRepository(Exception):
    """Indicates that no Git repository was found."""

    def __init__(self, *args, **kwargs):
        Exception.__init__(self, *args, **kwargs)
@


<<errors>>=
class NoIndexPresent(Exception):
    """No index is present."""
@

<<errors>>=
class CommitError(Exception):
    """An error occurred while performing a commit."""
@

<<errors>>=
class RefFormatError(Exception):
    """Indicates an invalid ref name."""
@

<<errors>>=
class HookError(Exception):
    """An error occurred while executing a hook."""
@





<<exception WrongObjectException>>=
class WrongObjectException(Exception):
    """Baseclass for all the _ is not a _ exceptions on objects.

    Do not instantiate directly.

    Subclasses should define a type_name attribute that indicates what
    was expected if they were raised.
    """

    def __init__(self, sha, *args, **kwargs):
        Exception.__init__(self, "%s is not a %s" % (sha, self.type_name))
@

<<[[WrongObjectException]] errors>>=
class NotCommitError(WrongObjectException):
    """Indicates that the sha requested does not point to a commit."""

    type_name = 'commit'
@

<<[[WrongObjectException]] errors>>=
class NotTreeError(WrongObjectException):
    """Indicates that the sha requested does not point to a tree."""

    type_name = 'tree'
@

<<[[WrongObjectException]] errors>>=
class NotTagError(WrongObjectException):
    """Indicates that the sha requested does not point to a tag."""

    type_name = 'tag'
@

<<[[WrongObjectException]] errors>>=
class NotBlobError(WrongObjectException):
    """Indicates that the sha requested does not point to a blob."""

    type_name = 'blob'
@




<<exception GitProtocolError>>=
class GitProtocolError(Exception):
    """Git protocol exception."""

    def __init__(self, *args, **kwargs):
        Exception.__init__(self, *args, **kwargs)
@

<<[[GitProtocolError]] errors>>=
class SendPackError(GitProtocolError):
    """An error occurred during send_pack."""

    def __init__(self, *args, **kwargs):
        Exception.__init__(self, *args, **kwargs)
@

<<[[GitProtocolError]] errors>>=
class UpdateRefsError(GitProtocolError):
    """The server reported errors updating refs."""

    def __init__(self, *args, **kwargs):
        self.ref_status = kwargs.pop('ref_status')
        Exception.__init__(self, *args, **kwargs)
@

<<[[GitProtocolError]] errors>>=
class HangupException(GitProtocolError):
    """Hangup exception."""

    def __init__(self):
        Exception.__init__(self,
            "The remote server unexpectedly closed the connection.")
@

<<[[GitProtocolError]] errors>>=
class UnexpectedCommandError(GitProtocolError):
    """Unexpected command received in a proto line."""

    def __init__(self, command):
        if command is None:
            command = 'flush-pkt'
        else:
            command = 'command %s' % command
        GitProtocolError.__init__(self, 'Protocol got unexpected %s' % command)
@


<<exception FileFormatException>>=
class FileFormatException(Exception):
    """Base class for exceptions relating to reading git file formats."""
@

<<[[FileFormatException]] errors>>=
class PackedRefsException(FileFormatException):
    """Indicates an error parsing a packed-refs file."""
@

<<[[FileFormatException]] errors>>=
class ObjectFormatException(FileFormatException):
    """Indicates an error parsing an object."""
@

\chapter{Extra Code}

\ifallcode
#include "VCS_extra.nw"
\fi

%\chapter{Changelog}
%\label{sec:changelog}

% code via make loc = X LOC
% orig VCS.nw = XX, just lpized and few comments, ?? pages pdf
% now: =~ XX LOC so added XX LOE (Lines of explanations)
% vcs in ocaml: XX LOC (but still miss some advanced-topics features)

\chapter{Glossary}
\label{sec:glossary}

\begin{verbatim}
SHA1 = 
\end{verbatim}

\chapter*{Indexes}
\addcontentsline{toc}{section}{Index}

%\chapter{References} 
\addcontentsline{toc}{section}{References}

\bibliography{../docs/latex/Principia}
\bibliographystyle{alpha}

%******************************************************************************
% Postlude
%******************************************************************************

\end{document}
